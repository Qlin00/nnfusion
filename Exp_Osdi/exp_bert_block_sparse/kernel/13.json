{"kernel_identifier": "kernel_13", "tvm_func_name": "Blocksparse with fused add", "op_type": "SparseDot", "parameters": {"arg0_shape": [256, 256], "arg1_shape": [256, 256], "arg2_shape": [256, 256], "arg3_shape": [256, 256], "arg4_shape": [256, 256], "out_shape": [256, 256], "transpose_A": false, "transpose_B": false}, "code": "\n\n\n__global__ void BLOCK_SPARSE_MATMUL(float* input0, float* input1,float* input2, float* input3, float* input4, float *output0){\n\n    const int BLOCK_SIZE_M=64;\n    const int BLOCK_SIZE_K=8;\n    const int BLOCK_SIZE_N=128;\n    const int THREAD_SIZE_M=8;\n    const int THREAD_SIZE_K=4;\n    const int THREAD_SIZE_N=8;\n    const int M=4096;\n    const int N=768;\n    const int K=768;\n    float * A = reinterpret_cast<float*>(input0);\n    float * W_val = reinterpret_cast<float*>(input1);\n    int * W_row = reinterpret_cast<int*>(input2);\n    int * W_col = reinterpret_cast<int*>(input3);\n    float * bias = reinterpret_cast<float*>(input4);\n    float * C = reinterpret_cast<float*>(output0);\n    /* \n    TESAID : 13\n    */\n    int by = blockIdx.y;\n    int bx = blockIdx.x;\n    int ty = threadIdx.y;\n    int tx = threadIdx.x;\n\n    __shared__ float As[BLOCK_SIZE_M * BLOCK_SIZE_K];\n    __shared__ float Bs[BLOCK_SIZE_N * BLOCK_SIZE_K];\n\n    float accum[THREAD_SIZE_N][THREAD_SIZE_M] = {0};\n    float a_frag[THREAD_SIZE_M][THREAD_SIZE_K];\n    float b_frag[THREAD_SIZE_N][THREAD_SIZE_K];\n\n    int A_THREAD_PER_ROW = BLOCK_SIZE_K / 4;\n    int B_THREAD_PER_ROW = BLOCK_SIZE_N / 4;\n\n    int bszy = BLOCK_SIZE_M / THREAD_SIZE_M;\n    int bszx = BLOCK_SIZE_N / THREAD_SIZE_N;\n\n    int THREADS_PER_BLOCK = bszy * bszx;\n\n    int A_TILE_ROW_STRIDE = THREADS_PER_BLOCK / A_THREAD_PER_ROW;\n    int B_TILE_ROW_STRIDE = THREADS_PER_BLOCK / B_THREAD_PER_ROW;\n\n    int tid = ty * bszx + tx;\n\n    int A_BLOCK_ROW_START = tid / A_THREAD_PER_ROW;\n    int B_BLOCK_ROW_START = tid / B_THREAD_PER_ROW;\n\n    int A_BLOCK_COL_START = tid % A_THREAD_PER_ROW * 4;\n    int B_BLOCK_COL_START = tid % B_THREAD_PER_ROW * 4;\n\n    int index_start = W_row[bx], index_end = W_row[bx+1];\n\n    const int vBLOCK_SIZE_M = BLOCK_SIZE_M / THREAD_SIZE_M;\n    const int vBLOCK_SIZE_N = BLOCK_SIZE_N / THREAD_SIZE_N;\n    for(int tile_block_idx = index_start; tile_block_idx < index_end; tile_block_idx += 1){\n        int tile_idx = W_col[tile_block_idx] * BLOCK_SIZE_K;\n        #pragma unroll\n        for(int k = 0; k < BLOCK_SIZE_M; k += A_TILE_ROW_STRIDE){\n            FETCH_FLOAT4(As[OFFSET(k+A_BLOCK_ROW_START, A_BLOCK_COL_START, BLOCK_SIZE_K)]) =\n                FETCH_FLOAT4(A[OFFSET(by*BLOCK_SIZE_M+k+A_BLOCK_ROW_START, tile_idx+A_BLOCK_COL_START, K)]);\n        }\n        /*\n        for(int k = 0; k < BLOCK_SIZE_K; k += A_TILE_ROW_STRIDE){\n            FETCH_FLOAT4(As[OFFSET(k+A_BLOCK_ROW_START, A_BLOCK_COL_START, BLOCK_SIZE_M)]) = \n                FETCH_FLOAT4(A[OFFSET(tile_idx+k+A_BLOCK_ROW_START, by*BLOCK_SIZE_M+A_BLOCK_COL_START, M)]);\n        }\n        */\n\n        #pragma unroll\n        for(int k = 0; k < BLOCK_SIZE_K; k += B_TILE_ROW_STRIDE){\n            FETCH_FLOAT4(Bs[OFFSET(k+B_BLOCK_ROW_START, B_BLOCK_COL_START, BLOCK_SIZE_N)]) = \n                FETCH_FLOAT4(W_val[tile_block_idx * BLOCK_SIZE_N * BLOCK_SIZE_K + (k+B_BLOCK_ROW_START) * BLOCK_SIZE_N + B_BLOCK_COL_START]);\n                // FETCH_FLOAT4(B[OFFSET(tile_idx+k+B_BLOCK_ROW_START, bx*BLOCK_SIZE_N+B_BLOCK_COL_START, N)]);\n        }\n\n        __syncthreads();\n\n        #pragma unroll\n        for(int k = 0; k < BLOCK_SIZE_K; k += THREAD_SIZE_K){\n            #pragma unroll\n            for(int i = 0; i < THREAD_SIZE_K; i++){\n                #pragma unroll\n                for(int j = 0; j < THREAD_SIZE_M; j += 1){\n                    a_frag[j][i] = As[OFFSET(ty + vBLOCK_SIZE_M * j, k+i, BLOCK_SIZE_K)];\n                    //a_frag[j][i] = As[OFFSET(k+i, ty + vBLOCK_SIZE_M * j, BLOCK_SIZE_M)];\n                }\n            }\n\n            #pragma unroll\n            for(int i = 0; i < THREAD_SIZE_K; i++){\n                #pragma unroll\n                for(int j = 0; j < THREAD_SIZE_N; j += 1){\n                    b_frag[j][i] = Bs[OFFSET(k+i, tx + vBLOCK_SIZE_N * j, BLOCK_SIZE_N)];\n                }\n            }\n\n            #pragma unroll\n            for(int i = 0; i < THREAD_SIZE_N; i++){\n                #pragma unroll\n                for(int j = 0; j < THREAD_SIZE_M; j++){\n                    #pragma unroll\n                    for(int k_in = 0; k_in < THREAD_SIZE_K; k_in++){\n                        // accum[i][j] = fma(a_frag[j][k_in], b_frag[i][k_in], accum[i][j]);\n                        accum[i][j] += a_frag[j][k_in] * b_frag[i][k_in];\n                    }\n                }\n            }\n        }\n\n        __syncthreads();\n    }\n\n    float bias_local[THREAD_SIZE_N];\n    for(int thread_x = 0; thread_x < THREAD_SIZE_N; thread_x++){\n        bias_local[thread_x] = bias[BLOCK_SIZE_N * bx + tx + thread_x * vBLOCK_SIZE_N];\n    }\n\n    #pragma unroll\n    for(int thread_x = 0; thread_x < THREAD_SIZE_N; thread_x++){\n        #pragma unroll\n        for(int thread_y = 0; thread_y < THREAD_SIZE_M; thread_y+=1){\n            C[OFFSET(\n                BLOCK_SIZE_M * by + ty + thread_y * vBLOCK_SIZE_M,\n                BLOCK_SIZE_N * bx + tx + thread_x * vBLOCK_SIZE_N,\n                N\n            )] = (accum[thread_x][thread_y]) + bias_local[thread_x];\n        }\n    }\n}             ", "gridDim": [6, 64, 1], "blockDim": [16, 8, 1]}