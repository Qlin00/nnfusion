{"bert.encoder.layer.0.attention.self.query": {"code": "__global__ void MatMul_TILE_THREAD_GENERAL(float *input0, float *input1, float *input2, float *input3, float *output0) {\n    /*\n    g_vec: A [M, K]\n    g_mat_data: condense B [K_sparse, N]\n    g_mat_index: condense B index [K_sparse, N]\n    g_data: C [M, N]\n    */\n\tfloat *g_vec = input0;\n\tfloat *g_mat_data = input1;\n\tint *g_mat_index = (int*)input2;\n\tfloat *bias = input3;\n\tfloat *g_data = output0;\n\n\tconst int M = 4096;\n\tconst int K = 768;\n\tconst int N = 768;\n\n    const int BLOCK_SIZE_M = 64;\n    const int BLOCK_SIZE_N = 128;\n    const int BLOCK_SIZE_K = 32;\n    const int THREAD_SIZE_M = 8;\n    const int THREAD_SIZE_N = 8;\n\n    const float SPARSITY = 0.875;\n    const int BANK_VAL = 32;\n    const int ALIGN_N = THREAD_SIZE_N;\n\n    const int BANK_NUM_PER_BLOCK = BLOCK_SIZE_K / BANK_VAL;\n    const int BLOCK_SIZE_K_SPARSE = int(BLOCK_SIZE_K * (1-SPARSITY));\n    const int LEN_OF_BANK_PER_SPARSE_BLOCK = BLOCK_SIZE_K_SPARSE / BANK_NUM_PER_BLOCK;\n\n\tconst int K_SPARSE = int(K * SPARSITY);\n\tint M_BLOCK_START = blockIdx.x * BLOCK_SIZE_M;\n\tint N_BLOCK_START = blockIdx.y * BLOCK_SIZE_N;\n\n\tconst int A_THREADS_PER_ROW = BLOCK_SIZE_K / 4;\n\tconst int B_THREADS_PER_ROW = BLOCK_SIZE_N / 4;\n\n\tconst int THREADS_PER_BLOCK = (BLOCK_SIZE_M / THREAD_SIZE_M) * (BLOCK_SIZE_N / THREAD_SIZE_N);\n\n\tconst int A_STRIDES = THREADS_PER_BLOCK / A_THREADS_PER_ROW;\n\tconst int B_STRIDES = THREADS_PER_BLOCK / B_THREADS_PER_ROW;\n\n\t__shared__ float A_shared[BLOCK_SIZE_M * BLOCK_SIZE_K];\n\t__shared__ float B_shared[BLOCK_SIZE_N * BLOCK_SIZE_K_SPARSE];\n\t__shared__ int B_index_shared[BLOCK_SIZE_N * BLOCK_SIZE_K_SPARSE];\n\n\tfloat A_reg[THREAD_SIZE_M];\n\tfloat B_reg[THREAD_SIZE_N];\n\tint B_reg_index;\n\tfloat C_reg[THREAD_SIZE_M][THREAD_SIZE_N] = {0};\n\n\tint tid = threadIdx.x;\n\n\tint t_N = tid % (BLOCK_SIZE_N / THREAD_SIZE_N);\n\tint t_M = tid / (BLOCK_SIZE_N / THREAD_SIZE_N);\n\n\tint A_BLOCK_ROW_START = tid / A_THREADS_PER_ROW;\n\tint B_BLOCK_ROW_START = tid / B_THREADS_PER_ROW;\n\n\tint A_BLOCK_COL_START = tid % A_THREADS_PER_ROW * 4;\n\tint B_BLOCK_COL_START = tid % B_THREADS_PER_ROW * 4;\n\n\tfor(int K_BLOCK_START = 0, K_SPARSE_BLOCK_START = 0; K_BLOCK_START < K; K_BLOCK_START += BLOCK_SIZE_K, K_SPARSE_BLOCK_START += BLOCK_SIZE_K_SPARSE){\n\t\tfloat *A_global_ptr = g_vec + M_BLOCK_START * K + K_BLOCK_START;\n\t\tfloat *B_global_ptr = g_mat_data + K_SPARSE_BLOCK_START * N + N_BLOCK_START;\n\t\tint *B_index_global_ptr = g_mat_index + K_SPARSE_BLOCK_START * N + N_BLOCK_START;\n\n\t\t__syncthreads();\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_M; i += A_STRIDES){\n\t\t\t*(float4 *)(A_shared + (i + A_BLOCK_ROW_START) * BLOCK_SIZE_K + A_BLOCK_COL_START) = \n\t\t\t\t*(float4 *)(A_global_ptr + (i + A_BLOCK_ROW_START) * K + A_BLOCK_COL_START);\n\t\t}\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_K_SPARSE; i += B_STRIDES){\n\t\t\t*(float4 *)(B_shared + (i + B_BLOCK_ROW_START) * BLOCK_SIZE_N + B_BLOCK_COL_START) =\n\t\t\t\t*(float4 *)(B_global_ptr + (i + B_BLOCK_ROW_START) * N + B_BLOCK_COL_START);\n\n\t\t\t*(float4 *)(B_index_shared + (i + B_BLOCK_ROW_START) * BLOCK_SIZE_N + B_BLOCK_COL_START) =\n\t\t\t\t*(float4 *)(B_index_global_ptr + (i + B_BLOCK_ROW_START) * N + B_BLOCK_COL_START);\n\t\t}\n\n\t\t__syncthreads();\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_K_SPARSE;i += 1){\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_N; k += 1){\n\t\t\t\tB_reg[k] = B_shared[i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k];\n\t\t\t\t//*(float4 *)(B_reg + k) = *(float4 *)(B_shared + i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k);\n\t\t\t\t//*(float4 *)(B_reg_index + k) = *(float4 *)(B_index_shared + i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k);\n\t\t\t}\n\t\t\tint bank_idx = i / LEN_OF_BANK_PER_SPARSE_BLOCK;\n\t\t\tB_reg_index = B_index_shared[i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N] % BANK_VAL+bank_idx * BANK_VAL;\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_M; k += 1){\n\t\t\t\tA_reg[k] = A_shared[(t_M * THREAD_SIZE_M+k) * BLOCK_SIZE_K + B_reg_index];\n\t\t\t}\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_N; k += 1){\n\t\t\t\t#pragma unroll\n\t\t\t\tfor(int j = 0; j < THREAD_SIZE_M; j += 1){\n\t\t\t\t\tC_reg[j][k] += B_reg[k] * A_reg[j];\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n    float bias_local[THREAD_SIZE_N];\n    for(int thread_x = 0; thread_x < THREAD_SIZE_N; thread_x++){\n        bias_local[thread_x] = bias[BLOCK_SIZE_N * blockIdx.y + THREAD_SIZE_N * t_N + thread_x];\n    }\n\n\t#pragma unroll\n\tfor(int i = 0; i < THREAD_SIZE_M; i += 1){\n\t\t#pragma unroll\n\t\tfor(int j = 0; j < THREAD_SIZE_N; j += 1){\n\t\t\tg_data[(BLOCK_SIZE_M * blockIdx.x + THREAD_SIZE_M * t_M + i) * N + BLOCK_SIZE_N * blockIdx.y + THREAD_SIZE_N * t_N + j] = C_reg[i][j] + bias_local[j];\n\t\t}\n\t}\n}", "launch_config": {"dimBlock": [128, 1], "dimGrid": [64, 6]}}, "bert.encoder.layer.0.attention.self.key": {"code": "__global__ void MatMul_TILE_THREAD_GENERAL(float *input0, float *input1, float *input2, float *input3, float *output0) {\n    /*\n    g_vec: A [M, K]\n    g_mat_data: condense B [K_sparse, N]\n    g_mat_index: condense B index [K_sparse, N]\n    g_data: C [M, N]\n    */\n\tfloat *g_vec = input0;\n\tfloat *g_mat_data = input1;\n\tint *g_mat_index = (int*)input2;\n\tfloat *bias = input3;\n\tfloat *g_data = output0;\n\n\tconst int M = 4096;\n\tconst int K = 768;\n\tconst int N = 768;\n\n    const int BLOCK_SIZE_M = 64;\n    const int BLOCK_SIZE_N = 128;\n    const int BLOCK_SIZE_K = 32;\n    const int THREAD_SIZE_M = 8;\n    const int THREAD_SIZE_N = 8;\n\n    const float SPARSITY = 0.875;\n    const int BANK_VAL = 32;\n    const int ALIGN_N = THREAD_SIZE_N;\n\n    const int BANK_NUM_PER_BLOCK = BLOCK_SIZE_K / BANK_VAL;\n    const int BLOCK_SIZE_K_SPARSE = int(BLOCK_SIZE_K * (1-SPARSITY));\n    const int LEN_OF_BANK_PER_SPARSE_BLOCK = BLOCK_SIZE_K_SPARSE / BANK_NUM_PER_BLOCK;\n\n\tconst int K_SPARSE = int(K * SPARSITY);\n\tint M_BLOCK_START = blockIdx.x * BLOCK_SIZE_M;\n\tint N_BLOCK_START = blockIdx.y * BLOCK_SIZE_N;\n\n\tconst int A_THREADS_PER_ROW = BLOCK_SIZE_K / 4;\n\tconst int B_THREADS_PER_ROW = BLOCK_SIZE_N / 4;\n\n\tconst int THREADS_PER_BLOCK = (BLOCK_SIZE_M / THREAD_SIZE_M) * (BLOCK_SIZE_N / THREAD_SIZE_N);\n\n\tconst int A_STRIDES = THREADS_PER_BLOCK / A_THREADS_PER_ROW;\n\tconst int B_STRIDES = THREADS_PER_BLOCK / B_THREADS_PER_ROW;\n\n\t__shared__ float A_shared[BLOCK_SIZE_M * BLOCK_SIZE_K];\n\t__shared__ float B_shared[BLOCK_SIZE_N * BLOCK_SIZE_K_SPARSE];\n\t__shared__ int B_index_shared[BLOCK_SIZE_N * BLOCK_SIZE_K_SPARSE];\n\n\tfloat A_reg[THREAD_SIZE_M];\n\tfloat B_reg[THREAD_SIZE_N];\n\tint B_reg_index;\n\tfloat C_reg[THREAD_SIZE_M][THREAD_SIZE_N] = {0};\n\n\tint tid = threadIdx.x;\n\n\tint t_N = tid % (BLOCK_SIZE_N / THREAD_SIZE_N);\n\tint t_M = tid / (BLOCK_SIZE_N / THREAD_SIZE_N);\n\n\tint A_BLOCK_ROW_START = tid / A_THREADS_PER_ROW;\n\tint B_BLOCK_ROW_START = tid / B_THREADS_PER_ROW;\n\n\tint A_BLOCK_COL_START = tid % A_THREADS_PER_ROW * 4;\n\tint B_BLOCK_COL_START = tid % B_THREADS_PER_ROW * 4;\n\n\tfor(int K_BLOCK_START = 0, K_SPARSE_BLOCK_START = 0; K_BLOCK_START < K; K_BLOCK_START += BLOCK_SIZE_K, K_SPARSE_BLOCK_START += BLOCK_SIZE_K_SPARSE){\n\t\tfloat *A_global_ptr = g_vec + M_BLOCK_START * K + K_BLOCK_START;\n\t\tfloat *B_global_ptr = g_mat_data + K_SPARSE_BLOCK_START * N + N_BLOCK_START;\n\t\tint *B_index_global_ptr = g_mat_index + K_SPARSE_BLOCK_START * N + N_BLOCK_START;\n\n\t\t__syncthreads();\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_M; i += A_STRIDES){\n\t\t\t*(float4 *)(A_shared + (i + A_BLOCK_ROW_START) * BLOCK_SIZE_K + A_BLOCK_COL_START) = \n\t\t\t\t*(float4 *)(A_global_ptr + (i + A_BLOCK_ROW_START) * K + A_BLOCK_COL_START);\n\t\t}\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_K_SPARSE; i += B_STRIDES){\n\t\t\t*(float4 *)(B_shared + (i + B_BLOCK_ROW_START) * BLOCK_SIZE_N + B_BLOCK_COL_START) =\n\t\t\t\t*(float4 *)(B_global_ptr + (i + B_BLOCK_ROW_START) * N + B_BLOCK_COL_START);\n\n\t\t\t*(float4 *)(B_index_shared + (i + B_BLOCK_ROW_START) * BLOCK_SIZE_N + B_BLOCK_COL_START) =\n\t\t\t\t*(float4 *)(B_index_global_ptr + (i + B_BLOCK_ROW_START) * N + B_BLOCK_COL_START);\n\t\t}\n\n\t\t__syncthreads();\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_K_SPARSE;i += 1){\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_N; k += 1){\n\t\t\t\tB_reg[k] = B_shared[i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k];\n\t\t\t\t//*(float4 *)(B_reg + k) = *(float4 *)(B_shared + i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k);\n\t\t\t\t//*(float4 *)(B_reg_index + k) = *(float4 *)(B_index_shared + i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k);\n\t\t\t}\n\t\t\tint bank_idx = i / LEN_OF_BANK_PER_SPARSE_BLOCK;\n\t\t\tB_reg_index = B_index_shared[i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N] % BANK_VAL+bank_idx * BANK_VAL;\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_M; k += 1){\n\t\t\t\tA_reg[k] = A_shared[(t_M * THREAD_SIZE_M+k) * BLOCK_SIZE_K + B_reg_index];\n\t\t\t}\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_N; k += 1){\n\t\t\t\t#pragma unroll\n\t\t\t\tfor(int j = 0; j < THREAD_SIZE_M; j += 1){\n\t\t\t\t\tC_reg[j][k] += B_reg[k] * A_reg[j];\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n    float bias_local[THREAD_SIZE_N];\n    for(int thread_x = 0; thread_x < THREAD_SIZE_N; thread_x++){\n        bias_local[thread_x] = bias[BLOCK_SIZE_N * blockIdx.y + THREAD_SIZE_N * t_N + thread_x];\n    }\n\n\t#pragma unroll\n\tfor(int i = 0; i < THREAD_SIZE_M; i += 1){\n\t\t#pragma unroll\n\t\tfor(int j = 0; j < THREAD_SIZE_N; j += 1){\n\t\t\tg_data[(BLOCK_SIZE_M * blockIdx.x + THREAD_SIZE_M * t_M + i) * N + BLOCK_SIZE_N * blockIdx.y + THREAD_SIZE_N * t_N + j] = C_reg[i][j] + bias_local[j];\n\t\t}\n\t}\n}", "launch_config": {"dimBlock": [128, 1], "dimGrid": [64, 6]}}, "bert.encoder.layer.0.attention.self.value": {"code": "__global__ void MatMul_TILE_THREAD_GENERAL(float *input0, float *input1, float *input2, float *input3, float *output0) {\n    /*\n    g_vec: A [M, K]\n    g_mat_data: condense B [K_sparse, N]\n    g_mat_index: condense B index [K_sparse, N]\n    g_data: C [M, N]\n    */\n\tfloat *g_vec = input0;\n\tfloat *g_mat_data = input1;\n\tint *g_mat_index = (int*)input2;\n\tfloat *bias = input3;\n\tfloat *g_data = output0;\n\n\tconst int M = 4096;\n\tconst int K = 768;\n\tconst int N = 768;\n\n    const int BLOCK_SIZE_M = 64;\n    const int BLOCK_SIZE_N = 128;\n    const int BLOCK_SIZE_K = 32;\n    const int THREAD_SIZE_M = 8;\n    const int THREAD_SIZE_N = 8;\n\n    const float SPARSITY = 0.875;\n    const int BANK_VAL = 32;\n    const int ALIGN_N = THREAD_SIZE_N;\n\n    const int BANK_NUM_PER_BLOCK = BLOCK_SIZE_K / BANK_VAL;\n    const int BLOCK_SIZE_K_SPARSE = int(BLOCK_SIZE_K * (1-SPARSITY));\n    const int LEN_OF_BANK_PER_SPARSE_BLOCK = BLOCK_SIZE_K_SPARSE / BANK_NUM_PER_BLOCK;\n\n\tconst int K_SPARSE = int(K * SPARSITY);\n\tint M_BLOCK_START = blockIdx.x * BLOCK_SIZE_M;\n\tint N_BLOCK_START = blockIdx.y * BLOCK_SIZE_N;\n\n\tconst int A_THREADS_PER_ROW = BLOCK_SIZE_K / 4;\n\tconst int B_THREADS_PER_ROW = BLOCK_SIZE_N / 4;\n\n\tconst int THREADS_PER_BLOCK = (BLOCK_SIZE_M / THREAD_SIZE_M) * (BLOCK_SIZE_N / THREAD_SIZE_N);\n\n\tconst int A_STRIDES = THREADS_PER_BLOCK / A_THREADS_PER_ROW;\n\tconst int B_STRIDES = THREADS_PER_BLOCK / B_THREADS_PER_ROW;\n\n\t__shared__ float A_shared[BLOCK_SIZE_M * BLOCK_SIZE_K];\n\t__shared__ float B_shared[BLOCK_SIZE_N * BLOCK_SIZE_K_SPARSE];\n\t__shared__ int B_index_shared[BLOCK_SIZE_N * BLOCK_SIZE_K_SPARSE];\n\n\tfloat A_reg[THREAD_SIZE_M];\n\tfloat B_reg[THREAD_SIZE_N];\n\tint B_reg_index;\n\tfloat C_reg[THREAD_SIZE_M][THREAD_SIZE_N] = {0};\n\n\tint tid = threadIdx.x;\n\n\tint t_N = tid % (BLOCK_SIZE_N / THREAD_SIZE_N);\n\tint t_M = tid / (BLOCK_SIZE_N / THREAD_SIZE_N);\n\n\tint A_BLOCK_ROW_START = tid / A_THREADS_PER_ROW;\n\tint B_BLOCK_ROW_START = tid / B_THREADS_PER_ROW;\n\n\tint A_BLOCK_COL_START = tid % A_THREADS_PER_ROW * 4;\n\tint B_BLOCK_COL_START = tid % B_THREADS_PER_ROW * 4;\n\n\tfor(int K_BLOCK_START = 0, K_SPARSE_BLOCK_START = 0; K_BLOCK_START < K; K_BLOCK_START += BLOCK_SIZE_K, K_SPARSE_BLOCK_START += BLOCK_SIZE_K_SPARSE){\n\t\tfloat *A_global_ptr = g_vec + M_BLOCK_START * K + K_BLOCK_START;\n\t\tfloat *B_global_ptr = g_mat_data + K_SPARSE_BLOCK_START * N + N_BLOCK_START;\n\t\tint *B_index_global_ptr = g_mat_index + K_SPARSE_BLOCK_START * N + N_BLOCK_START;\n\n\t\t__syncthreads();\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_M; i += A_STRIDES){\n\t\t\t*(float4 *)(A_shared + (i + A_BLOCK_ROW_START) * BLOCK_SIZE_K + A_BLOCK_COL_START) = \n\t\t\t\t*(float4 *)(A_global_ptr + (i + A_BLOCK_ROW_START) * K + A_BLOCK_COL_START);\n\t\t}\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_K_SPARSE; i += B_STRIDES){\n\t\t\t*(float4 *)(B_shared + (i + B_BLOCK_ROW_START) * BLOCK_SIZE_N + B_BLOCK_COL_START) =\n\t\t\t\t*(float4 *)(B_global_ptr + (i + B_BLOCK_ROW_START) * N + B_BLOCK_COL_START);\n\n\t\t\t*(float4 *)(B_index_shared + (i + B_BLOCK_ROW_START) * BLOCK_SIZE_N + B_BLOCK_COL_START) =\n\t\t\t\t*(float4 *)(B_index_global_ptr + (i + B_BLOCK_ROW_START) * N + B_BLOCK_COL_START);\n\t\t}\n\n\t\t__syncthreads();\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_K_SPARSE;i += 1){\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_N; k += 1){\n\t\t\t\tB_reg[k] = B_shared[i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k];\n\t\t\t\t//*(float4 *)(B_reg + k) = *(float4 *)(B_shared + i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k);\n\t\t\t\t//*(float4 *)(B_reg_index + k) = *(float4 *)(B_index_shared + i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k);\n\t\t\t}\n\t\t\tint bank_idx = i / LEN_OF_BANK_PER_SPARSE_BLOCK;\n\t\t\tB_reg_index = B_index_shared[i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N] % BANK_VAL+bank_idx * BANK_VAL;\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_M; k += 1){\n\t\t\t\tA_reg[k] = A_shared[(t_M * THREAD_SIZE_M+k) * BLOCK_SIZE_K + B_reg_index];\n\t\t\t}\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_N; k += 1){\n\t\t\t\t#pragma unroll\n\t\t\t\tfor(int j = 0; j < THREAD_SIZE_M; j += 1){\n\t\t\t\t\tC_reg[j][k] += B_reg[k] * A_reg[j];\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n    float bias_local[THREAD_SIZE_N];\n    for(int thread_x = 0; thread_x < THREAD_SIZE_N; thread_x++){\n        bias_local[thread_x] = bias[BLOCK_SIZE_N * blockIdx.y + THREAD_SIZE_N * t_N + thread_x];\n    }\n\n\t#pragma unroll\n\tfor(int i = 0; i < THREAD_SIZE_M; i += 1){\n\t\t#pragma unroll\n\t\tfor(int j = 0; j < THREAD_SIZE_N; j += 1){\n\t\t\tg_data[(BLOCK_SIZE_M * blockIdx.x + THREAD_SIZE_M * t_M + i) * N + BLOCK_SIZE_N * blockIdx.y + THREAD_SIZE_N * t_N + j] = C_reg[i][j] + bias_local[j];\n\t\t}\n\t}\n}", "launch_config": {"dimBlock": [128, 1], "dimGrid": [64, 6]}}, "bert.encoder.layer.0.attention.output.dense": {"code": "__global__ void MatMul_TILE_THREAD_GENERAL(float *input0, float *input1, float *input2, float *input3, float *output0) {\n    /*\n    g_vec: A [M, K]\n    g_mat_data: condense B [K_sparse, N]\n    g_mat_index: condense B index [K_sparse, N]\n    g_data: C [M, N]\n    */\n\tfloat *g_vec = input0;\n\tfloat *g_mat_data = input1;\n\tint *g_mat_index = (int*)input2;\n\tfloat *bias = input3;\n\tfloat *g_data = output0;\n\n\tconst int M = 4096;\n\tconst int K = 768;\n\tconst int N = 768;\n\n    const int BLOCK_SIZE_M = 64;\n    const int BLOCK_SIZE_N = 128;\n    const int BLOCK_SIZE_K = 32;\n    const int THREAD_SIZE_M = 8;\n    const int THREAD_SIZE_N = 8;\n\n    const float SPARSITY = 0.875;\n    const int BANK_VAL = 32;\n    const int ALIGN_N = THREAD_SIZE_N;\n\n    const int BANK_NUM_PER_BLOCK = BLOCK_SIZE_K / BANK_VAL;\n    const int BLOCK_SIZE_K_SPARSE = int(BLOCK_SIZE_K * (1-SPARSITY));\n    const int LEN_OF_BANK_PER_SPARSE_BLOCK = BLOCK_SIZE_K_SPARSE / BANK_NUM_PER_BLOCK;\n\n\tconst int K_SPARSE = int(K * SPARSITY);\n\tint M_BLOCK_START = blockIdx.x * BLOCK_SIZE_M;\n\tint N_BLOCK_START = blockIdx.y * BLOCK_SIZE_N;\n\n\tconst int A_THREADS_PER_ROW = BLOCK_SIZE_K / 4;\n\tconst int B_THREADS_PER_ROW = BLOCK_SIZE_N / 4;\n\n\tconst int THREADS_PER_BLOCK = (BLOCK_SIZE_M / THREAD_SIZE_M) * (BLOCK_SIZE_N / THREAD_SIZE_N);\n\n\tconst int A_STRIDES = THREADS_PER_BLOCK / A_THREADS_PER_ROW;\n\tconst int B_STRIDES = THREADS_PER_BLOCK / B_THREADS_PER_ROW;\n\n\t__shared__ float A_shared[BLOCK_SIZE_M * BLOCK_SIZE_K];\n\t__shared__ float B_shared[BLOCK_SIZE_N * BLOCK_SIZE_K_SPARSE];\n\t__shared__ int B_index_shared[BLOCK_SIZE_N * BLOCK_SIZE_K_SPARSE];\n\n\tfloat A_reg[THREAD_SIZE_M];\n\tfloat B_reg[THREAD_SIZE_N];\n\tint B_reg_index;\n\tfloat C_reg[THREAD_SIZE_M][THREAD_SIZE_N] = {0};\n\n\tint tid = threadIdx.x;\n\n\tint t_N = tid % (BLOCK_SIZE_N / THREAD_SIZE_N);\n\tint t_M = tid / (BLOCK_SIZE_N / THREAD_SIZE_N);\n\n\tint A_BLOCK_ROW_START = tid / A_THREADS_PER_ROW;\n\tint B_BLOCK_ROW_START = tid / B_THREADS_PER_ROW;\n\n\tint A_BLOCK_COL_START = tid % A_THREADS_PER_ROW * 4;\n\tint B_BLOCK_COL_START = tid % B_THREADS_PER_ROW * 4;\n\n\tfor(int K_BLOCK_START = 0, K_SPARSE_BLOCK_START = 0; K_BLOCK_START < K; K_BLOCK_START += BLOCK_SIZE_K, K_SPARSE_BLOCK_START += BLOCK_SIZE_K_SPARSE){\n\t\tfloat *A_global_ptr = g_vec + M_BLOCK_START * K + K_BLOCK_START;\n\t\tfloat *B_global_ptr = g_mat_data + K_SPARSE_BLOCK_START * N + N_BLOCK_START;\n\t\tint *B_index_global_ptr = g_mat_index + K_SPARSE_BLOCK_START * N + N_BLOCK_START;\n\n\t\t__syncthreads();\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_M; i += A_STRIDES){\n\t\t\t*(float4 *)(A_shared + (i + A_BLOCK_ROW_START) * BLOCK_SIZE_K + A_BLOCK_COL_START) = \n\t\t\t\t*(float4 *)(A_global_ptr + (i + A_BLOCK_ROW_START) * K + A_BLOCK_COL_START);\n\t\t}\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_K_SPARSE; i += B_STRIDES){\n\t\t\t*(float4 *)(B_shared + (i + B_BLOCK_ROW_START) * BLOCK_SIZE_N + B_BLOCK_COL_START) =\n\t\t\t\t*(float4 *)(B_global_ptr + (i + B_BLOCK_ROW_START) * N + B_BLOCK_COL_START);\n\n\t\t\t*(float4 *)(B_index_shared + (i + B_BLOCK_ROW_START) * BLOCK_SIZE_N + B_BLOCK_COL_START) =\n\t\t\t\t*(float4 *)(B_index_global_ptr + (i + B_BLOCK_ROW_START) * N + B_BLOCK_COL_START);\n\t\t}\n\n\t\t__syncthreads();\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_K_SPARSE;i += 1){\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_N; k += 1){\n\t\t\t\tB_reg[k] = B_shared[i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k];\n\t\t\t\t//*(float4 *)(B_reg + k) = *(float4 *)(B_shared + i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k);\n\t\t\t\t//*(float4 *)(B_reg_index + k) = *(float4 *)(B_index_shared + i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k);\n\t\t\t}\n\t\t\tint bank_idx = i / LEN_OF_BANK_PER_SPARSE_BLOCK;\n\t\t\tB_reg_index = B_index_shared[i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N] % BANK_VAL+bank_idx * BANK_VAL;\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_M; k += 1){\n\t\t\t\tA_reg[k] = A_shared[(t_M * THREAD_SIZE_M+k) * BLOCK_SIZE_K + B_reg_index];\n\t\t\t}\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_N; k += 1){\n\t\t\t\t#pragma unroll\n\t\t\t\tfor(int j = 0; j < THREAD_SIZE_M; j += 1){\n\t\t\t\t\tC_reg[j][k] += B_reg[k] * A_reg[j];\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n    float bias_local[THREAD_SIZE_N];\n    for(int thread_x = 0; thread_x < THREAD_SIZE_N; thread_x++){\n        bias_local[thread_x] = bias[BLOCK_SIZE_N * blockIdx.y + THREAD_SIZE_N * t_N + thread_x];\n    }\n\n\t#pragma unroll\n\tfor(int i = 0; i < THREAD_SIZE_M; i += 1){\n\t\t#pragma unroll\n\t\tfor(int j = 0; j < THREAD_SIZE_N; j += 1){\n\t\t\tg_data[(BLOCK_SIZE_M * blockIdx.x + THREAD_SIZE_M * t_M + i) * N + BLOCK_SIZE_N * blockIdx.y + THREAD_SIZE_N * t_N + j] = C_reg[i][j] + bias_local[j];\n\t\t}\n\t}\n}", "launch_config": {"dimBlock": [128, 1], "dimGrid": [64, 6]}}, "bert.encoder.layer.0.intermediate.dense": {"code": "__global__ void MatMul_TILE_THREAD_GENERAL(float *input0, float *input1, float *input2, float *input3, float *output0) {\n    /*\n    g_vec: A [M, K]\n    g_mat_data: condense B [K_sparse, N]\n    g_mat_index: condense B index [K_sparse, N]\n    g_data: C [M, N]\n    */\n\tfloat *g_vec = input0;\n\tfloat *g_mat_data = input1;\n\tint *g_mat_index = (int*)input2;\n\tfloat *bias = input3;\n\tfloat *g_data = output0;\n\n\tconst int M = 4096;\n\tconst int K = 768;\n\tconst int N = 3072;\n\n    const int BLOCK_SIZE_M = 64;\n    const int BLOCK_SIZE_N = 128;\n    const int BLOCK_SIZE_K = 32;\n    const int THREAD_SIZE_M = 8;\n    const int THREAD_SIZE_N = 8;\n\n    const float SPARSITY = 0.875;\n    const int BANK_VAL = 32;\n    const int ALIGN_N = THREAD_SIZE_N;\n\n    const int BANK_NUM_PER_BLOCK = BLOCK_SIZE_K / BANK_VAL;\n    const int BLOCK_SIZE_K_SPARSE = int(BLOCK_SIZE_K * (1-SPARSITY));\n    const int LEN_OF_BANK_PER_SPARSE_BLOCK = BLOCK_SIZE_K_SPARSE / BANK_NUM_PER_BLOCK;\n\n\tconst int K_SPARSE = int(K * SPARSITY);\n\tint M_BLOCK_START = blockIdx.x * BLOCK_SIZE_M;\n\tint N_BLOCK_START = blockIdx.y * BLOCK_SIZE_N;\n\n\tconst int A_THREADS_PER_ROW = BLOCK_SIZE_K / 4;\n\tconst int B_THREADS_PER_ROW = BLOCK_SIZE_N / 4;\n\n\tconst int THREADS_PER_BLOCK = (BLOCK_SIZE_M / THREAD_SIZE_M) * (BLOCK_SIZE_N / THREAD_SIZE_N);\n\n\tconst int A_STRIDES = THREADS_PER_BLOCK / A_THREADS_PER_ROW;\n\tconst int B_STRIDES = THREADS_PER_BLOCK / B_THREADS_PER_ROW;\n\n\t__shared__ float A_shared[BLOCK_SIZE_M * BLOCK_SIZE_K];\n\t__shared__ float B_shared[BLOCK_SIZE_N * BLOCK_SIZE_K_SPARSE];\n\t__shared__ int B_index_shared[BLOCK_SIZE_N * BLOCK_SIZE_K_SPARSE];\n\n\tfloat A_reg[THREAD_SIZE_M];\n\tfloat B_reg[THREAD_SIZE_N];\n\tint B_reg_index;\n\tfloat C_reg[THREAD_SIZE_M][THREAD_SIZE_N] = {0};\n\n\tint tid = threadIdx.x;\n\n\tint t_N = tid % (BLOCK_SIZE_N / THREAD_SIZE_N);\n\tint t_M = tid / (BLOCK_SIZE_N / THREAD_SIZE_N);\n\n\tint A_BLOCK_ROW_START = tid / A_THREADS_PER_ROW;\n\tint B_BLOCK_ROW_START = tid / B_THREADS_PER_ROW;\n\n\tint A_BLOCK_COL_START = tid % A_THREADS_PER_ROW * 4;\n\tint B_BLOCK_COL_START = tid % B_THREADS_PER_ROW * 4;\n\n\tfor(int K_BLOCK_START = 0, K_SPARSE_BLOCK_START = 0; K_BLOCK_START < K; K_BLOCK_START += BLOCK_SIZE_K, K_SPARSE_BLOCK_START += BLOCK_SIZE_K_SPARSE){\n\t\tfloat *A_global_ptr = g_vec + M_BLOCK_START * K + K_BLOCK_START;\n\t\tfloat *B_global_ptr = g_mat_data + K_SPARSE_BLOCK_START * N + N_BLOCK_START;\n\t\tint *B_index_global_ptr = g_mat_index + K_SPARSE_BLOCK_START * N + N_BLOCK_START;\n\n\t\t__syncthreads();\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_M; i += A_STRIDES){\n\t\t\t*(float4 *)(A_shared + (i + A_BLOCK_ROW_START) * BLOCK_SIZE_K + A_BLOCK_COL_START) = \n\t\t\t\t*(float4 *)(A_global_ptr + (i + A_BLOCK_ROW_START) * K + A_BLOCK_COL_START);\n\t\t}\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_K_SPARSE; i += B_STRIDES){\n\t\t\t*(float4 *)(B_shared + (i + B_BLOCK_ROW_START) * BLOCK_SIZE_N + B_BLOCK_COL_START) =\n\t\t\t\t*(float4 *)(B_global_ptr + (i + B_BLOCK_ROW_START) * N + B_BLOCK_COL_START);\n\n\t\t\t*(float4 *)(B_index_shared + (i + B_BLOCK_ROW_START) * BLOCK_SIZE_N + B_BLOCK_COL_START) =\n\t\t\t\t*(float4 *)(B_index_global_ptr + (i + B_BLOCK_ROW_START) * N + B_BLOCK_COL_START);\n\t\t}\n\n\t\t__syncthreads();\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_K_SPARSE;i += 1){\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_N; k += 1){\n\t\t\t\tB_reg[k] = B_shared[i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k];\n\t\t\t\t//*(float4 *)(B_reg + k) = *(float4 *)(B_shared + i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k);\n\t\t\t\t//*(float4 *)(B_reg_index + k) = *(float4 *)(B_index_shared + i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k);\n\t\t\t}\n\t\t\tint bank_idx = i / LEN_OF_BANK_PER_SPARSE_BLOCK;\n\t\t\tB_reg_index = B_index_shared[i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N] % BANK_VAL+bank_idx * BANK_VAL;\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_M; k += 1){\n\t\t\t\tA_reg[k] = A_shared[(t_M * THREAD_SIZE_M+k) * BLOCK_SIZE_K + B_reg_index];\n\t\t\t}\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_N; k += 1){\n\t\t\t\t#pragma unroll\n\t\t\t\tfor(int j = 0; j < THREAD_SIZE_M; j += 1){\n\t\t\t\t\tC_reg[j][k] += B_reg[k] * A_reg[j];\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n    float bias_local[THREAD_SIZE_N];\n    for(int thread_x = 0; thread_x < THREAD_SIZE_N; thread_x++){\n        bias_local[thread_x] = bias[BLOCK_SIZE_N * blockIdx.y + THREAD_SIZE_N * t_N + thread_x];\n    }\n\n\t#pragma unroll\n\tfor(int i = 0; i < THREAD_SIZE_M; i += 1){\n\t\t#pragma unroll\n\t\tfor(int j = 0; j < THREAD_SIZE_N; j += 1){\n\t\t\tg_data[(BLOCK_SIZE_M * blockIdx.x + THREAD_SIZE_M * t_M + i) * N + BLOCK_SIZE_N * blockIdx.y + THREAD_SIZE_N * t_N + j] = C_reg[i][j] + bias_local[j];\n\t\t}\n\t}\n}", "launch_config": {"dimBlock": [128, 1], "dimGrid": [64, 24]}}, "bert.encoder.layer.0.output.dense": {"code": "__global__ void MatMul_TILE_THREAD_GENERAL(float *input0, float *input1, float *input2, float *input3, float *output0) {\n    /*\n    g_vec: A [M, K]\n    g_mat_data: condense B [K_sparse, N]\n    g_mat_index: condense B index [K_sparse, N]\n    g_data: C [M, N]\n    */\n\tfloat *g_vec = input0;\n\tfloat *g_mat_data = input1;\n\tint *g_mat_index = (int*)input2;\n\tfloat *bias = input3;\n\tfloat *g_data = output0;\n\n\tconst int M = 4096;\n\tconst int K = 3072;\n\tconst int N = 768;\n\n    const int BLOCK_SIZE_M = 64;\n    const int BLOCK_SIZE_N = 128;\n    const int BLOCK_SIZE_K = 32;\n    const int THREAD_SIZE_M = 8;\n    const int THREAD_SIZE_N = 8;\n\n    const float SPARSITY = 0.875;\n    const int BANK_VAL = 32;\n    const int ALIGN_N = THREAD_SIZE_N;\n\n    const int BANK_NUM_PER_BLOCK = BLOCK_SIZE_K / BANK_VAL;\n    const int BLOCK_SIZE_K_SPARSE = int(BLOCK_SIZE_K * (1-SPARSITY));\n    const int LEN_OF_BANK_PER_SPARSE_BLOCK = BLOCK_SIZE_K_SPARSE / BANK_NUM_PER_BLOCK;\n\n\tconst int K_SPARSE = int(K * SPARSITY);\n\tint M_BLOCK_START = blockIdx.x * BLOCK_SIZE_M;\n\tint N_BLOCK_START = blockIdx.y * BLOCK_SIZE_N;\n\n\tconst int A_THREADS_PER_ROW = BLOCK_SIZE_K / 4;\n\tconst int B_THREADS_PER_ROW = BLOCK_SIZE_N / 4;\n\n\tconst int THREADS_PER_BLOCK = (BLOCK_SIZE_M / THREAD_SIZE_M) * (BLOCK_SIZE_N / THREAD_SIZE_N);\n\n\tconst int A_STRIDES = THREADS_PER_BLOCK / A_THREADS_PER_ROW;\n\tconst int B_STRIDES = THREADS_PER_BLOCK / B_THREADS_PER_ROW;\n\n\t__shared__ float A_shared[BLOCK_SIZE_M * BLOCK_SIZE_K];\n\t__shared__ float B_shared[BLOCK_SIZE_N * BLOCK_SIZE_K_SPARSE];\n\t__shared__ int B_index_shared[BLOCK_SIZE_N * BLOCK_SIZE_K_SPARSE];\n\n\tfloat A_reg[THREAD_SIZE_M];\n\tfloat B_reg[THREAD_SIZE_N];\n\tint B_reg_index;\n\tfloat C_reg[THREAD_SIZE_M][THREAD_SIZE_N] = {0};\n\n\tint tid = threadIdx.x;\n\n\tint t_N = tid % (BLOCK_SIZE_N / THREAD_SIZE_N);\n\tint t_M = tid / (BLOCK_SIZE_N / THREAD_SIZE_N);\n\n\tint A_BLOCK_ROW_START = tid / A_THREADS_PER_ROW;\n\tint B_BLOCK_ROW_START = tid / B_THREADS_PER_ROW;\n\n\tint A_BLOCK_COL_START = tid % A_THREADS_PER_ROW * 4;\n\tint B_BLOCK_COL_START = tid % B_THREADS_PER_ROW * 4;\n\n\tfor(int K_BLOCK_START = 0, K_SPARSE_BLOCK_START = 0; K_BLOCK_START < K; K_BLOCK_START += BLOCK_SIZE_K, K_SPARSE_BLOCK_START += BLOCK_SIZE_K_SPARSE){\n\t\tfloat *A_global_ptr = g_vec + M_BLOCK_START * K + K_BLOCK_START;\n\t\tfloat *B_global_ptr = g_mat_data + K_SPARSE_BLOCK_START * N + N_BLOCK_START;\n\t\tint *B_index_global_ptr = g_mat_index + K_SPARSE_BLOCK_START * N + N_BLOCK_START;\n\n\t\t__syncthreads();\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_M; i += A_STRIDES){\n\t\t\t*(float4 *)(A_shared + (i + A_BLOCK_ROW_START) * BLOCK_SIZE_K + A_BLOCK_COL_START) = \n\t\t\t\t*(float4 *)(A_global_ptr + (i + A_BLOCK_ROW_START) * K + A_BLOCK_COL_START);\n\t\t}\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_K_SPARSE; i += B_STRIDES){\n\t\t\t*(float4 *)(B_shared + (i + B_BLOCK_ROW_START) * BLOCK_SIZE_N + B_BLOCK_COL_START) =\n\t\t\t\t*(float4 *)(B_global_ptr + (i + B_BLOCK_ROW_START) * N + B_BLOCK_COL_START);\n\n\t\t\t*(float4 *)(B_index_shared + (i + B_BLOCK_ROW_START) * BLOCK_SIZE_N + B_BLOCK_COL_START) =\n\t\t\t\t*(float4 *)(B_index_global_ptr + (i + B_BLOCK_ROW_START) * N + B_BLOCK_COL_START);\n\t\t}\n\n\t\t__syncthreads();\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_K_SPARSE;i += 1){\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_N; k += 1){\n\t\t\t\tB_reg[k] = B_shared[i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k];\n\t\t\t\t//*(float4 *)(B_reg + k) = *(float4 *)(B_shared + i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k);\n\t\t\t\t//*(float4 *)(B_reg_index + k) = *(float4 *)(B_index_shared + i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k);\n\t\t\t}\n\t\t\tint bank_idx = i / LEN_OF_BANK_PER_SPARSE_BLOCK;\n\t\t\tB_reg_index = B_index_shared[i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N] % BANK_VAL+bank_idx * BANK_VAL;\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_M; k += 1){\n\t\t\t\tA_reg[k] = A_shared[(t_M * THREAD_SIZE_M+k) * BLOCK_SIZE_K + B_reg_index];\n\t\t\t}\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_N; k += 1){\n\t\t\t\t#pragma unroll\n\t\t\t\tfor(int j = 0; j < THREAD_SIZE_M; j += 1){\n\t\t\t\t\tC_reg[j][k] += B_reg[k] * A_reg[j];\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n    float bias_local[THREAD_SIZE_N];\n    for(int thread_x = 0; thread_x < THREAD_SIZE_N; thread_x++){\n        bias_local[thread_x] = bias[BLOCK_SIZE_N * blockIdx.y + THREAD_SIZE_N * t_N + thread_x];\n    }\n\n\t#pragma unroll\n\tfor(int i = 0; i < THREAD_SIZE_M; i += 1){\n\t\t#pragma unroll\n\t\tfor(int j = 0; j < THREAD_SIZE_N; j += 1){\n\t\t\tg_data[(BLOCK_SIZE_M * blockIdx.x + THREAD_SIZE_M * t_M + i) * N + BLOCK_SIZE_N * blockIdx.y + THREAD_SIZE_N * t_N + j] = C_reg[i][j] + bias_local[j];\n\t\t}\n\t}\n}", "launch_config": {"dimBlock": [128, 1], "dimGrid": [64, 6]}}, "bert.encoder.layer.1.attention.self.query": {"code": "__global__ void MatMul_TILE_THREAD_GENERAL(float *input0, float *input1, float *input2, float *input3, float *output0) {\n    /*\n    g_vec: A [M, K]\n    g_mat_data: condense B [K_sparse, N]\n    g_mat_index: condense B index [K_sparse, N]\n    g_data: C [M, N]\n    */\n\tfloat *g_vec = input0;\n\tfloat *g_mat_data = input1;\n\tint *g_mat_index = (int*)input2;\n\tfloat *bias = input3;\n\tfloat *g_data = output0;\n\n\tconst int M = 4096;\n\tconst int K = 768;\n\tconst int N = 768;\n\n    const int BLOCK_SIZE_M = 64;\n    const int BLOCK_SIZE_N = 128;\n    const int BLOCK_SIZE_K = 32;\n    const int THREAD_SIZE_M = 8;\n    const int THREAD_SIZE_N = 8;\n\n    const float SPARSITY = 0.875;\n    const int BANK_VAL = 32;\n    const int ALIGN_N = THREAD_SIZE_N;\n\n    const int BANK_NUM_PER_BLOCK = BLOCK_SIZE_K / BANK_VAL;\n    const int BLOCK_SIZE_K_SPARSE = int(BLOCK_SIZE_K * (1-SPARSITY));\n    const int LEN_OF_BANK_PER_SPARSE_BLOCK = BLOCK_SIZE_K_SPARSE / BANK_NUM_PER_BLOCK;\n\n\tconst int K_SPARSE = int(K * SPARSITY);\n\tint M_BLOCK_START = blockIdx.x * BLOCK_SIZE_M;\n\tint N_BLOCK_START = blockIdx.y * BLOCK_SIZE_N;\n\n\tconst int A_THREADS_PER_ROW = BLOCK_SIZE_K / 4;\n\tconst int B_THREADS_PER_ROW = BLOCK_SIZE_N / 4;\n\n\tconst int THREADS_PER_BLOCK = (BLOCK_SIZE_M / THREAD_SIZE_M) * (BLOCK_SIZE_N / THREAD_SIZE_N);\n\n\tconst int A_STRIDES = THREADS_PER_BLOCK / A_THREADS_PER_ROW;\n\tconst int B_STRIDES = THREADS_PER_BLOCK / B_THREADS_PER_ROW;\n\n\t__shared__ float A_shared[BLOCK_SIZE_M * BLOCK_SIZE_K];\n\t__shared__ float B_shared[BLOCK_SIZE_N * BLOCK_SIZE_K_SPARSE];\n\t__shared__ int B_index_shared[BLOCK_SIZE_N * BLOCK_SIZE_K_SPARSE];\n\n\tfloat A_reg[THREAD_SIZE_M];\n\tfloat B_reg[THREAD_SIZE_N];\n\tint B_reg_index;\n\tfloat C_reg[THREAD_SIZE_M][THREAD_SIZE_N] = {0};\n\n\tint tid = threadIdx.x;\n\n\tint t_N = tid % (BLOCK_SIZE_N / THREAD_SIZE_N);\n\tint t_M = tid / (BLOCK_SIZE_N / THREAD_SIZE_N);\n\n\tint A_BLOCK_ROW_START = tid / A_THREADS_PER_ROW;\n\tint B_BLOCK_ROW_START = tid / B_THREADS_PER_ROW;\n\n\tint A_BLOCK_COL_START = tid % A_THREADS_PER_ROW * 4;\n\tint B_BLOCK_COL_START = tid % B_THREADS_PER_ROW * 4;\n\n\tfor(int K_BLOCK_START = 0, K_SPARSE_BLOCK_START = 0; K_BLOCK_START < K; K_BLOCK_START += BLOCK_SIZE_K, K_SPARSE_BLOCK_START += BLOCK_SIZE_K_SPARSE){\n\t\tfloat *A_global_ptr = g_vec + M_BLOCK_START * K + K_BLOCK_START;\n\t\tfloat *B_global_ptr = g_mat_data + K_SPARSE_BLOCK_START * N + N_BLOCK_START;\n\t\tint *B_index_global_ptr = g_mat_index + K_SPARSE_BLOCK_START * N + N_BLOCK_START;\n\n\t\t__syncthreads();\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_M; i += A_STRIDES){\n\t\t\t*(float4 *)(A_shared + (i + A_BLOCK_ROW_START) * BLOCK_SIZE_K + A_BLOCK_COL_START) = \n\t\t\t\t*(float4 *)(A_global_ptr + (i + A_BLOCK_ROW_START) * K + A_BLOCK_COL_START);\n\t\t}\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_K_SPARSE; i += B_STRIDES){\n\t\t\t*(float4 *)(B_shared + (i + B_BLOCK_ROW_START) * BLOCK_SIZE_N + B_BLOCK_COL_START) =\n\t\t\t\t*(float4 *)(B_global_ptr + (i + B_BLOCK_ROW_START) * N + B_BLOCK_COL_START);\n\n\t\t\t*(float4 *)(B_index_shared + (i + B_BLOCK_ROW_START) * BLOCK_SIZE_N + B_BLOCK_COL_START) =\n\t\t\t\t*(float4 *)(B_index_global_ptr + (i + B_BLOCK_ROW_START) * N + B_BLOCK_COL_START);\n\t\t}\n\n\t\t__syncthreads();\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_K_SPARSE;i += 1){\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_N; k += 1){\n\t\t\t\tB_reg[k] = B_shared[i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k];\n\t\t\t\t//*(float4 *)(B_reg + k) = *(float4 *)(B_shared + i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k);\n\t\t\t\t//*(float4 *)(B_reg_index + k) = *(float4 *)(B_index_shared + i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k);\n\t\t\t}\n\t\t\tint bank_idx = i / LEN_OF_BANK_PER_SPARSE_BLOCK;\n\t\t\tB_reg_index = B_index_shared[i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N] % BANK_VAL+bank_idx * BANK_VAL;\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_M; k += 1){\n\t\t\t\tA_reg[k] = A_shared[(t_M * THREAD_SIZE_M+k) * BLOCK_SIZE_K + B_reg_index];\n\t\t\t}\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_N; k += 1){\n\t\t\t\t#pragma unroll\n\t\t\t\tfor(int j = 0; j < THREAD_SIZE_M; j += 1){\n\t\t\t\t\tC_reg[j][k] += B_reg[k] * A_reg[j];\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n    float bias_local[THREAD_SIZE_N];\n    for(int thread_x = 0; thread_x < THREAD_SIZE_N; thread_x++){\n        bias_local[thread_x] = bias[BLOCK_SIZE_N * blockIdx.y + THREAD_SIZE_N * t_N + thread_x];\n    }\n\n\t#pragma unroll\n\tfor(int i = 0; i < THREAD_SIZE_M; i += 1){\n\t\t#pragma unroll\n\t\tfor(int j = 0; j < THREAD_SIZE_N; j += 1){\n\t\t\tg_data[(BLOCK_SIZE_M * blockIdx.x + THREAD_SIZE_M * t_M + i) * N + BLOCK_SIZE_N * blockIdx.y + THREAD_SIZE_N * t_N + j] = C_reg[i][j] + bias_local[j];\n\t\t}\n\t}\n}", "launch_config": {"dimBlock": [128, 1], "dimGrid": [64, 6]}}, "bert.encoder.layer.1.attention.self.key": {"code": "__global__ void MatMul_TILE_THREAD_GENERAL(float *input0, float *input1, float *input2, float *input3, float *output0) {\n    /*\n    g_vec: A [M, K]\n    g_mat_data: condense B [K_sparse, N]\n    g_mat_index: condense B index [K_sparse, N]\n    g_data: C [M, N]\n    */\n\tfloat *g_vec = input0;\n\tfloat *g_mat_data = input1;\n\tint *g_mat_index = (int*)input2;\n\tfloat *bias = input3;\n\tfloat *g_data = output0;\n\n\tconst int M = 4096;\n\tconst int K = 768;\n\tconst int N = 768;\n\n    const int BLOCK_SIZE_M = 64;\n    const int BLOCK_SIZE_N = 128;\n    const int BLOCK_SIZE_K = 32;\n    const int THREAD_SIZE_M = 8;\n    const int THREAD_SIZE_N = 8;\n\n    const float SPARSITY = 0.875;\n    const int BANK_VAL = 32;\n    const int ALIGN_N = THREAD_SIZE_N;\n\n    const int BANK_NUM_PER_BLOCK = BLOCK_SIZE_K / BANK_VAL;\n    const int BLOCK_SIZE_K_SPARSE = int(BLOCK_SIZE_K * (1-SPARSITY));\n    const int LEN_OF_BANK_PER_SPARSE_BLOCK = BLOCK_SIZE_K_SPARSE / BANK_NUM_PER_BLOCK;\n\n\tconst int K_SPARSE = int(K * SPARSITY);\n\tint M_BLOCK_START = blockIdx.x * BLOCK_SIZE_M;\n\tint N_BLOCK_START = blockIdx.y * BLOCK_SIZE_N;\n\n\tconst int A_THREADS_PER_ROW = BLOCK_SIZE_K / 4;\n\tconst int B_THREADS_PER_ROW = BLOCK_SIZE_N / 4;\n\n\tconst int THREADS_PER_BLOCK = (BLOCK_SIZE_M / THREAD_SIZE_M) * (BLOCK_SIZE_N / THREAD_SIZE_N);\n\n\tconst int A_STRIDES = THREADS_PER_BLOCK / A_THREADS_PER_ROW;\n\tconst int B_STRIDES = THREADS_PER_BLOCK / B_THREADS_PER_ROW;\n\n\t__shared__ float A_shared[BLOCK_SIZE_M * BLOCK_SIZE_K];\n\t__shared__ float B_shared[BLOCK_SIZE_N * BLOCK_SIZE_K_SPARSE];\n\t__shared__ int B_index_shared[BLOCK_SIZE_N * BLOCK_SIZE_K_SPARSE];\n\n\tfloat A_reg[THREAD_SIZE_M];\n\tfloat B_reg[THREAD_SIZE_N];\n\tint B_reg_index;\n\tfloat C_reg[THREAD_SIZE_M][THREAD_SIZE_N] = {0};\n\n\tint tid = threadIdx.x;\n\n\tint t_N = tid % (BLOCK_SIZE_N / THREAD_SIZE_N);\n\tint t_M = tid / (BLOCK_SIZE_N / THREAD_SIZE_N);\n\n\tint A_BLOCK_ROW_START = tid / A_THREADS_PER_ROW;\n\tint B_BLOCK_ROW_START = tid / B_THREADS_PER_ROW;\n\n\tint A_BLOCK_COL_START = tid % A_THREADS_PER_ROW * 4;\n\tint B_BLOCK_COL_START = tid % B_THREADS_PER_ROW * 4;\n\n\tfor(int K_BLOCK_START = 0, K_SPARSE_BLOCK_START = 0; K_BLOCK_START < K; K_BLOCK_START += BLOCK_SIZE_K, K_SPARSE_BLOCK_START += BLOCK_SIZE_K_SPARSE){\n\t\tfloat *A_global_ptr = g_vec + M_BLOCK_START * K + K_BLOCK_START;\n\t\tfloat *B_global_ptr = g_mat_data + K_SPARSE_BLOCK_START * N + N_BLOCK_START;\n\t\tint *B_index_global_ptr = g_mat_index + K_SPARSE_BLOCK_START * N + N_BLOCK_START;\n\n\t\t__syncthreads();\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_M; i += A_STRIDES){\n\t\t\t*(float4 *)(A_shared + (i + A_BLOCK_ROW_START) * BLOCK_SIZE_K + A_BLOCK_COL_START) = \n\t\t\t\t*(float4 *)(A_global_ptr + (i + A_BLOCK_ROW_START) * K + A_BLOCK_COL_START);\n\t\t}\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_K_SPARSE; i += B_STRIDES){\n\t\t\t*(float4 *)(B_shared + (i + B_BLOCK_ROW_START) * BLOCK_SIZE_N + B_BLOCK_COL_START) =\n\t\t\t\t*(float4 *)(B_global_ptr + (i + B_BLOCK_ROW_START) * N + B_BLOCK_COL_START);\n\n\t\t\t*(float4 *)(B_index_shared + (i + B_BLOCK_ROW_START) * BLOCK_SIZE_N + B_BLOCK_COL_START) =\n\t\t\t\t*(float4 *)(B_index_global_ptr + (i + B_BLOCK_ROW_START) * N + B_BLOCK_COL_START);\n\t\t}\n\n\t\t__syncthreads();\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_K_SPARSE;i += 1){\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_N; k += 1){\n\t\t\t\tB_reg[k] = B_shared[i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k];\n\t\t\t\t//*(float4 *)(B_reg + k) = *(float4 *)(B_shared + i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k);\n\t\t\t\t//*(float4 *)(B_reg_index + k) = *(float4 *)(B_index_shared + i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k);\n\t\t\t}\n\t\t\tint bank_idx = i / LEN_OF_BANK_PER_SPARSE_BLOCK;\n\t\t\tB_reg_index = B_index_shared[i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N] % BANK_VAL+bank_idx * BANK_VAL;\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_M; k += 1){\n\t\t\t\tA_reg[k] = A_shared[(t_M * THREAD_SIZE_M+k) * BLOCK_SIZE_K + B_reg_index];\n\t\t\t}\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_N; k += 1){\n\t\t\t\t#pragma unroll\n\t\t\t\tfor(int j = 0; j < THREAD_SIZE_M; j += 1){\n\t\t\t\t\tC_reg[j][k] += B_reg[k] * A_reg[j];\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n    float bias_local[THREAD_SIZE_N];\n    for(int thread_x = 0; thread_x < THREAD_SIZE_N; thread_x++){\n        bias_local[thread_x] = bias[BLOCK_SIZE_N * blockIdx.y + THREAD_SIZE_N * t_N + thread_x];\n    }\n\n\t#pragma unroll\n\tfor(int i = 0; i < THREAD_SIZE_M; i += 1){\n\t\t#pragma unroll\n\t\tfor(int j = 0; j < THREAD_SIZE_N; j += 1){\n\t\t\tg_data[(BLOCK_SIZE_M * blockIdx.x + THREAD_SIZE_M * t_M + i) * N + BLOCK_SIZE_N * blockIdx.y + THREAD_SIZE_N * t_N + j] = C_reg[i][j] + bias_local[j];\n\t\t}\n\t}\n}", "launch_config": {"dimBlock": [128, 1], "dimGrid": [64, 6]}}, "bert.encoder.layer.1.attention.self.value": {"code": "__global__ void MatMul_TILE_THREAD_GENERAL(float *input0, float *input1, float *input2, float *input3, float *output0) {\n    /*\n    g_vec: A [M, K]\n    g_mat_data: condense B [K_sparse, N]\n    g_mat_index: condense B index [K_sparse, N]\n    g_data: C [M, N]\n    */\n\tfloat *g_vec = input0;\n\tfloat *g_mat_data = input1;\n\tint *g_mat_index = (int*)input2;\n\tfloat *bias = input3;\n\tfloat *g_data = output0;\n\n\tconst int M = 4096;\n\tconst int K = 768;\n\tconst int N = 768;\n\n    const int BLOCK_SIZE_M = 64;\n    const int BLOCK_SIZE_N = 128;\n    const int BLOCK_SIZE_K = 32;\n    const int THREAD_SIZE_M = 8;\n    const int THREAD_SIZE_N = 8;\n\n    const float SPARSITY = 0.875;\n    const int BANK_VAL = 32;\n    const int ALIGN_N = THREAD_SIZE_N;\n\n    const int BANK_NUM_PER_BLOCK = BLOCK_SIZE_K / BANK_VAL;\n    const int BLOCK_SIZE_K_SPARSE = int(BLOCK_SIZE_K * (1-SPARSITY));\n    const int LEN_OF_BANK_PER_SPARSE_BLOCK = BLOCK_SIZE_K_SPARSE / BANK_NUM_PER_BLOCK;\n\n\tconst int K_SPARSE = int(K * SPARSITY);\n\tint M_BLOCK_START = blockIdx.x * BLOCK_SIZE_M;\n\tint N_BLOCK_START = blockIdx.y * BLOCK_SIZE_N;\n\n\tconst int A_THREADS_PER_ROW = BLOCK_SIZE_K / 4;\n\tconst int B_THREADS_PER_ROW = BLOCK_SIZE_N / 4;\n\n\tconst int THREADS_PER_BLOCK = (BLOCK_SIZE_M / THREAD_SIZE_M) * (BLOCK_SIZE_N / THREAD_SIZE_N);\n\n\tconst int A_STRIDES = THREADS_PER_BLOCK / A_THREADS_PER_ROW;\n\tconst int B_STRIDES = THREADS_PER_BLOCK / B_THREADS_PER_ROW;\n\n\t__shared__ float A_shared[BLOCK_SIZE_M * BLOCK_SIZE_K];\n\t__shared__ float B_shared[BLOCK_SIZE_N * BLOCK_SIZE_K_SPARSE];\n\t__shared__ int B_index_shared[BLOCK_SIZE_N * BLOCK_SIZE_K_SPARSE];\n\n\tfloat A_reg[THREAD_SIZE_M];\n\tfloat B_reg[THREAD_SIZE_N];\n\tint B_reg_index;\n\tfloat C_reg[THREAD_SIZE_M][THREAD_SIZE_N] = {0};\n\n\tint tid = threadIdx.x;\n\n\tint t_N = tid % (BLOCK_SIZE_N / THREAD_SIZE_N);\n\tint t_M = tid / (BLOCK_SIZE_N / THREAD_SIZE_N);\n\n\tint A_BLOCK_ROW_START = tid / A_THREADS_PER_ROW;\n\tint B_BLOCK_ROW_START = tid / B_THREADS_PER_ROW;\n\n\tint A_BLOCK_COL_START = tid % A_THREADS_PER_ROW * 4;\n\tint B_BLOCK_COL_START = tid % B_THREADS_PER_ROW * 4;\n\n\tfor(int K_BLOCK_START = 0, K_SPARSE_BLOCK_START = 0; K_BLOCK_START < K; K_BLOCK_START += BLOCK_SIZE_K, K_SPARSE_BLOCK_START += BLOCK_SIZE_K_SPARSE){\n\t\tfloat *A_global_ptr = g_vec + M_BLOCK_START * K + K_BLOCK_START;\n\t\tfloat *B_global_ptr = g_mat_data + K_SPARSE_BLOCK_START * N + N_BLOCK_START;\n\t\tint *B_index_global_ptr = g_mat_index + K_SPARSE_BLOCK_START * N + N_BLOCK_START;\n\n\t\t__syncthreads();\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_M; i += A_STRIDES){\n\t\t\t*(float4 *)(A_shared + (i + A_BLOCK_ROW_START) * BLOCK_SIZE_K + A_BLOCK_COL_START) = \n\t\t\t\t*(float4 *)(A_global_ptr + (i + A_BLOCK_ROW_START) * K + A_BLOCK_COL_START);\n\t\t}\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_K_SPARSE; i += B_STRIDES){\n\t\t\t*(float4 *)(B_shared + (i + B_BLOCK_ROW_START) * BLOCK_SIZE_N + B_BLOCK_COL_START) =\n\t\t\t\t*(float4 *)(B_global_ptr + (i + B_BLOCK_ROW_START) * N + B_BLOCK_COL_START);\n\n\t\t\t*(float4 *)(B_index_shared + (i + B_BLOCK_ROW_START) * BLOCK_SIZE_N + B_BLOCK_COL_START) =\n\t\t\t\t*(float4 *)(B_index_global_ptr + (i + B_BLOCK_ROW_START) * N + B_BLOCK_COL_START);\n\t\t}\n\n\t\t__syncthreads();\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_K_SPARSE;i += 1){\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_N; k += 1){\n\t\t\t\tB_reg[k] = B_shared[i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k];\n\t\t\t\t//*(float4 *)(B_reg + k) = *(float4 *)(B_shared + i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k);\n\t\t\t\t//*(float4 *)(B_reg_index + k) = *(float4 *)(B_index_shared + i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k);\n\t\t\t}\n\t\t\tint bank_idx = i / LEN_OF_BANK_PER_SPARSE_BLOCK;\n\t\t\tB_reg_index = B_index_shared[i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N] % BANK_VAL+bank_idx * BANK_VAL;\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_M; k += 1){\n\t\t\t\tA_reg[k] = A_shared[(t_M * THREAD_SIZE_M+k) * BLOCK_SIZE_K + B_reg_index];\n\t\t\t}\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_N; k += 1){\n\t\t\t\t#pragma unroll\n\t\t\t\tfor(int j = 0; j < THREAD_SIZE_M; j += 1){\n\t\t\t\t\tC_reg[j][k] += B_reg[k] * A_reg[j];\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n    float bias_local[THREAD_SIZE_N];\n    for(int thread_x = 0; thread_x < THREAD_SIZE_N; thread_x++){\n        bias_local[thread_x] = bias[BLOCK_SIZE_N * blockIdx.y + THREAD_SIZE_N * t_N + thread_x];\n    }\n\n\t#pragma unroll\n\tfor(int i = 0; i < THREAD_SIZE_M; i += 1){\n\t\t#pragma unroll\n\t\tfor(int j = 0; j < THREAD_SIZE_N; j += 1){\n\t\t\tg_data[(BLOCK_SIZE_M * blockIdx.x + THREAD_SIZE_M * t_M + i) * N + BLOCK_SIZE_N * blockIdx.y + THREAD_SIZE_N * t_N + j] = C_reg[i][j] + bias_local[j];\n\t\t}\n\t}\n}", "launch_config": {"dimBlock": [128, 1], "dimGrid": [64, 6]}}, "bert.encoder.layer.1.attention.output.dense": {"code": "__global__ void MatMul_TILE_THREAD_GENERAL(float *input0, float *input1, float *input2, float *input3, float *output0) {\n    /*\n    g_vec: A [M, K]\n    g_mat_data: condense B [K_sparse, N]\n    g_mat_index: condense B index [K_sparse, N]\n    g_data: C [M, N]\n    */\n\tfloat *g_vec = input0;\n\tfloat *g_mat_data = input1;\n\tint *g_mat_index = (int*)input2;\n\tfloat *bias = input3;\n\tfloat *g_data = output0;\n\n\tconst int M = 4096;\n\tconst int K = 768;\n\tconst int N = 768;\n\n    const int BLOCK_SIZE_M = 64;\n    const int BLOCK_SIZE_N = 128;\n    const int BLOCK_SIZE_K = 32;\n    const int THREAD_SIZE_M = 8;\n    const int THREAD_SIZE_N = 8;\n\n    const float SPARSITY = 0.875;\n    const int BANK_VAL = 32;\n    const int ALIGN_N = THREAD_SIZE_N;\n\n    const int BANK_NUM_PER_BLOCK = BLOCK_SIZE_K / BANK_VAL;\n    const int BLOCK_SIZE_K_SPARSE = int(BLOCK_SIZE_K * (1-SPARSITY));\n    const int LEN_OF_BANK_PER_SPARSE_BLOCK = BLOCK_SIZE_K_SPARSE / BANK_NUM_PER_BLOCK;\n\n\tconst int K_SPARSE = int(K * SPARSITY);\n\tint M_BLOCK_START = blockIdx.x * BLOCK_SIZE_M;\n\tint N_BLOCK_START = blockIdx.y * BLOCK_SIZE_N;\n\n\tconst int A_THREADS_PER_ROW = BLOCK_SIZE_K / 4;\n\tconst int B_THREADS_PER_ROW = BLOCK_SIZE_N / 4;\n\n\tconst int THREADS_PER_BLOCK = (BLOCK_SIZE_M / THREAD_SIZE_M) * (BLOCK_SIZE_N / THREAD_SIZE_N);\n\n\tconst int A_STRIDES = THREADS_PER_BLOCK / A_THREADS_PER_ROW;\n\tconst int B_STRIDES = THREADS_PER_BLOCK / B_THREADS_PER_ROW;\n\n\t__shared__ float A_shared[BLOCK_SIZE_M * BLOCK_SIZE_K];\n\t__shared__ float B_shared[BLOCK_SIZE_N * BLOCK_SIZE_K_SPARSE];\n\t__shared__ int B_index_shared[BLOCK_SIZE_N * BLOCK_SIZE_K_SPARSE];\n\n\tfloat A_reg[THREAD_SIZE_M];\n\tfloat B_reg[THREAD_SIZE_N];\n\tint B_reg_index;\n\tfloat C_reg[THREAD_SIZE_M][THREAD_SIZE_N] = {0};\n\n\tint tid = threadIdx.x;\n\n\tint t_N = tid % (BLOCK_SIZE_N / THREAD_SIZE_N);\n\tint t_M = tid / (BLOCK_SIZE_N / THREAD_SIZE_N);\n\n\tint A_BLOCK_ROW_START = tid / A_THREADS_PER_ROW;\n\tint B_BLOCK_ROW_START = tid / B_THREADS_PER_ROW;\n\n\tint A_BLOCK_COL_START = tid % A_THREADS_PER_ROW * 4;\n\tint B_BLOCK_COL_START = tid % B_THREADS_PER_ROW * 4;\n\n\tfor(int K_BLOCK_START = 0, K_SPARSE_BLOCK_START = 0; K_BLOCK_START < K; K_BLOCK_START += BLOCK_SIZE_K, K_SPARSE_BLOCK_START += BLOCK_SIZE_K_SPARSE){\n\t\tfloat *A_global_ptr = g_vec + M_BLOCK_START * K + K_BLOCK_START;\n\t\tfloat *B_global_ptr = g_mat_data + K_SPARSE_BLOCK_START * N + N_BLOCK_START;\n\t\tint *B_index_global_ptr = g_mat_index + K_SPARSE_BLOCK_START * N + N_BLOCK_START;\n\n\t\t__syncthreads();\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_M; i += A_STRIDES){\n\t\t\t*(float4 *)(A_shared + (i + A_BLOCK_ROW_START) * BLOCK_SIZE_K + A_BLOCK_COL_START) = \n\t\t\t\t*(float4 *)(A_global_ptr + (i + A_BLOCK_ROW_START) * K + A_BLOCK_COL_START);\n\t\t}\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_K_SPARSE; i += B_STRIDES){\n\t\t\t*(float4 *)(B_shared + (i + B_BLOCK_ROW_START) * BLOCK_SIZE_N + B_BLOCK_COL_START) =\n\t\t\t\t*(float4 *)(B_global_ptr + (i + B_BLOCK_ROW_START) * N + B_BLOCK_COL_START);\n\n\t\t\t*(float4 *)(B_index_shared + (i + B_BLOCK_ROW_START) * BLOCK_SIZE_N + B_BLOCK_COL_START) =\n\t\t\t\t*(float4 *)(B_index_global_ptr + (i + B_BLOCK_ROW_START) * N + B_BLOCK_COL_START);\n\t\t}\n\n\t\t__syncthreads();\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_K_SPARSE;i += 1){\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_N; k += 1){\n\t\t\t\tB_reg[k] = B_shared[i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k];\n\t\t\t\t//*(float4 *)(B_reg + k) = *(float4 *)(B_shared + i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k);\n\t\t\t\t//*(float4 *)(B_reg_index + k) = *(float4 *)(B_index_shared + i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k);\n\t\t\t}\n\t\t\tint bank_idx = i / LEN_OF_BANK_PER_SPARSE_BLOCK;\n\t\t\tB_reg_index = B_index_shared[i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N] % BANK_VAL+bank_idx * BANK_VAL;\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_M; k += 1){\n\t\t\t\tA_reg[k] = A_shared[(t_M * THREAD_SIZE_M+k) * BLOCK_SIZE_K + B_reg_index];\n\t\t\t}\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_N; k += 1){\n\t\t\t\t#pragma unroll\n\t\t\t\tfor(int j = 0; j < THREAD_SIZE_M; j += 1){\n\t\t\t\t\tC_reg[j][k] += B_reg[k] * A_reg[j];\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n    float bias_local[THREAD_SIZE_N];\n    for(int thread_x = 0; thread_x < THREAD_SIZE_N; thread_x++){\n        bias_local[thread_x] = bias[BLOCK_SIZE_N * blockIdx.y + THREAD_SIZE_N * t_N + thread_x];\n    }\n\n\t#pragma unroll\n\tfor(int i = 0; i < THREAD_SIZE_M; i += 1){\n\t\t#pragma unroll\n\t\tfor(int j = 0; j < THREAD_SIZE_N; j += 1){\n\t\t\tg_data[(BLOCK_SIZE_M * blockIdx.x + THREAD_SIZE_M * t_M + i) * N + BLOCK_SIZE_N * blockIdx.y + THREAD_SIZE_N * t_N + j] = C_reg[i][j] + bias_local[j];\n\t\t}\n\t}\n}", "launch_config": {"dimBlock": [128, 1], "dimGrid": [64, 6]}}, "bert.encoder.layer.1.intermediate.dense": {"code": "__global__ void MatMul_TILE_THREAD_GENERAL(float *input0, float *input1, float *input2, float *input3, float *output0) {\n    /*\n    g_vec: A [M, K]\n    g_mat_data: condense B [K_sparse, N]\n    g_mat_index: condense B index [K_sparse, N]\n    g_data: C [M, N]\n    */\n\tfloat *g_vec = input0;\n\tfloat *g_mat_data = input1;\n\tint *g_mat_index = (int*)input2;\n\tfloat *bias = input3;\n\tfloat *g_data = output0;\n\n\tconst int M = 4096;\n\tconst int K = 768;\n\tconst int N = 3072;\n\n    const int BLOCK_SIZE_M = 64;\n    const int BLOCK_SIZE_N = 128;\n    const int BLOCK_SIZE_K = 32;\n    const int THREAD_SIZE_M = 8;\n    const int THREAD_SIZE_N = 8;\n\n    const float SPARSITY = 0.875;\n    const int BANK_VAL = 32;\n    const int ALIGN_N = THREAD_SIZE_N;\n\n    const int BANK_NUM_PER_BLOCK = BLOCK_SIZE_K / BANK_VAL;\n    const int BLOCK_SIZE_K_SPARSE = int(BLOCK_SIZE_K * (1-SPARSITY));\n    const int LEN_OF_BANK_PER_SPARSE_BLOCK = BLOCK_SIZE_K_SPARSE / BANK_NUM_PER_BLOCK;\n\n\tconst int K_SPARSE = int(K * SPARSITY);\n\tint M_BLOCK_START = blockIdx.x * BLOCK_SIZE_M;\n\tint N_BLOCK_START = blockIdx.y * BLOCK_SIZE_N;\n\n\tconst int A_THREADS_PER_ROW = BLOCK_SIZE_K / 4;\n\tconst int B_THREADS_PER_ROW = BLOCK_SIZE_N / 4;\n\n\tconst int THREADS_PER_BLOCK = (BLOCK_SIZE_M / THREAD_SIZE_M) * (BLOCK_SIZE_N / THREAD_SIZE_N);\n\n\tconst int A_STRIDES = THREADS_PER_BLOCK / A_THREADS_PER_ROW;\n\tconst int B_STRIDES = THREADS_PER_BLOCK / B_THREADS_PER_ROW;\n\n\t__shared__ float A_shared[BLOCK_SIZE_M * BLOCK_SIZE_K];\n\t__shared__ float B_shared[BLOCK_SIZE_N * BLOCK_SIZE_K_SPARSE];\n\t__shared__ int B_index_shared[BLOCK_SIZE_N * BLOCK_SIZE_K_SPARSE];\n\n\tfloat A_reg[THREAD_SIZE_M];\n\tfloat B_reg[THREAD_SIZE_N];\n\tint B_reg_index;\n\tfloat C_reg[THREAD_SIZE_M][THREAD_SIZE_N] = {0};\n\n\tint tid = threadIdx.x;\n\n\tint t_N = tid % (BLOCK_SIZE_N / THREAD_SIZE_N);\n\tint t_M = tid / (BLOCK_SIZE_N / THREAD_SIZE_N);\n\n\tint A_BLOCK_ROW_START = tid / A_THREADS_PER_ROW;\n\tint B_BLOCK_ROW_START = tid / B_THREADS_PER_ROW;\n\n\tint A_BLOCK_COL_START = tid % A_THREADS_PER_ROW * 4;\n\tint B_BLOCK_COL_START = tid % B_THREADS_PER_ROW * 4;\n\n\tfor(int K_BLOCK_START = 0, K_SPARSE_BLOCK_START = 0; K_BLOCK_START < K; K_BLOCK_START += BLOCK_SIZE_K, K_SPARSE_BLOCK_START += BLOCK_SIZE_K_SPARSE){\n\t\tfloat *A_global_ptr = g_vec + M_BLOCK_START * K + K_BLOCK_START;\n\t\tfloat *B_global_ptr = g_mat_data + K_SPARSE_BLOCK_START * N + N_BLOCK_START;\n\t\tint *B_index_global_ptr = g_mat_index + K_SPARSE_BLOCK_START * N + N_BLOCK_START;\n\n\t\t__syncthreads();\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_M; i += A_STRIDES){\n\t\t\t*(float4 *)(A_shared + (i + A_BLOCK_ROW_START) * BLOCK_SIZE_K + A_BLOCK_COL_START) = \n\t\t\t\t*(float4 *)(A_global_ptr + (i + A_BLOCK_ROW_START) * K + A_BLOCK_COL_START);\n\t\t}\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_K_SPARSE; i += B_STRIDES){\n\t\t\t*(float4 *)(B_shared + (i + B_BLOCK_ROW_START) * BLOCK_SIZE_N + B_BLOCK_COL_START) =\n\t\t\t\t*(float4 *)(B_global_ptr + (i + B_BLOCK_ROW_START) * N + B_BLOCK_COL_START);\n\n\t\t\t*(float4 *)(B_index_shared + (i + B_BLOCK_ROW_START) * BLOCK_SIZE_N + B_BLOCK_COL_START) =\n\t\t\t\t*(float4 *)(B_index_global_ptr + (i + B_BLOCK_ROW_START) * N + B_BLOCK_COL_START);\n\t\t}\n\n\t\t__syncthreads();\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_K_SPARSE;i += 1){\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_N; k += 1){\n\t\t\t\tB_reg[k] = B_shared[i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k];\n\t\t\t\t//*(float4 *)(B_reg + k) = *(float4 *)(B_shared + i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k);\n\t\t\t\t//*(float4 *)(B_reg_index + k) = *(float4 *)(B_index_shared + i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k);\n\t\t\t}\n\t\t\tint bank_idx = i / LEN_OF_BANK_PER_SPARSE_BLOCK;\n\t\t\tB_reg_index = B_index_shared[i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N] % BANK_VAL+bank_idx * BANK_VAL;\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_M; k += 1){\n\t\t\t\tA_reg[k] = A_shared[(t_M * THREAD_SIZE_M+k) * BLOCK_SIZE_K + B_reg_index];\n\t\t\t}\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_N; k += 1){\n\t\t\t\t#pragma unroll\n\t\t\t\tfor(int j = 0; j < THREAD_SIZE_M; j += 1){\n\t\t\t\t\tC_reg[j][k] += B_reg[k] * A_reg[j];\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n    float bias_local[THREAD_SIZE_N];\n    for(int thread_x = 0; thread_x < THREAD_SIZE_N; thread_x++){\n        bias_local[thread_x] = bias[BLOCK_SIZE_N * blockIdx.y + THREAD_SIZE_N * t_N + thread_x];\n    }\n\n\t#pragma unroll\n\tfor(int i = 0; i < THREAD_SIZE_M; i += 1){\n\t\t#pragma unroll\n\t\tfor(int j = 0; j < THREAD_SIZE_N; j += 1){\n\t\t\tg_data[(BLOCK_SIZE_M * blockIdx.x + THREAD_SIZE_M * t_M + i) * N + BLOCK_SIZE_N * blockIdx.y + THREAD_SIZE_N * t_N + j] = C_reg[i][j] + bias_local[j];\n\t\t}\n\t}\n}", "launch_config": {"dimBlock": [128, 1], "dimGrid": [64, 24]}}, "bert.encoder.layer.1.output.dense": {"code": "__global__ void MatMul_TILE_THREAD_GENERAL(float *input0, float *input1, float *input2, float *input3, float *output0) {\n    /*\n    g_vec: A [M, K]\n    g_mat_data: condense B [K_sparse, N]\n    g_mat_index: condense B index [K_sparse, N]\n    g_data: C [M, N]\n    */\n\tfloat *g_vec = input0;\n\tfloat *g_mat_data = input1;\n\tint *g_mat_index = (int*)input2;\n\tfloat *bias = input3;\n\tfloat *g_data = output0;\n\n\tconst int M = 4096;\n\tconst int K = 3072;\n\tconst int N = 768;\n\n    const int BLOCK_SIZE_M = 64;\n    const int BLOCK_SIZE_N = 128;\n    const int BLOCK_SIZE_K = 32;\n    const int THREAD_SIZE_M = 8;\n    const int THREAD_SIZE_N = 8;\n\n    const float SPARSITY = 0.875;\n    const int BANK_VAL = 32;\n    const int ALIGN_N = THREAD_SIZE_N;\n\n    const int BANK_NUM_PER_BLOCK = BLOCK_SIZE_K / BANK_VAL;\n    const int BLOCK_SIZE_K_SPARSE = int(BLOCK_SIZE_K * (1-SPARSITY));\n    const int LEN_OF_BANK_PER_SPARSE_BLOCK = BLOCK_SIZE_K_SPARSE / BANK_NUM_PER_BLOCK;\n\n\tconst int K_SPARSE = int(K * SPARSITY);\n\tint M_BLOCK_START = blockIdx.x * BLOCK_SIZE_M;\n\tint N_BLOCK_START = blockIdx.y * BLOCK_SIZE_N;\n\n\tconst int A_THREADS_PER_ROW = BLOCK_SIZE_K / 4;\n\tconst int B_THREADS_PER_ROW = BLOCK_SIZE_N / 4;\n\n\tconst int THREADS_PER_BLOCK = (BLOCK_SIZE_M / THREAD_SIZE_M) * (BLOCK_SIZE_N / THREAD_SIZE_N);\n\n\tconst int A_STRIDES = THREADS_PER_BLOCK / A_THREADS_PER_ROW;\n\tconst int B_STRIDES = THREADS_PER_BLOCK / B_THREADS_PER_ROW;\n\n\t__shared__ float A_shared[BLOCK_SIZE_M * BLOCK_SIZE_K];\n\t__shared__ float B_shared[BLOCK_SIZE_N * BLOCK_SIZE_K_SPARSE];\n\t__shared__ int B_index_shared[BLOCK_SIZE_N * BLOCK_SIZE_K_SPARSE];\n\n\tfloat A_reg[THREAD_SIZE_M];\n\tfloat B_reg[THREAD_SIZE_N];\n\tint B_reg_index;\n\tfloat C_reg[THREAD_SIZE_M][THREAD_SIZE_N] = {0};\n\n\tint tid = threadIdx.x;\n\n\tint t_N = tid % (BLOCK_SIZE_N / THREAD_SIZE_N);\n\tint t_M = tid / (BLOCK_SIZE_N / THREAD_SIZE_N);\n\n\tint A_BLOCK_ROW_START = tid / A_THREADS_PER_ROW;\n\tint B_BLOCK_ROW_START = tid / B_THREADS_PER_ROW;\n\n\tint A_BLOCK_COL_START = tid % A_THREADS_PER_ROW * 4;\n\tint B_BLOCK_COL_START = tid % B_THREADS_PER_ROW * 4;\n\n\tfor(int K_BLOCK_START = 0, K_SPARSE_BLOCK_START = 0; K_BLOCK_START < K; K_BLOCK_START += BLOCK_SIZE_K, K_SPARSE_BLOCK_START += BLOCK_SIZE_K_SPARSE){\n\t\tfloat *A_global_ptr = g_vec + M_BLOCK_START * K + K_BLOCK_START;\n\t\tfloat *B_global_ptr = g_mat_data + K_SPARSE_BLOCK_START * N + N_BLOCK_START;\n\t\tint *B_index_global_ptr = g_mat_index + K_SPARSE_BLOCK_START * N + N_BLOCK_START;\n\n\t\t__syncthreads();\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_M; i += A_STRIDES){\n\t\t\t*(float4 *)(A_shared + (i + A_BLOCK_ROW_START) * BLOCK_SIZE_K + A_BLOCK_COL_START) = \n\t\t\t\t*(float4 *)(A_global_ptr + (i + A_BLOCK_ROW_START) * K + A_BLOCK_COL_START);\n\t\t}\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_K_SPARSE; i += B_STRIDES){\n\t\t\t*(float4 *)(B_shared + (i + B_BLOCK_ROW_START) * BLOCK_SIZE_N + B_BLOCK_COL_START) =\n\t\t\t\t*(float4 *)(B_global_ptr + (i + B_BLOCK_ROW_START) * N + B_BLOCK_COL_START);\n\n\t\t\t*(float4 *)(B_index_shared + (i + B_BLOCK_ROW_START) * BLOCK_SIZE_N + B_BLOCK_COL_START) =\n\t\t\t\t*(float4 *)(B_index_global_ptr + (i + B_BLOCK_ROW_START) * N + B_BLOCK_COL_START);\n\t\t}\n\n\t\t__syncthreads();\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_K_SPARSE;i += 1){\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_N; k += 1){\n\t\t\t\tB_reg[k] = B_shared[i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k];\n\t\t\t\t//*(float4 *)(B_reg + k) = *(float4 *)(B_shared + i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k);\n\t\t\t\t//*(float4 *)(B_reg_index + k) = *(float4 *)(B_index_shared + i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k);\n\t\t\t}\n\t\t\tint bank_idx = i / LEN_OF_BANK_PER_SPARSE_BLOCK;\n\t\t\tB_reg_index = B_index_shared[i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N] % BANK_VAL+bank_idx * BANK_VAL;\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_M; k += 1){\n\t\t\t\tA_reg[k] = A_shared[(t_M * THREAD_SIZE_M+k) * BLOCK_SIZE_K + B_reg_index];\n\t\t\t}\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_N; k += 1){\n\t\t\t\t#pragma unroll\n\t\t\t\tfor(int j = 0; j < THREAD_SIZE_M; j += 1){\n\t\t\t\t\tC_reg[j][k] += B_reg[k] * A_reg[j];\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n    float bias_local[THREAD_SIZE_N];\n    for(int thread_x = 0; thread_x < THREAD_SIZE_N; thread_x++){\n        bias_local[thread_x] = bias[BLOCK_SIZE_N * blockIdx.y + THREAD_SIZE_N * t_N + thread_x];\n    }\n\n\t#pragma unroll\n\tfor(int i = 0; i < THREAD_SIZE_M; i += 1){\n\t\t#pragma unroll\n\t\tfor(int j = 0; j < THREAD_SIZE_N; j += 1){\n\t\t\tg_data[(BLOCK_SIZE_M * blockIdx.x + THREAD_SIZE_M * t_M + i) * N + BLOCK_SIZE_N * blockIdx.y + THREAD_SIZE_N * t_N + j] = C_reg[i][j] + bias_local[j];\n\t\t}\n\t}\n}", "launch_config": {"dimBlock": [128, 1], "dimGrid": [64, 6]}}, "bert.encoder.layer.2.attention.self.query": {"code": "__global__ void MatMul_TILE_THREAD_GENERAL(float *input0, float *input1, float *input2, float *input3, float *output0) {\n    /*\n    g_vec: A [M, K]\n    g_mat_data: condense B [K_sparse, N]\n    g_mat_index: condense B index [K_sparse, N]\n    g_data: C [M, N]\n    */\n\tfloat *g_vec = input0;\n\tfloat *g_mat_data = input1;\n\tint *g_mat_index = (int*)input2;\n\tfloat *bias = input3;\n\tfloat *g_data = output0;\n\n\tconst int M = 4096;\n\tconst int K = 768;\n\tconst int N = 768;\n\n    const int BLOCK_SIZE_M = 64;\n    const int BLOCK_SIZE_N = 128;\n    const int BLOCK_SIZE_K = 32;\n    const int THREAD_SIZE_M = 8;\n    const int THREAD_SIZE_N = 8;\n\n    const float SPARSITY = 0.875;\n    const int BANK_VAL = 32;\n    const int ALIGN_N = THREAD_SIZE_N;\n\n    const int BANK_NUM_PER_BLOCK = BLOCK_SIZE_K / BANK_VAL;\n    const int BLOCK_SIZE_K_SPARSE = int(BLOCK_SIZE_K * (1-SPARSITY));\n    const int LEN_OF_BANK_PER_SPARSE_BLOCK = BLOCK_SIZE_K_SPARSE / BANK_NUM_PER_BLOCK;\n\n\tconst int K_SPARSE = int(K * SPARSITY);\n\tint M_BLOCK_START = blockIdx.x * BLOCK_SIZE_M;\n\tint N_BLOCK_START = blockIdx.y * BLOCK_SIZE_N;\n\n\tconst int A_THREADS_PER_ROW = BLOCK_SIZE_K / 4;\n\tconst int B_THREADS_PER_ROW = BLOCK_SIZE_N / 4;\n\n\tconst int THREADS_PER_BLOCK = (BLOCK_SIZE_M / THREAD_SIZE_M) * (BLOCK_SIZE_N / THREAD_SIZE_N);\n\n\tconst int A_STRIDES = THREADS_PER_BLOCK / A_THREADS_PER_ROW;\n\tconst int B_STRIDES = THREADS_PER_BLOCK / B_THREADS_PER_ROW;\n\n\t__shared__ float A_shared[BLOCK_SIZE_M * BLOCK_SIZE_K];\n\t__shared__ float B_shared[BLOCK_SIZE_N * BLOCK_SIZE_K_SPARSE];\n\t__shared__ int B_index_shared[BLOCK_SIZE_N * BLOCK_SIZE_K_SPARSE];\n\n\tfloat A_reg[THREAD_SIZE_M];\n\tfloat B_reg[THREAD_SIZE_N];\n\tint B_reg_index;\n\tfloat C_reg[THREAD_SIZE_M][THREAD_SIZE_N] = {0};\n\n\tint tid = threadIdx.x;\n\n\tint t_N = tid % (BLOCK_SIZE_N / THREAD_SIZE_N);\n\tint t_M = tid / (BLOCK_SIZE_N / THREAD_SIZE_N);\n\n\tint A_BLOCK_ROW_START = tid / A_THREADS_PER_ROW;\n\tint B_BLOCK_ROW_START = tid / B_THREADS_PER_ROW;\n\n\tint A_BLOCK_COL_START = tid % A_THREADS_PER_ROW * 4;\n\tint B_BLOCK_COL_START = tid % B_THREADS_PER_ROW * 4;\n\n\tfor(int K_BLOCK_START = 0, K_SPARSE_BLOCK_START = 0; K_BLOCK_START < K; K_BLOCK_START += BLOCK_SIZE_K, K_SPARSE_BLOCK_START += BLOCK_SIZE_K_SPARSE){\n\t\tfloat *A_global_ptr = g_vec + M_BLOCK_START * K + K_BLOCK_START;\n\t\tfloat *B_global_ptr = g_mat_data + K_SPARSE_BLOCK_START * N + N_BLOCK_START;\n\t\tint *B_index_global_ptr = g_mat_index + K_SPARSE_BLOCK_START * N + N_BLOCK_START;\n\n\t\t__syncthreads();\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_M; i += A_STRIDES){\n\t\t\t*(float4 *)(A_shared + (i + A_BLOCK_ROW_START) * BLOCK_SIZE_K + A_BLOCK_COL_START) = \n\t\t\t\t*(float4 *)(A_global_ptr + (i + A_BLOCK_ROW_START) * K + A_BLOCK_COL_START);\n\t\t}\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_K_SPARSE; i += B_STRIDES){\n\t\t\t*(float4 *)(B_shared + (i + B_BLOCK_ROW_START) * BLOCK_SIZE_N + B_BLOCK_COL_START) =\n\t\t\t\t*(float4 *)(B_global_ptr + (i + B_BLOCK_ROW_START) * N + B_BLOCK_COL_START);\n\n\t\t\t*(float4 *)(B_index_shared + (i + B_BLOCK_ROW_START) * BLOCK_SIZE_N + B_BLOCK_COL_START) =\n\t\t\t\t*(float4 *)(B_index_global_ptr + (i + B_BLOCK_ROW_START) * N + B_BLOCK_COL_START);\n\t\t}\n\n\t\t__syncthreads();\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_K_SPARSE;i += 1){\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_N; k += 1){\n\t\t\t\tB_reg[k] = B_shared[i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k];\n\t\t\t\t//*(float4 *)(B_reg + k) = *(float4 *)(B_shared + i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k);\n\t\t\t\t//*(float4 *)(B_reg_index + k) = *(float4 *)(B_index_shared + i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k);\n\t\t\t}\n\t\t\tint bank_idx = i / LEN_OF_BANK_PER_SPARSE_BLOCK;\n\t\t\tB_reg_index = B_index_shared[i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N] % BANK_VAL+bank_idx * BANK_VAL;\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_M; k += 1){\n\t\t\t\tA_reg[k] = A_shared[(t_M * THREAD_SIZE_M+k) * BLOCK_SIZE_K + B_reg_index];\n\t\t\t}\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_N; k += 1){\n\t\t\t\t#pragma unroll\n\t\t\t\tfor(int j = 0; j < THREAD_SIZE_M; j += 1){\n\t\t\t\t\tC_reg[j][k] += B_reg[k] * A_reg[j];\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n    float bias_local[THREAD_SIZE_N];\n    for(int thread_x = 0; thread_x < THREAD_SIZE_N; thread_x++){\n        bias_local[thread_x] = bias[BLOCK_SIZE_N * blockIdx.y + THREAD_SIZE_N * t_N + thread_x];\n    }\n\n\t#pragma unroll\n\tfor(int i = 0; i < THREAD_SIZE_M; i += 1){\n\t\t#pragma unroll\n\t\tfor(int j = 0; j < THREAD_SIZE_N; j += 1){\n\t\t\tg_data[(BLOCK_SIZE_M * blockIdx.x + THREAD_SIZE_M * t_M + i) * N + BLOCK_SIZE_N * blockIdx.y + THREAD_SIZE_N * t_N + j] = C_reg[i][j] + bias_local[j];\n\t\t}\n\t}\n}", "launch_config": {"dimBlock": [128, 1], "dimGrid": [64, 6]}}, "bert.encoder.layer.2.attention.self.key": {"code": "__global__ void MatMul_TILE_THREAD_GENERAL(float *input0, float *input1, float *input2, float *input3, float *output0) {\n    /*\n    g_vec: A [M, K]\n    g_mat_data: condense B [K_sparse, N]\n    g_mat_index: condense B index [K_sparse, N]\n    g_data: C [M, N]\n    */\n\tfloat *g_vec = input0;\n\tfloat *g_mat_data = input1;\n\tint *g_mat_index = (int*)input2;\n\tfloat *bias = input3;\n\tfloat *g_data = output0;\n\n\tconst int M = 4096;\n\tconst int K = 768;\n\tconst int N = 768;\n\n    const int BLOCK_SIZE_M = 64;\n    const int BLOCK_SIZE_N = 128;\n    const int BLOCK_SIZE_K = 32;\n    const int THREAD_SIZE_M = 8;\n    const int THREAD_SIZE_N = 8;\n\n    const float SPARSITY = 0.875;\n    const int BANK_VAL = 32;\n    const int ALIGN_N = THREAD_SIZE_N;\n\n    const int BANK_NUM_PER_BLOCK = BLOCK_SIZE_K / BANK_VAL;\n    const int BLOCK_SIZE_K_SPARSE = int(BLOCK_SIZE_K * (1-SPARSITY));\n    const int LEN_OF_BANK_PER_SPARSE_BLOCK = BLOCK_SIZE_K_SPARSE / BANK_NUM_PER_BLOCK;\n\n\tconst int K_SPARSE = int(K * SPARSITY);\n\tint M_BLOCK_START = blockIdx.x * BLOCK_SIZE_M;\n\tint N_BLOCK_START = blockIdx.y * BLOCK_SIZE_N;\n\n\tconst int A_THREADS_PER_ROW = BLOCK_SIZE_K / 4;\n\tconst int B_THREADS_PER_ROW = BLOCK_SIZE_N / 4;\n\n\tconst int THREADS_PER_BLOCK = (BLOCK_SIZE_M / THREAD_SIZE_M) * (BLOCK_SIZE_N / THREAD_SIZE_N);\n\n\tconst int A_STRIDES = THREADS_PER_BLOCK / A_THREADS_PER_ROW;\n\tconst int B_STRIDES = THREADS_PER_BLOCK / B_THREADS_PER_ROW;\n\n\t__shared__ float A_shared[BLOCK_SIZE_M * BLOCK_SIZE_K];\n\t__shared__ float B_shared[BLOCK_SIZE_N * BLOCK_SIZE_K_SPARSE];\n\t__shared__ int B_index_shared[BLOCK_SIZE_N * BLOCK_SIZE_K_SPARSE];\n\n\tfloat A_reg[THREAD_SIZE_M];\n\tfloat B_reg[THREAD_SIZE_N];\n\tint B_reg_index;\n\tfloat C_reg[THREAD_SIZE_M][THREAD_SIZE_N] = {0};\n\n\tint tid = threadIdx.x;\n\n\tint t_N = tid % (BLOCK_SIZE_N / THREAD_SIZE_N);\n\tint t_M = tid / (BLOCK_SIZE_N / THREAD_SIZE_N);\n\n\tint A_BLOCK_ROW_START = tid / A_THREADS_PER_ROW;\n\tint B_BLOCK_ROW_START = tid / B_THREADS_PER_ROW;\n\n\tint A_BLOCK_COL_START = tid % A_THREADS_PER_ROW * 4;\n\tint B_BLOCK_COL_START = tid % B_THREADS_PER_ROW * 4;\n\n\tfor(int K_BLOCK_START = 0, K_SPARSE_BLOCK_START = 0; K_BLOCK_START < K; K_BLOCK_START += BLOCK_SIZE_K, K_SPARSE_BLOCK_START += BLOCK_SIZE_K_SPARSE){\n\t\tfloat *A_global_ptr = g_vec + M_BLOCK_START * K + K_BLOCK_START;\n\t\tfloat *B_global_ptr = g_mat_data + K_SPARSE_BLOCK_START * N + N_BLOCK_START;\n\t\tint *B_index_global_ptr = g_mat_index + K_SPARSE_BLOCK_START * N + N_BLOCK_START;\n\n\t\t__syncthreads();\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_M; i += A_STRIDES){\n\t\t\t*(float4 *)(A_shared + (i + A_BLOCK_ROW_START) * BLOCK_SIZE_K + A_BLOCK_COL_START) = \n\t\t\t\t*(float4 *)(A_global_ptr + (i + A_BLOCK_ROW_START) * K + A_BLOCK_COL_START);\n\t\t}\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_K_SPARSE; i += B_STRIDES){\n\t\t\t*(float4 *)(B_shared + (i + B_BLOCK_ROW_START) * BLOCK_SIZE_N + B_BLOCK_COL_START) =\n\t\t\t\t*(float4 *)(B_global_ptr + (i + B_BLOCK_ROW_START) * N + B_BLOCK_COL_START);\n\n\t\t\t*(float4 *)(B_index_shared + (i + B_BLOCK_ROW_START) * BLOCK_SIZE_N + B_BLOCK_COL_START) =\n\t\t\t\t*(float4 *)(B_index_global_ptr + (i + B_BLOCK_ROW_START) * N + B_BLOCK_COL_START);\n\t\t}\n\n\t\t__syncthreads();\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_K_SPARSE;i += 1){\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_N; k += 1){\n\t\t\t\tB_reg[k] = B_shared[i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k];\n\t\t\t\t//*(float4 *)(B_reg + k) = *(float4 *)(B_shared + i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k);\n\t\t\t\t//*(float4 *)(B_reg_index + k) = *(float4 *)(B_index_shared + i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k);\n\t\t\t}\n\t\t\tint bank_idx = i / LEN_OF_BANK_PER_SPARSE_BLOCK;\n\t\t\tB_reg_index = B_index_shared[i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N] % BANK_VAL+bank_idx * BANK_VAL;\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_M; k += 1){\n\t\t\t\tA_reg[k] = A_shared[(t_M * THREAD_SIZE_M+k) * BLOCK_SIZE_K + B_reg_index];\n\t\t\t}\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_N; k += 1){\n\t\t\t\t#pragma unroll\n\t\t\t\tfor(int j = 0; j < THREAD_SIZE_M; j += 1){\n\t\t\t\t\tC_reg[j][k] += B_reg[k] * A_reg[j];\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n    float bias_local[THREAD_SIZE_N];\n    for(int thread_x = 0; thread_x < THREAD_SIZE_N; thread_x++){\n        bias_local[thread_x] = bias[BLOCK_SIZE_N * blockIdx.y + THREAD_SIZE_N * t_N + thread_x];\n    }\n\n\t#pragma unroll\n\tfor(int i = 0; i < THREAD_SIZE_M; i += 1){\n\t\t#pragma unroll\n\t\tfor(int j = 0; j < THREAD_SIZE_N; j += 1){\n\t\t\tg_data[(BLOCK_SIZE_M * blockIdx.x + THREAD_SIZE_M * t_M + i) * N + BLOCK_SIZE_N * blockIdx.y + THREAD_SIZE_N * t_N + j] = C_reg[i][j] + bias_local[j];\n\t\t}\n\t}\n}", "launch_config": {"dimBlock": [128, 1], "dimGrid": [64, 6]}}, "bert.encoder.layer.2.attention.self.value": {"code": "__global__ void MatMul_TILE_THREAD_GENERAL(float *input0, float *input1, float *input2, float *input3, float *output0) {\n    /*\n    g_vec: A [M, K]\n    g_mat_data: condense B [K_sparse, N]\n    g_mat_index: condense B index [K_sparse, N]\n    g_data: C [M, N]\n    */\n\tfloat *g_vec = input0;\n\tfloat *g_mat_data = input1;\n\tint *g_mat_index = (int*)input2;\n\tfloat *bias = input3;\n\tfloat *g_data = output0;\n\n\tconst int M = 4096;\n\tconst int K = 768;\n\tconst int N = 768;\n\n    const int BLOCK_SIZE_M = 64;\n    const int BLOCK_SIZE_N = 128;\n    const int BLOCK_SIZE_K = 32;\n    const int THREAD_SIZE_M = 8;\n    const int THREAD_SIZE_N = 8;\n\n    const float SPARSITY = 0.875;\n    const int BANK_VAL = 32;\n    const int ALIGN_N = THREAD_SIZE_N;\n\n    const int BANK_NUM_PER_BLOCK = BLOCK_SIZE_K / BANK_VAL;\n    const int BLOCK_SIZE_K_SPARSE = int(BLOCK_SIZE_K * (1-SPARSITY));\n    const int LEN_OF_BANK_PER_SPARSE_BLOCK = BLOCK_SIZE_K_SPARSE / BANK_NUM_PER_BLOCK;\n\n\tconst int K_SPARSE = int(K * SPARSITY);\n\tint M_BLOCK_START = blockIdx.x * BLOCK_SIZE_M;\n\tint N_BLOCK_START = blockIdx.y * BLOCK_SIZE_N;\n\n\tconst int A_THREADS_PER_ROW = BLOCK_SIZE_K / 4;\n\tconst int B_THREADS_PER_ROW = BLOCK_SIZE_N / 4;\n\n\tconst int THREADS_PER_BLOCK = (BLOCK_SIZE_M / THREAD_SIZE_M) * (BLOCK_SIZE_N / THREAD_SIZE_N);\n\n\tconst int A_STRIDES = THREADS_PER_BLOCK / A_THREADS_PER_ROW;\n\tconst int B_STRIDES = THREADS_PER_BLOCK / B_THREADS_PER_ROW;\n\n\t__shared__ float A_shared[BLOCK_SIZE_M * BLOCK_SIZE_K];\n\t__shared__ float B_shared[BLOCK_SIZE_N * BLOCK_SIZE_K_SPARSE];\n\t__shared__ int B_index_shared[BLOCK_SIZE_N * BLOCK_SIZE_K_SPARSE];\n\n\tfloat A_reg[THREAD_SIZE_M];\n\tfloat B_reg[THREAD_SIZE_N];\n\tint B_reg_index;\n\tfloat C_reg[THREAD_SIZE_M][THREAD_SIZE_N] = {0};\n\n\tint tid = threadIdx.x;\n\n\tint t_N = tid % (BLOCK_SIZE_N / THREAD_SIZE_N);\n\tint t_M = tid / (BLOCK_SIZE_N / THREAD_SIZE_N);\n\n\tint A_BLOCK_ROW_START = tid / A_THREADS_PER_ROW;\n\tint B_BLOCK_ROW_START = tid / B_THREADS_PER_ROW;\n\n\tint A_BLOCK_COL_START = tid % A_THREADS_PER_ROW * 4;\n\tint B_BLOCK_COL_START = tid % B_THREADS_PER_ROW * 4;\n\n\tfor(int K_BLOCK_START = 0, K_SPARSE_BLOCK_START = 0; K_BLOCK_START < K; K_BLOCK_START += BLOCK_SIZE_K, K_SPARSE_BLOCK_START += BLOCK_SIZE_K_SPARSE){\n\t\tfloat *A_global_ptr = g_vec + M_BLOCK_START * K + K_BLOCK_START;\n\t\tfloat *B_global_ptr = g_mat_data + K_SPARSE_BLOCK_START * N + N_BLOCK_START;\n\t\tint *B_index_global_ptr = g_mat_index + K_SPARSE_BLOCK_START * N + N_BLOCK_START;\n\n\t\t__syncthreads();\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_M; i += A_STRIDES){\n\t\t\t*(float4 *)(A_shared + (i + A_BLOCK_ROW_START) * BLOCK_SIZE_K + A_BLOCK_COL_START) = \n\t\t\t\t*(float4 *)(A_global_ptr + (i + A_BLOCK_ROW_START) * K + A_BLOCK_COL_START);\n\t\t}\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_K_SPARSE; i += B_STRIDES){\n\t\t\t*(float4 *)(B_shared + (i + B_BLOCK_ROW_START) * BLOCK_SIZE_N + B_BLOCK_COL_START) =\n\t\t\t\t*(float4 *)(B_global_ptr + (i + B_BLOCK_ROW_START) * N + B_BLOCK_COL_START);\n\n\t\t\t*(float4 *)(B_index_shared + (i + B_BLOCK_ROW_START) * BLOCK_SIZE_N + B_BLOCK_COL_START) =\n\t\t\t\t*(float4 *)(B_index_global_ptr + (i + B_BLOCK_ROW_START) * N + B_BLOCK_COL_START);\n\t\t}\n\n\t\t__syncthreads();\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_K_SPARSE;i += 1){\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_N; k += 1){\n\t\t\t\tB_reg[k] = B_shared[i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k];\n\t\t\t\t//*(float4 *)(B_reg + k) = *(float4 *)(B_shared + i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k);\n\t\t\t\t//*(float4 *)(B_reg_index + k) = *(float4 *)(B_index_shared + i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k);\n\t\t\t}\n\t\t\tint bank_idx = i / LEN_OF_BANK_PER_SPARSE_BLOCK;\n\t\t\tB_reg_index = B_index_shared[i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N] % BANK_VAL+bank_idx * BANK_VAL;\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_M; k += 1){\n\t\t\t\tA_reg[k] = A_shared[(t_M * THREAD_SIZE_M+k) * BLOCK_SIZE_K + B_reg_index];\n\t\t\t}\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_N; k += 1){\n\t\t\t\t#pragma unroll\n\t\t\t\tfor(int j = 0; j < THREAD_SIZE_M; j += 1){\n\t\t\t\t\tC_reg[j][k] += B_reg[k] * A_reg[j];\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n    float bias_local[THREAD_SIZE_N];\n    for(int thread_x = 0; thread_x < THREAD_SIZE_N; thread_x++){\n        bias_local[thread_x] = bias[BLOCK_SIZE_N * blockIdx.y + THREAD_SIZE_N * t_N + thread_x];\n    }\n\n\t#pragma unroll\n\tfor(int i = 0; i < THREAD_SIZE_M; i += 1){\n\t\t#pragma unroll\n\t\tfor(int j = 0; j < THREAD_SIZE_N; j += 1){\n\t\t\tg_data[(BLOCK_SIZE_M * blockIdx.x + THREAD_SIZE_M * t_M + i) * N + BLOCK_SIZE_N * blockIdx.y + THREAD_SIZE_N * t_N + j] = C_reg[i][j] + bias_local[j];\n\t\t}\n\t}\n}", "launch_config": {"dimBlock": [128, 1], "dimGrid": [64, 6]}}, "bert.encoder.layer.2.attention.output.dense": {"code": "__global__ void MatMul_TILE_THREAD_GENERAL(float *input0, float *input1, float *input2, float *input3, float *output0) {\n    /*\n    g_vec: A [M, K]\n    g_mat_data: condense B [K_sparse, N]\n    g_mat_index: condense B index [K_sparse, N]\n    g_data: C [M, N]\n    */\n\tfloat *g_vec = input0;\n\tfloat *g_mat_data = input1;\n\tint *g_mat_index = (int*)input2;\n\tfloat *bias = input3;\n\tfloat *g_data = output0;\n\n\tconst int M = 4096;\n\tconst int K = 768;\n\tconst int N = 768;\n\n    const int BLOCK_SIZE_M = 64;\n    const int BLOCK_SIZE_N = 128;\n    const int BLOCK_SIZE_K = 32;\n    const int THREAD_SIZE_M = 8;\n    const int THREAD_SIZE_N = 8;\n\n    const float SPARSITY = 0.875;\n    const int BANK_VAL = 32;\n    const int ALIGN_N = THREAD_SIZE_N;\n\n    const int BANK_NUM_PER_BLOCK = BLOCK_SIZE_K / BANK_VAL;\n    const int BLOCK_SIZE_K_SPARSE = int(BLOCK_SIZE_K * (1-SPARSITY));\n    const int LEN_OF_BANK_PER_SPARSE_BLOCK = BLOCK_SIZE_K_SPARSE / BANK_NUM_PER_BLOCK;\n\n\tconst int K_SPARSE = int(K * SPARSITY);\n\tint M_BLOCK_START = blockIdx.x * BLOCK_SIZE_M;\n\tint N_BLOCK_START = blockIdx.y * BLOCK_SIZE_N;\n\n\tconst int A_THREADS_PER_ROW = BLOCK_SIZE_K / 4;\n\tconst int B_THREADS_PER_ROW = BLOCK_SIZE_N / 4;\n\n\tconst int THREADS_PER_BLOCK = (BLOCK_SIZE_M / THREAD_SIZE_M) * (BLOCK_SIZE_N / THREAD_SIZE_N);\n\n\tconst int A_STRIDES = THREADS_PER_BLOCK / A_THREADS_PER_ROW;\n\tconst int B_STRIDES = THREADS_PER_BLOCK / B_THREADS_PER_ROW;\n\n\t__shared__ float A_shared[BLOCK_SIZE_M * BLOCK_SIZE_K];\n\t__shared__ float B_shared[BLOCK_SIZE_N * BLOCK_SIZE_K_SPARSE];\n\t__shared__ int B_index_shared[BLOCK_SIZE_N * BLOCK_SIZE_K_SPARSE];\n\n\tfloat A_reg[THREAD_SIZE_M];\n\tfloat B_reg[THREAD_SIZE_N];\n\tint B_reg_index;\n\tfloat C_reg[THREAD_SIZE_M][THREAD_SIZE_N] = {0};\n\n\tint tid = threadIdx.x;\n\n\tint t_N = tid % (BLOCK_SIZE_N / THREAD_SIZE_N);\n\tint t_M = tid / (BLOCK_SIZE_N / THREAD_SIZE_N);\n\n\tint A_BLOCK_ROW_START = tid / A_THREADS_PER_ROW;\n\tint B_BLOCK_ROW_START = tid / B_THREADS_PER_ROW;\n\n\tint A_BLOCK_COL_START = tid % A_THREADS_PER_ROW * 4;\n\tint B_BLOCK_COL_START = tid % B_THREADS_PER_ROW * 4;\n\n\tfor(int K_BLOCK_START = 0, K_SPARSE_BLOCK_START = 0; K_BLOCK_START < K; K_BLOCK_START += BLOCK_SIZE_K, K_SPARSE_BLOCK_START += BLOCK_SIZE_K_SPARSE){\n\t\tfloat *A_global_ptr = g_vec + M_BLOCK_START * K + K_BLOCK_START;\n\t\tfloat *B_global_ptr = g_mat_data + K_SPARSE_BLOCK_START * N + N_BLOCK_START;\n\t\tint *B_index_global_ptr = g_mat_index + K_SPARSE_BLOCK_START * N + N_BLOCK_START;\n\n\t\t__syncthreads();\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_M; i += A_STRIDES){\n\t\t\t*(float4 *)(A_shared + (i + A_BLOCK_ROW_START) * BLOCK_SIZE_K + A_BLOCK_COL_START) = \n\t\t\t\t*(float4 *)(A_global_ptr + (i + A_BLOCK_ROW_START) * K + A_BLOCK_COL_START);\n\t\t}\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_K_SPARSE; i += B_STRIDES){\n\t\t\t*(float4 *)(B_shared + (i + B_BLOCK_ROW_START) * BLOCK_SIZE_N + B_BLOCK_COL_START) =\n\t\t\t\t*(float4 *)(B_global_ptr + (i + B_BLOCK_ROW_START) * N + B_BLOCK_COL_START);\n\n\t\t\t*(float4 *)(B_index_shared + (i + B_BLOCK_ROW_START) * BLOCK_SIZE_N + B_BLOCK_COL_START) =\n\t\t\t\t*(float4 *)(B_index_global_ptr + (i + B_BLOCK_ROW_START) * N + B_BLOCK_COL_START);\n\t\t}\n\n\t\t__syncthreads();\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_K_SPARSE;i += 1){\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_N; k += 1){\n\t\t\t\tB_reg[k] = B_shared[i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k];\n\t\t\t\t//*(float4 *)(B_reg + k) = *(float4 *)(B_shared + i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k);\n\t\t\t\t//*(float4 *)(B_reg_index + k) = *(float4 *)(B_index_shared + i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k);\n\t\t\t}\n\t\t\tint bank_idx = i / LEN_OF_BANK_PER_SPARSE_BLOCK;\n\t\t\tB_reg_index = B_index_shared[i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N] % BANK_VAL+bank_idx * BANK_VAL;\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_M; k += 1){\n\t\t\t\tA_reg[k] = A_shared[(t_M * THREAD_SIZE_M+k) * BLOCK_SIZE_K + B_reg_index];\n\t\t\t}\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_N; k += 1){\n\t\t\t\t#pragma unroll\n\t\t\t\tfor(int j = 0; j < THREAD_SIZE_M; j += 1){\n\t\t\t\t\tC_reg[j][k] += B_reg[k] * A_reg[j];\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n    float bias_local[THREAD_SIZE_N];\n    for(int thread_x = 0; thread_x < THREAD_SIZE_N; thread_x++){\n        bias_local[thread_x] = bias[BLOCK_SIZE_N * blockIdx.y + THREAD_SIZE_N * t_N + thread_x];\n    }\n\n\t#pragma unroll\n\tfor(int i = 0; i < THREAD_SIZE_M; i += 1){\n\t\t#pragma unroll\n\t\tfor(int j = 0; j < THREAD_SIZE_N; j += 1){\n\t\t\tg_data[(BLOCK_SIZE_M * blockIdx.x + THREAD_SIZE_M * t_M + i) * N + BLOCK_SIZE_N * blockIdx.y + THREAD_SIZE_N * t_N + j] = C_reg[i][j] + bias_local[j];\n\t\t}\n\t}\n}", "launch_config": {"dimBlock": [128, 1], "dimGrid": [64, 6]}}, "bert.encoder.layer.2.intermediate.dense": {"code": "__global__ void MatMul_TILE_THREAD_GENERAL(float *input0, float *input1, float *input2, float *input3, float *output0) {\n    /*\n    g_vec: A [M, K]\n    g_mat_data: condense B [K_sparse, N]\n    g_mat_index: condense B index [K_sparse, N]\n    g_data: C [M, N]\n    */\n\tfloat *g_vec = input0;\n\tfloat *g_mat_data = input1;\n\tint *g_mat_index = (int*)input2;\n\tfloat *bias = input3;\n\tfloat *g_data = output0;\n\n\tconst int M = 4096;\n\tconst int K = 768;\n\tconst int N = 3072;\n\n    const int BLOCK_SIZE_M = 64;\n    const int BLOCK_SIZE_N = 128;\n    const int BLOCK_SIZE_K = 32;\n    const int THREAD_SIZE_M = 8;\n    const int THREAD_SIZE_N = 8;\n\n    const float SPARSITY = 0.875;\n    const int BANK_VAL = 32;\n    const int ALIGN_N = THREAD_SIZE_N;\n\n    const int BANK_NUM_PER_BLOCK = BLOCK_SIZE_K / BANK_VAL;\n    const int BLOCK_SIZE_K_SPARSE = int(BLOCK_SIZE_K * (1-SPARSITY));\n    const int LEN_OF_BANK_PER_SPARSE_BLOCK = BLOCK_SIZE_K_SPARSE / BANK_NUM_PER_BLOCK;\n\n\tconst int K_SPARSE = int(K * SPARSITY);\n\tint M_BLOCK_START = blockIdx.x * BLOCK_SIZE_M;\n\tint N_BLOCK_START = blockIdx.y * BLOCK_SIZE_N;\n\n\tconst int A_THREADS_PER_ROW = BLOCK_SIZE_K / 4;\n\tconst int B_THREADS_PER_ROW = BLOCK_SIZE_N / 4;\n\n\tconst int THREADS_PER_BLOCK = (BLOCK_SIZE_M / THREAD_SIZE_M) * (BLOCK_SIZE_N / THREAD_SIZE_N);\n\n\tconst int A_STRIDES = THREADS_PER_BLOCK / A_THREADS_PER_ROW;\n\tconst int B_STRIDES = THREADS_PER_BLOCK / B_THREADS_PER_ROW;\n\n\t__shared__ float A_shared[BLOCK_SIZE_M * BLOCK_SIZE_K];\n\t__shared__ float B_shared[BLOCK_SIZE_N * BLOCK_SIZE_K_SPARSE];\n\t__shared__ int B_index_shared[BLOCK_SIZE_N * BLOCK_SIZE_K_SPARSE];\n\n\tfloat A_reg[THREAD_SIZE_M];\n\tfloat B_reg[THREAD_SIZE_N];\n\tint B_reg_index;\n\tfloat C_reg[THREAD_SIZE_M][THREAD_SIZE_N] = {0};\n\n\tint tid = threadIdx.x;\n\n\tint t_N = tid % (BLOCK_SIZE_N / THREAD_SIZE_N);\n\tint t_M = tid / (BLOCK_SIZE_N / THREAD_SIZE_N);\n\n\tint A_BLOCK_ROW_START = tid / A_THREADS_PER_ROW;\n\tint B_BLOCK_ROW_START = tid / B_THREADS_PER_ROW;\n\n\tint A_BLOCK_COL_START = tid % A_THREADS_PER_ROW * 4;\n\tint B_BLOCK_COL_START = tid % B_THREADS_PER_ROW * 4;\n\n\tfor(int K_BLOCK_START = 0, K_SPARSE_BLOCK_START = 0; K_BLOCK_START < K; K_BLOCK_START += BLOCK_SIZE_K, K_SPARSE_BLOCK_START += BLOCK_SIZE_K_SPARSE){\n\t\tfloat *A_global_ptr = g_vec + M_BLOCK_START * K + K_BLOCK_START;\n\t\tfloat *B_global_ptr = g_mat_data + K_SPARSE_BLOCK_START * N + N_BLOCK_START;\n\t\tint *B_index_global_ptr = g_mat_index + K_SPARSE_BLOCK_START * N + N_BLOCK_START;\n\n\t\t__syncthreads();\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_M; i += A_STRIDES){\n\t\t\t*(float4 *)(A_shared + (i + A_BLOCK_ROW_START) * BLOCK_SIZE_K + A_BLOCK_COL_START) = \n\t\t\t\t*(float4 *)(A_global_ptr + (i + A_BLOCK_ROW_START) * K + A_BLOCK_COL_START);\n\t\t}\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_K_SPARSE; i += B_STRIDES){\n\t\t\t*(float4 *)(B_shared + (i + B_BLOCK_ROW_START) * BLOCK_SIZE_N + B_BLOCK_COL_START) =\n\t\t\t\t*(float4 *)(B_global_ptr + (i + B_BLOCK_ROW_START) * N + B_BLOCK_COL_START);\n\n\t\t\t*(float4 *)(B_index_shared + (i + B_BLOCK_ROW_START) * BLOCK_SIZE_N + B_BLOCK_COL_START) =\n\t\t\t\t*(float4 *)(B_index_global_ptr + (i + B_BLOCK_ROW_START) * N + B_BLOCK_COL_START);\n\t\t}\n\n\t\t__syncthreads();\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_K_SPARSE;i += 1){\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_N; k += 1){\n\t\t\t\tB_reg[k] = B_shared[i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k];\n\t\t\t\t//*(float4 *)(B_reg + k) = *(float4 *)(B_shared + i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k);\n\t\t\t\t//*(float4 *)(B_reg_index + k) = *(float4 *)(B_index_shared + i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k);\n\t\t\t}\n\t\t\tint bank_idx = i / LEN_OF_BANK_PER_SPARSE_BLOCK;\n\t\t\tB_reg_index = B_index_shared[i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N] % BANK_VAL+bank_idx * BANK_VAL;\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_M; k += 1){\n\t\t\t\tA_reg[k] = A_shared[(t_M * THREAD_SIZE_M+k) * BLOCK_SIZE_K + B_reg_index];\n\t\t\t}\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_N; k += 1){\n\t\t\t\t#pragma unroll\n\t\t\t\tfor(int j = 0; j < THREAD_SIZE_M; j += 1){\n\t\t\t\t\tC_reg[j][k] += B_reg[k] * A_reg[j];\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n    float bias_local[THREAD_SIZE_N];\n    for(int thread_x = 0; thread_x < THREAD_SIZE_N; thread_x++){\n        bias_local[thread_x] = bias[BLOCK_SIZE_N * blockIdx.y + THREAD_SIZE_N * t_N + thread_x];\n    }\n\n\t#pragma unroll\n\tfor(int i = 0; i < THREAD_SIZE_M; i += 1){\n\t\t#pragma unroll\n\t\tfor(int j = 0; j < THREAD_SIZE_N; j += 1){\n\t\t\tg_data[(BLOCK_SIZE_M * blockIdx.x + THREAD_SIZE_M * t_M + i) * N + BLOCK_SIZE_N * blockIdx.y + THREAD_SIZE_N * t_N + j] = C_reg[i][j] + bias_local[j];\n\t\t}\n\t}\n}", "launch_config": {"dimBlock": [128, 1], "dimGrid": [64, 24]}}, "bert.encoder.layer.2.output.dense": {"code": "__global__ void MatMul_TILE_THREAD_GENERAL(float *input0, float *input1, float *input2, float *input3, float *output0) {\n    /*\n    g_vec: A [M, K]\n    g_mat_data: condense B [K_sparse, N]\n    g_mat_index: condense B index [K_sparse, N]\n    g_data: C [M, N]\n    */\n\tfloat *g_vec = input0;\n\tfloat *g_mat_data = input1;\n\tint *g_mat_index = (int*)input2;\n\tfloat *bias = input3;\n\tfloat *g_data = output0;\n\n\tconst int M = 4096;\n\tconst int K = 3072;\n\tconst int N = 768;\n\n    const int BLOCK_SIZE_M = 64;\n    const int BLOCK_SIZE_N = 128;\n    const int BLOCK_SIZE_K = 32;\n    const int THREAD_SIZE_M = 8;\n    const int THREAD_SIZE_N = 8;\n\n    const float SPARSITY = 0.875;\n    const int BANK_VAL = 32;\n    const int ALIGN_N = THREAD_SIZE_N;\n\n    const int BANK_NUM_PER_BLOCK = BLOCK_SIZE_K / BANK_VAL;\n    const int BLOCK_SIZE_K_SPARSE = int(BLOCK_SIZE_K * (1-SPARSITY));\n    const int LEN_OF_BANK_PER_SPARSE_BLOCK = BLOCK_SIZE_K_SPARSE / BANK_NUM_PER_BLOCK;\n\n\tconst int K_SPARSE = int(K * SPARSITY);\n\tint M_BLOCK_START = blockIdx.x * BLOCK_SIZE_M;\n\tint N_BLOCK_START = blockIdx.y * BLOCK_SIZE_N;\n\n\tconst int A_THREADS_PER_ROW = BLOCK_SIZE_K / 4;\n\tconst int B_THREADS_PER_ROW = BLOCK_SIZE_N / 4;\n\n\tconst int THREADS_PER_BLOCK = (BLOCK_SIZE_M / THREAD_SIZE_M) * (BLOCK_SIZE_N / THREAD_SIZE_N);\n\n\tconst int A_STRIDES = THREADS_PER_BLOCK / A_THREADS_PER_ROW;\n\tconst int B_STRIDES = THREADS_PER_BLOCK / B_THREADS_PER_ROW;\n\n\t__shared__ float A_shared[BLOCK_SIZE_M * BLOCK_SIZE_K];\n\t__shared__ float B_shared[BLOCK_SIZE_N * BLOCK_SIZE_K_SPARSE];\n\t__shared__ int B_index_shared[BLOCK_SIZE_N * BLOCK_SIZE_K_SPARSE];\n\n\tfloat A_reg[THREAD_SIZE_M];\n\tfloat B_reg[THREAD_SIZE_N];\n\tint B_reg_index;\n\tfloat C_reg[THREAD_SIZE_M][THREAD_SIZE_N] = {0};\n\n\tint tid = threadIdx.x;\n\n\tint t_N = tid % (BLOCK_SIZE_N / THREAD_SIZE_N);\n\tint t_M = tid / (BLOCK_SIZE_N / THREAD_SIZE_N);\n\n\tint A_BLOCK_ROW_START = tid / A_THREADS_PER_ROW;\n\tint B_BLOCK_ROW_START = tid / B_THREADS_PER_ROW;\n\n\tint A_BLOCK_COL_START = tid % A_THREADS_PER_ROW * 4;\n\tint B_BLOCK_COL_START = tid % B_THREADS_PER_ROW * 4;\n\n\tfor(int K_BLOCK_START = 0, K_SPARSE_BLOCK_START = 0; K_BLOCK_START < K; K_BLOCK_START += BLOCK_SIZE_K, K_SPARSE_BLOCK_START += BLOCK_SIZE_K_SPARSE){\n\t\tfloat *A_global_ptr = g_vec + M_BLOCK_START * K + K_BLOCK_START;\n\t\tfloat *B_global_ptr = g_mat_data + K_SPARSE_BLOCK_START * N + N_BLOCK_START;\n\t\tint *B_index_global_ptr = g_mat_index + K_SPARSE_BLOCK_START * N + N_BLOCK_START;\n\n\t\t__syncthreads();\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_M; i += A_STRIDES){\n\t\t\t*(float4 *)(A_shared + (i + A_BLOCK_ROW_START) * BLOCK_SIZE_K + A_BLOCK_COL_START) = \n\t\t\t\t*(float4 *)(A_global_ptr + (i + A_BLOCK_ROW_START) * K + A_BLOCK_COL_START);\n\t\t}\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_K_SPARSE; i += B_STRIDES){\n\t\t\t*(float4 *)(B_shared + (i + B_BLOCK_ROW_START) * BLOCK_SIZE_N + B_BLOCK_COL_START) =\n\t\t\t\t*(float4 *)(B_global_ptr + (i + B_BLOCK_ROW_START) * N + B_BLOCK_COL_START);\n\n\t\t\t*(float4 *)(B_index_shared + (i + B_BLOCK_ROW_START) * BLOCK_SIZE_N + B_BLOCK_COL_START) =\n\t\t\t\t*(float4 *)(B_index_global_ptr + (i + B_BLOCK_ROW_START) * N + B_BLOCK_COL_START);\n\t\t}\n\n\t\t__syncthreads();\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_K_SPARSE;i += 1){\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_N; k += 1){\n\t\t\t\tB_reg[k] = B_shared[i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k];\n\t\t\t\t//*(float4 *)(B_reg + k) = *(float4 *)(B_shared + i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k);\n\t\t\t\t//*(float4 *)(B_reg_index + k) = *(float4 *)(B_index_shared + i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k);\n\t\t\t}\n\t\t\tint bank_idx = i / LEN_OF_BANK_PER_SPARSE_BLOCK;\n\t\t\tB_reg_index = B_index_shared[i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N] % BANK_VAL+bank_idx * BANK_VAL;\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_M; k += 1){\n\t\t\t\tA_reg[k] = A_shared[(t_M * THREAD_SIZE_M+k) * BLOCK_SIZE_K + B_reg_index];\n\t\t\t}\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_N; k += 1){\n\t\t\t\t#pragma unroll\n\t\t\t\tfor(int j = 0; j < THREAD_SIZE_M; j += 1){\n\t\t\t\t\tC_reg[j][k] += B_reg[k] * A_reg[j];\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n    float bias_local[THREAD_SIZE_N];\n    for(int thread_x = 0; thread_x < THREAD_SIZE_N; thread_x++){\n        bias_local[thread_x] = bias[BLOCK_SIZE_N * blockIdx.y + THREAD_SIZE_N * t_N + thread_x];\n    }\n\n\t#pragma unroll\n\tfor(int i = 0; i < THREAD_SIZE_M; i += 1){\n\t\t#pragma unroll\n\t\tfor(int j = 0; j < THREAD_SIZE_N; j += 1){\n\t\t\tg_data[(BLOCK_SIZE_M * blockIdx.x + THREAD_SIZE_M * t_M + i) * N + BLOCK_SIZE_N * blockIdx.y + THREAD_SIZE_N * t_N + j] = C_reg[i][j] + bias_local[j];\n\t\t}\n\t}\n}", "launch_config": {"dimBlock": [128, 1], "dimGrid": [64, 6]}}, "bert.encoder.layer.3.attention.self.query": {"code": "__global__ void MatMul_TILE_THREAD_GENERAL(float *input0, float *input1, float *input2, float *input3, float *output0) {\n    /*\n    g_vec: A [M, K]\n    g_mat_data: condense B [K_sparse, N]\n    g_mat_index: condense B index [K_sparse, N]\n    g_data: C [M, N]\n    */\n\tfloat *g_vec = input0;\n\tfloat *g_mat_data = input1;\n\tint *g_mat_index = (int*)input2;\n\tfloat *bias = input3;\n\tfloat *g_data = output0;\n\n\tconst int M = 4096;\n\tconst int K = 768;\n\tconst int N = 768;\n\n    const int BLOCK_SIZE_M = 64;\n    const int BLOCK_SIZE_N = 128;\n    const int BLOCK_SIZE_K = 32;\n    const int THREAD_SIZE_M = 8;\n    const int THREAD_SIZE_N = 8;\n\n    const float SPARSITY = 0.875;\n    const int BANK_VAL = 32;\n    const int ALIGN_N = THREAD_SIZE_N;\n\n    const int BANK_NUM_PER_BLOCK = BLOCK_SIZE_K / BANK_VAL;\n    const int BLOCK_SIZE_K_SPARSE = int(BLOCK_SIZE_K * (1-SPARSITY));\n    const int LEN_OF_BANK_PER_SPARSE_BLOCK = BLOCK_SIZE_K_SPARSE / BANK_NUM_PER_BLOCK;\n\n\tconst int K_SPARSE = int(K * SPARSITY);\n\tint M_BLOCK_START = blockIdx.x * BLOCK_SIZE_M;\n\tint N_BLOCK_START = blockIdx.y * BLOCK_SIZE_N;\n\n\tconst int A_THREADS_PER_ROW = BLOCK_SIZE_K / 4;\n\tconst int B_THREADS_PER_ROW = BLOCK_SIZE_N / 4;\n\n\tconst int THREADS_PER_BLOCK = (BLOCK_SIZE_M / THREAD_SIZE_M) * (BLOCK_SIZE_N / THREAD_SIZE_N);\n\n\tconst int A_STRIDES = THREADS_PER_BLOCK / A_THREADS_PER_ROW;\n\tconst int B_STRIDES = THREADS_PER_BLOCK / B_THREADS_PER_ROW;\n\n\t__shared__ float A_shared[BLOCK_SIZE_M * BLOCK_SIZE_K];\n\t__shared__ float B_shared[BLOCK_SIZE_N * BLOCK_SIZE_K_SPARSE];\n\t__shared__ int B_index_shared[BLOCK_SIZE_N * BLOCK_SIZE_K_SPARSE];\n\n\tfloat A_reg[THREAD_SIZE_M];\n\tfloat B_reg[THREAD_SIZE_N];\n\tint B_reg_index;\n\tfloat C_reg[THREAD_SIZE_M][THREAD_SIZE_N] = {0};\n\n\tint tid = threadIdx.x;\n\n\tint t_N = tid % (BLOCK_SIZE_N / THREAD_SIZE_N);\n\tint t_M = tid / (BLOCK_SIZE_N / THREAD_SIZE_N);\n\n\tint A_BLOCK_ROW_START = tid / A_THREADS_PER_ROW;\n\tint B_BLOCK_ROW_START = tid / B_THREADS_PER_ROW;\n\n\tint A_BLOCK_COL_START = tid % A_THREADS_PER_ROW * 4;\n\tint B_BLOCK_COL_START = tid % B_THREADS_PER_ROW * 4;\n\n\tfor(int K_BLOCK_START = 0, K_SPARSE_BLOCK_START = 0; K_BLOCK_START < K; K_BLOCK_START += BLOCK_SIZE_K, K_SPARSE_BLOCK_START += BLOCK_SIZE_K_SPARSE){\n\t\tfloat *A_global_ptr = g_vec + M_BLOCK_START * K + K_BLOCK_START;\n\t\tfloat *B_global_ptr = g_mat_data + K_SPARSE_BLOCK_START * N + N_BLOCK_START;\n\t\tint *B_index_global_ptr = g_mat_index + K_SPARSE_BLOCK_START * N + N_BLOCK_START;\n\n\t\t__syncthreads();\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_M; i += A_STRIDES){\n\t\t\t*(float4 *)(A_shared + (i + A_BLOCK_ROW_START) * BLOCK_SIZE_K + A_BLOCK_COL_START) = \n\t\t\t\t*(float4 *)(A_global_ptr + (i + A_BLOCK_ROW_START) * K + A_BLOCK_COL_START);\n\t\t}\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_K_SPARSE; i += B_STRIDES){\n\t\t\t*(float4 *)(B_shared + (i + B_BLOCK_ROW_START) * BLOCK_SIZE_N + B_BLOCK_COL_START) =\n\t\t\t\t*(float4 *)(B_global_ptr + (i + B_BLOCK_ROW_START) * N + B_BLOCK_COL_START);\n\n\t\t\t*(float4 *)(B_index_shared + (i + B_BLOCK_ROW_START) * BLOCK_SIZE_N + B_BLOCK_COL_START) =\n\t\t\t\t*(float4 *)(B_index_global_ptr + (i + B_BLOCK_ROW_START) * N + B_BLOCK_COL_START);\n\t\t}\n\n\t\t__syncthreads();\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_K_SPARSE;i += 1){\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_N; k += 1){\n\t\t\t\tB_reg[k] = B_shared[i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k];\n\t\t\t\t//*(float4 *)(B_reg + k) = *(float4 *)(B_shared + i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k);\n\t\t\t\t//*(float4 *)(B_reg_index + k) = *(float4 *)(B_index_shared + i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k);\n\t\t\t}\n\t\t\tint bank_idx = i / LEN_OF_BANK_PER_SPARSE_BLOCK;\n\t\t\tB_reg_index = B_index_shared[i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N] % BANK_VAL+bank_idx * BANK_VAL;\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_M; k += 1){\n\t\t\t\tA_reg[k] = A_shared[(t_M * THREAD_SIZE_M+k) * BLOCK_SIZE_K + B_reg_index];\n\t\t\t}\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_N; k += 1){\n\t\t\t\t#pragma unroll\n\t\t\t\tfor(int j = 0; j < THREAD_SIZE_M; j += 1){\n\t\t\t\t\tC_reg[j][k] += B_reg[k] * A_reg[j];\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n    float bias_local[THREAD_SIZE_N];\n    for(int thread_x = 0; thread_x < THREAD_SIZE_N; thread_x++){\n        bias_local[thread_x] = bias[BLOCK_SIZE_N * blockIdx.y + THREAD_SIZE_N * t_N + thread_x];\n    }\n\n\t#pragma unroll\n\tfor(int i = 0; i < THREAD_SIZE_M; i += 1){\n\t\t#pragma unroll\n\t\tfor(int j = 0; j < THREAD_SIZE_N; j += 1){\n\t\t\tg_data[(BLOCK_SIZE_M * blockIdx.x + THREAD_SIZE_M * t_M + i) * N + BLOCK_SIZE_N * blockIdx.y + THREAD_SIZE_N * t_N + j] = C_reg[i][j] + bias_local[j];\n\t\t}\n\t}\n}", "launch_config": {"dimBlock": [128, 1], "dimGrid": [64, 6]}}, "bert.encoder.layer.3.attention.self.key": {"code": "__global__ void MatMul_TILE_THREAD_GENERAL(float *input0, float *input1, float *input2, float *input3, float *output0) {\n    /*\n    g_vec: A [M, K]\n    g_mat_data: condense B [K_sparse, N]\n    g_mat_index: condense B index [K_sparse, N]\n    g_data: C [M, N]\n    */\n\tfloat *g_vec = input0;\n\tfloat *g_mat_data = input1;\n\tint *g_mat_index = (int*)input2;\n\tfloat *bias = input3;\n\tfloat *g_data = output0;\n\n\tconst int M = 4096;\n\tconst int K = 768;\n\tconst int N = 768;\n\n    const int BLOCK_SIZE_M = 64;\n    const int BLOCK_SIZE_N = 128;\n    const int BLOCK_SIZE_K = 32;\n    const int THREAD_SIZE_M = 8;\n    const int THREAD_SIZE_N = 8;\n\n    const float SPARSITY = 0.875;\n    const int BANK_VAL = 32;\n    const int ALIGN_N = THREAD_SIZE_N;\n\n    const int BANK_NUM_PER_BLOCK = BLOCK_SIZE_K / BANK_VAL;\n    const int BLOCK_SIZE_K_SPARSE = int(BLOCK_SIZE_K * (1-SPARSITY));\n    const int LEN_OF_BANK_PER_SPARSE_BLOCK = BLOCK_SIZE_K_SPARSE / BANK_NUM_PER_BLOCK;\n\n\tconst int K_SPARSE = int(K * SPARSITY);\n\tint M_BLOCK_START = blockIdx.x * BLOCK_SIZE_M;\n\tint N_BLOCK_START = blockIdx.y * BLOCK_SIZE_N;\n\n\tconst int A_THREADS_PER_ROW = BLOCK_SIZE_K / 4;\n\tconst int B_THREADS_PER_ROW = BLOCK_SIZE_N / 4;\n\n\tconst int THREADS_PER_BLOCK = (BLOCK_SIZE_M / THREAD_SIZE_M) * (BLOCK_SIZE_N / THREAD_SIZE_N);\n\n\tconst int A_STRIDES = THREADS_PER_BLOCK / A_THREADS_PER_ROW;\n\tconst int B_STRIDES = THREADS_PER_BLOCK / B_THREADS_PER_ROW;\n\n\t__shared__ float A_shared[BLOCK_SIZE_M * BLOCK_SIZE_K];\n\t__shared__ float B_shared[BLOCK_SIZE_N * BLOCK_SIZE_K_SPARSE];\n\t__shared__ int B_index_shared[BLOCK_SIZE_N * BLOCK_SIZE_K_SPARSE];\n\n\tfloat A_reg[THREAD_SIZE_M];\n\tfloat B_reg[THREAD_SIZE_N];\n\tint B_reg_index;\n\tfloat C_reg[THREAD_SIZE_M][THREAD_SIZE_N] = {0};\n\n\tint tid = threadIdx.x;\n\n\tint t_N = tid % (BLOCK_SIZE_N / THREAD_SIZE_N);\n\tint t_M = tid / (BLOCK_SIZE_N / THREAD_SIZE_N);\n\n\tint A_BLOCK_ROW_START = tid / A_THREADS_PER_ROW;\n\tint B_BLOCK_ROW_START = tid / B_THREADS_PER_ROW;\n\n\tint A_BLOCK_COL_START = tid % A_THREADS_PER_ROW * 4;\n\tint B_BLOCK_COL_START = tid % B_THREADS_PER_ROW * 4;\n\n\tfor(int K_BLOCK_START = 0, K_SPARSE_BLOCK_START = 0; K_BLOCK_START < K; K_BLOCK_START += BLOCK_SIZE_K, K_SPARSE_BLOCK_START += BLOCK_SIZE_K_SPARSE){\n\t\tfloat *A_global_ptr = g_vec + M_BLOCK_START * K + K_BLOCK_START;\n\t\tfloat *B_global_ptr = g_mat_data + K_SPARSE_BLOCK_START * N + N_BLOCK_START;\n\t\tint *B_index_global_ptr = g_mat_index + K_SPARSE_BLOCK_START * N + N_BLOCK_START;\n\n\t\t__syncthreads();\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_M; i += A_STRIDES){\n\t\t\t*(float4 *)(A_shared + (i + A_BLOCK_ROW_START) * BLOCK_SIZE_K + A_BLOCK_COL_START) = \n\t\t\t\t*(float4 *)(A_global_ptr + (i + A_BLOCK_ROW_START) * K + A_BLOCK_COL_START);\n\t\t}\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_K_SPARSE; i += B_STRIDES){\n\t\t\t*(float4 *)(B_shared + (i + B_BLOCK_ROW_START) * BLOCK_SIZE_N + B_BLOCK_COL_START) =\n\t\t\t\t*(float4 *)(B_global_ptr + (i + B_BLOCK_ROW_START) * N + B_BLOCK_COL_START);\n\n\t\t\t*(float4 *)(B_index_shared + (i + B_BLOCK_ROW_START) * BLOCK_SIZE_N + B_BLOCK_COL_START) =\n\t\t\t\t*(float4 *)(B_index_global_ptr + (i + B_BLOCK_ROW_START) * N + B_BLOCK_COL_START);\n\t\t}\n\n\t\t__syncthreads();\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_K_SPARSE;i += 1){\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_N; k += 1){\n\t\t\t\tB_reg[k] = B_shared[i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k];\n\t\t\t\t//*(float4 *)(B_reg + k) = *(float4 *)(B_shared + i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k);\n\t\t\t\t//*(float4 *)(B_reg_index + k) = *(float4 *)(B_index_shared + i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k);\n\t\t\t}\n\t\t\tint bank_idx = i / LEN_OF_BANK_PER_SPARSE_BLOCK;\n\t\t\tB_reg_index = B_index_shared[i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N] % BANK_VAL+bank_idx * BANK_VAL;\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_M; k += 1){\n\t\t\t\tA_reg[k] = A_shared[(t_M * THREAD_SIZE_M+k) * BLOCK_SIZE_K + B_reg_index];\n\t\t\t}\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_N; k += 1){\n\t\t\t\t#pragma unroll\n\t\t\t\tfor(int j = 0; j < THREAD_SIZE_M; j += 1){\n\t\t\t\t\tC_reg[j][k] += B_reg[k] * A_reg[j];\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n    float bias_local[THREAD_SIZE_N];\n    for(int thread_x = 0; thread_x < THREAD_SIZE_N; thread_x++){\n        bias_local[thread_x] = bias[BLOCK_SIZE_N * blockIdx.y + THREAD_SIZE_N * t_N + thread_x];\n    }\n\n\t#pragma unroll\n\tfor(int i = 0; i < THREAD_SIZE_M; i += 1){\n\t\t#pragma unroll\n\t\tfor(int j = 0; j < THREAD_SIZE_N; j += 1){\n\t\t\tg_data[(BLOCK_SIZE_M * blockIdx.x + THREAD_SIZE_M * t_M + i) * N + BLOCK_SIZE_N * blockIdx.y + THREAD_SIZE_N * t_N + j] = C_reg[i][j] + bias_local[j];\n\t\t}\n\t}\n}", "launch_config": {"dimBlock": [128, 1], "dimGrid": [64, 6]}}, "bert.encoder.layer.3.attention.self.value": {"code": "__global__ void MatMul_TILE_THREAD_GENERAL(float *input0, float *input1, float *input2, float *input3, float *output0) {\n    /*\n    g_vec: A [M, K]\n    g_mat_data: condense B [K_sparse, N]\n    g_mat_index: condense B index [K_sparse, N]\n    g_data: C [M, N]\n    */\n\tfloat *g_vec = input0;\n\tfloat *g_mat_data = input1;\n\tint *g_mat_index = (int*)input2;\n\tfloat *bias = input3;\n\tfloat *g_data = output0;\n\n\tconst int M = 4096;\n\tconst int K = 768;\n\tconst int N = 768;\n\n    const int BLOCK_SIZE_M = 64;\n    const int BLOCK_SIZE_N = 128;\n    const int BLOCK_SIZE_K = 32;\n    const int THREAD_SIZE_M = 8;\n    const int THREAD_SIZE_N = 8;\n\n    const float SPARSITY = 0.875;\n    const int BANK_VAL = 32;\n    const int ALIGN_N = THREAD_SIZE_N;\n\n    const int BANK_NUM_PER_BLOCK = BLOCK_SIZE_K / BANK_VAL;\n    const int BLOCK_SIZE_K_SPARSE = int(BLOCK_SIZE_K * (1-SPARSITY));\n    const int LEN_OF_BANK_PER_SPARSE_BLOCK = BLOCK_SIZE_K_SPARSE / BANK_NUM_PER_BLOCK;\n\n\tconst int K_SPARSE = int(K * SPARSITY);\n\tint M_BLOCK_START = blockIdx.x * BLOCK_SIZE_M;\n\tint N_BLOCK_START = blockIdx.y * BLOCK_SIZE_N;\n\n\tconst int A_THREADS_PER_ROW = BLOCK_SIZE_K / 4;\n\tconst int B_THREADS_PER_ROW = BLOCK_SIZE_N / 4;\n\n\tconst int THREADS_PER_BLOCK = (BLOCK_SIZE_M / THREAD_SIZE_M) * (BLOCK_SIZE_N / THREAD_SIZE_N);\n\n\tconst int A_STRIDES = THREADS_PER_BLOCK / A_THREADS_PER_ROW;\n\tconst int B_STRIDES = THREADS_PER_BLOCK / B_THREADS_PER_ROW;\n\n\t__shared__ float A_shared[BLOCK_SIZE_M * BLOCK_SIZE_K];\n\t__shared__ float B_shared[BLOCK_SIZE_N * BLOCK_SIZE_K_SPARSE];\n\t__shared__ int B_index_shared[BLOCK_SIZE_N * BLOCK_SIZE_K_SPARSE];\n\n\tfloat A_reg[THREAD_SIZE_M];\n\tfloat B_reg[THREAD_SIZE_N];\n\tint B_reg_index;\n\tfloat C_reg[THREAD_SIZE_M][THREAD_SIZE_N] = {0};\n\n\tint tid = threadIdx.x;\n\n\tint t_N = tid % (BLOCK_SIZE_N / THREAD_SIZE_N);\n\tint t_M = tid / (BLOCK_SIZE_N / THREAD_SIZE_N);\n\n\tint A_BLOCK_ROW_START = tid / A_THREADS_PER_ROW;\n\tint B_BLOCK_ROW_START = tid / B_THREADS_PER_ROW;\n\n\tint A_BLOCK_COL_START = tid % A_THREADS_PER_ROW * 4;\n\tint B_BLOCK_COL_START = tid % B_THREADS_PER_ROW * 4;\n\n\tfor(int K_BLOCK_START = 0, K_SPARSE_BLOCK_START = 0; K_BLOCK_START < K; K_BLOCK_START += BLOCK_SIZE_K, K_SPARSE_BLOCK_START += BLOCK_SIZE_K_SPARSE){\n\t\tfloat *A_global_ptr = g_vec + M_BLOCK_START * K + K_BLOCK_START;\n\t\tfloat *B_global_ptr = g_mat_data + K_SPARSE_BLOCK_START * N + N_BLOCK_START;\n\t\tint *B_index_global_ptr = g_mat_index + K_SPARSE_BLOCK_START * N + N_BLOCK_START;\n\n\t\t__syncthreads();\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_M; i += A_STRIDES){\n\t\t\t*(float4 *)(A_shared + (i + A_BLOCK_ROW_START) * BLOCK_SIZE_K + A_BLOCK_COL_START) = \n\t\t\t\t*(float4 *)(A_global_ptr + (i + A_BLOCK_ROW_START) * K + A_BLOCK_COL_START);\n\t\t}\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_K_SPARSE; i += B_STRIDES){\n\t\t\t*(float4 *)(B_shared + (i + B_BLOCK_ROW_START) * BLOCK_SIZE_N + B_BLOCK_COL_START) =\n\t\t\t\t*(float4 *)(B_global_ptr + (i + B_BLOCK_ROW_START) * N + B_BLOCK_COL_START);\n\n\t\t\t*(float4 *)(B_index_shared + (i + B_BLOCK_ROW_START) * BLOCK_SIZE_N + B_BLOCK_COL_START) =\n\t\t\t\t*(float4 *)(B_index_global_ptr + (i + B_BLOCK_ROW_START) * N + B_BLOCK_COL_START);\n\t\t}\n\n\t\t__syncthreads();\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_K_SPARSE;i += 1){\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_N; k += 1){\n\t\t\t\tB_reg[k] = B_shared[i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k];\n\t\t\t\t//*(float4 *)(B_reg + k) = *(float4 *)(B_shared + i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k);\n\t\t\t\t//*(float4 *)(B_reg_index + k) = *(float4 *)(B_index_shared + i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k);\n\t\t\t}\n\t\t\tint bank_idx = i / LEN_OF_BANK_PER_SPARSE_BLOCK;\n\t\t\tB_reg_index = B_index_shared[i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N] % BANK_VAL+bank_idx * BANK_VAL;\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_M; k += 1){\n\t\t\t\tA_reg[k] = A_shared[(t_M * THREAD_SIZE_M+k) * BLOCK_SIZE_K + B_reg_index];\n\t\t\t}\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_N; k += 1){\n\t\t\t\t#pragma unroll\n\t\t\t\tfor(int j = 0; j < THREAD_SIZE_M; j += 1){\n\t\t\t\t\tC_reg[j][k] += B_reg[k] * A_reg[j];\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n    float bias_local[THREAD_SIZE_N];\n    for(int thread_x = 0; thread_x < THREAD_SIZE_N; thread_x++){\n        bias_local[thread_x] = bias[BLOCK_SIZE_N * blockIdx.y + THREAD_SIZE_N * t_N + thread_x];\n    }\n\n\t#pragma unroll\n\tfor(int i = 0; i < THREAD_SIZE_M; i += 1){\n\t\t#pragma unroll\n\t\tfor(int j = 0; j < THREAD_SIZE_N; j += 1){\n\t\t\tg_data[(BLOCK_SIZE_M * blockIdx.x + THREAD_SIZE_M * t_M + i) * N + BLOCK_SIZE_N * blockIdx.y + THREAD_SIZE_N * t_N + j] = C_reg[i][j] + bias_local[j];\n\t\t}\n\t}\n}", "launch_config": {"dimBlock": [128, 1], "dimGrid": [64, 6]}}, "bert.encoder.layer.3.attention.output.dense": {"code": "__global__ void MatMul_TILE_THREAD_GENERAL(float *input0, float *input1, float *input2, float *input3, float *output0) {\n    /*\n    g_vec: A [M, K]\n    g_mat_data: condense B [K_sparse, N]\n    g_mat_index: condense B index [K_sparse, N]\n    g_data: C [M, N]\n    */\n\tfloat *g_vec = input0;\n\tfloat *g_mat_data = input1;\n\tint *g_mat_index = (int*)input2;\n\tfloat *bias = input3;\n\tfloat *g_data = output0;\n\n\tconst int M = 4096;\n\tconst int K = 768;\n\tconst int N = 768;\n\n    const int BLOCK_SIZE_M = 64;\n    const int BLOCK_SIZE_N = 128;\n    const int BLOCK_SIZE_K = 32;\n    const int THREAD_SIZE_M = 8;\n    const int THREAD_SIZE_N = 8;\n\n    const float SPARSITY = 0.875;\n    const int BANK_VAL = 32;\n    const int ALIGN_N = THREAD_SIZE_N;\n\n    const int BANK_NUM_PER_BLOCK = BLOCK_SIZE_K / BANK_VAL;\n    const int BLOCK_SIZE_K_SPARSE = int(BLOCK_SIZE_K * (1-SPARSITY));\n    const int LEN_OF_BANK_PER_SPARSE_BLOCK = BLOCK_SIZE_K_SPARSE / BANK_NUM_PER_BLOCK;\n\n\tconst int K_SPARSE = int(K * SPARSITY);\n\tint M_BLOCK_START = blockIdx.x * BLOCK_SIZE_M;\n\tint N_BLOCK_START = blockIdx.y * BLOCK_SIZE_N;\n\n\tconst int A_THREADS_PER_ROW = BLOCK_SIZE_K / 4;\n\tconst int B_THREADS_PER_ROW = BLOCK_SIZE_N / 4;\n\n\tconst int THREADS_PER_BLOCK = (BLOCK_SIZE_M / THREAD_SIZE_M) * (BLOCK_SIZE_N / THREAD_SIZE_N);\n\n\tconst int A_STRIDES = THREADS_PER_BLOCK / A_THREADS_PER_ROW;\n\tconst int B_STRIDES = THREADS_PER_BLOCK / B_THREADS_PER_ROW;\n\n\t__shared__ float A_shared[BLOCK_SIZE_M * BLOCK_SIZE_K];\n\t__shared__ float B_shared[BLOCK_SIZE_N * BLOCK_SIZE_K_SPARSE];\n\t__shared__ int B_index_shared[BLOCK_SIZE_N * BLOCK_SIZE_K_SPARSE];\n\n\tfloat A_reg[THREAD_SIZE_M];\n\tfloat B_reg[THREAD_SIZE_N];\n\tint B_reg_index;\n\tfloat C_reg[THREAD_SIZE_M][THREAD_SIZE_N] = {0};\n\n\tint tid = threadIdx.x;\n\n\tint t_N = tid % (BLOCK_SIZE_N / THREAD_SIZE_N);\n\tint t_M = tid / (BLOCK_SIZE_N / THREAD_SIZE_N);\n\n\tint A_BLOCK_ROW_START = tid / A_THREADS_PER_ROW;\n\tint B_BLOCK_ROW_START = tid / B_THREADS_PER_ROW;\n\n\tint A_BLOCK_COL_START = tid % A_THREADS_PER_ROW * 4;\n\tint B_BLOCK_COL_START = tid % B_THREADS_PER_ROW * 4;\n\n\tfor(int K_BLOCK_START = 0, K_SPARSE_BLOCK_START = 0; K_BLOCK_START < K; K_BLOCK_START += BLOCK_SIZE_K, K_SPARSE_BLOCK_START += BLOCK_SIZE_K_SPARSE){\n\t\tfloat *A_global_ptr = g_vec + M_BLOCK_START * K + K_BLOCK_START;\n\t\tfloat *B_global_ptr = g_mat_data + K_SPARSE_BLOCK_START * N + N_BLOCK_START;\n\t\tint *B_index_global_ptr = g_mat_index + K_SPARSE_BLOCK_START * N + N_BLOCK_START;\n\n\t\t__syncthreads();\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_M; i += A_STRIDES){\n\t\t\t*(float4 *)(A_shared + (i + A_BLOCK_ROW_START) * BLOCK_SIZE_K + A_BLOCK_COL_START) = \n\t\t\t\t*(float4 *)(A_global_ptr + (i + A_BLOCK_ROW_START) * K + A_BLOCK_COL_START);\n\t\t}\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_K_SPARSE; i += B_STRIDES){\n\t\t\t*(float4 *)(B_shared + (i + B_BLOCK_ROW_START) * BLOCK_SIZE_N + B_BLOCK_COL_START) =\n\t\t\t\t*(float4 *)(B_global_ptr + (i + B_BLOCK_ROW_START) * N + B_BLOCK_COL_START);\n\n\t\t\t*(float4 *)(B_index_shared + (i + B_BLOCK_ROW_START) * BLOCK_SIZE_N + B_BLOCK_COL_START) =\n\t\t\t\t*(float4 *)(B_index_global_ptr + (i + B_BLOCK_ROW_START) * N + B_BLOCK_COL_START);\n\t\t}\n\n\t\t__syncthreads();\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_K_SPARSE;i += 1){\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_N; k += 1){\n\t\t\t\tB_reg[k] = B_shared[i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k];\n\t\t\t\t//*(float4 *)(B_reg + k) = *(float4 *)(B_shared + i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k);\n\t\t\t\t//*(float4 *)(B_reg_index + k) = *(float4 *)(B_index_shared + i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k);\n\t\t\t}\n\t\t\tint bank_idx = i / LEN_OF_BANK_PER_SPARSE_BLOCK;\n\t\t\tB_reg_index = B_index_shared[i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N] % BANK_VAL+bank_idx * BANK_VAL;\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_M; k += 1){\n\t\t\t\tA_reg[k] = A_shared[(t_M * THREAD_SIZE_M+k) * BLOCK_SIZE_K + B_reg_index];\n\t\t\t}\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_N; k += 1){\n\t\t\t\t#pragma unroll\n\t\t\t\tfor(int j = 0; j < THREAD_SIZE_M; j += 1){\n\t\t\t\t\tC_reg[j][k] += B_reg[k] * A_reg[j];\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n    float bias_local[THREAD_SIZE_N];\n    for(int thread_x = 0; thread_x < THREAD_SIZE_N; thread_x++){\n        bias_local[thread_x] = bias[BLOCK_SIZE_N * blockIdx.y + THREAD_SIZE_N * t_N + thread_x];\n    }\n\n\t#pragma unroll\n\tfor(int i = 0; i < THREAD_SIZE_M; i += 1){\n\t\t#pragma unroll\n\t\tfor(int j = 0; j < THREAD_SIZE_N; j += 1){\n\t\t\tg_data[(BLOCK_SIZE_M * blockIdx.x + THREAD_SIZE_M * t_M + i) * N + BLOCK_SIZE_N * blockIdx.y + THREAD_SIZE_N * t_N + j] = C_reg[i][j] + bias_local[j];\n\t\t}\n\t}\n}", "launch_config": {"dimBlock": [128, 1], "dimGrid": [64, 6]}}, "bert.encoder.layer.3.intermediate.dense": {"code": "__global__ void MatMul_TILE_THREAD_GENERAL(float *input0, float *input1, float *input2, float *input3, float *output0) {\n    /*\n    g_vec: A [M, K]\n    g_mat_data: condense B [K_sparse, N]\n    g_mat_index: condense B index [K_sparse, N]\n    g_data: C [M, N]\n    */\n\tfloat *g_vec = input0;\n\tfloat *g_mat_data = input1;\n\tint *g_mat_index = (int*)input2;\n\tfloat *bias = input3;\n\tfloat *g_data = output0;\n\n\tconst int M = 4096;\n\tconst int K = 768;\n\tconst int N = 3072;\n\n    const int BLOCK_SIZE_M = 64;\n    const int BLOCK_SIZE_N = 128;\n    const int BLOCK_SIZE_K = 32;\n    const int THREAD_SIZE_M = 8;\n    const int THREAD_SIZE_N = 8;\n\n    const float SPARSITY = 0.875;\n    const int BANK_VAL = 32;\n    const int ALIGN_N = THREAD_SIZE_N;\n\n    const int BANK_NUM_PER_BLOCK = BLOCK_SIZE_K / BANK_VAL;\n    const int BLOCK_SIZE_K_SPARSE = int(BLOCK_SIZE_K * (1-SPARSITY));\n    const int LEN_OF_BANK_PER_SPARSE_BLOCK = BLOCK_SIZE_K_SPARSE / BANK_NUM_PER_BLOCK;\n\n\tconst int K_SPARSE = int(K * SPARSITY);\n\tint M_BLOCK_START = blockIdx.x * BLOCK_SIZE_M;\n\tint N_BLOCK_START = blockIdx.y * BLOCK_SIZE_N;\n\n\tconst int A_THREADS_PER_ROW = BLOCK_SIZE_K / 4;\n\tconst int B_THREADS_PER_ROW = BLOCK_SIZE_N / 4;\n\n\tconst int THREADS_PER_BLOCK = (BLOCK_SIZE_M / THREAD_SIZE_M) * (BLOCK_SIZE_N / THREAD_SIZE_N);\n\n\tconst int A_STRIDES = THREADS_PER_BLOCK / A_THREADS_PER_ROW;\n\tconst int B_STRIDES = THREADS_PER_BLOCK / B_THREADS_PER_ROW;\n\n\t__shared__ float A_shared[BLOCK_SIZE_M * BLOCK_SIZE_K];\n\t__shared__ float B_shared[BLOCK_SIZE_N * BLOCK_SIZE_K_SPARSE];\n\t__shared__ int B_index_shared[BLOCK_SIZE_N * BLOCK_SIZE_K_SPARSE];\n\n\tfloat A_reg[THREAD_SIZE_M];\n\tfloat B_reg[THREAD_SIZE_N];\n\tint B_reg_index;\n\tfloat C_reg[THREAD_SIZE_M][THREAD_SIZE_N] = {0};\n\n\tint tid = threadIdx.x;\n\n\tint t_N = tid % (BLOCK_SIZE_N / THREAD_SIZE_N);\n\tint t_M = tid / (BLOCK_SIZE_N / THREAD_SIZE_N);\n\n\tint A_BLOCK_ROW_START = tid / A_THREADS_PER_ROW;\n\tint B_BLOCK_ROW_START = tid / B_THREADS_PER_ROW;\n\n\tint A_BLOCK_COL_START = tid % A_THREADS_PER_ROW * 4;\n\tint B_BLOCK_COL_START = tid % B_THREADS_PER_ROW * 4;\n\n\tfor(int K_BLOCK_START = 0, K_SPARSE_BLOCK_START = 0; K_BLOCK_START < K; K_BLOCK_START += BLOCK_SIZE_K, K_SPARSE_BLOCK_START += BLOCK_SIZE_K_SPARSE){\n\t\tfloat *A_global_ptr = g_vec + M_BLOCK_START * K + K_BLOCK_START;\n\t\tfloat *B_global_ptr = g_mat_data + K_SPARSE_BLOCK_START * N + N_BLOCK_START;\n\t\tint *B_index_global_ptr = g_mat_index + K_SPARSE_BLOCK_START * N + N_BLOCK_START;\n\n\t\t__syncthreads();\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_M; i += A_STRIDES){\n\t\t\t*(float4 *)(A_shared + (i + A_BLOCK_ROW_START) * BLOCK_SIZE_K + A_BLOCK_COL_START) = \n\t\t\t\t*(float4 *)(A_global_ptr + (i + A_BLOCK_ROW_START) * K + A_BLOCK_COL_START);\n\t\t}\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_K_SPARSE; i += B_STRIDES){\n\t\t\t*(float4 *)(B_shared + (i + B_BLOCK_ROW_START) * BLOCK_SIZE_N + B_BLOCK_COL_START) =\n\t\t\t\t*(float4 *)(B_global_ptr + (i + B_BLOCK_ROW_START) * N + B_BLOCK_COL_START);\n\n\t\t\t*(float4 *)(B_index_shared + (i + B_BLOCK_ROW_START) * BLOCK_SIZE_N + B_BLOCK_COL_START) =\n\t\t\t\t*(float4 *)(B_index_global_ptr + (i + B_BLOCK_ROW_START) * N + B_BLOCK_COL_START);\n\t\t}\n\n\t\t__syncthreads();\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_K_SPARSE;i += 1){\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_N; k += 1){\n\t\t\t\tB_reg[k] = B_shared[i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k];\n\t\t\t\t//*(float4 *)(B_reg + k) = *(float4 *)(B_shared + i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k);\n\t\t\t\t//*(float4 *)(B_reg_index + k) = *(float4 *)(B_index_shared + i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k);\n\t\t\t}\n\t\t\tint bank_idx = i / LEN_OF_BANK_PER_SPARSE_BLOCK;\n\t\t\tB_reg_index = B_index_shared[i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N] % BANK_VAL+bank_idx * BANK_VAL;\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_M; k += 1){\n\t\t\t\tA_reg[k] = A_shared[(t_M * THREAD_SIZE_M+k) * BLOCK_SIZE_K + B_reg_index];\n\t\t\t}\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_N; k += 1){\n\t\t\t\t#pragma unroll\n\t\t\t\tfor(int j = 0; j < THREAD_SIZE_M; j += 1){\n\t\t\t\t\tC_reg[j][k] += B_reg[k] * A_reg[j];\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n    float bias_local[THREAD_SIZE_N];\n    for(int thread_x = 0; thread_x < THREAD_SIZE_N; thread_x++){\n        bias_local[thread_x] = bias[BLOCK_SIZE_N * blockIdx.y + THREAD_SIZE_N * t_N + thread_x];\n    }\n\n\t#pragma unroll\n\tfor(int i = 0; i < THREAD_SIZE_M; i += 1){\n\t\t#pragma unroll\n\t\tfor(int j = 0; j < THREAD_SIZE_N; j += 1){\n\t\t\tg_data[(BLOCK_SIZE_M * blockIdx.x + THREAD_SIZE_M * t_M + i) * N + BLOCK_SIZE_N * blockIdx.y + THREAD_SIZE_N * t_N + j] = C_reg[i][j] + bias_local[j];\n\t\t}\n\t}\n}", "launch_config": {"dimBlock": [128, 1], "dimGrid": [64, 24]}}, "bert.encoder.layer.3.output.dense": {"code": "__global__ void MatMul_TILE_THREAD_GENERAL(float *input0, float *input1, float *input2, float *input3, float *output0) {\n    /*\n    g_vec: A [M, K]\n    g_mat_data: condense B [K_sparse, N]\n    g_mat_index: condense B index [K_sparse, N]\n    g_data: C [M, N]\n    */\n\tfloat *g_vec = input0;\n\tfloat *g_mat_data = input1;\n\tint *g_mat_index = (int*)input2;\n\tfloat *bias = input3;\n\tfloat *g_data = output0;\n\n\tconst int M = 4096;\n\tconst int K = 3072;\n\tconst int N = 768;\n\n    const int BLOCK_SIZE_M = 64;\n    const int BLOCK_SIZE_N = 128;\n    const int BLOCK_SIZE_K = 32;\n    const int THREAD_SIZE_M = 8;\n    const int THREAD_SIZE_N = 8;\n\n    const float SPARSITY = 0.875;\n    const int BANK_VAL = 32;\n    const int ALIGN_N = THREAD_SIZE_N;\n\n    const int BANK_NUM_PER_BLOCK = BLOCK_SIZE_K / BANK_VAL;\n    const int BLOCK_SIZE_K_SPARSE = int(BLOCK_SIZE_K * (1-SPARSITY));\n    const int LEN_OF_BANK_PER_SPARSE_BLOCK = BLOCK_SIZE_K_SPARSE / BANK_NUM_PER_BLOCK;\n\n\tconst int K_SPARSE = int(K * SPARSITY);\n\tint M_BLOCK_START = blockIdx.x * BLOCK_SIZE_M;\n\tint N_BLOCK_START = blockIdx.y * BLOCK_SIZE_N;\n\n\tconst int A_THREADS_PER_ROW = BLOCK_SIZE_K / 4;\n\tconst int B_THREADS_PER_ROW = BLOCK_SIZE_N / 4;\n\n\tconst int THREADS_PER_BLOCK = (BLOCK_SIZE_M / THREAD_SIZE_M) * (BLOCK_SIZE_N / THREAD_SIZE_N);\n\n\tconst int A_STRIDES = THREADS_PER_BLOCK / A_THREADS_PER_ROW;\n\tconst int B_STRIDES = THREADS_PER_BLOCK / B_THREADS_PER_ROW;\n\n\t__shared__ float A_shared[BLOCK_SIZE_M * BLOCK_SIZE_K];\n\t__shared__ float B_shared[BLOCK_SIZE_N * BLOCK_SIZE_K_SPARSE];\n\t__shared__ int B_index_shared[BLOCK_SIZE_N * BLOCK_SIZE_K_SPARSE];\n\n\tfloat A_reg[THREAD_SIZE_M];\n\tfloat B_reg[THREAD_SIZE_N];\n\tint B_reg_index;\n\tfloat C_reg[THREAD_SIZE_M][THREAD_SIZE_N] = {0};\n\n\tint tid = threadIdx.x;\n\n\tint t_N = tid % (BLOCK_SIZE_N / THREAD_SIZE_N);\n\tint t_M = tid / (BLOCK_SIZE_N / THREAD_SIZE_N);\n\n\tint A_BLOCK_ROW_START = tid / A_THREADS_PER_ROW;\n\tint B_BLOCK_ROW_START = tid / B_THREADS_PER_ROW;\n\n\tint A_BLOCK_COL_START = tid % A_THREADS_PER_ROW * 4;\n\tint B_BLOCK_COL_START = tid % B_THREADS_PER_ROW * 4;\n\n\tfor(int K_BLOCK_START = 0, K_SPARSE_BLOCK_START = 0; K_BLOCK_START < K; K_BLOCK_START += BLOCK_SIZE_K, K_SPARSE_BLOCK_START += BLOCK_SIZE_K_SPARSE){\n\t\tfloat *A_global_ptr = g_vec + M_BLOCK_START * K + K_BLOCK_START;\n\t\tfloat *B_global_ptr = g_mat_data + K_SPARSE_BLOCK_START * N + N_BLOCK_START;\n\t\tint *B_index_global_ptr = g_mat_index + K_SPARSE_BLOCK_START * N + N_BLOCK_START;\n\n\t\t__syncthreads();\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_M; i += A_STRIDES){\n\t\t\t*(float4 *)(A_shared + (i + A_BLOCK_ROW_START) * BLOCK_SIZE_K + A_BLOCK_COL_START) = \n\t\t\t\t*(float4 *)(A_global_ptr + (i + A_BLOCK_ROW_START) * K + A_BLOCK_COL_START);\n\t\t}\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_K_SPARSE; i += B_STRIDES){\n\t\t\t*(float4 *)(B_shared + (i + B_BLOCK_ROW_START) * BLOCK_SIZE_N + B_BLOCK_COL_START) =\n\t\t\t\t*(float4 *)(B_global_ptr + (i + B_BLOCK_ROW_START) * N + B_BLOCK_COL_START);\n\n\t\t\t*(float4 *)(B_index_shared + (i + B_BLOCK_ROW_START) * BLOCK_SIZE_N + B_BLOCK_COL_START) =\n\t\t\t\t*(float4 *)(B_index_global_ptr + (i + B_BLOCK_ROW_START) * N + B_BLOCK_COL_START);\n\t\t}\n\n\t\t__syncthreads();\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_K_SPARSE;i += 1){\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_N; k += 1){\n\t\t\t\tB_reg[k] = B_shared[i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k];\n\t\t\t\t//*(float4 *)(B_reg + k) = *(float4 *)(B_shared + i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k);\n\t\t\t\t//*(float4 *)(B_reg_index + k) = *(float4 *)(B_index_shared + i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k);\n\t\t\t}\n\t\t\tint bank_idx = i / LEN_OF_BANK_PER_SPARSE_BLOCK;\n\t\t\tB_reg_index = B_index_shared[i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N] % BANK_VAL+bank_idx * BANK_VAL;\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_M; k += 1){\n\t\t\t\tA_reg[k] = A_shared[(t_M * THREAD_SIZE_M+k) * BLOCK_SIZE_K + B_reg_index];\n\t\t\t}\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_N; k += 1){\n\t\t\t\t#pragma unroll\n\t\t\t\tfor(int j = 0; j < THREAD_SIZE_M; j += 1){\n\t\t\t\t\tC_reg[j][k] += B_reg[k] * A_reg[j];\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n    float bias_local[THREAD_SIZE_N];\n    for(int thread_x = 0; thread_x < THREAD_SIZE_N; thread_x++){\n        bias_local[thread_x] = bias[BLOCK_SIZE_N * blockIdx.y + THREAD_SIZE_N * t_N + thread_x];\n    }\n\n\t#pragma unroll\n\tfor(int i = 0; i < THREAD_SIZE_M; i += 1){\n\t\t#pragma unroll\n\t\tfor(int j = 0; j < THREAD_SIZE_N; j += 1){\n\t\t\tg_data[(BLOCK_SIZE_M * blockIdx.x + THREAD_SIZE_M * t_M + i) * N + BLOCK_SIZE_N * blockIdx.y + THREAD_SIZE_N * t_N + j] = C_reg[i][j] + bias_local[j];\n\t\t}\n\t}\n}", "launch_config": {"dimBlock": [128, 1], "dimGrid": [64, 6]}}, "bert.encoder.layer.4.attention.self.query": {"code": "__global__ void MatMul_TILE_THREAD_GENERAL(float *input0, float *input1, float *input2, float *input3, float *output0) {\n    /*\n    g_vec: A [M, K]\n    g_mat_data: condense B [K_sparse, N]\n    g_mat_index: condense B index [K_sparse, N]\n    g_data: C [M, N]\n    */\n\tfloat *g_vec = input0;\n\tfloat *g_mat_data = input1;\n\tint *g_mat_index = (int*)input2;\n\tfloat *bias = input3;\n\tfloat *g_data = output0;\n\n\tconst int M = 4096;\n\tconst int K = 768;\n\tconst int N = 768;\n\n    const int BLOCK_SIZE_M = 64;\n    const int BLOCK_SIZE_N = 128;\n    const int BLOCK_SIZE_K = 32;\n    const int THREAD_SIZE_M = 8;\n    const int THREAD_SIZE_N = 8;\n\n    const float SPARSITY = 0.875;\n    const int BANK_VAL = 32;\n    const int ALIGN_N = THREAD_SIZE_N;\n\n    const int BANK_NUM_PER_BLOCK = BLOCK_SIZE_K / BANK_VAL;\n    const int BLOCK_SIZE_K_SPARSE = int(BLOCK_SIZE_K * (1-SPARSITY));\n    const int LEN_OF_BANK_PER_SPARSE_BLOCK = BLOCK_SIZE_K_SPARSE / BANK_NUM_PER_BLOCK;\n\n\tconst int K_SPARSE = int(K * SPARSITY);\n\tint M_BLOCK_START = blockIdx.x * BLOCK_SIZE_M;\n\tint N_BLOCK_START = blockIdx.y * BLOCK_SIZE_N;\n\n\tconst int A_THREADS_PER_ROW = BLOCK_SIZE_K / 4;\n\tconst int B_THREADS_PER_ROW = BLOCK_SIZE_N / 4;\n\n\tconst int THREADS_PER_BLOCK = (BLOCK_SIZE_M / THREAD_SIZE_M) * (BLOCK_SIZE_N / THREAD_SIZE_N);\n\n\tconst int A_STRIDES = THREADS_PER_BLOCK / A_THREADS_PER_ROW;\n\tconst int B_STRIDES = THREADS_PER_BLOCK / B_THREADS_PER_ROW;\n\n\t__shared__ float A_shared[BLOCK_SIZE_M * BLOCK_SIZE_K];\n\t__shared__ float B_shared[BLOCK_SIZE_N * BLOCK_SIZE_K_SPARSE];\n\t__shared__ int B_index_shared[BLOCK_SIZE_N * BLOCK_SIZE_K_SPARSE];\n\n\tfloat A_reg[THREAD_SIZE_M];\n\tfloat B_reg[THREAD_SIZE_N];\n\tint B_reg_index;\n\tfloat C_reg[THREAD_SIZE_M][THREAD_SIZE_N] = {0};\n\n\tint tid = threadIdx.x;\n\n\tint t_N = tid % (BLOCK_SIZE_N / THREAD_SIZE_N);\n\tint t_M = tid / (BLOCK_SIZE_N / THREAD_SIZE_N);\n\n\tint A_BLOCK_ROW_START = tid / A_THREADS_PER_ROW;\n\tint B_BLOCK_ROW_START = tid / B_THREADS_PER_ROW;\n\n\tint A_BLOCK_COL_START = tid % A_THREADS_PER_ROW * 4;\n\tint B_BLOCK_COL_START = tid % B_THREADS_PER_ROW * 4;\n\n\tfor(int K_BLOCK_START = 0, K_SPARSE_BLOCK_START = 0; K_BLOCK_START < K; K_BLOCK_START += BLOCK_SIZE_K, K_SPARSE_BLOCK_START += BLOCK_SIZE_K_SPARSE){\n\t\tfloat *A_global_ptr = g_vec + M_BLOCK_START * K + K_BLOCK_START;\n\t\tfloat *B_global_ptr = g_mat_data + K_SPARSE_BLOCK_START * N + N_BLOCK_START;\n\t\tint *B_index_global_ptr = g_mat_index + K_SPARSE_BLOCK_START * N + N_BLOCK_START;\n\n\t\t__syncthreads();\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_M; i += A_STRIDES){\n\t\t\t*(float4 *)(A_shared + (i + A_BLOCK_ROW_START) * BLOCK_SIZE_K + A_BLOCK_COL_START) = \n\t\t\t\t*(float4 *)(A_global_ptr + (i + A_BLOCK_ROW_START) * K + A_BLOCK_COL_START);\n\t\t}\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_K_SPARSE; i += B_STRIDES){\n\t\t\t*(float4 *)(B_shared + (i + B_BLOCK_ROW_START) * BLOCK_SIZE_N + B_BLOCK_COL_START) =\n\t\t\t\t*(float4 *)(B_global_ptr + (i + B_BLOCK_ROW_START) * N + B_BLOCK_COL_START);\n\n\t\t\t*(float4 *)(B_index_shared + (i + B_BLOCK_ROW_START) * BLOCK_SIZE_N + B_BLOCK_COL_START) =\n\t\t\t\t*(float4 *)(B_index_global_ptr + (i + B_BLOCK_ROW_START) * N + B_BLOCK_COL_START);\n\t\t}\n\n\t\t__syncthreads();\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_K_SPARSE;i += 1){\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_N; k += 1){\n\t\t\t\tB_reg[k] = B_shared[i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k];\n\t\t\t\t//*(float4 *)(B_reg + k) = *(float4 *)(B_shared + i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k);\n\t\t\t\t//*(float4 *)(B_reg_index + k) = *(float4 *)(B_index_shared + i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k);\n\t\t\t}\n\t\t\tint bank_idx = i / LEN_OF_BANK_PER_SPARSE_BLOCK;\n\t\t\tB_reg_index = B_index_shared[i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N] % BANK_VAL+bank_idx * BANK_VAL;\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_M; k += 1){\n\t\t\t\tA_reg[k] = A_shared[(t_M * THREAD_SIZE_M+k) * BLOCK_SIZE_K + B_reg_index];\n\t\t\t}\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_N; k += 1){\n\t\t\t\t#pragma unroll\n\t\t\t\tfor(int j = 0; j < THREAD_SIZE_M; j += 1){\n\t\t\t\t\tC_reg[j][k] += B_reg[k] * A_reg[j];\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n    float bias_local[THREAD_SIZE_N];\n    for(int thread_x = 0; thread_x < THREAD_SIZE_N; thread_x++){\n        bias_local[thread_x] = bias[BLOCK_SIZE_N * blockIdx.y + THREAD_SIZE_N * t_N + thread_x];\n    }\n\n\t#pragma unroll\n\tfor(int i = 0; i < THREAD_SIZE_M; i += 1){\n\t\t#pragma unroll\n\t\tfor(int j = 0; j < THREAD_SIZE_N; j += 1){\n\t\t\tg_data[(BLOCK_SIZE_M * blockIdx.x + THREAD_SIZE_M * t_M + i) * N + BLOCK_SIZE_N * blockIdx.y + THREAD_SIZE_N * t_N + j] = C_reg[i][j] + bias_local[j];\n\t\t}\n\t}\n}", "launch_config": {"dimBlock": [128, 1], "dimGrid": [64, 6]}}, "bert.encoder.layer.4.attention.self.key": {"code": "__global__ void MatMul_TILE_THREAD_GENERAL(float *input0, float *input1, float *input2, float *input3, float *output0) {\n    /*\n    g_vec: A [M, K]\n    g_mat_data: condense B [K_sparse, N]\n    g_mat_index: condense B index [K_sparse, N]\n    g_data: C [M, N]\n    */\n\tfloat *g_vec = input0;\n\tfloat *g_mat_data = input1;\n\tint *g_mat_index = (int*)input2;\n\tfloat *bias = input3;\n\tfloat *g_data = output0;\n\n\tconst int M = 4096;\n\tconst int K = 768;\n\tconst int N = 768;\n\n    const int BLOCK_SIZE_M = 64;\n    const int BLOCK_SIZE_N = 128;\n    const int BLOCK_SIZE_K = 32;\n    const int THREAD_SIZE_M = 8;\n    const int THREAD_SIZE_N = 8;\n\n    const float SPARSITY = 0.875;\n    const int BANK_VAL = 32;\n    const int ALIGN_N = THREAD_SIZE_N;\n\n    const int BANK_NUM_PER_BLOCK = BLOCK_SIZE_K / BANK_VAL;\n    const int BLOCK_SIZE_K_SPARSE = int(BLOCK_SIZE_K * (1-SPARSITY));\n    const int LEN_OF_BANK_PER_SPARSE_BLOCK = BLOCK_SIZE_K_SPARSE / BANK_NUM_PER_BLOCK;\n\n\tconst int K_SPARSE = int(K * SPARSITY);\n\tint M_BLOCK_START = blockIdx.x * BLOCK_SIZE_M;\n\tint N_BLOCK_START = blockIdx.y * BLOCK_SIZE_N;\n\n\tconst int A_THREADS_PER_ROW = BLOCK_SIZE_K / 4;\n\tconst int B_THREADS_PER_ROW = BLOCK_SIZE_N / 4;\n\n\tconst int THREADS_PER_BLOCK = (BLOCK_SIZE_M / THREAD_SIZE_M) * (BLOCK_SIZE_N / THREAD_SIZE_N);\n\n\tconst int A_STRIDES = THREADS_PER_BLOCK / A_THREADS_PER_ROW;\n\tconst int B_STRIDES = THREADS_PER_BLOCK / B_THREADS_PER_ROW;\n\n\t__shared__ float A_shared[BLOCK_SIZE_M * BLOCK_SIZE_K];\n\t__shared__ float B_shared[BLOCK_SIZE_N * BLOCK_SIZE_K_SPARSE];\n\t__shared__ int B_index_shared[BLOCK_SIZE_N * BLOCK_SIZE_K_SPARSE];\n\n\tfloat A_reg[THREAD_SIZE_M];\n\tfloat B_reg[THREAD_SIZE_N];\n\tint B_reg_index;\n\tfloat C_reg[THREAD_SIZE_M][THREAD_SIZE_N] = {0};\n\n\tint tid = threadIdx.x;\n\n\tint t_N = tid % (BLOCK_SIZE_N / THREAD_SIZE_N);\n\tint t_M = tid / (BLOCK_SIZE_N / THREAD_SIZE_N);\n\n\tint A_BLOCK_ROW_START = tid / A_THREADS_PER_ROW;\n\tint B_BLOCK_ROW_START = tid / B_THREADS_PER_ROW;\n\n\tint A_BLOCK_COL_START = tid % A_THREADS_PER_ROW * 4;\n\tint B_BLOCK_COL_START = tid % B_THREADS_PER_ROW * 4;\n\n\tfor(int K_BLOCK_START = 0, K_SPARSE_BLOCK_START = 0; K_BLOCK_START < K; K_BLOCK_START += BLOCK_SIZE_K, K_SPARSE_BLOCK_START += BLOCK_SIZE_K_SPARSE){\n\t\tfloat *A_global_ptr = g_vec + M_BLOCK_START * K + K_BLOCK_START;\n\t\tfloat *B_global_ptr = g_mat_data + K_SPARSE_BLOCK_START * N + N_BLOCK_START;\n\t\tint *B_index_global_ptr = g_mat_index + K_SPARSE_BLOCK_START * N + N_BLOCK_START;\n\n\t\t__syncthreads();\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_M; i += A_STRIDES){\n\t\t\t*(float4 *)(A_shared + (i + A_BLOCK_ROW_START) * BLOCK_SIZE_K + A_BLOCK_COL_START) = \n\t\t\t\t*(float4 *)(A_global_ptr + (i + A_BLOCK_ROW_START) * K + A_BLOCK_COL_START);\n\t\t}\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_K_SPARSE; i += B_STRIDES){\n\t\t\t*(float4 *)(B_shared + (i + B_BLOCK_ROW_START) * BLOCK_SIZE_N + B_BLOCK_COL_START) =\n\t\t\t\t*(float4 *)(B_global_ptr + (i + B_BLOCK_ROW_START) * N + B_BLOCK_COL_START);\n\n\t\t\t*(float4 *)(B_index_shared + (i + B_BLOCK_ROW_START) * BLOCK_SIZE_N + B_BLOCK_COL_START) =\n\t\t\t\t*(float4 *)(B_index_global_ptr + (i + B_BLOCK_ROW_START) * N + B_BLOCK_COL_START);\n\t\t}\n\n\t\t__syncthreads();\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_K_SPARSE;i += 1){\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_N; k += 1){\n\t\t\t\tB_reg[k] = B_shared[i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k];\n\t\t\t\t//*(float4 *)(B_reg + k) = *(float4 *)(B_shared + i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k);\n\t\t\t\t//*(float4 *)(B_reg_index + k) = *(float4 *)(B_index_shared + i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k);\n\t\t\t}\n\t\t\tint bank_idx = i / LEN_OF_BANK_PER_SPARSE_BLOCK;\n\t\t\tB_reg_index = B_index_shared[i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N] % BANK_VAL+bank_idx * BANK_VAL;\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_M; k += 1){\n\t\t\t\tA_reg[k] = A_shared[(t_M * THREAD_SIZE_M+k) * BLOCK_SIZE_K + B_reg_index];\n\t\t\t}\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_N; k += 1){\n\t\t\t\t#pragma unroll\n\t\t\t\tfor(int j = 0; j < THREAD_SIZE_M; j += 1){\n\t\t\t\t\tC_reg[j][k] += B_reg[k] * A_reg[j];\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n    float bias_local[THREAD_SIZE_N];\n    for(int thread_x = 0; thread_x < THREAD_SIZE_N; thread_x++){\n        bias_local[thread_x] = bias[BLOCK_SIZE_N * blockIdx.y + THREAD_SIZE_N * t_N + thread_x];\n    }\n\n\t#pragma unroll\n\tfor(int i = 0; i < THREAD_SIZE_M; i += 1){\n\t\t#pragma unroll\n\t\tfor(int j = 0; j < THREAD_SIZE_N; j += 1){\n\t\t\tg_data[(BLOCK_SIZE_M * blockIdx.x + THREAD_SIZE_M * t_M + i) * N + BLOCK_SIZE_N * blockIdx.y + THREAD_SIZE_N * t_N + j] = C_reg[i][j] + bias_local[j];\n\t\t}\n\t}\n}", "launch_config": {"dimBlock": [128, 1], "dimGrid": [64, 6]}}, "bert.encoder.layer.4.attention.self.value": {"code": "__global__ void MatMul_TILE_THREAD_GENERAL(float *input0, float *input1, float *input2, float *input3, float *output0) {\n    /*\n    g_vec: A [M, K]\n    g_mat_data: condense B [K_sparse, N]\n    g_mat_index: condense B index [K_sparse, N]\n    g_data: C [M, N]\n    */\n\tfloat *g_vec = input0;\n\tfloat *g_mat_data = input1;\n\tint *g_mat_index = (int*)input2;\n\tfloat *bias = input3;\n\tfloat *g_data = output0;\n\n\tconst int M = 4096;\n\tconst int K = 768;\n\tconst int N = 768;\n\n    const int BLOCK_SIZE_M = 64;\n    const int BLOCK_SIZE_N = 128;\n    const int BLOCK_SIZE_K = 32;\n    const int THREAD_SIZE_M = 8;\n    const int THREAD_SIZE_N = 8;\n\n    const float SPARSITY = 0.875;\n    const int BANK_VAL = 32;\n    const int ALIGN_N = THREAD_SIZE_N;\n\n    const int BANK_NUM_PER_BLOCK = BLOCK_SIZE_K / BANK_VAL;\n    const int BLOCK_SIZE_K_SPARSE = int(BLOCK_SIZE_K * (1-SPARSITY));\n    const int LEN_OF_BANK_PER_SPARSE_BLOCK = BLOCK_SIZE_K_SPARSE / BANK_NUM_PER_BLOCK;\n\n\tconst int K_SPARSE = int(K * SPARSITY);\n\tint M_BLOCK_START = blockIdx.x * BLOCK_SIZE_M;\n\tint N_BLOCK_START = blockIdx.y * BLOCK_SIZE_N;\n\n\tconst int A_THREADS_PER_ROW = BLOCK_SIZE_K / 4;\n\tconst int B_THREADS_PER_ROW = BLOCK_SIZE_N / 4;\n\n\tconst int THREADS_PER_BLOCK = (BLOCK_SIZE_M / THREAD_SIZE_M) * (BLOCK_SIZE_N / THREAD_SIZE_N);\n\n\tconst int A_STRIDES = THREADS_PER_BLOCK / A_THREADS_PER_ROW;\n\tconst int B_STRIDES = THREADS_PER_BLOCK / B_THREADS_PER_ROW;\n\n\t__shared__ float A_shared[BLOCK_SIZE_M * BLOCK_SIZE_K];\n\t__shared__ float B_shared[BLOCK_SIZE_N * BLOCK_SIZE_K_SPARSE];\n\t__shared__ int B_index_shared[BLOCK_SIZE_N * BLOCK_SIZE_K_SPARSE];\n\n\tfloat A_reg[THREAD_SIZE_M];\n\tfloat B_reg[THREAD_SIZE_N];\n\tint B_reg_index;\n\tfloat C_reg[THREAD_SIZE_M][THREAD_SIZE_N] = {0};\n\n\tint tid = threadIdx.x;\n\n\tint t_N = tid % (BLOCK_SIZE_N / THREAD_SIZE_N);\n\tint t_M = tid / (BLOCK_SIZE_N / THREAD_SIZE_N);\n\n\tint A_BLOCK_ROW_START = tid / A_THREADS_PER_ROW;\n\tint B_BLOCK_ROW_START = tid / B_THREADS_PER_ROW;\n\n\tint A_BLOCK_COL_START = tid % A_THREADS_PER_ROW * 4;\n\tint B_BLOCK_COL_START = tid % B_THREADS_PER_ROW * 4;\n\n\tfor(int K_BLOCK_START = 0, K_SPARSE_BLOCK_START = 0; K_BLOCK_START < K; K_BLOCK_START += BLOCK_SIZE_K, K_SPARSE_BLOCK_START += BLOCK_SIZE_K_SPARSE){\n\t\tfloat *A_global_ptr = g_vec + M_BLOCK_START * K + K_BLOCK_START;\n\t\tfloat *B_global_ptr = g_mat_data + K_SPARSE_BLOCK_START * N + N_BLOCK_START;\n\t\tint *B_index_global_ptr = g_mat_index + K_SPARSE_BLOCK_START * N + N_BLOCK_START;\n\n\t\t__syncthreads();\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_M; i += A_STRIDES){\n\t\t\t*(float4 *)(A_shared + (i + A_BLOCK_ROW_START) * BLOCK_SIZE_K + A_BLOCK_COL_START) = \n\t\t\t\t*(float4 *)(A_global_ptr + (i + A_BLOCK_ROW_START) * K + A_BLOCK_COL_START);\n\t\t}\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_K_SPARSE; i += B_STRIDES){\n\t\t\t*(float4 *)(B_shared + (i + B_BLOCK_ROW_START) * BLOCK_SIZE_N + B_BLOCK_COL_START) =\n\t\t\t\t*(float4 *)(B_global_ptr + (i + B_BLOCK_ROW_START) * N + B_BLOCK_COL_START);\n\n\t\t\t*(float4 *)(B_index_shared + (i + B_BLOCK_ROW_START) * BLOCK_SIZE_N + B_BLOCK_COL_START) =\n\t\t\t\t*(float4 *)(B_index_global_ptr + (i + B_BLOCK_ROW_START) * N + B_BLOCK_COL_START);\n\t\t}\n\n\t\t__syncthreads();\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_K_SPARSE;i += 1){\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_N; k += 1){\n\t\t\t\tB_reg[k] = B_shared[i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k];\n\t\t\t\t//*(float4 *)(B_reg + k) = *(float4 *)(B_shared + i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k);\n\t\t\t\t//*(float4 *)(B_reg_index + k) = *(float4 *)(B_index_shared + i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k);\n\t\t\t}\n\t\t\tint bank_idx = i / LEN_OF_BANK_PER_SPARSE_BLOCK;\n\t\t\tB_reg_index = B_index_shared[i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N] % BANK_VAL+bank_idx * BANK_VAL;\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_M; k += 1){\n\t\t\t\tA_reg[k] = A_shared[(t_M * THREAD_SIZE_M+k) * BLOCK_SIZE_K + B_reg_index];\n\t\t\t}\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_N; k += 1){\n\t\t\t\t#pragma unroll\n\t\t\t\tfor(int j = 0; j < THREAD_SIZE_M; j += 1){\n\t\t\t\t\tC_reg[j][k] += B_reg[k] * A_reg[j];\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n    float bias_local[THREAD_SIZE_N];\n    for(int thread_x = 0; thread_x < THREAD_SIZE_N; thread_x++){\n        bias_local[thread_x] = bias[BLOCK_SIZE_N * blockIdx.y + THREAD_SIZE_N * t_N + thread_x];\n    }\n\n\t#pragma unroll\n\tfor(int i = 0; i < THREAD_SIZE_M; i += 1){\n\t\t#pragma unroll\n\t\tfor(int j = 0; j < THREAD_SIZE_N; j += 1){\n\t\t\tg_data[(BLOCK_SIZE_M * blockIdx.x + THREAD_SIZE_M * t_M + i) * N + BLOCK_SIZE_N * blockIdx.y + THREAD_SIZE_N * t_N + j] = C_reg[i][j] + bias_local[j];\n\t\t}\n\t}\n}", "launch_config": {"dimBlock": [128, 1], "dimGrid": [64, 6]}}, "bert.encoder.layer.4.attention.output.dense": {"code": "__global__ void MatMul_TILE_THREAD_GENERAL(float *input0, float *input1, float *input2, float *input3, float *output0) {\n    /*\n    g_vec: A [M, K]\n    g_mat_data: condense B [K_sparse, N]\n    g_mat_index: condense B index [K_sparse, N]\n    g_data: C [M, N]\n    */\n\tfloat *g_vec = input0;\n\tfloat *g_mat_data = input1;\n\tint *g_mat_index = (int*)input2;\n\tfloat *bias = input3;\n\tfloat *g_data = output0;\n\n\tconst int M = 4096;\n\tconst int K = 768;\n\tconst int N = 768;\n\n    const int BLOCK_SIZE_M = 64;\n    const int BLOCK_SIZE_N = 128;\n    const int BLOCK_SIZE_K = 32;\n    const int THREAD_SIZE_M = 8;\n    const int THREAD_SIZE_N = 8;\n\n    const float SPARSITY = 0.875;\n    const int BANK_VAL = 32;\n    const int ALIGN_N = THREAD_SIZE_N;\n\n    const int BANK_NUM_PER_BLOCK = BLOCK_SIZE_K / BANK_VAL;\n    const int BLOCK_SIZE_K_SPARSE = int(BLOCK_SIZE_K * (1-SPARSITY));\n    const int LEN_OF_BANK_PER_SPARSE_BLOCK = BLOCK_SIZE_K_SPARSE / BANK_NUM_PER_BLOCK;\n\n\tconst int K_SPARSE = int(K * SPARSITY);\n\tint M_BLOCK_START = blockIdx.x * BLOCK_SIZE_M;\n\tint N_BLOCK_START = blockIdx.y * BLOCK_SIZE_N;\n\n\tconst int A_THREADS_PER_ROW = BLOCK_SIZE_K / 4;\n\tconst int B_THREADS_PER_ROW = BLOCK_SIZE_N / 4;\n\n\tconst int THREADS_PER_BLOCK = (BLOCK_SIZE_M / THREAD_SIZE_M) * (BLOCK_SIZE_N / THREAD_SIZE_N);\n\n\tconst int A_STRIDES = THREADS_PER_BLOCK / A_THREADS_PER_ROW;\n\tconst int B_STRIDES = THREADS_PER_BLOCK / B_THREADS_PER_ROW;\n\n\t__shared__ float A_shared[BLOCK_SIZE_M * BLOCK_SIZE_K];\n\t__shared__ float B_shared[BLOCK_SIZE_N * BLOCK_SIZE_K_SPARSE];\n\t__shared__ int B_index_shared[BLOCK_SIZE_N * BLOCK_SIZE_K_SPARSE];\n\n\tfloat A_reg[THREAD_SIZE_M];\n\tfloat B_reg[THREAD_SIZE_N];\n\tint B_reg_index;\n\tfloat C_reg[THREAD_SIZE_M][THREAD_SIZE_N] = {0};\n\n\tint tid = threadIdx.x;\n\n\tint t_N = tid % (BLOCK_SIZE_N / THREAD_SIZE_N);\n\tint t_M = tid / (BLOCK_SIZE_N / THREAD_SIZE_N);\n\n\tint A_BLOCK_ROW_START = tid / A_THREADS_PER_ROW;\n\tint B_BLOCK_ROW_START = tid / B_THREADS_PER_ROW;\n\n\tint A_BLOCK_COL_START = tid % A_THREADS_PER_ROW * 4;\n\tint B_BLOCK_COL_START = tid % B_THREADS_PER_ROW * 4;\n\n\tfor(int K_BLOCK_START = 0, K_SPARSE_BLOCK_START = 0; K_BLOCK_START < K; K_BLOCK_START += BLOCK_SIZE_K, K_SPARSE_BLOCK_START += BLOCK_SIZE_K_SPARSE){\n\t\tfloat *A_global_ptr = g_vec + M_BLOCK_START * K + K_BLOCK_START;\n\t\tfloat *B_global_ptr = g_mat_data + K_SPARSE_BLOCK_START * N + N_BLOCK_START;\n\t\tint *B_index_global_ptr = g_mat_index + K_SPARSE_BLOCK_START * N + N_BLOCK_START;\n\n\t\t__syncthreads();\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_M; i += A_STRIDES){\n\t\t\t*(float4 *)(A_shared + (i + A_BLOCK_ROW_START) * BLOCK_SIZE_K + A_BLOCK_COL_START) = \n\t\t\t\t*(float4 *)(A_global_ptr + (i + A_BLOCK_ROW_START) * K + A_BLOCK_COL_START);\n\t\t}\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_K_SPARSE; i += B_STRIDES){\n\t\t\t*(float4 *)(B_shared + (i + B_BLOCK_ROW_START) * BLOCK_SIZE_N + B_BLOCK_COL_START) =\n\t\t\t\t*(float4 *)(B_global_ptr + (i + B_BLOCK_ROW_START) * N + B_BLOCK_COL_START);\n\n\t\t\t*(float4 *)(B_index_shared + (i + B_BLOCK_ROW_START) * BLOCK_SIZE_N + B_BLOCK_COL_START) =\n\t\t\t\t*(float4 *)(B_index_global_ptr + (i + B_BLOCK_ROW_START) * N + B_BLOCK_COL_START);\n\t\t}\n\n\t\t__syncthreads();\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_K_SPARSE;i += 1){\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_N; k += 1){\n\t\t\t\tB_reg[k] = B_shared[i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k];\n\t\t\t\t//*(float4 *)(B_reg + k) = *(float4 *)(B_shared + i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k);\n\t\t\t\t//*(float4 *)(B_reg_index + k) = *(float4 *)(B_index_shared + i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k);\n\t\t\t}\n\t\t\tint bank_idx = i / LEN_OF_BANK_PER_SPARSE_BLOCK;\n\t\t\tB_reg_index = B_index_shared[i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N] % BANK_VAL+bank_idx * BANK_VAL;\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_M; k += 1){\n\t\t\t\tA_reg[k] = A_shared[(t_M * THREAD_SIZE_M+k) * BLOCK_SIZE_K + B_reg_index];\n\t\t\t}\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_N; k += 1){\n\t\t\t\t#pragma unroll\n\t\t\t\tfor(int j = 0; j < THREAD_SIZE_M; j += 1){\n\t\t\t\t\tC_reg[j][k] += B_reg[k] * A_reg[j];\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n    float bias_local[THREAD_SIZE_N];\n    for(int thread_x = 0; thread_x < THREAD_SIZE_N; thread_x++){\n        bias_local[thread_x] = bias[BLOCK_SIZE_N * blockIdx.y + THREAD_SIZE_N * t_N + thread_x];\n    }\n\n\t#pragma unroll\n\tfor(int i = 0; i < THREAD_SIZE_M; i += 1){\n\t\t#pragma unroll\n\t\tfor(int j = 0; j < THREAD_SIZE_N; j += 1){\n\t\t\tg_data[(BLOCK_SIZE_M * blockIdx.x + THREAD_SIZE_M * t_M + i) * N + BLOCK_SIZE_N * blockIdx.y + THREAD_SIZE_N * t_N + j] = C_reg[i][j] + bias_local[j];\n\t\t}\n\t}\n}", "launch_config": {"dimBlock": [128, 1], "dimGrid": [64, 6]}}, "bert.encoder.layer.4.intermediate.dense": {"code": "__global__ void MatMul_TILE_THREAD_GENERAL(float *input0, float *input1, float *input2, float *input3, float *output0) {\n    /*\n    g_vec: A [M, K]\n    g_mat_data: condense B [K_sparse, N]\n    g_mat_index: condense B index [K_sparse, N]\n    g_data: C [M, N]\n    */\n\tfloat *g_vec = input0;\n\tfloat *g_mat_data = input1;\n\tint *g_mat_index = (int*)input2;\n\tfloat *bias = input3;\n\tfloat *g_data = output0;\n\n\tconst int M = 4096;\n\tconst int K = 768;\n\tconst int N = 3072;\n\n    const int BLOCK_SIZE_M = 64;\n    const int BLOCK_SIZE_N = 128;\n    const int BLOCK_SIZE_K = 32;\n    const int THREAD_SIZE_M = 8;\n    const int THREAD_SIZE_N = 8;\n\n    const float SPARSITY = 0.875;\n    const int BANK_VAL = 32;\n    const int ALIGN_N = THREAD_SIZE_N;\n\n    const int BANK_NUM_PER_BLOCK = BLOCK_SIZE_K / BANK_VAL;\n    const int BLOCK_SIZE_K_SPARSE = int(BLOCK_SIZE_K * (1-SPARSITY));\n    const int LEN_OF_BANK_PER_SPARSE_BLOCK = BLOCK_SIZE_K_SPARSE / BANK_NUM_PER_BLOCK;\n\n\tconst int K_SPARSE = int(K * SPARSITY);\n\tint M_BLOCK_START = blockIdx.x * BLOCK_SIZE_M;\n\tint N_BLOCK_START = blockIdx.y * BLOCK_SIZE_N;\n\n\tconst int A_THREADS_PER_ROW = BLOCK_SIZE_K / 4;\n\tconst int B_THREADS_PER_ROW = BLOCK_SIZE_N / 4;\n\n\tconst int THREADS_PER_BLOCK = (BLOCK_SIZE_M / THREAD_SIZE_M) * (BLOCK_SIZE_N / THREAD_SIZE_N);\n\n\tconst int A_STRIDES = THREADS_PER_BLOCK / A_THREADS_PER_ROW;\n\tconst int B_STRIDES = THREADS_PER_BLOCK / B_THREADS_PER_ROW;\n\n\t__shared__ float A_shared[BLOCK_SIZE_M * BLOCK_SIZE_K];\n\t__shared__ float B_shared[BLOCK_SIZE_N * BLOCK_SIZE_K_SPARSE];\n\t__shared__ int B_index_shared[BLOCK_SIZE_N * BLOCK_SIZE_K_SPARSE];\n\n\tfloat A_reg[THREAD_SIZE_M];\n\tfloat B_reg[THREAD_SIZE_N];\n\tint B_reg_index;\n\tfloat C_reg[THREAD_SIZE_M][THREAD_SIZE_N] = {0};\n\n\tint tid = threadIdx.x;\n\n\tint t_N = tid % (BLOCK_SIZE_N / THREAD_SIZE_N);\n\tint t_M = tid / (BLOCK_SIZE_N / THREAD_SIZE_N);\n\n\tint A_BLOCK_ROW_START = tid / A_THREADS_PER_ROW;\n\tint B_BLOCK_ROW_START = tid / B_THREADS_PER_ROW;\n\n\tint A_BLOCK_COL_START = tid % A_THREADS_PER_ROW * 4;\n\tint B_BLOCK_COL_START = tid % B_THREADS_PER_ROW * 4;\n\n\tfor(int K_BLOCK_START = 0, K_SPARSE_BLOCK_START = 0; K_BLOCK_START < K; K_BLOCK_START += BLOCK_SIZE_K, K_SPARSE_BLOCK_START += BLOCK_SIZE_K_SPARSE){\n\t\tfloat *A_global_ptr = g_vec + M_BLOCK_START * K + K_BLOCK_START;\n\t\tfloat *B_global_ptr = g_mat_data + K_SPARSE_BLOCK_START * N + N_BLOCK_START;\n\t\tint *B_index_global_ptr = g_mat_index + K_SPARSE_BLOCK_START * N + N_BLOCK_START;\n\n\t\t__syncthreads();\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_M; i += A_STRIDES){\n\t\t\t*(float4 *)(A_shared + (i + A_BLOCK_ROW_START) * BLOCK_SIZE_K + A_BLOCK_COL_START) = \n\t\t\t\t*(float4 *)(A_global_ptr + (i + A_BLOCK_ROW_START) * K + A_BLOCK_COL_START);\n\t\t}\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_K_SPARSE; i += B_STRIDES){\n\t\t\t*(float4 *)(B_shared + (i + B_BLOCK_ROW_START) * BLOCK_SIZE_N + B_BLOCK_COL_START) =\n\t\t\t\t*(float4 *)(B_global_ptr + (i + B_BLOCK_ROW_START) * N + B_BLOCK_COL_START);\n\n\t\t\t*(float4 *)(B_index_shared + (i + B_BLOCK_ROW_START) * BLOCK_SIZE_N + B_BLOCK_COL_START) =\n\t\t\t\t*(float4 *)(B_index_global_ptr + (i + B_BLOCK_ROW_START) * N + B_BLOCK_COL_START);\n\t\t}\n\n\t\t__syncthreads();\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_K_SPARSE;i += 1){\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_N; k += 1){\n\t\t\t\tB_reg[k] = B_shared[i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k];\n\t\t\t\t//*(float4 *)(B_reg + k) = *(float4 *)(B_shared + i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k);\n\t\t\t\t//*(float4 *)(B_reg_index + k) = *(float4 *)(B_index_shared + i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k);\n\t\t\t}\n\t\t\tint bank_idx = i / LEN_OF_BANK_PER_SPARSE_BLOCK;\n\t\t\tB_reg_index = B_index_shared[i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N] % BANK_VAL+bank_idx * BANK_VAL;\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_M; k += 1){\n\t\t\t\tA_reg[k] = A_shared[(t_M * THREAD_SIZE_M+k) * BLOCK_SIZE_K + B_reg_index];\n\t\t\t}\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_N; k += 1){\n\t\t\t\t#pragma unroll\n\t\t\t\tfor(int j = 0; j < THREAD_SIZE_M; j += 1){\n\t\t\t\t\tC_reg[j][k] += B_reg[k] * A_reg[j];\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n    float bias_local[THREAD_SIZE_N];\n    for(int thread_x = 0; thread_x < THREAD_SIZE_N; thread_x++){\n        bias_local[thread_x] = bias[BLOCK_SIZE_N * blockIdx.y + THREAD_SIZE_N * t_N + thread_x];\n    }\n\n\t#pragma unroll\n\tfor(int i = 0; i < THREAD_SIZE_M; i += 1){\n\t\t#pragma unroll\n\t\tfor(int j = 0; j < THREAD_SIZE_N; j += 1){\n\t\t\tg_data[(BLOCK_SIZE_M * blockIdx.x + THREAD_SIZE_M * t_M + i) * N + BLOCK_SIZE_N * blockIdx.y + THREAD_SIZE_N * t_N + j] = C_reg[i][j] + bias_local[j];\n\t\t}\n\t}\n}", "launch_config": {"dimBlock": [128, 1], "dimGrid": [64, 24]}}, "bert.encoder.layer.4.output.dense": {"code": "__global__ void MatMul_TILE_THREAD_GENERAL(float *input0, float *input1, float *input2, float *input3, float *output0) {\n    /*\n    g_vec: A [M, K]\n    g_mat_data: condense B [K_sparse, N]\n    g_mat_index: condense B index [K_sparse, N]\n    g_data: C [M, N]\n    */\n\tfloat *g_vec = input0;\n\tfloat *g_mat_data = input1;\n\tint *g_mat_index = (int*)input2;\n\tfloat *bias = input3;\n\tfloat *g_data = output0;\n\n\tconst int M = 4096;\n\tconst int K = 3072;\n\tconst int N = 768;\n\n    const int BLOCK_SIZE_M = 64;\n    const int BLOCK_SIZE_N = 128;\n    const int BLOCK_SIZE_K = 32;\n    const int THREAD_SIZE_M = 8;\n    const int THREAD_SIZE_N = 8;\n\n    const float SPARSITY = 0.875;\n    const int BANK_VAL = 32;\n    const int ALIGN_N = THREAD_SIZE_N;\n\n    const int BANK_NUM_PER_BLOCK = BLOCK_SIZE_K / BANK_VAL;\n    const int BLOCK_SIZE_K_SPARSE = int(BLOCK_SIZE_K * (1-SPARSITY));\n    const int LEN_OF_BANK_PER_SPARSE_BLOCK = BLOCK_SIZE_K_SPARSE / BANK_NUM_PER_BLOCK;\n\n\tconst int K_SPARSE = int(K * SPARSITY);\n\tint M_BLOCK_START = blockIdx.x * BLOCK_SIZE_M;\n\tint N_BLOCK_START = blockIdx.y * BLOCK_SIZE_N;\n\n\tconst int A_THREADS_PER_ROW = BLOCK_SIZE_K / 4;\n\tconst int B_THREADS_PER_ROW = BLOCK_SIZE_N / 4;\n\n\tconst int THREADS_PER_BLOCK = (BLOCK_SIZE_M / THREAD_SIZE_M) * (BLOCK_SIZE_N / THREAD_SIZE_N);\n\n\tconst int A_STRIDES = THREADS_PER_BLOCK / A_THREADS_PER_ROW;\n\tconst int B_STRIDES = THREADS_PER_BLOCK / B_THREADS_PER_ROW;\n\n\t__shared__ float A_shared[BLOCK_SIZE_M * BLOCK_SIZE_K];\n\t__shared__ float B_shared[BLOCK_SIZE_N * BLOCK_SIZE_K_SPARSE];\n\t__shared__ int B_index_shared[BLOCK_SIZE_N * BLOCK_SIZE_K_SPARSE];\n\n\tfloat A_reg[THREAD_SIZE_M];\n\tfloat B_reg[THREAD_SIZE_N];\n\tint B_reg_index;\n\tfloat C_reg[THREAD_SIZE_M][THREAD_SIZE_N] = {0};\n\n\tint tid = threadIdx.x;\n\n\tint t_N = tid % (BLOCK_SIZE_N / THREAD_SIZE_N);\n\tint t_M = tid / (BLOCK_SIZE_N / THREAD_SIZE_N);\n\n\tint A_BLOCK_ROW_START = tid / A_THREADS_PER_ROW;\n\tint B_BLOCK_ROW_START = tid / B_THREADS_PER_ROW;\n\n\tint A_BLOCK_COL_START = tid % A_THREADS_PER_ROW * 4;\n\tint B_BLOCK_COL_START = tid % B_THREADS_PER_ROW * 4;\n\n\tfor(int K_BLOCK_START = 0, K_SPARSE_BLOCK_START = 0; K_BLOCK_START < K; K_BLOCK_START += BLOCK_SIZE_K, K_SPARSE_BLOCK_START += BLOCK_SIZE_K_SPARSE){\n\t\tfloat *A_global_ptr = g_vec + M_BLOCK_START * K + K_BLOCK_START;\n\t\tfloat *B_global_ptr = g_mat_data + K_SPARSE_BLOCK_START * N + N_BLOCK_START;\n\t\tint *B_index_global_ptr = g_mat_index + K_SPARSE_BLOCK_START * N + N_BLOCK_START;\n\n\t\t__syncthreads();\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_M; i += A_STRIDES){\n\t\t\t*(float4 *)(A_shared + (i + A_BLOCK_ROW_START) * BLOCK_SIZE_K + A_BLOCK_COL_START) = \n\t\t\t\t*(float4 *)(A_global_ptr + (i + A_BLOCK_ROW_START) * K + A_BLOCK_COL_START);\n\t\t}\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_K_SPARSE; i += B_STRIDES){\n\t\t\t*(float4 *)(B_shared + (i + B_BLOCK_ROW_START) * BLOCK_SIZE_N + B_BLOCK_COL_START) =\n\t\t\t\t*(float4 *)(B_global_ptr + (i + B_BLOCK_ROW_START) * N + B_BLOCK_COL_START);\n\n\t\t\t*(float4 *)(B_index_shared + (i + B_BLOCK_ROW_START) * BLOCK_SIZE_N + B_BLOCK_COL_START) =\n\t\t\t\t*(float4 *)(B_index_global_ptr + (i + B_BLOCK_ROW_START) * N + B_BLOCK_COL_START);\n\t\t}\n\n\t\t__syncthreads();\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_K_SPARSE;i += 1){\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_N; k += 1){\n\t\t\t\tB_reg[k] = B_shared[i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k];\n\t\t\t\t//*(float4 *)(B_reg + k) = *(float4 *)(B_shared + i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k);\n\t\t\t\t//*(float4 *)(B_reg_index + k) = *(float4 *)(B_index_shared + i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k);\n\t\t\t}\n\t\t\tint bank_idx = i / LEN_OF_BANK_PER_SPARSE_BLOCK;\n\t\t\tB_reg_index = B_index_shared[i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N] % BANK_VAL+bank_idx * BANK_VAL;\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_M; k += 1){\n\t\t\t\tA_reg[k] = A_shared[(t_M * THREAD_SIZE_M+k) * BLOCK_SIZE_K + B_reg_index];\n\t\t\t}\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_N; k += 1){\n\t\t\t\t#pragma unroll\n\t\t\t\tfor(int j = 0; j < THREAD_SIZE_M; j += 1){\n\t\t\t\t\tC_reg[j][k] += B_reg[k] * A_reg[j];\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n    float bias_local[THREAD_SIZE_N];\n    for(int thread_x = 0; thread_x < THREAD_SIZE_N; thread_x++){\n        bias_local[thread_x] = bias[BLOCK_SIZE_N * blockIdx.y + THREAD_SIZE_N * t_N + thread_x];\n    }\n\n\t#pragma unroll\n\tfor(int i = 0; i < THREAD_SIZE_M; i += 1){\n\t\t#pragma unroll\n\t\tfor(int j = 0; j < THREAD_SIZE_N; j += 1){\n\t\t\tg_data[(BLOCK_SIZE_M * blockIdx.x + THREAD_SIZE_M * t_M + i) * N + BLOCK_SIZE_N * blockIdx.y + THREAD_SIZE_N * t_N + j] = C_reg[i][j] + bias_local[j];\n\t\t}\n\t}\n}", "launch_config": {"dimBlock": [128, 1], "dimGrid": [64, 6]}}, "bert.encoder.layer.5.attention.self.query": {"code": "__global__ void MatMul_TILE_THREAD_GENERAL(float *input0, float *input1, float *input2, float *input3, float *output0) {\n    /*\n    g_vec: A [M, K]\n    g_mat_data: condense B [K_sparse, N]\n    g_mat_index: condense B index [K_sparse, N]\n    g_data: C [M, N]\n    */\n\tfloat *g_vec = input0;\n\tfloat *g_mat_data = input1;\n\tint *g_mat_index = (int*)input2;\n\tfloat *bias = input3;\n\tfloat *g_data = output0;\n\n\tconst int M = 4096;\n\tconst int K = 768;\n\tconst int N = 768;\n\n    const int BLOCK_SIZE_M = 64;\n    const int BLOCK_SIZE_N = 128;\n    const int BLOCK_SIZE_K = 32;\n    const int THREAD_SIZE_M = 8;\n    const int THREAD_SIZE_N = 8;\n\n    const float SPARSITY = 0.875;\n    const int BANK_VAL = 32;\n    const int ALIGN_N = THREAD_SIZE_N;\n\n    const int BANK_NUM_PER_BLOCK = BLOCK_SIZE_K / BANK_VAL;\n    const int BLOCK_SIZE_K_SPARSE = int(BLOCK_SIZE_K * (1-SPARSITY));\n    const int LEN_OF_BANK_PER_SPARSE_BLOCK = BLOCK_SIZE_K_SPARSE / BANK_NUM_PER_BLOCK;\n\n\tconst int K_SPARSE = int(K * SPARSITY);\n\tint M_BLOCK_START = blockIdx.x * BLOCK_SIZE_M;\n\tint N_BLOCK_START = blockIdx.y * BLOCK_SIZE_N;\n\n\tconst int A_THREADS_PER_ROW = BLOCK_SIZE_K / 4;\n\tconst int B_THREADS_PER_ROW = BLOCK_SIZE_N / 4;\n\n\tconst int THREADS_PER_BLOCK = (BLOCK_SIZE_M / THREAD_SIZE_M) * (BLOCK_SIZE_N / THREAD_SIZE_N);\n\n\tconst int A_STRIDES = THREADS_PER_BLOCK / A_THREADS_PER_ROW;\n\tconst int B_STRIDES = THREADS_PER_BLOCK / B_THREADS_PER_ROW;\n\n\t__shared__ float A_shared[BLOCK_SIZE_M * BLOCK_SIZE_K];\n\t__shared__ float B_shared[BLOCK_SIZE_N * BLOCK_SIZE_K_SPARSE];\n\t__shared__ int B_index_shared[BLOCK_SIZE_N * BLOCK_SIZE_K_SPARSE];\n\n\tfloat A_reg[THREAD_SIZE_M];\n\tfloat B_reg[THREAD_SIZE_N];\n\tint B_reg_index;\n\tfloat C_reg[THREAD_SIZE_M][THREAD_SIZE_N] = {0};\n\n\tint tid = threadIdx.x;\n\n\tint t_N = tid % (BLOCK_SIZE_N / THREAD_SIZE_N);\n\tint t_M = tid / (BLOCK_SIZE_N / THREAD_SIZE_N);\n\n\tint A_BLOCK_ROW_START = tid / A_THREADS_PER_ROW;\n\tint B_BLOCK_ROW_START = tid / B_THREADS_PER_ROW;\n\n\tint A_BLOCK_COL_START = tid % A_THREADS_PER_ROW * 4;\n\tint B_BLOCK_COL_START = tid % B_THREADS_PER_ROW * 4;\n\n\tfor(int K_BLOCK_START = 0, K_SPARSE_BLOCK_START = 0; K_BLOCK_START < K; K_BLOCK_START += BLOCK_SIZE_K, K_SPARSE_BLOCK_START += BLOCK_SIZE_K_SPARSE){\n\t\tfloat *A_global_ptr = g_vec + M_BLOCK_START * K + K_BLOCK_START;\n\t\tfloat *B_global_ptr = g_mat_data + K_SPARSE_BLOCK_START * N + N_BLOCK_START;\n\t\tint *B_index_global_ptr = g_mat_index + K_SPARSE_BLOCK_START * N + N_BLOCK_START;\n\n\t\t__syncthreads();\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_M; i += A_STRIDES){\n\t\t\t*(float4 *)(A_shared + (i + A_BLOCK_ROW_START) * BLOCK_SIZE_K + A_BLOCK_COL_START) = \n\t\t\t\t*(float4 *)(A_global_ptr + (i + A_BLOCK_ROW_START) * K + A_BLOCK_COL_START);\n\t\t}\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_K_SPARSE; i += B_STRIDES){\n\t\t\t*(float4 *)(B_shared + (i + B_BLOCK_ROW_START) * BLOCK_SIZE_N + B_BLOCK_COL_START) =\n\t\t\t\t*(float4 *)(B_global_ptr + (i + B_BLOCK_ROW_START) * N + B_BLOCK_COL_START);\n\n\t\t\t*(float4 *)(B_index_shared + (i + B_BLOCK_ROW_START) * BLOCK_SIZE_N + B_BLOCK_COL_START) =\n\t\t\t\t*(float4 *)(B_index_global_ptr + (i + B_BLOCK_ROW_START) * N + B_BLOCK_COL_START);\n\t\t}\n\n\t\t__syncthreads();\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_K_SPARSE;i += 1){\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_N; k += 1){\n\t\t\t\tB_reg[k] = B_shared[i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k];\n\t\t\t\t//*(float4 *)(B_reg + k) = *(float4 *)(B_shared + i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k);\n\t\t\t\t//*(float4 *)(B_reg_index + k) = *(float4 *)(B_index_shared + i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k);\n\t\t\t}\n\t\t\tint bank_idx = i / LEN_OF_BANK_PER_SPARSE_BLOCK;\n\t\t\tB_reg_index = B_index_shared[i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N] % BANK_VAL+bank_idx * BANK_VAL;\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_M; k += 1){\n\t\t\t\tA_reg[k] = A_shared[(t_M * THREAD_SIZE_M+k) * BLOCK_SIZE_K + B_reg_index];\n\t\t\t}\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_N; k += 1){\n\t\t\t\t#pragma unroll\n\t\t\t\tfor(int j = 0; j < THREAD_SIZE_M; j += 1){\n\t\t\t\t\tC_reg[j][k] += B_reg[k] * A_reg[j];\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n    float bias_local[THREAD_SIZE_N];\n    for(int thread_x = 0; thread_x < THREAD_SIZE_N; thread_x++){\n        bias_local[thread_x] = bias[BLOCK_SIZE_N * blockIdx.y + THREAD_SIZE_N * t_N + thread_x];\n    }\n\n\t#pragma unroll\n\tfor(int i = 0; i < THREAD_SIZE_M; i += 1){\n\t\t#pragma unroll\n\t\tfor(int j = 0; j < THREAD_SIZE_N; j += 1){\n\t\t\tg_data[(BLOCK_SIZE_M * blockIdx.x + THREAD_SIZE_M * t_M + i) * N + BLOCK_SIZE_N * blockIdx.y + THREAD_SIZE_N * t_N + j] = C_reg[i][j] + bias_local[j];\n\t\t}\n\t}\n}", "launch_config": {"dimBlock": [128, 1], "dimGrid": [64, 6]}}, "bert.encoder.layer.5.attention.self.key": {"code": "__global__ void MatMul_TILE_THREAD_GENERAL(float *input0, float *input1, float *input2, float *input3, float *output0) {\n    /*\n    g_vec: A [M, K]\n    g_mat_data: condense B [K_sparse, N]\n    g_mat_index: condense B index [K_sparse, N]\n    g_data: C [M, N]\n    */\n\tfloat *g_vec = input0;\n\tfloat *g_mat_data = input1;\n\tint *g_mat_index = (int*)input2;\n\tfloat *bias = input3;\n\tfloat *g_data = output0;\n\n\tconst int M = 4096;\n\tconst int K = 768;\n\tconst int N = 768;\n\n    const int BLOCK_SIZE_M = 64;\n    const int BLOCK_SIZE_N = 128;\n    const int BLOCK_SIZE_K = 32;\n    const int THREAD_SIZE_M = 8;\n    const int THREAD_SIZE_N = 8;\n\n    const float SPARSITY = 0.875;\n    const int BANK_VAL = 32;\n    const int ALIGN_N = THREAD_SIZE_N;\n\n    const int BANK_NUM_PER_BLOCK = BLOCK_SIZE_K / BANK_VAL;\n    const int BLOCK_SIZE_K_SPARSE = int(BLOCK_SIZE_K * (1-SPARSITY));\n    const int LEN_OF_BANK_PER_SPARSE_BLOCK = BLOCK_SIZE_K_SPARSE / BANK_NUM_PER_BLOCK;\n\n\tconst int K_SPARSE = int(K * SPARSITY);\n\tint M_BLOCK_START = blockIdx.x * BLOCK_SIZE_M;\n\tint N_BLOCK_START = blockIdx.y * BLOCK_SIZE_N;\n\n\tconst int A_THREADS_PER_ROW = BLOCK_SIZE_K / 4;\n\tconst int B_THREADS_PER_ROW = BLOCK_SIZE_N / 4;\n\n\tconst int THREADS_PER_BLOCK = (BLOCK_SIZE_M / THREAD_SIZE_M) * (BLOCK_SIZE_N / THREAD_SIZE_N);\n\n\tconst int A_STRIDES = THREADS_PER_BLOCK / A_THREADS_PER_ROW;\n\tconst int B_STRIDES = THREADS_PER_BLOCK / B_THREADS_PER_ROW;\n\n\t__shared__ float A_shared[BLOCK_SIZE_M * BLOCK_SIZE_K];\n\t__shared__ float B_shared[BLOCK_SIZE_N * BLOCK_SIZE_K_SPARSE];\n\t__shared__ int B_index_shared[BLOCK_SIZE_N * BLOCK_SIZE_K_SPARSE];\n\n\tfloat A_reg[THREAD_SIZE_M];\n\tfloat B_reg[THREAD_SIZE_N];\n\tint B_reg_index;\n\tfloat C_reg[THREAD_SIZE_M][THREAD_SIZE_N] = {0};\n\n\tint tid = threadIdx.x;\n\n\tint t_N = tid % (BLOCK_SIZE_N / THREAD_SIZE_N);\n\tint t_M = tid / (BLOCK_SIZE_N / THREAD_SIZE_N);\n\n\tint A_BLOCK_ROW_START = tid / A_THREADS_PER_ROW;\n\tint B_BLOCK_ROW_START = tid / B_THREADS_PER_ROW;\n\n\tint A_BLOCK_COL_START = tid % A_THREADS_PER_ROW * 4;\n\tint B_BLOCK_COL_START = tid % B_THREADS_PER_ROW * 4;\n\n\tfor(int K_BLOCK_START = 0, K_SPARSE_BLOCK_START = 0; K_BLOCK_START < K; K_BLOCK_START += BLOCK_SIZE_K, K_SPARSE_BLOCK_START += BLOCK_SIZE_K_SPARSE){\n\t\tfloat *A_global_ptr = g_vec + M_BLOCK_START * K + K_BLOCK_START;\n\t\tfloat *B_global_ptr = g_mat_data + K_SPARSE_BLOCK_START * N + N_BLOCK_START;\n\t\tint *B_index_global_ptr = g_mat_index + K_SPARSE_BLOCK_START * N + N_BLOCK_START;\n\n\t\t__syncthreads();\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_M; i += A_STRIDES){\n\t\t\t*(float4 *)(A_shared + (i + A_BLOCK_ROW_START) * BLOCK_SIZE_K + A_BLOCK_COL_START) = \n\t\t\t\t*(float4 *)(A_global_ptr + (i + A_BLOCK_ROW_START) * K + A_BLOCK_COL_START);\n\t\t}\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_K_SPARSE; i += B_STRIDES){\n\t\t\t*(float4 *)(B_shared + (i + B_BLOCK_ROW_START) * BLOCK_SIZE_N + B_BLOCK_COL_START) =\n\t\t\t\t*(float4 *)(B_global_ptr + (i + B_BLOCK_ROW_START) * N + B_BLOCK_COL_START);\n\n\t\t\t*(float4 *)(B_index_shared + (i + B_BLOCK_ROW_START) * BLOCK_SIZE_N + B_BLOCK_COL_START) =\n\t\t\t\t*(float4 *)(B_index_global_ptr + (i + B_BLOCK_ROW_START) * N + B_BLOCK_COL_START);\n\t\t}\n\n\t\t__syncthreads();\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_K_SPARSE;i += 1){\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_N; k += 1){\n\t\t\t\tB_reg[k] = B_shared[i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k];\n\t\t\t\t//*(float4 *)(B_reg + k) = *(float4 *)(B_shared + i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k);\n\t\t\t\t//*(float4 *)(B_reg_index + k) = *(float4 *)(B_index_shared + i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k);\n\t\t\t}\n\t\t\tint bank_idx = i / LEN_OF_BANK_PER_SPARSE_BLOCK;\n\t\t\tB_reg_index = B_index_shared[i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N] % BANK_VAL+bank_idx * BANK_VAL;\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_M; k += 1){\n\t\t\t\tA_reg[k] = A_shared[(t_M * THREAD_SIZE_M+k) * BLOCK_SIZE_K + B_reg_index];\n\t\t\t}\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_N; k += 1){\n\t\t\t\t#pragma unroll\n\t\t\t\tfor(int j = 0; j < THREAD_SIZE_M; j += 1){\n\t\t\t\t\tC_reg[j][k] += B_reg[k] * A_reg[j];\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n    float bias_local[THREAD_SIZE_N];\n    for(int thread_x = 0; thread_x < THREAD_SIZE_N; thread_x++){\n        bias_local[thread_x] = bias[BLOCK_SIZE_N * blockIdx.y + THREAD_SIZE_N * t_N + thread_x];\n    }\n\n\t#pragma unroll\n\tfor(int i = 0; i < THREAD_SIZE_M; i += 1){\n\t\t#pragma unroll\n\t\tfor(int j = 0; j < THREAD_SIZE_N; j += 1){\n\t\t\tg_data[(BLOCK_SIZE_M * blockIdx.x + THREAD_SIZE_M * t_M + i) * N + BLOCK_SIZE_N * blockIdx.y + THREAD_SIZE_N * t_N + j] = C_reg[i][j] + bias_local[j];\n\t\t}\n\t}\n}", "launch_config": {"dimBlock": [128, 1], "dimGrid": [64, 6]}}, "bert.encoder.layer.5.attention.self.value": {"code": "__global__ void MatMul_TILE_THREAD_GENERAL(float *input0, float *input1, float *input2, float *input3, float *output0) {\n    /*\n    g_vec: A [M, K]\n    g_mat_data: condense B [K_sparse, N]\n    g_mat_index: condense B index [K_sparse, N]\n    g_data: C [M, N]\n    */\n\tfloat *g_vec = input0;\n\tfloat *g_mat_data = input1;\n\tint *g_mat_index = (int*)input2;\n\tfloat *bias = input3;\n\tfloat *g_data = output0;\n\n\tconst int M = 4096;\n\tconst int K = 768;\n\tconst int N = 768;\n\n    const int BLOCK_SIZE_M = 64;\n    const int BLOCK_SIZE_N = 128;\n    const int BLOCK_SIZE_K = 32;\n    const int THREAD_SIZE_M = 8;\n    const int THREAD_SIZE_N = 8;\n\n    const float SPARSITY = 0.875;\n    const int BANK_VAL = 32;\n    const int ALIGN_N = THREAD_SIZE_N;\n\n    const int BANK_NUM_PER_BLOCK = BLOCK_SIZE_K / BANK_VAL;\n    const int BLOCK_SIZE_K_SPARSE = int(BLOCK_SIZE_K * (1-SPARSITY));\n    const int LEN_OF_BANK_PER_SPARSE_BLOCK = BLOCK_SIZE_K_SPARSE / BANK_NUM_PER_BLOCK;\n\n\tconst int K_SPARSE = int(K * SPARSITY);\n\tint M_BLOCK_START = blockIdx.x * BLOCK_SIZE_M;\n\tint N_BLOCK_START = blockIdx.y * BLOCK_SIZE_N;\n\n\tconst int A_THREADS_PER_ROW = BLOCK_SIZE_K / 4;\n\tconst int B_THREADS_PER_ROW = BLOCK_SIZE_N / 4;\n\n\tconst int THREADS_PER_BLOCK = (BLOCK_SIZE_M / THREAD_SIZE_M) * (BLOCK_SIZE_N / THREAD_SIZE_N);\n\n\tconst int A_STRIDES = THREADS_PER_BLOCK / A_THREADS_PER_ROW;\n\tconst int B_STRIDES = THREADS_PER_BLOCK / B_THREADS_PER_ROW;\n\n\t__shared__ float A_shared[BLOCK_SIZE_M * BLOCK_SIZE_K];\n\t__shared__ float B_shared[BLOCK_SIZE_N * BLOCK_SIZE_K_SPARSE];\n\t__shared__ int B_index_shared[BLOCK_SIZE_N * BLOCK_SIZE_K_SPARSE];\n\n\tfloat A_reg[THREAD_SIZE_M];\n\tfloat B_reg[THREAD_SIZE_N];\n\tint B_reg_index;\n\tfloat C_reg[THREAD_SIZE_M][THREAD_SIZE_N] = {0};\n\n\tint tid = threadIdx.x;\n\n\tint t_N = tid % (BLOCK_SIZE_N / THREAD_SIZE_N);\n\tint t_M = tid / (BLOCK_SIZE_N / THREAD_SIZE_N);\n\n\tint A_BLOCK_ROW_START = tid / A_THREADS_PER_ROW;\n\tint B_BLOCK_ROW_START = tid / B_THREADS_PER_ROW;\n\n\tint A_BLOCK_COL_START = tid % A_THREADS_PER_ROW * 4;\n\tint B_BLOCK_COL_START = tid % B_THREADS_PER_ROW * 4;\n\n\tfor(int K_BLOCK_START = 0, K_SPARSE_BLOCK_START = 0; K_BLOCK_START < K; K_BLOCK_START += BLOCK_SIZE_K, K_SPARSE_BLOCK_START += BLOCK_SIZE_K_SPARSE){\n\t\tfloat *A_global_ptr = g_vec + M_BLOCK_START * K + K_BLOCK_START;\n\t\tfloat *B_global_ptr = g_mat_data + K_SPARSE_BLOCK_START * N + N_BLOCK_START;\n\t\tint *B_index_global_ptr = g_mat_index + K_SPARSE_BLOCK_START * N + N_BLOCK_START;\n\n\t\t__syncthreads();\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_M; i += A_STRIDES){\n\t\t\t*(float4 *)(A_shared + (i + A_BLOCK_ROW_START) * BLOCK_SIZE_K + A_BLOCK_COL_START) = \n\t\t\t\t*(float4 *)(A_global_ptr + (i + A_BLOCK_ROW_START) * K + A_BLOCK_COL_START);\n\t\t}\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_K_SPARSE; i += B_STRIDES){\n\t\t\t*(float4 *)(B_shared + (i + B_BLOCK_ROW_START) * BLOCK_SIZE_N + B_BLOCK_COL_START) =\n\t\t\t\t*(float4 *)(B_global_ptr + (i + B_BLOCK_ROW_START) * N + B_BLOCK_COL_START);\n\n\t\t\t*(float4 *)(B_index_shared + (i + B_BLOCK_ROW_START) * BLOCK_SIZE_N + B_BLOCK_COL_START) =\n\t\t\t\t*(float4 *)(B_index_global_ptr + (i + B_BLOCK_ROW_START) * N + B_BLOCK_COL_START);\n\t\t}\n\n\t\t__syncthreads();\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_K_SPARSE;i += 1){\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_N; k += 1){\n\t\t\t\tB_reg[k] = B_shared[i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k];\n\t\t\t\t//*(float4 *)(B_reg + k) = *(float4 *)(B_shared + i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k);\n\t\t\t\t//*(float4 *)(B_reg_index + k) = *(float4 *)(B_index_shared + i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k);\n\t\t\t}\n\t\t\tint bank_idx = i / LEN_OF_BANK_PER_SPARSE_BLOCK;\n\t\t\tB_reg_index = B_index_shared[i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N] % BANK_VAL+bank_idx * BANK_VAL;\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_M; k += 1){\n\t\t\t\tA_reg[k] = A_shared[(t_M * THREAD_SIZE_M+k) * BLOCK_SIZE_K + B_reg_index];\n\t\t\t}\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_N; k += 1){\n\t\t\t\t#pragma unroll\n\t\t\t\tfor(int j = 0; j < THREAD_SIZE_M; j += 1){\n\t\t\t\t\tC_reg[j][k] += B_reg[k] * A_reg[j];\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n    float bias_local[THREAD_SIZE_N];\n    for(int thread_x = 0; thread_x < THREAD_SIZE_N; thread_x++){\n        bias_local[thread_x] = bias[BLOCK_SIZE_N * blockIdx.y + THREAD_SIZE_N * t_N + thread_x];\n    }\n\n\t#pragma unroll\n\tfor(int i = 0; i < THREAD_SIZE_M; i += 1){\n\t\t#pragma unroll\n\t\tfor(int j = 0; j < THREAD_SIZE_N; j += 1){\n\t\t\tg_data[(BLOCK_SIZE_M * blockIdx.x + THREAD_SIZE_M * t_M + i) * N + BLOCK_SIZE_N * blockIdx.y + THREAD_SIZE_N * t_N + j] = C_reg[i][j] + bias_local[j];\n\t\t}\n\t}\n}", "launch_config": {"dimBlock": [128, 1], "dimGrid": [64, 6]}}, "bert.encoder.layer.5.attention.output.dense": {"code": "__global__ void MatMul_TILE_THREAD_GENERAL(float *input0, float *input1, float *input2, float *input3, float *output0) {\n    /*\n    g_vec: A [M, K]\n    g_mat_data: condense B [K_sparse, N]\n    g_mat_index: condense B index [K_sparse, N]\n    g_data: C [M, N]\n    */\n\tfloat *g_vec = input0;\n\tfloat *g_mat_data = input1;\n\tint *g_mat_index = (int*)input2;\n\tfloat *bias = input3;\n\tfloat *g_data = output0;\n\n\tconst int M = 4096;\n\tconst int K = 768;\n\tconst int N = 768;\n\n    const int BLOCK_SIZE_M = 64;\n    const int BLOCK_SIZE_N = 128;\n    const int BLOCK_SIZE_K = 32;\n    const int THREAD_SIZE_M = 8;\n    const int THREAD_SIZE_N = 8;\n\n    const float SPARSITY = 0.875;\n    const int BANK_VAL = 32;\n    const int ALIGN_N = THREAD_SIZE_N;\n\n    const int BANK_NUM_PER_BLOCK = BLOCK_SIZE_K / BANK_VAL;\n    const int BLOCK_SIZE_K_SPARSE = int(BLOCK_SIZE_K * (1-SPARSITY));\n    const int LEN_OF_BANK_PER_SPARSE_BLOCK = BLOCK_SIZE_K_SPARSE / BANK_NUM_PER_BLOCK;\n\n\tconst int K_SPARSE = int(K * SPARSITY);\n\tint M_BLOCK_START = blockIdx.x * BLOCK_SIZE_M;\n\tint N_BLOCK_START = blockIdx.y * BLOCK_SIZE_N;\n\n\tconst int A_THREADS_PER_ROW = BLOCK_SIZE_K / 4;\n\tconst int B_THREADS_PER_ROW = BLOCK_SIZE_N / 4;\n\n\tconst int THREADS_PER_BLOCK = (BLOCK_SIZE_M / THREAD_SIZE_M) * (BLOCK_SIZE_N / THREAD_SIZE_N);\n\n\tconst int A_STRIDES = THREADS_PER_BLOCK / A_THREADS_PER_ROW;\n\tconst int B_STRIDES = THREADS_PER_BLOCK / B_THREADS_PER_ROW;\n\n\t__shared__ float A_shared[BLOCK_SIZE_M * BLOCK_SIZE_K];\n\t__shared__ float B_shared[BLOCK_SIZE_N * BLOCK_SIZE_K_SPARSE];\n\t__shared__ int B_index_shared[BLOCK_SIZE_N * BLOCK_SIZE_K_SPARSE];\n\n\tfloat A_reg[THREAD_SIZE_M];\n\tfloat B_reg[THREAD_SIZE_N];\n\tint B_reg_index;\n\tfloat C_reg[THREAD_SIZE_M][THREAD_SIZE_N] = {0};\n\n\tint tid = threadIdx.x;\n\n\tint t_N = tid % (BLOCK_SIZE_N / THREAD_SIZE_N);\n\tint t_M = tid / (BLOCK_SIZE_N / THREAD_SIZE_N);\n\n\tint A_BLOCK_ROW_START = tid / A_THREADS_PER_ROW;\n\tint B_BLOCK_ROW_START = tid / B_THREADS_PER_ROW;\n\n\tint A_BLOCK_COL_START = tid % A_THREADS_PER_ROW * 4;\n\tint B_BLOCK_COL_START = tid % B_THREADS_PER_ROW * 4;\n\n\tfor(int K_BLOCK_START = 0, K_SPARSE_BLOCK_START = 0; K_BLOCK_START < K; K_BLOCK_START += BLOCK_SIZE_K, K_SPARSE_BLOCK_START += BLOCK_SIZE_K_SPARSE){\n\t\tfloat *A_global_ptr = g_vec + M_BLOCK_START * K + K_BLOCK_START;\n\t\tfloat *B_global_ptr = g_mat_data + K_SPARSE_BLOCK_START * N + N_BLOCK_START;\n\t\tint *B_index_global_ptr = g_mat_index + K_SPARSE_BLOCK_START * N + N_BLOCK_START;\n\n\t\t__syncthreads();\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_M; i += A_STRIDES){\n\t\t\t*(float4 *)(A_shared + (i + A_BLOCK_ROW_START) * BLOCK_SIZE_K + A_BLOCK_COL_START) = \n\t\t\t\t*(float4 *)(A_global_ptr + (i + A_BLOCK_ROW_START) * K + A_BLOCK_COL_START);\n\t\t}\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_K_SPARSE; i += B_STRIDES){\n\t\t\t*(float4 *)(B_shared + (i + B_BLOCK_ROW_START) * BLOCK_SIZE_N + B_BLOCK_COL_START) =\n\t\t\t\t*(float4 *)(B_global_ptr + (i + B_BLOCK_ROW_START) * N + B_BLOCK_COL_START);\n\n\t\t\t*(float4 *)(B_index_shared + (i + B_BLOCK_ROW_START) * BLOCK_SIZE_N + B_BLOCK_COL_START) =\n\t\t\t\t*(float4 *)(B_index_global_ptr + (i + B_BLOCK_ROW_START) * N + B_BLOCK_COL_START);\n\t\t}\n\n\t\t__syncthreads();\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_K_SPARSE;i += 1){\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_N; k += 1){\n\t\t\t\tB_reg[k] = B_shared[i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k];\n\t\t\t\t//*(float4 *)(B_reg + k) = *(float4 *)(B_shared + i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k);\n\t\t\t\t//*(float4 *)(B_reg_index + k) = *(float4 *)(B_index_shared + i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k);\n\t\t\t}\n\t\t\tint bank_idx = i / LEN_OF_BANK_PER_SPARSE_BLOCK;\n\t\t\tB_reg_index = B_index_shared[i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N] % BANK_VAL+bank_idx * BANK_VAL;\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_M; k += 1){\n\t\t\t\tA_reg[k] = A_shared[(t_M * THREAD_SIZE_M+k) * BLOCK_SIZE_K + B_reg_index];\n\t\t\t}\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_N; k += 1){\n\t\t\t\t#pragma unroll\n\t\t\t\tfor(int j = 0; j < THREAD_SIZE_M; j += 1){\n\t\t\t\t\tC_reg[j][k] += B_reg[k] * A_reg[j];\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n    float bias_local[THREAD_SIZE_N];\n    for(int thread_x = 0; thread_x < THREAD_SIZE_N; thread_x++){\n        bias_local[thread_x] = bias[BLOCK_SIZE_N * blockIdx.y + THREAD_SIZE_N * t_N + thread_x];\n    }\n\n\t#pragma unroll\n\tfor(int i = 0; i < THREAD_SIZE_M; i += 1){\n\t\t#pragma unroll\n\t\tfor(int j = 0; j < THREAD_SIZE_N; j += 1){\n\t\t\tg_data[(BLOCK_SIZE_M * blockIdx.x + THREAD_SIZE_M * t_M + i) * N + BLOCK_SIZE_N * blockIdx.y + THREAD_SIZE_N * t_N + j] = C_reg[i][j] + bias_local[j];\n\t\t}\n\t}\n}", "launch_config": {"dimBlock": [128, 1], "dimGrid": [64, 6]}}, "bert.encoder.layer.5.intermediate.dense": {"code": "__global__ void MatMul_TILE_THREAD_GENERAL(float *input0, float *input1, float *input2, float *input3, float *output0) {\n    /*\n    g_vec: A [M, K]\n    g_mat_data: condense B [K_sparse, N]\n    g_mat_index: condense B index [K_sparse, N]\n    g_data: C [M, N]\n    */\n\tfloat *g_vec = input0;\n\tfloat *g_mat_data = input1;\n\tint *g_mat_index = (int*)input2;\n\tfloat *bias = input3;\n\tfloat *g_data = output0;\n\n\tconst int M = 4096;\n\tconst int K = 768;\n\tconst int N = 3072;\n\n    const int BLOCK_SIZE_M = 64;\n    const int BLOCK_SIZE_N = 128;\n    const int BLOCK_SIZE_K = 32;\n    const int THREAD_SIZE_M = 8;\n    const int THREAD_SIZE_N = 8;\n\n    const float SPARSITY = 0.875;\n    const int BANK_VAL = 32;\n    const int ALIGN_N = THREAD_SIZE_N;\n\n    const int BANK_NUM_PER_BLOCK = BLOCK_SIZE_K / BANK_VAL;\n    const int BLOCK_SIZE_K_SPARSE = int(BLOCK_SIZE_K * (1-SPARSITY));\n    const int LEN_OF_BANK_PER_SPARSE_BLOCK = BLOCK_SIZE_K_SPARSE / BANK_NUM_PER_BLOCK;\n\n\tconst int K_SPARSE = int(K * SPARSITY);\n\tint M_BLOCK_START = blockIdx.x * BLOCK_SIZE_M;\n\tint N_BLOCK_START = blockIdx.y * BLOCK_SIZE_N;\n\n\tconst int A_THREADS_PER_ROW = BLOCK_SIZE_K / 4;\n\tconst int B_THREADS_PER_ROW = BLOCK_SIZE_N / 4;\n\n\tconst int THREADS_PER_BLOCK = (BLOCK_SIZE_M / THREAD_SIZE_M) * (BLOCK_SIZE_N / THREAD_SIZE_N);\n\n\tconst int A_STRIDES = THREADS_PER_BLOCK / A_THREADS_PER_ROW;\n\tconst int B_STRIDES = THREADS_PER_BLOCK / B_THREADS_PER_ROW;\n\n\t__shared__ float A_shared[BLOCK_SIZE_M * BLOCK_SIZE_K];\n\t__shared__ float B_shared[BLOCK_SIZE_N * BLOCK_SIZE_K_SPARSE];\n\t__shared__ int B_index_shared[BLOCK_SIZE_N * BLOCK_SIZE_K_SPARSE];\n\n\tfloat A_reg[THREAD_SIZE_M];\n\tfloat B_reg[THREAD_SIZE_N];\n\tint B_reg_index;\n\tfloat C_reg[THREAD_SIZE_M][THREAD_SIZE_N] = {0};\n\n\tint tid = threadIdx.x;\n\n\tint t_N = tid % (BLOCK_SIZE_N / THREAD_SIZE_N);\n\tint t_M = tid / (BLOCK_SIZE_N / THREAD_SIZE_N);\n\n\tint A_BLOCK_ROW_START = tid / A_THREADS_PER_ROW;\n\tint B_BLOCK_ROW_START = tid / B_THREADS_PER_ROW;\n\n\tint A_BLOCK_COL_START = tid % A_THREADS_PER_ROW * 4;\n\tint B_BLOCK_COL_START = tid % B_THREADS_PER_ROW * 4;\n\n\tfor(int K_BLOCK_START = 0, K_SPARSE_BLOCK_START = 0; K_BLOCK_START < K; K_BLOCK_START += BLOCK_SIZE_K, K_SPARSE_BLOCK_START += BLOCK_SIZE_K_SPARSE){\n\t\tfloat *A_global_ptr = g_vec + M_BLOCK_START * K + K_BLOCK_START;\n\t\tfloat *B_global_ptr = g_mat_data + K_SPARSE_BLOCK_START * N + N_BLOCK_START;\n\t\tint *B_index_global_ptr = g_mat_index + K_SPARSE_BLOCK_START * N + N_BLOCK_START;\n\n\t\t__syncthreads();\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_M; i += A_STRIDES){\n\t\t\t*(float4 *)(A_shared + (i + A_BLOCK_ROW_START) * BLOCK_SIZE_K + A_BLOCK_COL_START) = \n\t\t\t\t*(float4 *)(A_global_ptr + (i + A_BLOCK_ROW_START) * K + A_BLOCK_COL_START);\n\t\t}\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_K_SPARSE; i += B_STRIDES){\n\t\t\t*(float4 *)(B_shared + (i + B_BLOCK_ROW_START) * BLOCK_SIZE_N + B_BLOCK_COL_START) =\n\t\t\t\t*(float4 *)(B_global_ptr + (i + B_BLOCK_ROW_START) * N + B_BLOCK_COL_START);\n\n\t\t\t*(float4 *)(B_index_shared + (i + B_BLOCK_ROW_START) * BLOCK_SIZE_N + B_BLOCK_COL_START) =\n\t\t\t\t*(float4 *)(B_index_global_ptr + (i + B_BLOCK_ROW_START) * N + B_BLOCK_COL_START);\n\t\t}\n\n\t\t__syncthreads();\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_K_SPARSE;i += 1){\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_N; k += 1){\n\t\t\t\tB_reg[k] = B_shared[i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k];\n\t\t\t\t//*(float4 *)(B_reg + k) = *(float4 *)(B_shared + i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k);\n\t\t\t\t//*(float4 *)(B_reg_index + k) = *(float4 *)(B_index_shared + i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k);\n\t\t\t}\n\t\t\tint bank_idx = i / LEN_OF_BANK_PER_SPARSE_BLOCK;\n\t\t\tB_reg_index = B_index_shared[i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N] % BANK_VAL+bank_idx * BANK_VAL;\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_M; k += 1){\n\t\t\t\tA_reg[k] = A_shared[(t_M * THREAD_SIZE_M+k) * BLOCK_SIZE_K + B_reg_index];\n\t\t\t}\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_N; k += 1){\n\t\t\t\t#pragma unroll\n\t\t\t\tfor(int j = 0; j < THREAD_SIZE_M; j += 1){\n\t\t\t\t\tC_reg[j][k] += B_reg[k] * A_reg[j];\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n    float bias_local[THREAD_SIZE_N];\n    for(int thread_x = 0; thread_x < THREAD_SIZE_N; thread_x++){\n        bias_local[thread_x] = bias[BLOCK_SIZE_N * blockIdx.y + THREAD_SIZE_N * t_N + thread_x];\n    }\n\n\t#pragma unroll\n\tfor(int i = 0; i < THREAD_SIZE_M; i += 1){\n\t\t#pragma unroll\n\t\tfor(int j = 0; j < THREAD_SIZE_N; j += 1){\n\t\t\tg_data[(BLOCK_SIZE_M * blockIdx.x + THREAD_SIZE_M * t_M + i) * N + BLOCK_SIZE_N * blockIdx.y + THREAD_SIZE_N * t_N + j] = C_reg[i][j] + bias_local[j];\n\t\t}\n\t}\n}", "launch_config": {"dimBlock": [128, 1], "dimGrid": [64, 24]}}, "bert.encoder.layer.5.output.dense": {"code": "__global__ void MatMul_TILE_THREAD_GENERAL(float *input0, float *input1, float *input2, float *input3, float *output0) {\n    /*\n    g_vec: A [M, K]\n    g_mat_data: condense B [K_sparse, N]\n    g_mat_index: condense B index [K_sparse, N]\n    g_data: C [M, N]\n    */\n\tfloat *g_vec = input0;\n\tfloat *g_mat_data = input1;\n\tint *g_mat_index = (int*)input2;\n\tfloat *bias = input3;\n\tfloat *g_data = output0;\n\n\tconst int M = 4096;\n\tconst int K = 3072;\n\tconst int N = 768;\n\n    const int BLOCK_SIZE_M = 64;\n    const int BLOCK_SIZE_N = 128;\n    const int BLOCK_SIZE_K = 32;\n    const int THREAD_SIZE_M = 8;\n    const int THREAD_SIZE_N = 8;\n\n    const float SPARSITY = 0.875;\n    const int BANK_VAL = 32;\n    const int ALIGN_N = THREAD_SIZE_N;\n\n    const int BANK_NUM_PER_BLOCK = BLOCK_SIZE_K / BANK_VAL;\n    const int BLOCK_SIZE_K_SPARSE = int(BLOCK_SIZE_K * (1-SPARSITY));\n    const int LEN_OF_BANK_PER_SPARSE_BLOCK = BLOCK_SIZE_K_SPARSE / BANK_NUM_PER_BLOCK;\n\n\tconst int K_SPARSE = int(K * SPARSITY);\n\tint M_BLOCK_START = blockIdx.x * BLOCK_SIZE_M;\n\tint N_BLOCK_START = blockIdx.y * BLOCK_SIZE_N;\n\n\tconst int A_THREADS_PER_ROW = BLOCK_SIZE_K / 4;\n\tconst int B_THREADS_PER_ROW = BLOCK_SIZE_N / 4;\n\n\tconst int THREADS_PER_BLOCK = (BLOCK_SIZE_M / THREAD_SIZE_M) * (BLOCK_SIZE_N / THREAD_SIZE_N);\n\n\tconst int A_STRIDES = THREADS_PER_BLOCK / A_THREADS_PER_ROW;\n\tconst int B_STRIDES = THREADS_PER_BLOCK / B_THREADS_PER_ROW;\n\n\t__shared__ float A_shared[BLOCK_SIZE_M * BLOCK_SIZE_K];\n\t__shared__ float B_shared[BLOCK_SIZE_N * BLOCK_SIZE_K_SPARSE];\n\t__shared__ int B_index_shared[BLOCK_SIZE_N * BLOCK_SIZE_K_SPARSE];\n\n\tfloat A_reg[THREAD_SIZE_M];\n\tfloat B_reg[THREAD_SIZE_N];\n\tint B_reg_index;\n\tfloat C_reg[THREAD_SIZE_M][THREAD_SIZE_N] = {0};\n\n\tint tid = threadIdx.x;\n\n\tint t_N = tid % (BLOCK_SIZE_N / THREAD_SIZE_N);\n\tint t_M = tid / (BLOCK_SIZE_N / THREAD_SIZE_N);\n\n\tint A_BLOCK_ROW_START = tid / A_THREADS_PER_ROW;\n\tint B_BLOCK_ROW_START = tid / B_THREADS_PER_ROW;\n\n\tint A_BLOCK_COL_START = tid % A_THREADS_PER_ROW * 4;\n\tint B_BLOCK_COL_START = tid % B_THREADS_PER_ROW * 4;\n\n\tfor(int K_BLOCK_START = 0, K_SPARSE_BLOCK_START = 0; K_BLOCK_START < K; K_BLOCK_START += BLOCK_SIZE_K, K_SPARSE_BLOCK_START += BLOCK_SIZE_K_SPARSE){\n\t\tfloat *A_global_ptr = g_vec + M_BLOCK_START * K + K_BLOCK_START;\n\t\tfloat *B_global_ptr = g_mat_data + K_SPARSE_BLOCK_START * N + N_BLOCK_START;\n\t\tint *B_index_global_ptr = g_mat_index + K_SPARSE_BLOCK_START * N + N_BLOCK_START;\n\n\t\t__syncthreads();\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_M; i += A_STRIDES){\n\t\t\t*(float4 *)(A_shared + (i + A_BLOCK_ROW_START) * BLOCK_SIZE_K + A_BLOCK_COL_START) = \n\t\t\t\t*(float4 *)(A_global_ptr + (i + A_BLOCK_ROW_START) * K + A_BLOCK_COL_START);\n\t\t}\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_K_SPARSE; i += B_STRIDES){\n\t\t\t*(float4 *)(B_shared + (i + B_BLOCK_ROW_START) * BLOCK_SIZE_N + B_BLOCK_COL_START) =\n\t\t\t\t*(float4 *)(B_global_ptr + (i + B_BLOCK_ROW_START) * N + B_BLOCK_COL_START);\n\n\t\t\t*(float4 *)(B_index_shared + (i + B_BLOCK_ROW_START) * BLOCK_SIZE_N + B_BLOCK_COL_START) =\n\t\t\t\t*(float4 *)(B_index_global_ptr + (i + B_BLOCK_ROW_START) * N + B_BLOCK_COL_START);\n\t\t}\n\n\t\t__syncthreads();\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_K_SPARSE;i += 1){\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_N; k += 1){\n\t\t\t\tB_reg[k] = B_shared[i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k];\n\t\t\t\t//*(float4 *)(B_reg + k) = *(float4 *)(B_shared + i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k);\n\t\t\t\t//*(float4 *)(B_reg_index + k) = *(float4 *)(B_index_shared + i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k);\n\t\t\t}\n\t\t\tint bank_idx = i / LEN_OF_BANK_PER_SPARSE_BLOCK;\n\t\t\tB_reg_index = B_index_shared[i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N] % BANK_VAL+bank_idx * BANK_VAL;\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_M; k += 1){\n\t\t\t\tA_reg[k] = A_shared[(t_M * THREAD_SIZE_M+k) * BLOCK_SIZE_K + B_reg_index];\n\t\t\t}\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_N; k += 1){\n\t\t\t\t#pragma unroll\n\t\t\t\tfor(int j = 0; j < THREAD_SIZE_M; j += 1){\n\t\t\t\t\tC_reg[j][k] += B_reg[k] * A_reg[j];\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n    float bias_local[THREAD_SIZE_N];\n    for(int thread_x = 0; thread_x < THREAD_SIZE_N; thread_x++){\n        bias_local[thread_x] = bias[BLOCK_SIZE_N * blockIdx.y + THREAD_SIZE_N * t_N + thread_x];\n    }\n\n\t#pragma unroll\n\tfor(int i = 0; i < THREAD_SIZE_M; i += 1){\n\t\t#pragma unroll\n\t\tfor(int j = 0; j < THREAD_SIZE_N; j += 1){\n\t\t\tg_data[(BLOCK_SIZE_M * blockIdx.x + THREAD_SIZE_M * t_M + i) * N + BLOCK_SIZE_N * blockIdx.y + THREAD_SIZE_N * t_N + j] = C_reg[i][j] + bias_local[j];\n\t\t}\n\t}\n}", "launch_config": {"dimBlock": [128, 1], "dimGrid": [64, 6]}}, "bert.encoder.layer.6.attention.self.query": {"code": "__global__ void MatMul_TILE_THREAD_GENERAL(float *input0, float *input1, float *input2, float *input3, float *output0) {\n    /*\n    g_vec: A [M, K]\n    g_mat_data: condense B [K_sparse, N]\n    g_mat_index: condense B index [K_sparse, N]\n    g_data: C [M, N]\n    */\n\tfloat *g_vec = input0;\n\tfloat *g_mat_data = input1;\n\tint *g_mat_index = (int*)input2;\n\tfloat *bias = input3;\n\tfloat *g_data = output0;\n\n\tconst int M = 4096;\n\tconst int K = 768;\n\tconst int N = 768;\n\n    const int BLOCK_SIZE_M = 64;\n    const int BLOCK_SIZE_N = 128;\n    const int BLOCK_SIZE_K = 32;\n    const int THREAD_SIZE_M = 8;\n    const int THREAD_SIZE_N = 8;\n\n    const float SPARSITY = 0.875;\n    const int BANK_VAL = 32;\n    const int ALIGN_N = THREAD_SIZE_N;\n\n    const int BANK_NUM_PER_BLOCK = BLOCK_SIZE_K / BANK_VAL;\n    const int BLOCK_SIZE_K_SPARSE = int(BLOCK_SIZE_K * (1-SPARSITY));\n    const int LEN_OF_BANK_PER_SPARSE_BLOCK = BLOCK_SIZE_K_SPARSE / BANK_NUM_PER_BLOCK;\n\n\tconst int K_SPARSE = int(K * SPARSITY);\n\tint M_BLOCK_START = blockIdx.x * BLOCK_SIZE_M;\n\tint N_BLOCK_START = blockIdx.y * BLOCK_SIZE_N;\n\n\tconst int A_THREADS_PER_ROW = BLOCK_SIZE_K / 4;\n\tconst int B_THREADS_PER_ROW = BLOCK_SIZE_N / 4;\n\n\tconst int THREADS_PER_BLOCK = (BLOCK_SIZE_M / THREAD_SIZE_M) * (BLOCK_SIZE_N / THREAD_SIZE_N);\n\n\tconst int A_STRIDES = THREADS_PER_BLOCK / A_THREADS_PER_ROW;\n\tconst int B_STRIDES = THREADS_PER_BLOCK / B_THREADS_PER_ROW;\n\n\t__shared__ float A_shared[BLOCK_SIZE_M * BLOCK_SIZE_K];\n\t__shared__ float B_shared[BLOCK_SIZE_N * BLOCK_SIZE_K_SPARSE];\n\t__shared__ int B_index_shared[BLOCK_SIZE_N * BLOCK_SIZE_K_SPARSE];\n\n\tfloat A_reg[THREAD_SIZE_M];\n\tfloat B_reg[THREAD_SIZE_N];\n\tint B_reg_index;\n\tfloat C_reg[THREAD_SIZE_M][THREAD_SIZE_N] = {0};\n\n\tint tid = threadIdx.x;\n\n\tint t_N = tid % (BLOCK_SIZE_N / THREAD_SIZE_N);\n\tint t_M = tid / (BLOCK_SIZE_N / THREAD_SIZE_N);\n\n\tint A_BLOCK_ROW_START = tid / A_THREADS_PER_ROW;\n\tint B_BLOCK_ROW_START = tid / B_THREADS_PER_ROW;\n\n\tint A_BLOCK_COL_START = tid % A_THREADS_PER_ROW * 4;\n\tint B_BLOCK_COL_START = tid % B_THREADS_PER_ROW * 4;\n\n\tfor(int K_BLOCK_START = 0, K_SPARSE_BLOCK_START = 0; K_BLOCK_START < K; K_BLOCK_START += BLOCK_SIZE_K, K_SPARSE_BLOCK_START += BLOCK_SIZE_K_SPARSE){\n\t\tfloat *A_global_ptr = g_vec + M_BLOCK_START * K + K_BLOCK_START;\n\t\tfloat *B_global_ptr = g_mat_data + K_SPARSE_BLOCK_START * N + N_BLOCK_START;\n\t\tint *B_index_global_ptr = g_mat_index + K_SPARSE_BLOCK_START * N + N_BLOCK_START;\n\n\t\t__syncthreads();\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_M; i += A_STRIDES){\n\t\t\t*(float4 *)(A_shared + (i + A_BLOCK_ROW_START) * BLOCK_SIZE_K + A_BLOCK_COL_START) = \n\t\t\t\t*(float4 *)(A_global_ptr + (i + A_BLOCK_ROW_START) * K + A_BLOCK_COL_START);\n\t\t}\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_K_SPARSE; i += B_STRIDES){\n\t\t\t*(float4 *)(B_shared + (i + B_BLOCK_ROW_START) * BLOCK_SIZE_N + B_BLOCK_COL_START) =\n\t\t\t\t*(float4 *)(B_global_ptr + (i + B_BLOCK_ROW_START) * N + B_BLOCK_COL_START);\n\n\t\t\t*(float4 *)(B_index_shared + (i + B_BLOCK_ROW_START) * BLOCK_SIZE_N + B_BLOCK_COL_START) =\n\t\t\t\t*(float4 *)(B_index_global_ptr + (i + B_BLOCK_ROW_START) * N + B_BLOCK_COL_START);\n\t\t}\n\n\t\t__syncthreads();\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_K_SPARSE;i += 1){\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_N; k += 1){\n\t\t\t\tB_reg[k] = B_shared[i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k];\n\t\t\t\t//*(float4 *)(B_reg + k) = *(float4 *)(B_shared + i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k);\n\t\t\t\t//*(float4 *)(B_reg_index + k) = *(float4 *)(B_index_shared + i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k);\n\t\t\t}\n\t\t\tint bank_idx = i / LEN_OF_BANK_PER_SPARSE_BLOCK;\n\t\t\tB_reg_index = B_index_shared[i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N] % BANK_VAL+bank_idx * BANK_VAL;\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_M; k += 1){\n\t\t\t\tA_reg[k] = A_shared[(t_M * THREAD_SIZE_M+k) * BLOCK_SIZE_K + B_reg_index];\n\t\t\t}\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_N; k += 1){\n\t\t\t\t#pragma unroll\n\t\t\t\tfor(int j = 0; j < THREAD_SIZE_M; j += 1){\n\t\t\t\t\tC_reg[j][k] += B_reg[k] * A_reg[j];\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n    float bias_local[THREAD_SIZE_N];\n    for(int thread_x = 0; thread_x < THREAD_SIZE_N; thread_x++){\n        bias_local[thread_x] = bias[BLOCK_SIZE_N * blockIdx.y + THREAD_SIZE_N * t_N + thread_x];\n    }\n\n\t#pragma unroll\n\tfor(int i = 0; i < THREAD_SIZE_M; i += 1){\n\t\t#pragma unroll\n\t\tfor(int j = 0; j < THREAD_SIZE_N; j += 1){\n\t\t\tg_data[(BLOCK_SIZE_M * blockIdx.x + THREAD_SIZE_M * t_M + i) * N + BLOCK_SIZE_N * blockIdx.y + THREAD_SIZE_N * t_N + j] = C_reg[i][j] + bias_local[j];\n\t\t}\n\t}\n}", "launch_config": {"dimBlock": [128, 1], "dimGrid": [64, 6]}}, "bert.encoder.layer.6.attention.self.key": {"code": "__global__ void MatMul_TILE_THREAD_GENERAL(float *input0, float *input1, float *input2, float *input3, float *output0) {\n    /*\n    g_vec: A [M, K]\n    g_mat_data: condense B [K_sparse, N]\n    g_mat_index: condense B index [K_sparse, N]\n    g_data: C [M, N]\n    */\n\tfloat *g_vec = input0;\n\tfloat *g_mat_data = input1;\n\tint *g_mat_index = (int*)input2;\n\tfloat *bias = input3;\n\tfloat *g_data = output0;\n\n\tconst int M = 4096;\n\tconst int K = 768;\n\tconst int N = 768;\n\n    const int BLOCK_SIZE_M = 64;\n    const int BLOCK_SIZE_N = 128;\n    const int BLOCK_SIZE_K = 32;\n    const int THREAD_SIZE_M = 8;\n    const int THREAD_SIZE_N = 8;\n\n    const float SPARSITY = 0.875;\n    const int BANK_VAL = 32;\n    const int ALIGN_N = THREAD_SIZE_N;\n\n    const int BANK_NUM_PER_BLOCK = BLOCK_SIZE_K / BANK_VAL;\n    const int BLOCK_SIZE_K_SPARSE = int(BLOCK_SIZE_K * (1-SPARSITY));\n    const int LEN_OF_BANK_PER_SPARSE_BLOCK = BLOCK_SIZE_K_SPARSE / BANK_NUM_PER_BLOCK;\n\n\tconst int K_SPARSE = int(K * SPARSITY);\n\tint M_BLOCK_START = blockIdx.x * BLOCK_SIZE_M;\n\tint N_BLOCK_START = blockIdx.y * BLOCK_SIZE_N;\n\n\tconst int A_THREADS_PER_ROW = BLOCK_SIZE_K / 4;\n\tconst int B_THREADS_PER_ROW = BLOCK_SIZE_N / 4;\n\n\tconst int THREADS_PER_BLOCK = (BLOCK_SIZE_M / THREAD_SIZE_M) * (BLOCK_SIZE_N / THREAD_SIZE_N);\n\n\tconst int A_STRIDES = THREADS_PER_BLOCK / A_THREADS_PER_ROW;\n\tconst int B_STRIDES = THREADS_PER_BLOCK / B_THREADS_PER_ROW;\n\n\t__shared__ float A_shared[BLOCK_SIZE_M * BLOCK_SIZE_K];\n\t__shared__ float B_shared[BLOCK_SIZE_N * BLOCK_SIZE_K_SPARSE];\n\t__shared__ int B_index_shared[BLOCK_SIZE_N * BLOCK_SIZE_K_SPARSE];\n\n\tfloat A_reg[THREAD_SIZE_M];\n\tfloat B_reg[THREAD_SIZE_N];\n\tint B_reg_index;\n\tfloat C_reg[THREAD_SIZE_M][THREAD_SIZE_N] = {0};\n\n\tint tid = threadIdx.x;\n\n\tint t_N = tid % (BLOCK_SIZE_N / THREAD_SIZE_N);\n\tint t_M = tid / (BLOCK_SIZE_N / THREAD_SIZE_N);\n\n\tint A_BLOCK_ROW_START = tid / A_THREADS_PER_ROW;\n\tint B_BLOCK_ROW_START = tid / B_THREADS_PER_ROW;\n\n\tint A_BLOCK_COL_START = tid % A_THREADS_PER_ROW * 4;\n\tint B_BLOCK_COL_START = tid % B_THREADS_PER_ROW * 4;\n\n\tfor(int K_BLOCK_START = 0, K_SPARSE_BLOCK_START = 0; K_BLOCK_START < K; K_BLOCK_START += BLOCK_SIZE_K, K_SPARSE_BLOCK_START += BLOCK_SIZE_K_SPARSE){\n\t\tfloat *A_global_ptr = g_vec + M_BLOCK_START * K + K_BLOCK_START;\n\t\tfloat *B_global_ptr = g_mat_data + K_SPARSE_BLOCK_START * N + N_BLOCK_START;\n\t\tint *B_index_global_ptr = g_mat_index + K_SPARSE_BLOCK_START * N + N_BLOCK_START;\n\n\t\t__syncthreads();\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_M; i += A_STRIDES){\n\t\t\t*(float4 *)(A_shared + (i + A_BLOCK_ROW_START) * BLOCK_SIZE_K + A_BLOCK_COL_START) = \n\t\t\t\t*(float4 *)(A_global_ptr + (i + A_BLOCK_ROW_START) * K + A_BLOCK_COL_START);\n\t\t}\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_K_SPARSE; i += B_STRIDES){\n\t\t\t*(float4 *)(B_shared + (i + B_BLOCK_ROW_START) * BLOCK_SIZE_N + B_BLOCK_COL_START) =\n\t\t\t\t*(float4 *)(B_global_ptr + (i + B_BLOCK_ROW_START) * N + B_BLOCK_COL_START);\n\n\t\t\t*(float4 *)(B_index_shared + (i + B_BLOCK_ROW_START) * BLOCK_SIZE_N + B_BLOCK_COL_START) =\n\t\t\t\t*(float4 *)(B_index_global_ptr + (i + B_BLOCK_ROW_START) * N + B_BLOCK_COL_START);\n\t\t}\n\n\t\t__syncthreads();\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_K_SPARSE;i += 1){\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_N; k += 1){\n\t\t\t\tB_reg[k] = B_shared[i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k];\n\t\t\t\t//*(float4 *)(B_reg + k) = *(float4 *)(B_shared + i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k);\n\t\t\t\t//*(float4 *)(B_reg_index + k) = *(float4 *)(B_index_shared + i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k);\n\t\t\t}\n\t\t\tint bank_idx = i / LEN_OF_BANK_PER_SPARSE_BLOCK;\n\t\t\tB_reg_index = B_index_shared[i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N] % BANK_VAL+bank_idx * BANK_VAL;\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_M; k += 1){\n\t\t\t\tA_reg[k] = A_shared[(t_M * THREAD_SIZE_M+k) * BLOCK_SIZE_K + B_reg_index];\n\t\t\t}\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_N; k += 1){\n\t\t\t\t#pragma unroll\n\t\t\t\tfor(int j = 0; j < THREAD_SIZE_M; j += 1){\n\t\t\t\t\tC_reg[j][k] += B_reg[k] * A_reg[j];\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n    float bias_local[THREAD_SIZE_N];\n    for(int thread_x = 0; thread_x < THREAD_SIZE_N; thread_x++){\n        bias_local[thread_x] = bias[BLOCK_SIZE_N * blockIdx.y + THREAD_SIZE_N * t_N + thread_x];\n    }\n\n\t#pragma unroll\n\tfor(int i = 0; i < THREAD_SIZE_M; i += 1){\n\t\t#pragma unroll\n\t\tfor(int j = 0; j < THREAD_SIZE_N; j += 1){\n\t\t\tg_data[(BLOCK_SIZE_M * blockIdx.x + THREAD_SIZE_M * t_M + i) * N + BLOCK_SIZE_N * blockIdx.y + THREAD_SIZE_N * t_N + j] = C_reg[i][j] + bias_local[j];\n\t\t}\n\t}\n}", "launch_config": {"dimBlock": [128, 1], "dimGrid": [64, 6]}}, "bert.encoder.layer.6.attention.self.value": {"code": "__global__ void MatMul_TILE_THREAD_GENERAL(float *input0, float *input1, float *input2, float *input3, float *output0) {\n    /*\n    g_vec: A [M, K]\n    g_mat_data: condense B [K_sparse, N]\n    g_mat_index: condense B index [K_sparse, N]\n    g_data: C [M, N]\n    */\n\tfloat *g_vec = input0;\n\tfloat *g_mat_data = input1;\n\tint *g_mat_index = (int*)input2;\n\tfloat *bias = input3;\n\tfloat *g_data = output0;\n\n\tconst int M = 4096;\n\tconst int K = 768;\n\tconst int N = 768;\n\n    const int BLOCK_SIZE_M = 64;\n    const int BLOCK_SIZE_N = 128;\n    const int BLOCK_SIZE_K = 32;\n    const int THREAD_SIZE_M = 8;\n    const int THREAD_SIZE_N = 8;\n\n    const float SPARSITY = 0.875;\n    const int BANK_VAL = 32;\n    const int ALIGN_N = THREAD_SIZE_N;\n\n    const int BANK_NUM_PER_BLOCK = BLOCK_SIZE_K / BANK_VAL;\n    const int BLOCK_SIZE_K_SPARSE = int(BLOCK_SIZE_K * (1-SPARSITY));\n    const int LEN_OF_BANK_PER_SPARSE_BLOCK = BLOCK_SIZE_K_SPARSE / BANK_NUM_PER_BLOCK;\n\n\tconst int K_SPARSE = int(K * SPARSITY);\n\tint M_BLOCK_START = blockIdx.x * BLOCK_SIZE_M;\n\tint N_BLOCK_START = blockIdx.y * BLOCK_SIZE_N;\n\n\tconst int A_THREADS_PER_ROW = BLOCK_SIZE_K / 4;\n\tconst int B_THREADS_PER_ROW = BLOCK_SIZE_N / 4;\n\n\tconst int THREADS_PER_BLOCK = (BLOCK_SIZE_M / THREAD_SIZE_M) * (BLOCK_SIZE_N / THREAD_SIZE_N);\n\n\tconst int A_STRIDES = THREADS_PER_BLOCK / A_THREADS_PER_ROW;\n\tconst int B_STRIDES = THREADS_PER_BLOCK / B_THREADS_PER_ROW;\n\n\t__shared__ float A_shared[BLOCK_SIZE_M * BLOCK_SIZE_K];\n\t__shared__ float B_shared[BLOCK_SIZE_N * BLOCK_SIZE_K_SPARSE];\n\t__shared__ int B_index_shared[BLOCK_SIZE_N * BLOCK_SIZE_K_SPARSE];\n\n\tfloat A_reg[THREAD_SIZE_M];\n\tfloat B_reg[THREAD_SIZE_N];\n\tint B_reg_index;\n\tfloat C_reg[THREAD_SIZE_M][THREAD_SIZE_N] = {0};\n\n\tint tid = threadIdx.x;\n\n\tint t_N = tid % (BLOCK_SIZE_N / THREAD_SIZE_N);\n\tint t_M = tid / (BLOCK_SIZE_N / THREAD_SIZE_N);\n\n\tint A_BLOCK_ROW_START = tid / A_THREADS_PER_ROW;\n\tint B_BLOCK_ROW_START = tid / B_THREADS_PER_ROW;\n\n\tint A_BLOCK_COL_START = tid % A_THREADS_PER_ROW * 4;\n\tint B_BLOCK_COL_START = tid % B_THREADS_PER_ROW * 4;\n\n\tfor(int K_BLOCK_START = 0, K_SPARSE_BLOCK_START = 0; K_BLOCK_START < K; K_BLOCK_START += BLOCK_SIZE_K, K_SPARSE_BLOCK_START += BLOCK_SIZE_K_SPARSE){\n\t\tfloat *A_global_ptr = g_vec + M_BLOCK_START * K + K_BLOCK_START;\n\t\tfloat *B_global_ptr = g_mat_data + K_SPARSE_BLOCK_START * N + N_BLOCK_START;\n\t\tint *B_index_global_ptr = g_mat_index + K_SPARSE_BLOCK_START * N + N_BLOCK_START;\n\n\t\t__syncthreads();\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_M; i += A_STRIDES){\n\t\t\t*(float4 *)(A_shared + (i + A_BLOCK_ROW_START) * BLOCK_SIZE_K + A_BLOCK_COL_START) = \n\t\t\t\t*(float4 *)(A_global_ptr + (i + A_BLOCK_ROW_START) * K + A_BLOCK_COL_START);\n\t\t}\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_K_SPARSE; i += B_STRIDES){\n\t\t\t*(float4 *)(B_shared + (i + B_BLOCK_ROW_START) * BLOCK_SIZE_N + B_BLOCK_COL_START) =\n\t\t\t\t*(float4 *)(B_global_ptr + (i + B_BLOCK_ROW_START) * N + B_BLOCK_COL_START);\n\n\t\t\t*(float4 *)(B_index_shared + (i + B_BLOCK_ROW_START) * BLOCK_SIZE_N + B_BLOCK_COL_START) =\n\t\t\t\t*(float4 *)(B_index_global_ptr + (i + B_BLOCK_ROW_START) * N + B_BLOCK_COL_START);\n\t\t}\n\n\t\t__syncthreads();\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_K_SPARSE;i += 1){\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_N; k += 1){\n\t\t\t\tB_reg[k] = B_shared[i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k];\n\t\t\t\t//*(float4 *)(B_reg + k) = *(float4 *)(B_shared + i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k);\n\t\t\t\t//*(float4 *)(B_reg_index + k) = *(float4 *)(B_index_shared + i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k);\n\t\t\t}\n\t\t\tint bank_idx = i / LEN_OF_BANK_PER_SPARSE_BLOCK;\n\t\t\tB_reg_index = B_index_shared[i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N] % BANK_VAL+bank_idx * BANK_VAL;\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_M; k += 1){\n\t\t\t\tA_reg[k] = A_shared[(t_M * THREAD_SIZE_M+k) * BLOCK_SIZE_K + B_reg_index];\n\t\t\t}\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_N; k += 1){\n\t\t\t\t#pragma unroll\n\t\t\t\tfor(int j = 0; j < THREAD_SIZE_M; j += 1){\n\t\t\t\t\tC_reg[j][k] += B_reg[k] * A_reg[j];\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n    float bias_local[THREAD_SIZE_N];\n    for(int thread_x = 0; thread_x < THREAD_SIZE_N; thread_x++){\n        bias_local[thread_x] = bias[BLOCK_SIZE_N * blockIdx.y + THREAD_SIZE_N * t_N + thread_x];\n    }\n\n\t#pragma unroll\n\tfor(int i = 0; i < THREAD_SIZE_M; i += 1){\n\t\t#pragma unroll\n\t\tfor(int j = 0; j < THREAD_SIZE_N; j += 1){\n\t\t\tg_data[(BLOCK_SIZE_M * blockIdx.x + THREAD_SIZE_M * t_M + i) * N + BLOCK_SIZE_N * blockIdx.y + THREAD_SIZE_N * t_N + j] = C_reg[i][j] + bias_local[j];\n\t\t}\n\t}\n}", "launch_config": {"dimBlock": [128, 1], "dimGrid": [64, 6]}}, "bert.encoder.layer.6.attention.output.dense": {"code": "__global__ void MatMul_TILE_THREAD_GENERAL(float *input0, float *input1, float *input2, float *input3, float *output0) {\n    /*\n    g_vec: A [M, K]\n    g_mat_data: condense B [K_sparse, N]\n    g_mat_index: condense B index [K_sparse, N]\n    g_data: C [M, N]\n    */\n\tfloat *g_vec = input0;\n\tfloat *g_mat_data = input1;\n\tint *g_mat_index = (int*)input2;\n\tfloat *bias = input3;\n\tfloat *g_data = output0;\n\n\tconst int M = 4096;\n\tconst int K = 768;\n\tconst int N = 768;\n\n    const int BLOCK_SIZE_M = 64;\n    const int BLOCK_SIZE_N = 128;\n    const int BLOCK_SIZE_K = 32;\n    const int THREAD_SIZE_M = 8;\n    const int THREAD_SIZE_N = 8;\n\n    const float SPARSITY = 0.875;\n    const int BANK_VAL = 32;\n    const int ALIGN_N = THREAD_SIZE_N;\n\n    const int BANK_NUM_PER_BLOCK = BLOCK_SIZE_K / BANK_VAL;\n    const int BLOCK_SIZE_K_SPARSE = int(BLOCK_SIZE_K * (1-SPARSITY));\n    const int LEN_OF_BANK_PER_SPARSE_BLOCK = BLOCK_SIZE_K_SPARSE / BANK_NUM_PER_BLOCK;\n\n\tconst int K_SPARSE = int(K * SPARSITY);\n\tint M_BLOCK_START = blockIdx.x * BLOCK_SIZE_M;\n\tint N_BLOCK_START = blockIdx.y * BLOCK_SIZE_N;\n\n\tconst int A_THREADS_PER_ROW = BLOCK_SIZE_K / 4;\n\tconst int B_THREADS_PER_ROW = BLOCK_SIZE_N / 4;\n\n\tconst int THREADS_PER_BLOCK = (BLOCK_SIZE_M / THREAD_SIZE_M) * (BLOCK_SIZE_N / THREAD_SIZE_N);\n\n\tconst int A_STRIDES = THREADS_PER_BLOCK / A_THREADS_PER_ROW;\n\tconst int B_STRIDES = THREADS_PER_BLOCK / B_THREADS_PER_ROW;\n\n\t__shared__ float A_shared[BLOCK_SIZE_M * BLOCK_SIZE_K];\n\t__shared__ float B_shared[BLOCK_SIZE_N * BLOCK_SIZE_K_SPARSE];\n\t__shared__ int B_index_shared[BLOCK_SIZE_N * BLOCK_SIZE_K_SPARSE];\n\n\tfloat A_reg[THREAD_SIZE_M];\n\tfloat B_reg[THREAD_SIZE_N];\n\tint B_reg_index;\n\tfloat C_reg[THREAD_SIZE_M][THREAD_SIZE_N] = {0};\n\n\tint tid = threadIdx.x;\n\n\tint t_N = tid % (BLOCK_SIZE_N / THREAD_SIZE_N);\n\tint t_M = tid / (BLOCK_SIZE_N / THREAD_SIZE_N);\n\n\tint A_BLOCK_ROW_START = tid / A_THREADS_PER_ROW;\n\tint B_BLOCK_ROW_START = tid / B_THREADS_PER_ROW;\n\n\tint A_BLOCK_COL_START = tid % A_THREADS_PER_ROW * 4;\n\tint B_BLOCK_COL_START = tid % B_THREADS_PER_ROW * 4;\n\n\tfor(int K_BLOCK_START = 0, K_SPARSE_BLOCK_START = 0; K_BLOCK_START < K; K_BLOCK_START += BLOCK_SIZE_K, K_SPARSE_BLOCK_START += BLOCK_SIZE_K_SPARSE){\n\t\tfloat *A_global_ptr = g_vec + M_BLOCK_START * K + K_BLOCK_START;\n\t\tfloat *B_global_ptr = g_mat_data + K_SPARSE_BLOCK_START * N + N_BLOCK_START;\n\t\tint *B_index_global_ptr = g_mat_index + K_SPARSE_BLOCK_START * N + N_BLOCK_START;\n\n\t\t__syncthreads();\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_M; i += A_STRIDES){\n\t\t\t*(float4 *)(A_shared + (i + A_BLOCK_ROW_START) * BLOCK_SIZE_K + A_BLOCK_COL_START) = \n\t\t\t\t*(float4 *)(A_global_ptr + (i + A_BLOCK_ROW_START) * K + A_BLOCK_COL_START);\n\t\t}\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_K_SPARSE; i += B_STRIDES){\n\t\t\t*(float4 *)(B_shared + (i + B_BLOCK_ROW_START) * BLOCK_SIZE_N + B_BLOCK_COL_START) =\n\t\t\t\t*(float4 *)(B_global_ptr + (i + B_BLOCK_ROW_START) * N + B_BLOCK_COL_START);\n\n\t\t\t*(float4 *)(B_index_shared + (i + B_BLOCK_ROW_START) * BLOCK_SIZE_N + B_BLOCK_COL_START) =\n\t\t\t\t*(float4 *)(B_index_global_ptr + (i + B_BLOCK_ROW_START) * N + B_BLOCK_COL_START);\n\t\t}\n\n\t\t__syncthreads();\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_K_SPARSE;i += 1){\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_N; k += 1){\n\t\t\t\tB_reg[k] = B_shared[i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k];\n\t\t\t\t//*(float4 *)(B_reg + k) = *(float4 *)(B_shared + i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k);\n\t\t\t\t//*(float4 *)(B_reg_index + k) = *(float4 *)(B_index_shared + i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k);\n\t\t\t}\n\t\t\tint bank_idx = i / LEN_OF_BANK_PER_SPARSE_BLOCK;\n\t\t\tB_reg_index = B_index_shared[i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N] % BANK_VAL+bank_idx * BANK_VAL;\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_M; k += 1){\n\t\t\t\tA_reg[k] = A_shared[(t_M * THREAD_SIZE_M+k) * BLOCK_SIZE_K + B_reg_index];\n\t\t\t}\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_N; k += 1){\n\t\t\t\t#pragma unroll\n\t\t\t\tfor(int j = 0; j < THREAD_SIZE_M; j += 1){\n\t\t\t\t\tC_reg[j][k] += B_reg[k] * A_reg[j];\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n    float bias_local[THREAD_SIZE_N];\n    for(int thread_x = 0; thread_x < THREAD_SIZE_N; thread_x++){\n        bias_local[thread_x] = bias[BLOCK_SIZE_N * blockIdx.y + THREAD_SIZE_N * t_N + thread_x];\n    }\n\n\t#pragma unroll\n\tfor(int i = 0; i < THREAD_SIZE_M; i += 1){\n\t\t#pragma unroll\n\t\tfor(int j = 0; j < THREAD_SIZE_N; j += 1){\n\t\t\tg_data[(BLOCK_SIZE_M * blockIdx.x + THREAD_SIZE_M * t_M + i) * N + BLOCK_SIZE_N * blockIdx.y + THREAD_SIZE_N * t_N + j] = C_reg[i][j] + bias_local[j];\n\t\t}\n\t}\n}", "launch_config": {"dimBlock": [128, 1], "dimGrid": [64, 6]}}, "bert.encoder.layer.6.intermediate.dense": {"code": "__global__ void MatMul_TILE_THREAD_GENERAL(float *input0, float *input1, float *input2, float *input3, float *output0) {\n    /*\n    g_vec: A [M, K]\n    g_mat_data: condense B [K_sparse, N]\n    g_mat_index: condense B index [K_sparse, N]\n    g_data: C [M, N]\n    */\n\tfloat *g_vec = input0;\n\tfloat *g_mat_data = input1;\n\tint *g_mat_index = (int*)input2;\n\tfloat *bias = input3;\n\tfloat *g_data = output0;\n\n\tconst int M = 4096;\n\tconst int K = 768;\n\tconst int N = 3072;\n\n    const int BLOCK_SIZE_M = 64;\n    const int BLOCK_SIZE_N = 128;\n    const int BLOCK_SIZE_K = 32;\n    const int THREAD_SIZE_M = 8;\n    const int THREAD_SIZE_N = 8;\n\n    const float SPARSITY = 0.875;\n    const int BANK_VAL = 32;\n    const int ALIGN_N = THREAD_SIZE_N;\n\n    const int BANK_NUM_PER_BLOCK = BLOCK_SIZE_K / BANK_VAL;\n    const int BLOCK_SIZE_K_SPARSE = int(BLOCK_SIZE_K * (1-SPARSITY));\n    const int LEN_OF_BANK_PER_SPARSE_BLOCK = BLOCK_SIZE_K_SPARSE / BANK_NUM_PER_BLOCK;\n\n\tconst int K_SPARSE = int(K * SPARSITY);\n\tint M_BLOCK_START = blockIdx.x * BLOCK_SIZE_M;\n\tint N_BLOCK_START = blockIdx.y * BLOCK_SIZE_N;\n\n\tconst int A_THREADS_PER_ROW = BLOCK_SIZE_K / 4;\n\tconst int B_THREADS_PER_ROW = BLOCK_SIZE_N / 4;\n\n\tconst int THREADS_PER_BLOCK = (BLOCK_SIZE_M / THREAD_SIZE_M) * (BLOCK_SIZE_N / THREAD_SIZE_N);\n\n\tconst int A_STRIDES = THREADS_PER_BLOCK / A_THREADS_PER_ROW;\n\tconst int B_STRIDES = THREADS_PER_BLOCK / B_THREADS_PER_ROW;\n\n\t__shared__ float A_shared[BLOCK_SIZE_M * BLOCK_SIZE_K];\n\t__shared__ float B_shared[BLOCK_SIZE_N * BLOCK_SIZE_K_SPARSE];\n\t__shared__ int B_index_shared[BLOCK_SIZE_N * BLOCK_SIZE_K_SPARSE];\n\n\tfloat A_reg[THREAD_SIZE_M];\n\tfloat B_reg[THREAD_SIZE_N];\n\tint B_reg_index;\n\tfloat C_reg[THREAD_SIZE_M][THREAD_SIZE_N] = {0};\n\n\tint tid = threadIdx.x;\n\n\tint t_N = tid % (BLOCK_SIZE_N / THREAD_SIZE_N);\n\tint t_M = tid / (BLOCK_SIZE_N / THREAD_SIZE_N);\n\n\tint A_BLOCK_ROW_START = tid / A_THREADS_PER_ROW;\n\tint B_BLOCK_ROW_START = tid / B_THREADS_PER_ROW;\n\n\tint A_BLOCK_COL_START = tid % A_THREADS_PER_ROW * 4;\n\tint B_BLOCK_COL_START = tid % B_THREADS_PER_ROW * 4;\n\n\tfor(int K_BLOCK_START = 0, K_SPARSE_BLOCK_START = 0; K_BLOCK_START < K; K_BLOCK_START += BLOCK_SIZE_K, K_SPARSE_BLOCK_START += BLOCK_SIZE_K_SPARSE){\n\t\tfloat *A_global_ptr = g_vec + M_BLOCK_START * K + K_BLOCK_START;\n\t\tfloat *B_global_ptr = g_mat_data + K_SPARSE_BLOCK_START * N + N_BLOCK_START;\n\t\tint *B_index_global_ptr = g_mat_index + K_SPARSE_BLOCK_START * N + N_BLOCK_START;\n\n\t\t__syncthreads();\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_M; i += A_STRIDES){\n\t\t\t*(float4 *)(A_shared + (i + A_BLOCK_ROW_START) * BLOCK_SIZE_K + A_BLOCK_COL_START) = \n\t\t\t\t*(float4 *)(A_global_ptr + (i + A_BLOCK_ROW_START) * K + A_BLOCK_COL_START);\n\t\t}\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_K_SPARSE; i += B_STRIDES){\n\t\t\t*(float4 *)(B_shared + (i + B_BLOCK_ROW_START) * BLOCK_SIZE_N + B_BLOCK_COL_START) =\n\t\t\t\t*(float4 *)(B_global_ptr + (i + B_BLOCK_ROW_START) * N + B_BLOCK_COL_START);\n\n\t\t\t*(float4 *)(B_index_shared + (i + B_BLOCK_ROW_START) * BLOCK_SIZE_N + B_BLOCK_COL_START) =\n\t\t\t\t*(float4 *)(B_index_global_ptr + (i + B_BLOCK_ROW_START) * N + B_BLOCK_COL_START);\n\t\t}\n\n\t\t__syncthreads();\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_K_SPARSE;i += 1){\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_N; k += 1){\n\t\t\t\tB_reg[k] = B_shared[i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k];\n\t\t\t\t//*(float4 *)(B_reg + k) = *(float4 *)(B_shared + i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k);\n\t\t\t\t//*(float4 *)(B_reg_index + k) = *(float4 *)(B_index_shared + i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k);\n\t\t\t}\n\t\t\tint bank_idx = i / LEN_OF_BANK_PER_SPARSE_BLOCK;\n\t\t\tB_reg_index = B_index_shared[i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N] % BANK_VAL+bank_idx * BANK_VAL;\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_M; k += 1){\n\t\t\t\tA_reg[k] = A_shared[(t_M * THREAD_SIZE_M+k) * BLOCK_SIZE_K + B_reg_index];\n\t\t\t}\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_N; k += 1){\n\t\t\t\t#pragma unroll\n\t\t\t\tfor(int j = 0; j < THREAD_SIZE_M; j += 1){\n\t\t\t\t\tC_reg[j][k] += B_reg[k] * A_reg[j];\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n    float bias_local[THREAD_SIZE_N];\n    for(int thread_x = 0; thread_x < THREAD_SIZE_N; thread_x++){\n        bias_local[thread_x] = bias[BLOCK_SIZE_N * blockIdx.y + THREAD_SIZE_N * t_N + thread_x];\n    }\n\n\t#pragma unroll\n\tfor(int i = 0; i < THREAD_SIZE_M; i += 1){\n\t\t#pragma unroll\n\t\tfor(int j = 0; j < THREAD_SIZE_N; j += 1){\n\t\t\tg_data[(BLOCK_SIZE_M * blockIdx.x + THREAD_SIZE_M * t_M + i) * N + BLOCK_SIZE_N * blockIdx.y + THREAD_SIZE_N * t_N + j] = C_reg[i][j] + bias_local[j];\n\t\t}\n\t}\n}", "launch_config": {"dimBlock": [128, 1], "dimGrid": [64, 24]}}, "bert.encoder.layer.6.output.dense": {"code": "__global__ void MatMul_TILE_THREAD_GENERAL(float *input0, float *input1, float *input2, float *input3, float *output0) {\n    /*\n    g_vec: A [M, K]\n    g_mat_data: condense B [K_sparse, N]\n    g_mat_index: condense B index [K_sparse, N]\n    g_data: C [M, N]\n    */\n\tfloat *g_vec = input0;\n\tfloat *g_mat_data = input1;\n\tint *g_mat_index = (int*)input2;\n\tfloat *bias = input3;\n\tfloat *g_data = output0;\n\n\tconst int M = 4096;\n\tconst int K = 3072;\n\tconst int N = 768;\n\n    const int BLOCK_SIZE_M = 64;\n    const int BLOCK_SIZE_N = 128;\n    const int BLOCK_SIZE_K = 32;\n    const int THREAD_SIZE_M = 8;\n    const int THREAD_SIZE_N = 8;\n\n    const float SPARSITY = 0.875;\n    const int BANK_VAL = 32;\n    const int ALIGN_N = THREAD_SIZE_N;\n\n    const int BANK_NUM_PER_BLOCK = BLOCK_SIZE_K / BANK_VAL;\n    const int BLOCK_SIZE_K_SPARSE = int(BLOCK_SIZE_K * (1-SPARSITY));\n    const int LEN_OF_BANK_PER_SPARSE_BLOCK = BLOCK_SIZE_K_SPARSE / BANK_NUM_PER_BLOCK;\n\n\tconst int K_SPARSE = int(K * SPARSITY);\n\tint M_BLOCK_START = blockIdx.x * BLOCK_SIZE_M;\n\tint N_BLOCK_START = blockIdx.y * BLOCK_SIZE_N;\n\n\tconst int A_THREADS_PER_ROW = BLOCK_SIZE_K / 4;\n\tconst int B_THREADS_PER_ROW = BLOCK_SIZE_N / 4;\n\n\tconst int THREADS_PER_BLOCK = (BLOCK_SIZE_M / THREAD_SIZE_M) * (BLOCK_SIZE_N / THREAD_SIZE_N);\n\n\tconst int A_STRIDES = THREADS_PER_BLOCK / A_THREADS_PER_ROW;\n\tconst int B_STRIDES = THREADS_PER_BLOCK / B_THREADS_PER_ROW;\n\n\t__shared__ float A_shared[BLOCK_SIZE_M * BLOCK_SIZE_K];\n\t__shared__ float B_shared[BLOCK_SIZE_N * BLOCK_SIZE_K_SPARSE];\n\t__shared__ int B_index_shared[BLOCK_SIZE_N * BLOCK_SIZE_K_SPARSE];\n\n\tfloat A_reg[THREAD_SIZE_M];\n\tfloat B_reg[THREAD_SIZE_N];\n\tint B_reg_index;\n\tfloat C_reg[THREAD_SIZE_M][THREAD_SIZE_N] = {0};\n\n\tint tid = threadIdx.x;\n\n\tint t_N = tid % (BLOCK_SIZE_N / THREAD_SIZE_N);\n\tint t_M = tid / (BLOCK_SIZE_N / THREAD_SIZE_N);\n\n\tint A_BLOCK_ROW_START = tid / A_THREADS_PER_ROW;\n\tint B_BLOCK_ROW_START = tid / B_THREADS_PER_ROW;\n\n\tint A_BLOCK_COL_START = tid % A_THREADS_PER_ROW * 4;\n\tint B_BLOCK_COL_START = tid % B_THREADS_PER_ROW * 4;\n\n\tfor(int K_BLOCK_START = 0, K_SPARSE_BLOCK_START = 0; K_BLOCK_START < K; K_BLOCK_START += BLOCK_SIZE_K, K_SPARSE_BLOCK_START += BLOCK_SIZE_K_SPARSE){\n\t\tfloat *A_global_ptr = g_vec + M_BLOCK_START * K + K_BLOCK_START;\n\t\tfloat *B_global_ptr = g_mat_data + K_SPARSE_BLOCK_START * N + N_BLOCK_START;\n\t\tint *B_index_global_ptr = g_mat_index + K_SPARSE_BLOCK_START * N + N_BLOCK_START;\n\n\t\t__syncthreads();\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_M; i += A_STRIDES){\n\t\t\t*(float4 *)(A_shared + (i + A_BLOCK_ROW_START) * BLOCK_SIZE_K + A_BLOCK_COL_START) = \n\t\t\t\t*(float4 *)(A_global_ptr + (i + A_BLOCK_ROW_START) * K + A_BLOCK_COL_START);\n\t\t}\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_K_SPARSE; i += B_STRIDES){\n\t\t\t*(float4 *)(B_shared + (i + B_BLOCK_ROW_START) * BLOCK_SIZE_N + B_BLOCK_COL_START) =\n\t\t\t\t*(float4 *)(B_global_ptr + (i + B_BLOCK_ROW_START) * N + B_BLOCK_COL_START);\n\n\t\t\t*(float4 *)(B_index_shared + (i + B_BLOCK_ROW_START) * BLOCK_SIZE_N + B_BLOCK_COL_START) =\n\t\t\t\t*(float4 *)(B_index_global_ptr + (i + B_BLOCK_ROW_START) * N + B_BLOCK_COL_START);\n\t\t}\n\n\t\t__syncthreads();\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_K_SPARSE;i += 1){\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_N; k += 1){\n\t\t\t\tB_reg[k] = B_shared[i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k];\n\t\t\t\t//*(float4 *)(B_reg + k) = *(float4 *)(B_shared + i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k);\n\t\t\t\t//*(float4 *)(B_reg_index + k) = *(float4 *)(B_index_shared + i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k);\n\t\t\t}\n\t\t\tint bank_idx = i / LEN_OF_BANK_PER_SPARSE_BLOCK;\n\t\t\tB_reg_index = B_index_shared[i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N] % BANK_VAL+bank_idx * BANK_VAL;\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_M; k += 1){\n\t\t\t\tA_reg[k] = A_shared[(t_M * THREAD_SIZE_M+k) * BLOCK_SIZE_K + B_reg_index];\n\t\t\t}\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_N; k += 1){\n\t\t\t\t#pragma unroll\n\t\t\t\tfor(int j = 0; j < THREAD_SIZE_M; j += 1){\n\t\t\t\t\tC_reg[j][k] += B_reg[k] * A_reg[j];\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n    float bias_local[THREAD_SIZE_N];\n    for(int thread_x = 0; thread_x < THREAD_SIZE_N; thread_x++){\n        bias_local[thread_x] = bias[BLOCK_SIZE_N * blockIdx.y + THREAD_SIZE_N * t_N + thread_x];\n    }\n\n\t#pragma unroll\n\tfor(int i = 0; i < THREAD_SIZE_M; i += 1){\n\t\t#pragma unroll\n\t\tfor(int j = 0; j < THREAD_SIZE_N; j += 1){\n\t\t\tg_data[(BLOCK_SIZE_M * blockIdx.x + THREAD_SIZE_M * t_M + i) * N + BLOCK_SIZE_N * blockIdx.y + THREAD_SIZE_N * t_N + j] = C_reg[i][j] + bias_local[j];\n\t\t}\n\t}\n}", "launch_config": {"dimBlock": [128, 1], "dimGrid": [64, 6]}}, "bert.encoder.layer.7.attention.self.query": {"code": "__global__ void MatMul_TILE_THREAD_GENERAL(float *input0, float *input1, float *input2, float *input3, float *output0) {\n    /*\n    g_vec: A [M, K]\n    g_mat_data: condense B [K_sparse, N]\n    g_mat_index: condense B index [K_sparse, N]\n    g_data: C [M, N]\n    */\n\tfloat *g_vec = input0;\n\tfloat *g_mat_data = input1;\n\tint *g_mat_index = (int*)input2;\n\tfloat *bias = input3;\n\tfloat *g_data = output0;\n\n\tconst int M = 4096;\n\tconst int K = 768;\n\tconst int N = 768;\n\n    const int BLOCK_SIZE_M = 64;\n    const int BLOCK_SIZE_N = 128;\n    const int BLOCK_SIZE_K = 32;\n    const int THREAD_SIZE_M = 8;\n    const int THREAD_SIZE_N = 8;\n\n    const float SPARSITY = 0.875;\n    const int BANK_VAL = 32;\n    const int ALIGN_N = THREAD_SIZE_N;\n\n    const int BANK_NUM_PER_BLOCK = BLOCK_SIZE_K / BANK_VAL;\n    const int BLOCK_SIZE_K_SPARSE = int(BLOCK_SIZE_K * (1-SPARSITY));\n    const int LEN_OF_BANK_PER_SPARSE_BLOCK = BLOCK_SIZE_K_SPARSE / BANK_NUM_PER_BLOCK;\n\n\tconst int K_SPARSE = int(K * SPARSITY);\n\tint M_BLOCK_START = blockIdx.x * BLOCK_SIZE_M;\n\tint N_BLOCK_START = blockIdx.y * BLOCK_SIZE_N;\n\n\tconst int A_THREADS_PER_ROW = BLOCK_SIZE_K / 4;\n\tconst int B_THREADS_PER_ROW = BLOCK_SIZE_N / 4;\n\n\tconst int THREADS_PER_BLOCK = (BLOCK_SIZE_M / THREAD_SIZE_M) * (BLOCK_SIZE_N / THREAD_SIZE_N);\n\n\tconst int A_STRIDES = THREADS_PER_BLOCK / A_THREADS_PER_ROW;\n\tconst int B_STRIDES = THREADS_PER_BLOCK / B_THREADS_PER_ROW;\n\n\t__shared__ float A_shared[BLOCK_SIZE_M * BLOCK_SIZE_K];\n\t__shared__ float B_shared[BLOCK_SIZE_N * BLOCK_SIZE_K_SPARSE];\n\t__shared__ int B_index_shared[BLOCK_SIZE_N * BLOCK_SIZE_K_SPARSE];\n\n\tfloat A_reg[THREAD_SIZE_M];\n\tfloat B_reg[THREAD_SIZE_N];\n\tint B_reg_index;\n\tfloat C_reg[THREAD_SIZE_M][THREAD_SIZE_N] = {0};\n\n\tint tid = threadIdx.x;\n\n\tint t_N = tid % (BLOCK_SIZE_N / THREAD_SIZE_N);\n\tint t_M = tid / (BLOCK_SIZE_N / THREAD_SIZE_N);\n\n\tint A_BLOCK_ROW_START = tid / A_THREADS_PER_ROW;\n\tint B_BLOCK_ROW_START = tid / B_THREADS_PER_ROW;\n\n\tint A_BLOCK_COL_START = tid % A_THREADS_PER_ROW * 4;\n\tint B_BLOCK_COL_START = tid % B_THREADS_PER_ROW * 4;\n\n\tfor(int K_BLOCK_START = 0, K_SPARSE_BLOCK_START = 0; K_BLOCK_START < K; K_BLOCK_START += BLOCK_SIZE_K, K_SPARSE_BLOCK_START += BLOCK_SIZE_K_SPARSE){\n\t\tfloat *A_global_ptr = g_vec + M_BLOCK_START * K + K_BLOCK_START;\n\t\tfloat *B_global_ptr = g_mat_data + K_SPARSE_BLOCK_START * N + N_BLOCK_START;\n\t\tint *B_index_global_ptr = g_mat_index + K_SPARSE_BLOCK_START * N + N_BLOCK_START;\n\n\t\t__syncthreads();\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_M; i += A_STRIDES){\n\t\t\t*(float4 *)(A_shared + (i + A_BLOCK_ROW_START) * BLOCK_SIZE_K + A_BLOCK_COL_START) = \n\t\t\t\t*(float4 *)(A_global_ptr + (i + A_BLOCK_ROW_START) * K + A_BLOCK_COL_START);\n\t\t}\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_K_SPARSE; i += B_STRIDES){\n\t\t\t*(float4 *)(B_shared + (i + B_BLOCK_ROW_START) * BLOCK_SIZE_N + B_BLOCK_COL_START) =\n\t\t\t\t*(float4 *)(B_global_ptr + (i + B_BLOCK_ROW_START) * N + B_BLOCK_COL_START);\n\n\t\t\t*(float4 *)(B_index_shared + (i + B_BLOCK_ROW_START) * BLOCK_SIZE_N + B_BLOCK_COL_START) =\n\t\t\t\t*(float4 *)(B_index_global_ptr + (i + B_BLOCK_ROW_START) * N + B_BLOCK_COL_START);\n\t\t}\n\n\t\t__syncthreads();\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_K_SPARSE;i += 1){\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_N; k += 1){\n\t\t\t\tB_reg[k] = B_shared[i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k];\n\t\t\t\t//*(float4 *)(B_reg + k) = *(float4 *)(B_shared + i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k);\n\t\t\t\t//*(float4 *)(B_reg_index + k) = *(float4 *)(B_index_shared + i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k);\n\t\t\t}\n\t\t\tint bank_idx = i / LEN_OF_BANK_PER_SPARSE_BLOCK;\n\t\t\tB_reg_index = B_index_shared[i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N] % BANK_VAL+bank_idx * BANK_VAL;\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_M; k += 1){\n\t\t\t\tA_reg[k] = A_shared[(t_M * THREAD_SIZE_M+k) * BLOCK_SIZE_K + B_reg_index];\n\t\t\t}\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_N; k += 1){\n\t\t\t\t#pragma unroll\n\t\t\t\tfor(int j = 0; j < THREAD_SIZE_M; j += 1){\n\t\t\t\t\tC_reg[j][k] += B_reg[k] * A_reg[j];\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n    float bias_local[THREAD_SIZE_N];\n    for(int thread_x = 0; thread_x < THREAD_SIZE_N; thread_x++){\n        bias_local[thread_x] = bias[BLOCK_SIZE_N * blockIdx.y + THREAD_SIZE_N * t_N + thread_x];\n    }\n\n\t#pragma unroll\n\tfor(int i = 0; i < THREAD_SIZE_M; i += 1){\n\t\t#pragma unroll\n\t\tfor(int j = 0; j < THREAD_SIZE_N; j += 1){\n\t\t\tg_data[(BLOCK_SIZE_M * blockIdx.x + THREAD_SIZE_M * t_M + i) * N + BLOCK_SIZE_N * blockIdx.y + THREAD_SIZE_N * t_N + j] = C_reg[i][j] + bias_local[j];\n\t\t}\n\t}\n}", "launch_config": {"dimBlock": [128, 1], "dimGrid": [64, 6]}}, "bert.encoder.layer.7.attention.self.key": {"code": "__global__ void MatMul_TILE_THREAD_GENERAL(float *input0, float *input1, float *input2, float *input3, float *output0) {\n    /*\n    g_vec: A [M, K]\n    g_mat_data: condense B [K_sparse, N]\n    g_mat_index: condense B index [K_sparse, N]\n    g_data: C [M, N]\n    */\n\tfloat *g_vec = input0;\n\tfloat *g_mat_data = input1;\n\tint *g_mat_index = (int*)input2;\n\tfloat *bias = input3;\n\tfloat *g_data = output0;\n\n\tconst int M = 4096;\n\tconst int K = 768;\n\tconst int N = 768;\n\n    const int BLOCK_SIZE_M = 64;\n    const int BLOCK_SIZE_N = 128;\n    const int BLOCK_SIZE_K = 32;\n    const int THREAD_SIZE_M = 8;\n    const int THREAD_SIZE_N = 8;\n\n    const float SPARSITY = 0.875;\n    const int BANK_VAL = 32;\n    const int ALIGN_N = THREAD_SIZE_N;\n\n    const int BANK_NUM_PER_BLOCK = BLOCK_SIZE_K / BANK_VAL;\n    const int BLOCK_SIZE_K_SPARSE = int(BLOCK_SIZE_K * (1-SPARSITY));\n    const int LEN_OF_BANK_PER_SPARSE_BLOCK = BLOCK_SIZE_K_SPARSE / BANK_NUM_PER_BLOCK;\n\n\tconst int K_SPARSE = int(K * SPARSITY);\n\tint M_BLOCK_START = blockIdx.x * BLOCK_SIZE_M;\n\tint N_BLOCK_START = blockIdx.y * BLOCK_SIZE_N;\n\n\tconst int A_THREADS_PER_ROW = BLOCK_SIZE_K / 4;\n\tconst int B_THREADS_PER_ROW = BLOCK_SIZE_N / 4;\n\n\tconst int THREADS_PER_BLOCK = (BLOCK_SIZE_M / THREAD_SIZE_M) * (BLOCK_SIZE_N / THREAD_SIZE_N);\n\n\tconst int A_STRIDES = THREADS_PER_BLOCK / A_THREADS_PER_ROW;\n\tconst int B_STRIDES = THREADS_PER_BLOCK / B_THREADS_PER_ROW;\n\n\t__shared__ float A_shared[BLOCK_SIZE_M * BLOCK_SIZE_K];\n\t__shared__ float B_shared[BLOCK_SIZE_N * BLOCK_SIZE_K_SPARSE];\n\t__shared__ int B_index_shared[BLOCK_SIZE_N * BLOCK_SIZE_K_SPARSE];\n\n\tfloat A_reg[THREAD_SIZE_M];\n\tfloat B_reg[THREAD_SIZE_N];\n\tint B_reg_index;\n\tfloat C_reg[THREAD_SIZE_M][THREAD_SIZE_N] = {0};\n\n\tint tid = threadIdx.x;\n\n\tint t_N = tid % (BLOCK_SIZE_N / THREAD_SIZE_N);\n\tint t_M = tid / (BLOCK_SIZE_N / THREAD_SIZE_N);\n\n\tint A_BLOCK_ROW_START = tid / A_THREADS_PER_ROW;\n\tint B_BLOCK_ROW_START = tid / B_THREADS_PER_ROW;\n\n\tint A_BLOCK_COL_START = tid % A_THREADS_PER_ROW * 4;\n\tint B_BLOCK_COL_START = tid % B_THREADS_PER_ROW * 4;\n\n\tfor(int K_BLOCK_START = 0, K_SPARSE_BLOCK_START = 0; K_BLOCK_START < K; K_BLOCK_START += BLOCK_SIZE_K, K_SPARSE_BLOCK_START += BLOCK_SIZE_K_SPARSE){\n\t\tfloat *A_global_ptr = g_vec + M_BLOCK_START * K + K_BLOCK_START;\n\t\tfloat *B_global_ptr = g_mat_data + K_SPARSE_BLOCK_START * N + N_BLOCK_START;\n\t\tint *B_index_global_ptr = g_mat_index + K_SPARSE_BLOCK_START * N + N_BLOCK_START;\n\n\t\t__syncthreads();\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_M; i += A_STRIDES){\n\t\t\t*(float4 *)(A_shared + (i + A_BLOCK_ROW_START) * BLOCK_SIZE_K + A_BLOCK_COL_START) = \n\t\t\t\t*(float4 *)(A_global_ptr + (i + A_BLOCK_ROW_START) * K + A_BLOCK_COL_START);\n\t\t}\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_K_SPARSE; i += B_STRIDES){\n\t\t\t*(float4 *)(B_shared + (i + B_BLOCK_ROW_START) * BLOCK_SIZE_N + B_BLOCK_COL_START) =\n\t\t\t\t*(float4 *)(B_global_ptr + (i + B_BLOCK_ROW_START) * N + B_BLOCK_COL_START);\n\n\t\t\t*(float4 *)(B_index_shared + (i + B_BLOCK_ROW_START) * BLOCK_SIZE_N + B_BLOCK_COL_START) =\n\t\t\t\t*(float4 *)(B_index_global_ptr + (i + B_BLOCK_ROW_START) * N + B_BLOCK_COL_START);\n\t\t}\n\n\t\t__syncthreads();\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_K_SPARSE;i += 1){\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_N; k += 1){\n\t\t\t\tB_reg[k] = B_shared[i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k];\n\t\t\t\t//*(float4 *)(B_reg + k) = *(float4 *)(B_shared + i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k);\n\t\t\t\t//*(float4 *)(B_reg_index + k) = *(float4 *)(B_index_shared + i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k);\n\t\t\t}\n\t\t\tint bank_idx = i / LEN_OF_BANK_PER_SPARSE_BLOCK;\n\t\t\tB_reg_index = B_index_shared[i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N] % BANK_VAL+bank_idx * BANK_VAL;\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_M; k += 1){\n\t\t\t\tA_reg[k] = A_shared[(t_M * THREAD_SIZE_M+k) * BLOCK_SIZE_K + B_reg_index];\n\t\t\t}\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_N; k += 1){\n\t\t\t\t#pragma unroll\n\t\t\t\tfor(int j = 0; j < THREAD_SIZE_M; j += 1){\n\t\t\t\t\tC_reg[j][k] += B_reg[k] * A_reg[j];\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n    float bias_local[THREAD_SIZE_N];\n    for(int thread_x = 0; thread_x < THREAD_SIZE_N; thread_x++){\n        bias_local[thread_x] = bias[BLOCK_SIZE_N * blockIdx.y + THREAD_SIZE_N * t_N + thread_x];\n    }\n\n\t#pragma unroll\n\tfor(int i = 0; i < THREAD_SIZE_M; i += 1){\n\t\t#pragma unroll\n\t\tfor(int j = 0; j < THREAD_SIZE_N; j += 1){\n\t\t\tg_data[(BLOCK_SIZE_M * blockIdx.x + THREAD_SIZE_M * t_M + i) * N + BLOCK_SIZE_N * blockIdx.y + THREAD_SIZE_N * t_N + j] = C_reg[i][j] + bias_local[j];\n\t\t}\n\t}\n}", "launch_config": {"dimBlock": [128, 1], "dimGrid": [64, 6]}}, "bert.encoder.layer.7.attention.self.value": {"code": "__global__ void MatMul_TILE_THREAD_GENERAL(float *input0, float *input1, float *input2, float *input3, float *output0) {\n    /*\n    g_vec: A [M, K]\n    g_mat_data: condense B [K_sparse, N]\n    g_mat_index: condense B index [K_sparse, N]\n    g_data: C [M, N]\n    */\n\tfloat *g_vec = input0;\n\tfloat *g_mat_data = input1;\n\tint *g_mat_index = (int*)input2;\n\tfloat *bias = input3;\n\tfloat *g_data = output0;\n\n\tconst int M = 4096;\n\tconst int K = 768;\n\tconst int N = 768;\n\n    const int BLOCK_SIZE_M = 64;\n    const int BLOCK_SIZE_N = 128;\n    const int BLOCK_SIZE_K = 32;\n    const int THREAD_SIZE_M = 8;\n    const int THREAD_SIZE_N = 8;\n\n    const float SPARSITY = 0.875;\n    const int BANK_VAL = 32;\n    const int ALIGN_N = THREAD_SIZE_N;\n\n    const int BANK_NUM_PER_BLOCK = BLOCK_SIZE_K / BANK_VAL;\n    const int BLOCK_SIZE_K_SPARSE = int(BLOCK_SIZE_K * (1-SPARSITY));\n    const int LEN_OF_BANK_PER_SPARSE_BLOCK = BLOCK_SIZE_K_SPARSE / BANK_NUM_PER_BLOCK;\n\n\tconst int K_SPARSE = int(K * SPARSITY);\n\tint M_BLOCK_START = blockIdx.x * BLOCK_SIZE_M;\n\tint N_BLOCK_START = blockIdx.y * BLOCK_SIZE_N;\n\n\tconst int A_THREADS_PER_ROW = BLOCK_SIZE_K / 4;\n\tconst int B_THREADS_PER_ROW = BLOCK_SIZE_N / 4;\n\n\tconst int THREADS_PER_BLOCK = (BLOCK_SIZE_M / THREAD_SIZE_M) * (BLOCK_SIZE_N / THREAD_SIZE_N);\n\n\tconst int A_STRIDES = THREADS_PER_BLOCK / A_THREADS_PER_ROW;\n\tconst int B_STRIDES = THREADS_PER_BLOCK / B_THREADS_PER_ROW;\n\n\t__shared__ float A_shared[BLOCK_SIZE_M * BLOCK_SIZE_K];\n\t__shared__ float B_shared[BLOCK_SIZE_N * BLOCK_SIZE_K_SPARSE];\n\t__shared__ int B_index_shared[BLOCK_SIZE_N * BLOCK_SIZE_K_SPARSE];\n\n\tfloat A_reg[THREAD_SIZE_M];\n\tfloat B_reg[THREAD_SIZE_N];\n\tint B_reg_index;\n\tfloat C_reg[THREAD_SIZE_M][THREAD_SIZE_N] = {0};\n\n\tint tid = threadIdx.x;\n\n\tint t_N = tid % (BLOCK_SIZE_N / THREAD_SIZE_N);\n\tint t_M = tid / (BLOCK_SIZE_N / THREAD_SIZE_N);\n\n\tint A_BLOCK_ROW_START = tid / A_THREADS_PER_ROW;\n\tint B_BLOCK_ROW_START = tid / B_THREADS_PER_ROW;\n\n\tint A_BLOCK_COL_START = tid % A_THREADS_PER_ROW * 4;\n\tint B_BLOCK_COL_START = tid % B_THREADS_PER_ROW * 4;\n\n\tfor(int K_BLOCK_START = 0, K_SPARSE_BLOCK_START = 0; K_BLOCK_START < K; K_BLOCK_START += BLOCK_SIZE_K, K_SPARSE_BLOCK_START += BLOCK_SIZE_K_SPARSE){\n\t\tfloat *A_global_ptr = g_vec + M_BLOCK_START * K + K_BLOCK_START;\n\t\tfloat *B_global_ptr = g_mat_data + K_SPARSE_BLOCK_START * N + N_BLOCK_START;\n\t\tint *B_index_global_ptr = g_mat_index + K_SPARSE_BLOCK_START * N + N_BLOCK_START;\n\n\t\t__syncthreads();\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_M; i += A_STRIDES){\n\t\t\t*(float4 *)(A_shared + (i + A_BLOCK_ROW_START) * BLOCK_SIZE_K + A_BLOCK_COL_START) = \n\t\t\t\t*(float4 *)(A_global_ptr + (i + A_BLOCK_ROW_START) * K + A_BLOCK_COL_START);\n\t\t}\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_K_SPARSE; i += B_STRIDES){\n\t\t\t*(float4 *)(B_shared + (i + B_BLOCK_ROW_START) * BLOCK_SIZE_N + B_BLOCK_COL_START) =\n\t\t\t\t*(float4 *)(B_global_ptr + (i + B_BLOCK_ROW_START) * N + B_BLOCK_COL_START);\n\n\t\t\t*(float4 *)(B_index_shared + (i + B_BLOCK_ROW_START) * BLOCK_SIZE_N + B_BLOCK_COL_START) =\n\t\t\t\t*(float4 *)(B_index_global_ptr + (i + B_BLOCK_ROW_START) * N + B_BLOCK_COL_START);\n\t\t}\n\n\t\t__syncthreads();\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_K_SPARSE;i += 1){\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_N; k += 1){\n\t\t\t\tB_reg[k] = B_shared[i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k];\n\t\t\t\t//*(float4 *)(B_reg + k) = *(float4 *)(B_shared + i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k);\n\t\t\t\t//*(float4 *)(B_reg_index + k) = *(float4 *)(B_index_shared + i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k);\n\t\t\t}\n\t\t\tint bank_idx = i / LEN_OF_BANK_PER_SPARSE_BLOCK;\n\t\t\tB_reg_index = B_index_shared[i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N] % BANK_VAL+bank_idx * BANK_VAL;\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_M; k += 1){\n\t\t\t\tA_reg[k] = A_shared[(t_M * THREAD_SIZE_M+k) * BLOCK_SIZE_K + B_reg_index];\n\t\t\t}\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_N; k += 1){\n\t\t\t\t#pragma unroll\n\t\t\t\tfor(int j = 0; j < THREAD_SIZE_M; j += 1){\n\t\t\t\t\tC_reg[j][k] += B_reg[k] * A_reg[j];\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n    float bias_local[THREAD_SIZE_N];\n    for(int thread_x = 0; thread_x < THREAD_SIZE_N; thread_x++){\n        bias_local[thread_x] = bias[BLOCK_SIZE_N * blockIdx.y + THREAD_SIZE_N * t_N + thread_x];\n    }\n\n\t#pragma unroll\n\tfor(int i = 0; i < THREAD_SIZE_M; i += 1){\n\t\t#pragma unroll\n\t\tfor(int j = 0; j < THREAD_SIZE_N; j += 1){\n\t\t\tg_data[(BLOCK_SIZE_M * blockIdx.x + THREAD_SIZE_M * t_M + i) * N + BLOCK_SIZE_N * blockIdx.y + THREAD_SIZE_N * t_N + j] = C_reg[i][j] + bias_local[j];\n\t\t}\n\t}\n}", "launch_config": {"dimBlock": [128, 1], "dimGrid": [64, 6]}}, "bert.encoder.layer.7.attention.output.dense": {"code": "__global__ void MatMul_TILE_THREAD_GENERAL(float *input0, float *input1, float *input2, float *input3, float *output0) {\n    /*\n    g_vec: A [M, K]\n    g_mat_data: condense B [K_sparse, N]\n    g_mat_index: condense B index [K_sparse, N]\n    g_data: C [M, N]\n    */\n\tfloat *g_vec = input0;\n\tfloat *g_mat_data = input1;\n\tint *g_mat_index = (int*)input2;\n\tfloat *bias = input3;\n\tfloat *g_data = output0;\n\n\tconst int M = 4096;\n\tconst int K = 768;\n\tconst int N = 768;\n\n    const int BLOCK_SIZE_M = 64;\n    const int BLOCK_SIZE_N = 128;\n    const int BLOCK_SIZE_K = 32;\n    const int THREAD_SIZE_M = 8;\n    const int THREAD_SIZE_N = 8;\n\n    const float SPARSITY = 0.875;\n    const int BANK_VAL = 32;\n    const int ALIGN_N = THREAD_SIZE_N;\n\n    const int BANK_NUM_PER_BLOCK = BLOCK_SIZE_K / BANK_VAL;\n    const int BLOCK_SIZE_K_SPARSE = int(BLOCK_SIZE_K * (1-SPARSITY));\n    const int LEN_OF_BANK_PER_SPARSE_BLOCK = BLOCK_SIZE_K_SPARSE / BANK_NUM_PER_BLOCK;\n\n\tconst int K_SPARSE = int(K * SPARSITY);\n\tint M_BLOCK_START = blockIdx.x * BLOCK_SIZE_M;\n\tint N_BLOCK_START = blockIdx.y * BLOCK_SIZE_N;\n\n\tconst int A_THREADS_PER_ROW = BLOCK_SIZE_K / 4;\n\tconst int B_THREADS_PER_ROW = BLOCK_SIZE_N / 4;\n\n\tconst int THREADS_PER_BLOCK = (BLOCK_SIZE_M / THREAD_SIZE_M) * (BLOCK_SIZE_N / THREAD_SIZE_N);\n\n\tconst int A_STRIDES = THREADS_PER_BLOCK / A_THREADS_PER_ROW;\n\tconst int B_STRIDES = THREADS_PER_BLOCK / B_THREADS_PER_ROW;\n\n\t__shared__ float A_shared[BLOCK_SIZE_M * BLOCK_SIZE_K];\n\t__shared__ float B_shared[BLOCK_SIZE_N * BLOCK_SIZE_K_SPARSE];\n\t__shared__ int B_index_shared[BLOCK_SIZE_N * BLOCK_SIZE_K_SPARSE];\n\n\tfloat A_reg[THREAD_SIZE_M];\n\tfloat B_reg[THREAD_SIZE_N];\n\tint B_reg_index;\n\tfloat C_reg[THREAD_SIZE_M][THREAD_SIZE_N] = {0};\n\n\tint tid = threadIdx.x;\n\n\tint t_N = tid % (BLOCK_SIZE_N / THREAD_SIZE_N);\n\tint t_M = tid / (BLOCK_SIZE_N / THREAD_SIZE_N);\n\n\tint A_BLOCK_ROW_START = tid / A_THREADS_PER_ROW;\n\tint B_BLOCK_ROW_START = tid / B_THREADS_PER_ROW;\n\n\tint A_BLOCK_COL_START = tid % A_THREADS_PER_ROW * 4;\n\tint B_BLOCK_COL_START = tid % B_THREADS_PER_ROW * 4;\n\n\tfor(int K_BLOCK_START = 0, K_SPARSE_BLOCK_START = 0; K_BLOCK_START < K; K_BLOCK_START += BLOCK_SIZE_K, K_SPARSE_BLOCK_START += BLOCK_SIZE_K_SPARSE){\n\t\tfloat *A_global_ptr = g_vec + M_BLOCK_START * K + K_BLOCK_START;\n\t\tfloat *B_global_ptr = g_mat_data + K_SPARSE_BLOCK_START * N + N_BLOCK_START;\n\t\tint *B_index_global_ptr = g_mat_index + K_SPARSE_BLOCK_START * N + N_BLOCK_START;\n\n\t\t__syncthreads();\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_M; i += A_STRIDES){\n\t\t\t*(float4 *)(A_shared + (i + A_BLOCK_ROW_START) * BLOCK_SIZE_K + A_BLOCK_COL_START) = \n\t\t\t\t*(float4 *)(A_global_ptr + (i + A_BLOCK_ROW_START) * K + A_BLOCK_COL_START);\n\t\t}\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_K_SPARSE; i += B_STRIDES){\n\t\t\t*(float4 *)(B_shared + (i + B_BLOCK_ROW_START) * BLOCK_SIZE_N + B_BLOCK_COL_START) =\n\t\t\t\t*(float4 *)(B_global_ptr + (i + B_BLOCK_ROW_START) * N + B_BLOCK_COL_START);\n\n\t\t\t*(float4 *)(B_index_shared + (i + B_BLOCK_ROW_START) * BLOCK_SIZE_N + B_BLOCK_COL_START) =\n\t\t\t\t*(float4 *)(B_index_global_ptr + (i + B_BLOCK_ROW_START) * N + B_BLOCK_COL_START);\n\t\t}\n\n\t\t__syncthreads();\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_K_SPARSE;i += 1){\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_N; k += 1){\n\t\t\t\tB_reg[k] = B_shared[i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k];\n\t\t\t\t//*(float4 *)(B_reg + k) = *(float4 *)(B_shared + i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k);\n\t\t\t\t//*(float4 *)(B_reg_index + k) = *(float4 *)(B_index_shared + i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k);\n\t\t\t}\n\t\t\tint bank_idx = i / LEN_OF_BANK_PER_SPARSE_BLOCK;\n\t\t\tB_reg_index = B_index_shared[i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N] % BANK_VAL+bank_idx * BANK_VAL;\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_M; k += 1){\n\t\t\t\tA_reg[k] = A_shared[(t_M * THREAD_SIZE_M+k) * BLOCK_SIZE_K + B_reg_index];\n\t\t\t}\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_N; k += 1){\n\t\t\t\t#pragma unroll\n\t\t\t\tfor(int j = 0; j < THREAD_SIZE_M; j += 1){\n\t\t\t\t\tC_reg[j][k] += B_reg[k] * A_reg[j];\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n    float bias_local[THREAD_SIZE_N];\n    for(int thread_x = 0; thread_x < THREAD_SIZE_N; thread_x++){\n        bias_local[thread_x] = bias[BLOCK_SIZE_N * blockIdx.y + THREAD_SIZE_N * t_N + thread_x];\n    }\n\n\t#pragma unroll\n\tfor(int i = 0; i < THREAD_SIZE_M; i += 1){\n\t\t#pragma unroll\n\t\tfor(int j = 0; j < THREAD_SIZE_N; j += 1){\n\t\t\tg_data[(BLOCK_SIZE_M * blockIdx.x + THREAD_SIZE_M * t_M + i) * N + BLOCK_SIZE_N * blockIdx.y + THREAD_SIZE_N * t_N + j] = C_reg[i][j] + bias_local[j];\n\t\t}\n\t}\n}", "launch_config": {"dimBlock": [128, 1], "dimGrid": [64, 6]}}, "bert.encoder.layer.7.intermediate.dense": {"code": "__global__ void MatMul_TILE_THREAD_GENERAL(float *input0, float *input1, float *input2, float *input3, float *output0) {\n    /*\n    g_vec: A [M, K]\n    g_mat_data: condense B [K_sparse, N]\n    g_mat_index: condense B index [K_sparse, N]\n    g_data: C [M, N]\n    */\n\tfloat *g_vec = input0;\n\tfloat *g_mat_data = input1;\n\tint *g_mat_index = (int*)input2;\n\tfloat *bias = input3;\n\tfloat *g_data = output0;\n\n\tconst int M = 4096;\n\tconst int K = 768;\n\tconst int N = 3072;\n\n    const int BLOCK_SIZE_M = 64;\n    const int BLOCK_SIZE_N = 128;\n    const int BLOCK_SIZE_K = 32;\n    const int THREAD_SIZE_M = 8;\n    const int THREAD_SIZE_N = 8;\n\n    const float SPARSITY = 0.875;\n    const int BANK_VAL = 32;\n    const int ALIGN_N = THREAD_SIZE_N;\n\n    const int BANK_NUM_PER_BLOCK = BLOCK_SIZE_K / BANK_VAL;\n    const int BLOCK_SIZE_K_SPARSE = int(BLOCK_SIZE_K * (1-SPARSITY));\n    const int LEN_OF_BANK_PER_SPARSE_BLOCK = BLOCK_SIZE_K_SPARSE / BANK_NUM_PER_BLOCK;\n\n\tconst int K_SPARSE = int(K * SPARSITY);\n\tint M_BLOCK_START = blockIdx.x * BLOCK_SIZE_M;\n\tint N_BLOCK_START = blockIdx.y * BLOCK_SIZE_N;\n\n\tconst int A_THREADS_PER_ROW = BLOCK_SIZE_K / 4;\n\tconst int B_THREADS_PER_ROW = BLOCK_SIZE_N / 4;\n\n\tconst int THREADS_PER_BLOCK = (BLOCK_SIZE_M / THREAD_SIZE_M) * (BLOCK_SIZE_N / THREAD_SIZE_N);\n\n\tconst int A_STRIDES = THREADS_PER_BLOCK / A_THREADS_PER_ROW;\n\tconst int B_STRIDES = THREADS_PER_BLOCK / B_THREADS_PER_ROW;\n\n\t__shared__ float A_shared[BLOCK_SIZE_M * BLOCK_SIZE_K];\n\t__shared__ float B_shared[BLOCK_SIZE_N * BLOCK_SIZE_K_SPARSE];\n\t__shared__ int B_index_shared[BLOCK_SIZE_N * BLOCK_SIZE_K_SPARSE];\n\n\tfloat A_reg[THREAD_SIZE_M];\n\tfloat B_reg[THREAD_SIZE_N];\n\tint B_reg_index;\n\tfloat C_reg[THREAD_SIZE_M][THREAD_SIZE_N] = {0};\n\n\tint tid = threadIdx.x;\n\n\tint t_N = tid % (BLOCK_SIZE_N / THREAD_SIZE_N);\n\tint t_M = tid / (BLOCK_SIZE_N / THREAD_SIZE_N);\n\n\tint A_BLOCK_ROW_START = tid / A_THREADS_PER_ROW;\n\tint B_BLOCK_ROW_START = tid / B_THREADS_PER_ROW;\n\n\tint A_BLOCK_COL_START = tid % A_THREADS_PER_ROW * 4;\n\tint B_BLOCK_COL_START = tid % B_THREADS_PER_ROW * 4;\n\n\tfor(int K_BLOCK_START = 0, K_SPARSE_BLOCK_START = 0; K_BLOCK_START < K; K_BLOCK_START += BLOCK_SIZE_K, K_SPARSE_BLOCK_START += BLOCK_SIZE_K_SPARSE){\n\t\tfloat *A_global_ptr = g_vec + M_BLOCK_START * K + K_BLOCK_START;\n\t\tfloat *B_global_ptr = g_mat_data + K_SPARSE_BLOCK_START * N + N_BLOCK_START;\n\t\tint *B_index_global_ptr = g_mat_index + K_SPARSE_BLOCK_START * N + N_BLOCK_START;\n\n\t\t__syncthreads();\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_M; i += A_STRIDES){\n\t\t\t*(float4 *)(A_shared + (i + A_BLOCK_ROW_START) * BLOCK_SIZE_K + A_BLOCK_COL_START) = \n\t\t\t\t*(float4 *)(A_global_ptr + (i + A_BLOCK_ROW_START) * K + A_BLOCK_COL_START);\n\t\t}\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_K_SPARSE; i += B_STRIDES){\n\t\t\t*(float4 *)(B_shared + (i + B_BLOCK_ROW_START) * BLOCK_SIZE_N + B_BLOCK_COL_START) =\n\t\t\t\t*(float4 *)(B_global_ptr + (i + B_BLOCK_ROW_START) * N + B_BLOCK_COL_START);\n\n\t\t\t*(float4 *)(B_index_shared + (i + B_BLOCK_ROW_START) * BLOCK_SIZE_N + B_BLOCK_COL_START) =\n\t\t\t\t*(float4 *)(B_index_global_ptr + (i + B_BLOCK_ROW_START) * N + B_BLOCK_COL_START);\n\t\t}\n\n\t\t__syncthreads();\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_K_SPARSE;i += 1){\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_N; k += 1){\n\t\t\t\tB_reg[k] = B_shared[i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k];\n\t\t\t\t//*(float4 *)(B_reg + k) = *(float4 *)(B_shared + i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k);\n\t\t\t\t//*(float4 *)(B_reg_index + k) = *(float4 *)(B_index_shared + i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k);\n\t\t\t}\n\t\t\tint bank_idx = i / LEN_OF_BANK_PER_SPARSE_BLOCK;\n\t\t\tB_reg_index = B_index_shared[i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N] % BANK_VAL+bank_idx * BANK_VAL;\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_M; k += 1){\n\t\t\t\tA_reg[k] = A_shared[(t_M * THREAD_SIZE_M+k) * BLOCK_SIZE_K + B_reg_index];\n\t\t\t}\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_N; k += 1){\n\t\t\t\t#pragma unroll\n\t\t\t\tfor(int j = 0; j < THREAD_SIZE_M; j += 1){\n\t\t\t\t\tC_reg[j][k] += B_reg[k] * A_reg[j];\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n    float bias_local[THREAD_SIZE_N];\n    for(int thread_x = 0; thread_x < THREAD_SIZE_N; thread_x++){\n        bias_local[thread_x] = bias[BLOCK_SIZE_N * blockIdx.y + THREAD_SIZE_N * t_N + thread_x];\n    }\n\n\t#pragma unroll\n\tfor(int i = 0; i < THREAD_SIZE_M; i += 1){\n\t\t#pragma unroll\n\t\tfor(int j = 0; j < THREAD_SIZE_N; j += 1){\n\t\t\tg_data[(BLOCK_SIZE_M * blockIdx.x + THREAD_SIZE_M * t_M + i) * N + BLOCK_SIZE_N * blockIdx.y + THREAD_SIZE_N * t_N + j] = C_reg[i][j] + bias_local[j];\n\t\t}\n\t}\n}", "launch_config": {"dimBlock": [128, 1], "dimGrid": [64, 24]}}, "bert.encoder.layer.7.output.dense": {"code": "__global__ void MatMul_TILE_THREAD_GENERAL(float *input0, float *input1, float *input2, float *input3, float *output0) {\n    /*\n    g_vec: A [M, K]\n    g_mat_data: condense B [K_sparse, N]\n    g_mat_index: condense B index [K_sparse, N]\n    g_data: C [M, N]\n    */\n\tfloat *g_vec = input0;\n\tfloat *g_mat_data = input1;\n\tint *g_mat_index = (int*)input2;\n\tfloat *bias = input3;\n\tfloat *g_data = output0;\n\n\tconst int M = 4096;\n\tconst int K = 3072;\n\tconst int N = 768;\n\n    const int BLOCK_SIZE_M = 64;\n    const int BLOCK_SIZE_N = 128;\n    const int BLOCK_SIZE_K = 32;\n    const int THREAD_SIZE_M = 8;\n    const int THREAD_SIZE_N = 8;\n\n    const float SPARSITY = 0.875;\n    const int BANK_VAL = 32;\n    const int ALIGN_N = THREAD_SIZE_N;\n\n    const int BANK_NUM_PER_BLOCK = BLOCK_SIZE_K / BANK_VAL;\n    const int BLOCK_SIZE_K_SPARSE = int(BLOCK_SIZE_K * (1-SPARSITY));\n    const int LEN_OF_BANK_PER_SPARSE_BLOCK = BLOCK_SIZE_K_SPARSE / BANK_NUM_PER_BLOCK;\n\n\tconst int K_SPARSE = int(K * SPARSITY);\n\tint M_BLOCK_START = blockIdx.x * BLOCK_SIZE_M;\n\tint N_BLOCK_START = blockIdx.y * BLOCK_SIZE_N;\n\n\tconst int A_THREADS_PER_ROW = BLOCK_SIZE_K / 4;\n\tconst int B_THREADS_PER_ROW = BLOCK_SIZE_N / 4;\n\n\tconst int THREADS_PER_BLOCK = (BLOCK_SIZE_M / THREAD_SIZE_M) * (BLOCK_SIZE_N / THREAD_SIZE_N);\n\n\tconst int A_STRIDES = THREADS_PER_BLOCK / A_THREADS_PER_ROW;\n\tconst int B_STRIDES = THREADS_PER_BLOCK / B_THREADS_PER_ROW;\n\n\t__shared__ float A_shared[BLOCK_SIZE_M * BLOCK_SIZE_K];\n\t__shared__ float B_shared[BLOCK_SIZE_N * BLOCK_SIZE_K_SPARSE];\n\t__shared__ int B_index_shared[BLOCK_SIZE_N * BLOCK_SIZE_K_SPARSE];\n\n\tfloat A_reg[THREAD_SIZE_M];\n\tfloat B_reg[THREAD_SIZE_N];\n\tint B_reg_index;\n\tfloat C_reg[THREAD_SIZE_M][THREAD_SIZE_N] = {0};\n\n\tint tid = threadIdx.x;\n\n\tint t_N = tid % (BLOCK_SIZE_N / THREAD_SIZE_N);\n\tint t_M = tid / (BLOCK_SIZE_N / THREAD_SIZE_N);\n\n\tint A_BLOCK_ROW_START = tid / A_THREADS_PER_ROW;\n\tint B_BLOCK_ROW_START = tid / B_THREADS_PER_ROW;\n\n\tint A_BLOCK_COL_START = tid % A_THREADS_PER_ROW * 4;\n\tint B_BLOCK_COL_START = tid % B_THREADS_PER_ROW * 4;\n\n\tfor(int K_BLOCK_START = 0, K_SPARSE_BLOCK_START = 0; K_BLOCK_START < K; K_BLOCK_START += BLOCK_SIZE_K, K_SPARSE_BLOCK_START += BLOCK_SIZE_K_SPARSE){\n\t\tfloat *A_global_ptr = g_vec + M_BLOCK_START * K + K_BLOCK_START;\n\t\tfloat *B_global_ptr = g_mat_data + K_SPARSE_BLOCK_START * N + N_BLOCK_START;\n\t\tint *B_index_global_ptr = g_mat_index + K_SPARSE_BLOCK_START * N + N_BLOCK_START;\n\n\t\t__syncthreads();\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_M; i += A_STRIDES){\n\t\t\t*(float4 *)(A_shared + (i + A_BLOCK_ROW_START) * BLOCK_SIZE_K + A_BLOCK_COL_START) = \n\t\t\t\t*(float4 *)(A_global_ptr + (i + A_BLOCK_ROW_START) * K + A_BLOCK_COL_START);\n\t\t}\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_K_SPARSE; i += B_STRIDES){\n\t\t\t*(float4 *)(B_shared + (i + B_BLOCK_ROW_START) * BLOCK_SIZE_N + B_BLOCK_COL_START) =\n\t\t\t\t*(float4 *)(B_global_ptr + (i + B_BLOCK_ROW_START) * N + B_BLOCK_COL_START);\n\n\t\t\t*(float4 *)(B_index_shared + (i + B_BLOCK_ROW_START) * BLOCK_SIZE_N + B_BLOCK_COL_START) =\n\t\t\t\t*(float4 *)(B_index_global_ptr + (i + B_BLOCK_ROW_START) * N + B_BLOCK_COL_START);\n\t\t}\n\n\t\t__syncthreads();\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_K_SPARSE;i += 1){\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_N; k += 1){\n\t\t\t\tB_reg[k] = B_shared[i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k];\n\t\t\t\t//*(float4 *)(B_reg + k) = *(float4 *)(B_shared + i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k);\n\t\t\t\t//*(float4 *)(B_reg_index + k) = *(float4 *)(B_index_shared + i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k);\n\t\t\t}\n\t\t\tint bank_idx = i / LEN_OF_BANK_PER_SPARSE_BLOCK;\n\t\t\tB_reg_index = B_index_shared[i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N] % BANK_VAL+bank_idx * BANK_VAL;\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_M; k += 1){\n\t\t\t\tA_reg[k] = A_shared[(t_M * THREAD_SIZE_M+k) * BLOCK_SIZE_K + B_reg_index];\n\t\t\t}\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_N; k += 1){\n\t\t\t\t#pragma unroll\n\t\t\t\tfor(int j = 0; j < THREAD_SIZE_M; j += 1){\n\t\t\t\t\tC_reg[j][k] += B_reg[k] * A_reg[j];\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n    float bias_local[THREAD_SIZE_N];\n    for(int thread_x = 0; thread_x < THREAD_SIZE_N; thread_x++){\n        bias_local[thread_x] = bias[BLOCK_SIZE_N * blockIdx.y + THREAD_SIZE_N * t_N + thread_x];\n    }\n\n\t#pragma unroll\n\tfor(int i = 0; i < THREAD_SIZE_M; i += 1){\n\t\t#pragma unroll\n\t\tfor(int j = 0; j < THREAD_SIZE_N; j += 1){\n\t\t\tg_data[(BLOCK_SIZE_M * blockIdx.x + THREAD_SIZE_M * t_M + i) * N + BLOCK_SIZE_N * blockIdx.y + THREAD_SIZE_N * t_N + j] = C_reg[i][j] + bias_local[j];\n\t\t}\n\t}\n}", "launch_config": {"dimBlock": [128, 1], "dimGrid": [64, 6]}}, "bert.encoder.layer.8.attention.self.query": {"code": "__global__ void MatMul_TILE_THREAD_GENERAL(float *input0, float *input1, float *input2, float *input3, float *output0) {\n    /*\n    g_vec: A [M, K]\n    g_mat_data: condense B [K_sparse, N]\n    g_mat_index: condense B index [K_sparse, N]\n    g_data: C [M, N]\n    */\n\tfloat *g_vec = input0;\n\tfloat *g_mat_data = input1;\n\tint *g_mat_index = (int*)input2;\n\tfloat *bias = input3;\n\tfloat *g_data = output0;\n\n\tconst int M = 4096;\n\tconst int K = 768;\n\tconst int N = 768;\n\n    const int BLOCK_SIZE_M = 64;\n    const int BLOCK_SIZE_N = 128;\n    const int BLOCK_SIZE_K = 32;\n    const int THREAD_SIZE_M = 8;\n    const int THREAD_SIZE_N = 8;\n\n    const float SPARSITY = 0.875;\n    const int BANK_VAL = 32;\n    const int ALIGN_N = THREAD_SIZE_N;\n\n    const int BANK_NUM_PER_BLOCK = BLOCK_SIZE_K / BANK_VAL;\n    const int BLOCK_SIZE_K_SPARSE = int(BLOCK_SIZE_K * (1-SPARSITY));\n    const int LEN_OF_BANK_PER_SPARSE_BLOCK = BLOCK_SIZE_K_SPARSE / BANK_NUM_PER_BLOCK;\n\n\tconst int K_SPARSE = int(K * SPARSITY);\n\tint M_BLOCK_START = blockIdx.x * BLOCK_SIZE_M;\n\tint N_BLOCK_START = blockIdx.y * BLOCK_SIZE_N;\n\n\tconst int A_THREADS_PER_ROW = BLOCK_SIZE_K / 4;\n\tconst int B_THREADS_PER_ROW = BLOCK_SIZE_N / 4;\n\n\tconst int THREADS_PER_BLOCK = (BLOCK_SIZE_M / THREAD_SIZE_M) * (BLOCK_SIZE_N / THREAD_SIZE_N);\n\n\tconst int A_STRIDES = THREADS_PER_BLOCK / A_THREADS_PER_ROW;\n\tconst int B_STRIDES = THREADS_PER_BLOCK / B_THREADS_PER_ROW;\n\n\t__shared__ float A_shared[BLOCK_SIZE_M * BLOCK_SIZE_K];\n\t__shared__ float B_shared[BLOCK_SIZE_N * BLOCK_SIZE_K_SPARSE];\n\t__shared__ int B_index_shared[BLOCK_SIZE_N * BLOCK_SIZE_K_SPARSE];\n\n\tfloat A_reg[THREAD_SIZE_M];\n\tfloat B_reg[THREAD_SIZE_N];\n\tint B_reg_index;\n\tfloat C_reg[THREAD_SIZE_M][THREAD_SIZE_N] = {0};\n\n\tint tid = threadIdx.x;\n\n\tint t_N = tid % (BLOCK_SIZE_N / THREAD_SIZE_N);\n\tint t_M = tid / (BLOCK_SIZE_N / THREAD_SIZE_N);\n\n\tint A_BLOCK_ROW_START = tid / A_THREADS_PER_ROW;\n\tint B_BLOCK_ROW_START = tid / B_THREADS_PER_ROW;\n\n\tint A_BLOCK_COL_START = tid % A_THREADS_PER_ROW * 4;\n\tint B_BLOCK_COL_START = tid % B_THREADS_PER_ROW * 4;\n\n\tfor(int K_BLOCK_START = 0, K_SPARSE_BLOCK_START = 0; K_BLOCK_START < K; K_BLOCK_START += BLOCK_SIZE_K, K_SPARSE_BLOCK_START += BLOCK_SIZE_K_SPARSE){\n\t\tfloat *A_global_ptr = g_vec + M_BLOCK_START * K + K_BLOCK_START;\n\t\tfloat *B_global_ptr = g_mat_data + K_SPARSE_BLOCK_START * N + N_BLOCK_START;\n\t\tint *B_index_global_ptr = g_mat_index + K_SPARSE_BLOCK_START * N + N_BLOCK_START;\n\n\t\t__syncthreads();\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_M; i += A_STRIDES){\n\t\t\t*(float4 *)(A_shared + (i + A_BLOCK_ROW_START) * BLOCK_SIZE_K + A_BLOCK_COL_START) = \n\t\t\t\t*(float4 *)(A_global_ptr + (i + A_BLOCK_ROW_START) * K + A_BLOCK_COL_START);\n\t\t}\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_K_SPARSE; i += B_STRIDES){\n\t\t\t*(float4 *)(B_shared + (i + B_BLOCK_ROW_START) * BLOCK_SIZE_N + B_BLOCK_COL_START) =\n\t\t\t\t*(float4 *)(B_global_ptr + (i + B_BLOCK_ROW_START) * N + B_BLOCK_COL_START);\n\n\t\t\t*(float4 *)(B_index_shared + (i + B_BLOCK_ROW_START) * BLOCK_SIZE_N + B_BLOCK_COL_START) =\n\t\t\t\t*(float4 *)(B_index_global_ptr + (i + B_BLOCK_ROW_START) * N + B_BLOCK_COL_START);\n\t\t}\n\n\t\t__syncthreads();\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_K_SPARSE;i += 1){\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_N; k += 1){\n\t\t\t\tB_reg[k] = B_shared[i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k];\n\t\t\t\t//*(float4 *)(B_reg + k) = *(float4 *)(B_shared + i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k);\n\t\t\t\t//*(float4 *)(B_reg_index + k) = *(float4 *)(B_index_shared + i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k);\n\t\t\t}\n\t\t\tint bank_idx = i / LEN_OF_BANK_PER_SPARSE_BLOCK;\n\t\t\tB_reg_index = B_index_shared[i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N] % BANK_VAL+bank_idx * BANK_VAL;\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_M; k += 1){\n\t\t\t\tA_reg[k] = A_shared[(t_M * THREAD_SIZE_M+k) * BLOCK_SIZE_K + B_reg_index];\n\t\t\t}\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_N; k += 1){\n\t\t\t\t#pragma unroll\n\t\t\t\tfor(int j = 0; j < THREAD_SIZE_M; j += 1){\n\t\t\t\t\tC_reg[j][k] += B_reg[k] * A_reg[j];\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n    float bias_local[THREAD_SIZE_N];\n    for(int thread_x = 0; thread_x < THREAD_SIZE_N; thread_x++){\n        bias_local[thread_x] = bias[BLOCK_SIZE_N * blockIdx.y + THREAD_SIZE_N * t_N + thread_x];\n    }\n\n\t#pragma unroll\n\tfor(int i = 0; i < THREAD_SIZE_M; i += 1){\n\t\t#pragma unroll\n\t\tfor(int j = 0; j < THREAD_SIZE_N; j += 1){\n\t\t\tg_data[(BLOCK_SIZE_M * blockIdx.x + THREAD_SIZE_M * t_M + i) * N + BLOCK_SIZE_N * blockIdx.y + THREAD_SIZE_N * t_N + j] = C_reg[i][j] + bias_local[j];\n\t\t}\n\t}\n}", "launch_config": {"dimBlock": [128, 1], "dimGrid": [64, 6]}}, "bert.encoder.layer.8.attention.self.key": {"code": "__global__ void MatMul_TILE_THREAD_GENERAL(float *input0, float *input1, float *input2, float *input3, float *output0) {\n    /*\n    g_vec: A [M, K]\n    g_mat_data: condense B [K_sparse, N]\n    g_mat_index: condense B index [K_sparse, N]\n    g_data: C [M, N]\n    */\n\tfloat *g_vec = input0;\n\tfloat *g_mat_data = input1;\n\tint *g_mat_index = (int*)input2;\n\tfloat *bias = input3;\n\tfloat *g_data = output0;\n\n\tconst int M = 4096;\n\tconst int K = 768;\n\tconst int N = 768;\n\n    const int BLOCK_SIZE_M = 64;\n    const int BLOCK_SIZE_N = 128;\n    const int BLOCK_SIZE_K = 32;\n    const int THREAD_SIZE_M = 8;\n    const int THREAD_SIZE_N = 8;\n\n    const float SPARSITY = 0.875;\n    const int BANK_VAL = 32;\n    const int ALIGN_N = THREAD_SIZE_N;\n\n    const int BANK_NUM_PER_BLOCK = BLOCK_SIZE_K / BANK_VAL;\n    const int BLOCK_SIZE_K_SPARSE = int(BLOCK_SIZE_K * (1-SPARSITY));\n    const int LEN_OF_BANK_PER_SPARSE_BLOCK = BLOCK_SIZE_K_SPARSE / BANK_NUM_PER_BLOCK;\n\n\tconst int K_SPARSE = int(K * SPARSITY);\n\tint M_BLOCK_START = blockIdx.x * BLOCK_SIZE_M;\n\tint N_BLOCK_START = blockIdx.y * BLOCK_SIZE_N;\n\n\tconst int A_THREADS_PER_ROW = BLOCK_SIZE_K / 4;\n\tconst int B_THREADS_PER_ROW = BLOCK_SIZE_N / 4;\n\n\tconst int THREADS_PER_BLOCK = (BLOCK_SIZE_M / THREAD_SIZE_M) * (BLOCK_SIZE_N / THREAD_SIZE_N);\n\n\tconst int A_STRIDES = THREADS_PER_BLOCK / A_THREADS_PER_ROW;\n\tconst int B_STRIDES = THREADS_PER_BLOCK / B_THREADS_PER_ROW;\n\n\t__shared__ float A_shared[BLOCK_SIZE_M * BLOCK_SIZE_K];\n\t__shared__ float B_shared[BLOCK_SIZE_N * BLOCK_SIZE_K_SPARSE];\n\t__shared__ int B_index_shared[BLOCK_SIZE_N * BLOCK_SIZE_K_SPARSE];\n\n\tfloat A_reg[THREAD_SIZE_M];\n\tfloat B_reg[THREAD_SIZE_N];\n\tint B_reg_index;\n\tfloat C_reg[THREAD_SIZE_M][THREAD_SIZE_N] = {0};\n\n\tint tid = threadIdx.x;\n\n\tint t_N = tid % (BLOCK_SIZE_N / THREAD_SIZE_N);\n\tint t_M = tid / (BLOCK_SIZE_N / THREAD_SIZE_N);\n\n\tint A_BLOCK_ROW_START = tid / A_THREADS_PER_ROW;\n\tint B_BLOCK_ROW_START = tid / B_THREADS_PER_ROW;\n\n\tint A_BLOCK_COL_START = tid % A_THREADS_PER_ROW * 4;\n\tint B_BLOCK_COL_START = tid % B_THREADS_PER_ROW * 4;\n\n\tfor(int K_BLOCK_START = 0, K_SPARSE_BLOCK_START = 0; K_BLOCK_START < K; K_BLOCK_START += BLOCK_SIZE_K, K_SPARSE_BLOCK_START += BLOCK_SIZE_K_SPARSE){\n\t\tfloat *A_global_ptr = g_vec + M_BLOCK_START * K + K_BLOCK_START;\n\t\tfloat *B_global_ptr = g_mat_data + K_SPARSE_BLOCK_START * N + N_BLOCK_START;\n\t\tint *B_index_global_ptr = g_mat_index + K_SPARSE_BLOCK_START * N + N_BLOCK_START;\n\n\t\t__syncthreads();\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_M; i += A_STRIDES){\n\t\t\t*(float4 *)(A_shared + (i + A_BLOCK_ROW_START) * BLOCK_SIZE_K + A_BLOCK_COL_START) = \n\t\t\t\t*(float4 *)(A_global_ptr + (i + A_BLOCK_ROW_START) * K + A_BLOCK_COL_START);\n\t\t}\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_K_SPARSE; i += B_STRIDES){\n\t\t\t*(float4 *)(B_shared + (i + B_BLOCK_ROW_START) * BLOCK_SIZE_N + B_BLOCK_COL_START) =\n\t\t\t\t*(float4 *)(B_global_ptr + (i + B_BLOCK_ROW_START) * N + B_BLOCK_COL_START);\n\n\t\t\t*(float4 *)(B_index_shared + (i + B_BLOCK_ROW_START) * BLOCK_SIZE_N + B_BLOCK_COL_START) =\n\t\t\t\t*(float4 *)(B_index_global_ptr + (i + B_BLOCK_ROW_START) * N + B_BLOCK_COL_START);\n\t\t}\n\n\t\t__syncthreads();\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_K_SPARSE;i += 1){\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_N; k += 1){\n\t\t\t\tB_reg[k] = B_shared[i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k];\n\t\t\t\t//*(float4 *)(B_reg + k) = *(float4 *)(B_shared + i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k);\n\t\t\t\t//*(float4 *)(B_reg_index + k) = *(float4 *)(B_index_shared + i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k);\n\t\t\t}\n\t\t\tint bank_idx = i / LEN_OF_BANK_PER_SPARSE_BLOCK;\n\t\t\tB_reg_index = B_index_shared[i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N] % BANK_VAL+bank_idx * BANK_VAL;\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_M; k += 1){\n\t\t\t\tA_reg[k] = A_shared[(t_M * THREAD_SIZE_M+k) * BLOCK_SIZE_K + B_reg_index];\n\t\t\t}\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_N; k += 1){\n\t\t\t\t#pragma unroll\n\t\t\t\tfor(int j = 0; j < THREAD_SIZE_M; j += 1){\n\t\t\t\t\tC_reg[j][k] += B_reg[k] * A_reg[j];\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n    float bias_local[THREAD_SIZE_N];\n    for(int thread_x = 0; thread_x < THREAD_SIZE_N; thread_x++){\n        bias_local[thread_x] = bias[BLOCK_SIZE_N * blockIdx.y + THREAD_SIZE_N * t_N + thread_x];\n    }\n\n\t#pragma unroll\n\tfor(int i = 0; i < THREAD_SIZE_M; i += 1){\n\t\t#pragma unroll\n\t\tfor(int j = 0; j < THREAD_SIZE_N; j += 1){\n\t\t\tg_data[(BLOCK_SIZE_M * blockIdx.x + THREAD_SIZE_M * t_M + i) * N + BLOCK_SIZE_N * blockIdx.y + THREAD_SIZE_N * t_N + j] = C_reg[i][j] + bias_local[j];\n\t\t}\n\t}\n}", "launch_config": {"dimBlock": [128, 1], "dimGrid": [64, 6]}}, "bert.encoder.layer.8.attention.self.value": {"code": "__global__ void MatMul_TILE_THREAD_GENERAL(float *input0, float *input1, float *input2, float *input3, float *output0) {\n    /*\n    g_vec: A [M, K]\n    g_mat_data: condense B [K_sparse, N]\n    g_mat_index: condense B index [K_sparse, N]\n    g_data: C [M, N]\n    */\n\tfloat *g_vec = input0;\n\tfloat *g_mat_data = input1;\n\tint *g_mat_index = (int*)input2;\n\tfloat *bias = input3;\n\tfloat *g_data = output0;\n\n\tconst int M = 4096;\n\tconst int K = 768;\n\tconst int N = 768;\n\n    const int BLOCK_SIZE_M = 64;\n    const int BLOCK_SIZE_N = 128;\n    const int BLOCK_SIZE_K = 32;\n    const int THREAD_SIZE_M = 8;\n    const int THREAD_SIZE_N = 8;\n\n    const float SPARSITY = 0.875;\n    const int BANK_VAL = 32;\n    const int ALIGN_N = THREAD_SIZE_N;\n\n    const int BANK_NUM_PER_BLOCK = BLOCK_SIZE_K / BANK_VAL;\n    const int BLOCK_SIZE_K_SPARSE = int(BLOCK_SIZE_K * (1-SPARSITY));\n    const int LEN_OF_BANK_PER_SPARSE_BLOCK = BLOCK_SIZE_K_SPARSE / BANK_NUM_PER_BLOCK;\n\n\tconst int K_SPARSE = int(K * SPARSITY);\n\tint M_BLOCK_START = blockIdx.x * BLOCK_SIZE_M;\n\tint N_BLOCK_START = blockIdx.y * BLOCK_SIZE_N;\n\n\tconst int A_THREADS_PER_ROW = BLOCK_SIZE_K / 4;\n\tconst int B_THREADS_PER_ROW = BLOCK_SIZE_N / 4;\n\n\tconst int THREADS_PER_BLOCK = (BLOCK_SIZE_M / THREAD_SIZE_M) * (BLOCK_SIZE_N / THREAD_SIZE_N);\n\n\tconst int A_STRIDES = THREADS_PER_BLOCK / A_THREADS_PER_ROW;\n\tconst int B_STRIDES = THREADS_PER_BLOCK / B_THREADS_PER_ROW;\n\n\t__shared__ float A_shared[BLOCK_SIZE_M * BLOCK_SIZE_K];\n\t__shared__ float B_shared[BLOCK_SIZE_N * BLOCK_SIZE_K_SPARSE];\n\t__shared__ int B_index_shared[BLOCK_SIZE_N * BLOCK_SIZE_K_SPARSE];\n\n\tfloat A_reg[THREAD_SIZE_M];\n\tfloat B_reg[THREAD_SIZE_N];\n\tint B_reg_index;\n\tfloat C_reg[THREAD_SIZE_M][THREAD_SIZE_N] = {0};\n\n\tint tid = threadIdx.x;\n\n\tint t_N = tid % (BLOCK_SIZE_N / THREAD_SIZE_N);\n\tint t_M = tid / (BLOCK_SIZE_N / THREAD_SIZE_N);\n\n\tint A_BLOCK_ROW_START = tid / A_THREADS_PER_ROW;\n\tint B_BLOCK_ROW_START = tid / B_THREADS_PER_ROW;\n\n\tint A_BLOCK_COL_START = tid % A_THREADS_PER_ROW * 4;\n\tint B_BLOCK_COL_START = tid % B_THREADS_PER_ROW * 4;\n\n\tfor(int K_BLOCK_START = 0, K_SPARSE_BLOCK_START = 0; K_BLOCK_START < K; K_BLOCK_START += BLOCK_SIZE_K, K_SPARSE_BLOCK_START += BLOCK_SIZE_K_SPARSE){\n\t\tfloat *A_global_ptr = g_vec + M_BLOCK_START * K + K_BLOCK_START;\n\t\tfloat *B_global_ptr = g_mat_data + K_SPARSE_BLOCK_START * N + N_BLOCK_START;\n\t\tint *B_index_global_ptr = g_mat_index + K_SPARSE_BLOCK_START * N + N_BLOCK_START;\n\n\t\t__syncthreads();\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_M; i += A_STRIDES){\n\t\t\t*(float4 *)(A_shared + (i + A_BLOCK_ROW_START) * BLOCK_SIZE_K + A_BLOCK_COL_START) = \n\t\t\t\t*(float4 *)(A_global_ptr + (i + A_BLOCK_ROW_START) * K + A_BLOCK_COL_START);\n\t\t}\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_K_SPARSE; i += B_STRIDES){\n\t\t\t*(float4 *)(B_shared + (i + B_BLOCK_ROW_START) * BLOCK_SIZE_N + B_BLOCK_COL_START) =\n\t\t\t\t*(float4 *)(B_global_ptr + (i + B_BLOCK_ROW_START) * N + B_BLOCK_COL_START);\n\n\t\t\t*(float4 *)(B_index_shared + (i + B_BLOCK_ROW_START) * BLOCK_SIZE_N + B_BLOCK_COL_START) =\n\t\t\t\t*(float4 *)(B_index_global_ptr + (i + B_BLOCK_ROW_START) * N + B_BLOCK_COL_START);\n\t\t}\n\n\t\t__syncthreads();\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_K_SPARSE;i += 1){\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_N; k += 1){\n\t\t\t\tB_reg[k] = B_shared[i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k];\n\t\t\t\t//*(float4 *)(B_reg + k) = *(float4 *)(B_shared + i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k);\n\t\t\t\t//*(float4 *)(B_reg_index + k) = *(float4 *)(B_index_shared + i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k);\n\t\t\t}\n\t\t\tint bank_idx = i / LEN_OF_BANK_PER_SPARSE_BLOCK;\n\t\t\tB_reg_index = B_index_shared[i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N] % BANK_VAL+bank_idx * BANK_VAL;\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_M; k += 1){\n\t\t\t\tA_reg[k] = A_shared[(t_M * THREAD_SIZE_M+k) * BLOCK_SIZE_K + B_reg_index];\n\t\t\t}\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_N; k += 1){\n\t\t\t\t#pragma unroll\n\t\t\t\tfor(int j = 0; j < THREAD_SIZE_M; j += 1){\n\t\t\t\t\tC_reg[j][k] += B_reg[k] * A_reg[j];\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n    float bias_local[THREAD_SIZE_N];\n    for(int thread_x = 0; thread_x < THREAD_SIZE_N; thread_x++){\n        bias_local[thread_x] = bias[BLOCK_SIZE_N * blockIdx.y + THREAD_SIZE_N * t_N + thread_x];\n    }\n\n\t#pragma unroll\n\tfor(int i = 0; i < THREAD_SIZE_M; i += 1){\n\t\t#pragma unroll\n\t\tfor(int j = 0; j < THREAD_SIZE_N; j += 1){\n\t\t\tg_data[(BLOCK_SIZE_M * blockIdx.x + THREAD_SIZE_M * t_M + i) * N + BLOCK_SIZE_N * blockIdx.y + THREAD_SIZE_N * t_N + j] = C_reg[i][j] + bias_local[j];\n\t\t}\n\t}\n}", "launch_config": {"dimBlock": [128, 1], "dimGrid": [64, 6]}}, "bert.encoder.layer.8.attention.output.dense": {"code": "__global__ void MatMul_TILE_THREAD_GENERAL(float *input0, float *input1, float *input2, float *input3, float *output0) {\n    /*\n    g_vec: A [M, K]\n    g_mat_data: condense B [K_sparse, N]\n    g_mat_index: condense B index [K_sparse, N]\n    g_data: C [M, N]\n    */\n\tfloat *g_vec = input0;\n\tfloat *g_mat_data = input1;\n\tint *g_mat_index = (int*)input2;\n\tfloat *bias = input3;\n\tfloat *g_data = output0;\n\n\tconst int M = 4096;\n\tconst int K = 768;\n\tconst int N = 768;\n\n    const int BLOCK_SIZE_M = 64;\n    const int BLOCK_SIZE_N = 128;\n    const int BLOCK_SIZE_K = 32;\n    const int THREAD_SIZE_M = 8;\n    const int THREAD_SIZE_N = 8;\n\n    const float SPARSITY = 0.875;\n    const int BANK_VAL = 32;\n    const int ALIGN_N = THREAD_SIZE_N;\n\n    const int BANK_NUM_PER_BLOCK = BLOCK_SIZE_K / BANK_VAL;\n    const int BLOCK_SIZE_K_SPARSE = int(BLOCK_SIZE_K * (1-SPARSITY));\n    const int LEN_OF_BANK_PER_SPARSE_BLOCK = BLOCK_SIZE_K_SPARSE / BANK_NUM_PER_BLOCK;\n\n\tconst int K_SPARSE = int(K * SPARSITY);\n\tint M_BLOCK_START = blockIdx.x * BLOCK_SIZE_M;\n\tint N_BLOCK_START = blockIdx.y * BLOCK_SIZE_N;\n\n\tconst int A_THREADS_PER_ROW = BLOCK_SIZE_K / 4;\n\tconst int B_THREADS_PER_ROW = BLOCK_SIZE_N / 4;\n\n\tconst int THREADS_PER_BLOCK = (BLOCK_SIZE_M / THREAD_SIZE_M) * (BLOCK_SIZE_N / THREAD_SIZE_N);\n\n\tconst int A_STRIDES = THREADS_PER_BLOCK / A_THREADS_PER_ROW;\n\tconst int B_STRIDES = THREADS_PER_BLOCK / B_THREADS_PER_ROW;\n\n\t__shared__ float A_shared[BLOCK_SIZE_M * BLOCK_SIZE_K];\n\t__shared__ float B_shared[BLOCK_SIZE_N * BLOCK_SIZE_K_SPARSE];\n\t__shared__ int B_index_shared[BLOCK_SIZE_N * BLOCK_SIZE_K_SPARSE];\n\n\tfloat A_reg[THREAD_SIZE_M];\n\tfloat B_reg[THREAD_SIZE_N];\n\tint B_reg_index;\n\tfloat C_reg[THREAD_SIZE_M][THREAD_SIZE_N] = {0};\n\n\tint tid = threadIdx.x;\n\n\tint t_N = tid % (BLOCK_SIZE_N / THREAD_SIZE_N);\n\tint t_M = tid / (BLOCK_SIZE_N / THREAD_SIZE_N);\n\n\tint A_BLOCK_ROW_START = tid / A_THREADS_PER_ROW;\n\tint B_BLOCK_ROW_START = tid / B_THREADS_PER_ROW;\n\n\tint A_BLOCK_COL_START = tid % A_THREADS_PER_ROW * 4;\n\tint B_BLOCK_COL_START = tid % B_THREADS_PER_ROW * 4;\n\n\tfor(int K_BLOCK_START = 0, K_SPARSE_BLOCK_START = 0; K_BLOCK_START < K; K_BLOCK_START += BLOCK_SIZE_K, K_SPARSE_BLOCK_START += BLOCK_SIZE_K_SPARSE){\n\t\tfloat *A_global_ptr = g_vec + M_BLOCK_START * K + K_BLOCK_START;\n\t\tfloat *B_global_ptr = g_mat_data + K_SPARSE_BLOCK_START * N + N_BLOCK_START;\n\t\tint *B_index_global_ptr = g_mat_index + K_SPARSE_BLOCK_START * N + N_BLOCK_START;\n\n\t\t__syncthreads();\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_M; i += A_STRIDES){\n\t\t\t*(float4 *)(A_shared + (i + A_BLOCK_ROW_START) * BLOCK_SIZE_K + A_BLOCK_COL_START) = \n\t\t\t\t*(float4 *)(A_global_ptr + (i + A_BLOCK_ROW_START) * K + A_BLOCK_COL_START);\n\t\t}\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_K_SPARSE; i += B_STRIDES){\n\t\t\t*(float4 *)(B_shared + (i + B_BLOCK_ROW_START) * BLOCK_SIZE_N + B_BLOCK_COL_START) =\n\t\t\t\t*(float4 *)(B_global_ptr + (i + B_BLOCK_ROW_START) * N + B_BLOCK_COL_START);\n\n\t\t\t*(float4 *)(B_index_shared + (i + B_BLOCK_ROW_START) * BLOCK_SIZE_N + B_BLOCK_COL_START) =\n\t\t\t\t*(float4 *)(B_index_global_ptr + (i + B_BLOCK_ROW_START) * N + B_BLOCK_COL_START);\n\t\t}\n\n\t\t__syncthreads();\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_K_SPARSE;i += 1){\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_N; k += 1){\n\t\t\t\tB_reg[k] = B_shared[i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k];\n\t\t\t\t//*(float4 *)(B_reg + k) = *(float4 *)(B_shared + i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k);\n\t\t\t\t//*(float4 *)(B_reg_index + k) = *(float4 *)(B_index_shared + i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k);\n\t\t\t}\n\t\t\tint bank_idx = i / LEN_OF_BANK_PER_SPARSE_BLOCK;\n\t\t\tB_reg_index = B_index_shared[i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N] % BANK_VAL+bank_idx * BANK_VAL;\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_M; k += 1){\n\t\t\t\tA_reg[k] = A_shared[(t_M * THREAD_SIZE_M+k) * BLOCK_SIZE_K + B_reg_index];\n\t\t\t}\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_N; k += 1){\n\t\t\t\t#pragma unroll\n\t\t\t\tfor(int j = 0; j < THREAD_SIZE_M; j += 1){\n\t\t\t\t\tC_reg[j][k] += B_reg[k] * A_reg[j];\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n    float bias_local[THREAD_SIZE_N];\n    for(int thread_x = 0; thread_x < THREAD_SIZE_N; thread_x++){\n        bias_local[thread_x] = bias[BLOCK_SIZE_N * blockIdx.y + THREAD_SIZE_N * t_N + thread_x];\n    }\n\n\t#pragma unroll\n\tfor(int i = 0; i < THREAD_SIZE_M; i += 1){\n\t\t#pragma unroll\n\t\tfor(int j = 0; j < THREAD_SIZE_N; j += 1){\n\t\t\tg_data[(BLOCK_SIZE_M * blockIdx.x + THREAD_SIZE_M * t_M + i) * N + BLOCK_SIZE_N * blockIdx.y + THREAD_SIZE_N * t_N + j] = C_reg[i][j] + bias_local[j];\n\t\t}\n\t}\n}", "launch_config": {"dimBlock": [128, 1], "dimGrid": [64, 6]}}, "bert.encoder.layer.8.intermediate.dense": {"code": "__global__ void MatMul_TILE_THREAD_GENERAL(float *input0, float *input1, float *input2, float *input3, float *output0) {\n    /*\n    g_vec: A [M, K]\n    g_mat_data: condense B [K_sparse, N]\n    g_mat_index: condense B index [K_sparse, N]\n    g_data: C [M, N]\n    */\n\tfloat *g_vec = input0;\n\tfloat *g_mat_data = input1;\n\tint *g_mat_index = (int*)input2;\n\tfloat *bias = input3;\n\tfloat *g_data = output0;\n\n\tconst int M = 4096;\n\tconst int K = 768;\n\tconst int N = 3072;\n\n    const int BLOCK_SIZE_M = 64;\n    const int BLOCK_SIZE_N = 128;\n    const int BLOCK_SIZE_K = 32;\n    const int THREAD_SIZE_M = 8;\n    const int THREAD_SIZE_N = 8;\n\n    const float SPARSITY = 0.875;\n    const int BANK_VAL = 32;\n    const int ALIGN_N = THREAD_SIZE_N;\n\n    const int BANK_NUM_PER_BLOCK = BLOCK_SIZE_K / BANK_VAL;\n    const int BLOCK_SIZE_K_SPARSE = int(BLOCK_SIZE_K * (1-SPARSITY));\n    const int LEN_OF_BANK_PER_SPARSE_BLOCK = BLOCK_SIZE_K_SPARSE / BANK_NUM_PER_BLOCK;\n\n\tconst int K_SPARSE = int(K * SPARSITY);\n\tint M_BLOCK_START = blockIdx.x * BLOCK_SIZE_M;\n\tint N_BLOCK_START = blockIdx.y * BLOCK_SIZE_N;\n\n\tconst int A_THREADS_PER_ROW = BLOCK_SIZE_K / 4;\n\tconst int B_THREADS_PER_ROW = BLOCK_SIZE_N / 4;\n\n\tconst int THREADS_PER_BLOCK = (BLOCK_SIZE_M / THREAD_SIZE_M) * (BLOCK_SIZE_N / THREAD_SIZE_N);\n\n\tconst int A_STRIDES = THREADS_PER_BLOCK / A_THREADS_PER_ROW;\n\tconst int B_STRIDES = THREADS_PER_BLOCK / B_THREADS_PER_ROW;\n\n\t__shared__ float A_shared[BLOCK_SIZE_M * BLOCK_SIZE_K];\n\t__shared__ float B_shared[BLOCK_SIZE_N * BLOCK_SIZE_K_SPARSE];\n\t__shared__ int B_index_shared[BLOCK_SIZE_N * BLOCK_SIZE_K_SPARSE];\n\n\tfloat A_reg[THREAD_SIZE_M];\n\tfloat B_reg[THREAD_SIZE_N];\n\tint B_reg_index;\n\tfloat C_reg[THREAD_SIZE_M][THREAD_SIZE_N] = {0};\n\n\tint tid = threadIdx.x;\n\n\tint t_N = tid % (BLOCK_SIZE_N / THREAD_SIZE_N);\n\tint t_M = tid / (BLOCK_SIZE_N / THREAD_SIZE_N);\n\n\tint A_BLOCK_ROW_START = tid / A_THREADS_PER_ROW;\n\tint B_BLOCK_ROW_START = tid / B_THREADS_PER_ROW;\n\n\tint A_BLOCK_COL_START = tid % A_THREADS_PER_ROW * 4;\n\tint B_BLOCK_COL_START = tid % B_THREADS_PER_ROW * 4;\n\n\tfor(int K_BLOCK_START = 0, K_SPARSE_BLOCK_START = 0; K_BLOCK_START < K; K_BLOCK_START += BLOCK_SIZE_K, K_SPARSE_BLOCK_START += BLOCK_SIZE_K_SPARSE){\n\t\tfloat *A_global_ptr = g_vec + M_BLOCK_START * K + K_BLOCK_START;\n\t\tfloat *B_global_ptr = g_mat_data + K_SPARSE_BLOCK_START * N + N_BLOCK_START;\n\t\tint *B_index_global_ptr = g_mat_index + K_SPARSE_BLOCK_START * N + N_BLOCK_START;\n\n\t\t__syncthreads();\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_M; i += A_STRIDES){\n\t\t\t*(float4 *)(A_shared + (i + A_BLOCK_ROW_START) * BLOCK_SIZE_K + A_BLOCK_COL_START) = \n\t\t\t\t*(float4 *)(A_global_ptr + (i + A_BLOCK_ROW_START) * K + A_BLOCK_COL_START);\n\t\t}\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_K_SPARSE; i += B_STRIDES){\n\t\t\t*(float4 *)(B_shared + (i + B_BLOCK_ROW_START) * BLOCK_SIZE_N + B_BLOCK_COL_START) =\n\t\t\t\t*(float4 *)(B_global_ptr + (i + B_BLOCK_ROW_START) * N + B_BLOCK_COL_START);\n\n\t\t\t*(float4 *)(B_index_shared + (i + B_BLOCK_ROW_START) * BLOCK_SIZE_N + B_BLOCK_COL_START) =\n\t\t\t\t*(float4 *)(B_index_global_ptr + (i + B_BLOCK_ROW_START) * N + B_BLOCK_COL_START);\n\t\t}\n\n\t\t__syncthreads();\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_K_SPARSE;i += 1){\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_N; k += 1){\n\t\t\t\tB_reg[k] = B_shared[i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k];\n\t\t\t\t//*(float4 *)(B_reg + k) = *(float4 *)(B_shared + i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k);\n\t\t\t\t//*(float4 *)(B_reg_index + k) = *(float4 *)(B_index_shared + i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k);\n\t\t\t}\n\t\t\tint bank_idx = i / LEN_OF_BANK_PER_SPARSE_BLOCK;\n\t\t\tB_reg_index = B_index_shared[i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N] % BANK_VAL+bank_idx * BANK_VAL;\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_M; k += 1){\n\t\t\t\tA_reg[k] = A_shared[(t_M * THREAD_SIZE_M+k) * BLOCK_SIZE_K + B_reg_index];\n\t\t\t}\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_N; k += 1){\n\t\t\t\t#pragma unroll\n\t\t\t\tfor(int j = 0; j < THREAD_SIZE_M; j += 1){\n\t\t\t\t\tC_reg[j][k] += B_reg[k] * A_reg[j];\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n    float bias_local[THREAD_SIZE_N];\n    for(int thread_x = 0; thread_x < THREAD_SIZE_N; thread_x++){\n        bias_local[thread_x] = bias[BLOCK_SIZE_N * blockIdx.y + THREAD_SIZE_N * t_N + thread_x];\n    }\n\n\t#pragma unroll\n\tfor(int i = 0; i < THREAD_SIZE_M; i += 1){\n\t\t#pragma unroll\n\t\tfor(int j = 0; j < THREAD_SIZE_N; j += 1){\n\t\t\tg_data[(BLOCK_SIZE_M * blockIdx.x + THREAD_SIZE_M * t_M + i) * N + BLOCK_SIZE_N * blockIdx.y + THREAD_SIZE_N * t_N + j] = C_reg[i][j] + bias_local[j];\n\t\t}\n\t}\n}", "launch_config": {"dimBlock": [128, 1], "dimGrid": [64, 24]}}, "bert.encoder.layer.8.output.dense": {"code": "__global__ void MatMul_TILE_THREAD_GENERAL(float *input0, float *input1, float *input2, float *input3, float *output0) {\n    /*\n    g_vec: A [M, K]\n    g_mat_data: condense B [K_sparse, N]\n    g_mat_index: condense B index [K_sparse, N]\n    g_data: C [M, N]\n    */\n\tfloat *g_vec = input0;\n\tfloat *g_mat_data = input1;\n\tint *g_mat_index = (int*)input2;\n\tfloat *bias = input3;\n\tfloat *g_data = output0;\n\n\tconst int M = 4096;\n\tconst int K = 3072;\n\tconst int N = 768;\n\n    const int BLOCK_SIZE_M = 64;\n    const int BLOCK_SIZE_N = 128;\n    const int BLOCK_SIZE_K = 32;\n    const int THREAD_SIZE_M = 8;\n    const int THREAD_SIZE_N = 8;\n\n    const float SPARSITY = 0.875;\n    const int BANK_VAL = 32;\n    const int ALIGN_N = THREAD_SIZE_N;\n\n    const int BANK_NUM_PER_BLOCK = BLOCK_SIZE_K / BANK_VAL;\n    const int BLOCK_SIZE_K_SPARSE = int(BLOCK_SIZE_K * (1-SPARSITY));\n    const int LEN_OF_BANK_PER_SPARSE_BLOCK = BLOCK_SIZE_K_SPARSE / BANK_NUM_PER_BLOCK;\n\n\tconst int K_SPARSE = int(K * SPARSITY);\n\tint M_BLOCK_START = blockIdx.x * BLOCK_SIZE_M;\n\tint N_BLOCK_START = blockIdx.y * BLOCK_SIZE_N;\n\n\tconst int A_THREADS_PER_ROW = BLOCK_SIZE_K / 4;\n\tconst int B_THREADS_PER_ROW = BLOCK_SIZE_N / 4;\n\n\tconst int THREADS_PER_BLOCK = (BLOCK_SIZE_M / THREAD_SIZE_M) * (BLOCK_SIZE_N / THREAD_SIZE_N);\n\n\tconst int A_STRIDES = THREADS_PER_BLOCK / A_THREADS_PER_ROW;\n\tconst int B_STRIDES = THREADS_PER_BLOCK / B_THREADS_PER_ROW;\n\n\t__shared__ float A_shared[BLOCK_SIZE_M * BLOCK_SIZE_K];\n\t__shared__ float B_shared[BLOCK_SIZE_N * BLOCK_SIZE_K_SPARSE];\n\t__shared__ int B_index_shared[BLOCK_SIZE_N * BLOCK_SIZE_K_SPARSE];\n\n\tfloat A_reg[THREAD_SIZE_M];\n\tfloat B_reg[THREAD_SIZE_N];\n\tint B_reg_index;\n\tfloat C_reg[THREAD_SIZE_M][THREAD_SIZE_N] = {0};\n\n\tint tid = threadIdx.x;\n\n\tint t_N = tid % (BLOCK_SIZE_N / THREAD_SIZE_N);\n\tint t_M = tid / (BLOCK_SIZE_N / THREAD_SIZE_N);\n\n\tint A_BLOCK_ROW_START = tid / A_THREADS_PER_ROW;\n\tint B_BLOCK_ROW_START = tid / B_THREADS_PER_ROW;\n\n\tint A_BLOCK_COL_START = tid % A_THREADS_PER_ROW * 4;\n\tint B_BLOCK_COL_START = tid % B_THREADS_PER_ROW * 4;\n\n\tfor(int K_BLOCK_START = 0, K_SPARSE_BLOCK_START = 0; K_BLOCK_START < K; K_BLOCK_START += BLOCK_SIZE_K, K_SPARSE_BLOCK_START += BLOCK_SIZE_K_SPARSE){\n\t\tfloat *A_global_ptr = g_vec + M_BLOCK_START * K + K_BLOCK_START;\n\t\tfloat *B_global_ptr = g_mat_data + K_SPARSE_BLOCK_START * N + N_BLOCK_START;\n\t\tint *B_index_global_ptr = g_mat_index + K_SPARSE_BLOCK_START * N + N_BLOCK_START;\n\n\t\t__syncthreads();\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_M; i += A_STRIDES){\n\t\t\t*(float4 *)(A_shared + (i + A_BLOCK_ROW_START) * BLOCK_SIZE_K + A_BLOCK_COL_START) = \n\t\t\t\t*(float4 *)(A_global_ptr + (i + A_BLOCK_ROW_START) * K + A_BLOCK_COL_START);\n\t\t}\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_K_SPARSE; i += B_STRIDES){\n\t\t\t*(float4 *)(B_shared + (i + B_BLOCK_ROW_START) * BLOCK_SIZE_N + B_BLOCK_COL_START) =\n\t\t\t\t*(float4 *)(B_global_ptr + (i + B_BLOCK_ROW_START) * N + B_BLOCK_COL_START);\n\n\t\t\t*(float4 *)(B_index_shared + (i + B_BLOCK_ROW_START) * BLOCK_SIZE_N + B_BLOCK_COL_START) =\n\t\t\t\t*(float4 *)(B_index_global_ptr + (i + B_BLOCK_ROW_START) * N + B_BLOCK_COL_START);\n\t\t}\n\n\t\t__syncthreads();\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_K_SPARSE;i += 1){\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_N; k += 1){\n\t\t\t\tB_reg[k] = B_shared[i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k];\n\t\t\t\t//*(float4 *)(B_reg + k) = *(float4 *)(B_shared + i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k);\n\t\t\t\t//*(float4 *)(B_reg_index + k) = *(float4 *)(B_index_shared + i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k);\n\t\t\t}\n\t\t\tint bank_idx = i / LEN_OF_BANK_PER_SPARSE_BLOCK;\n\t\t\tB_reg_index = B_index_shared[i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N] % BANK_VAL+bank_idx * BANK_VAL;\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_M; k += 1){\n\t\t\t\tA_reg[k] = A_shared[(t_M * THREAD_SIZE_M+k) * BLOCK_SIZE_K + B_reg_index];\n\t\t\t}\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_N; k += 1){\n\t\t\t\t#pragma unroll\n\t\t\t\tfor(int j = 0; j < THREAD_SIZE_M; j += 1){\n\t\t\t\t\tC_reg[j][k] += B_reg[k] * A_reg[j];\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n    float bias_local[THREAD_SIZE_N];\n    for(int thread_x = 0; thread_x < THREAD_SIZE_N; thread_x++){\n        bias_local[thread_x] = bias[BLOCK_SIZE_N * blockIdx.y + THREAD_SIZE_N * t_N + thread_x];\n    }\n\n\t#pragma unroll\n\tfor(int i = 0; i < THREAD_SIZE_M; i += 1){\n\t\t#pragma unroll\n\t\tfor(int j = 0; j < THREAD_SIZE_N; j += 1){\n\t\t\tg_data[(BLOCK_SIZE_M * blockIdx.x + THREAD_SIZE_M * t_M + i) * N + BLOCK_SIZE_N * blockIdx.y + THREAD_SIZE_N * t_N + j] = C_reg[i][j] + bias_local[j];\n\t\t}\n\t}\n}", "launch_config": {"dimBlock": [128, 1], "dimGrid": [64, 6]}}, "bert.encoder.layer.9.attention.self.query": {"code": "__global__ void MatMul_TILE_THREAD_GENERAL(float *input0, float *input1, float *input2, float *input3, float *output0) {\n    /*\n    g_vec: A [M, K]\n    g_mat_data: condense B [K_sparse, N]\n    g_mat_index: condense B index [K_sparse, N]\n    g_data: C [M, N]\n    */\n\tfloat *g_vec = input0;\n\tfloat *g_mat_data = input1;\n\tint *g_mat_index = (int*)input2;\n\tfloat *bias = input3;\n\tfloat *g_data = output0;\n\n\tconst int M = 4096;\n\tconst int K = 768;\n\tconst int N = 768;\n\n    const int BLOCK_SIZE_M = 64;\n    const int BLOCK_SIZE_N = 128;\n    const int BLOCK_SIZE_K = 32;\n    const int THREAD_SIZE_M = 8;\n    const int THREAD_SIZE_N = 8;\n\n    const float SPARSITY = 0.875;\n    const int BANK_VAL = 32;\n    const int ALIGN_N = THREAD_SIZE_N;\n\n    const int BANK_NUM_PER_BLOCK = BLOCK_SIZE_K / BANK_VAL;\n    const int BLOCK_SIZE_K_SPARSE = int(BLOCK_SIZE_K * (1-SPARSITY));\n    const int LEN_OF_BANK_PER_SPARSE_BLOCK = BLOCK_SIZE_K_SPARSE / BANK_NUM_PER_BLOCK;\n\n\tconst int K_SPARSE = int(K * SPARSITY);\n\tint M_BLOCK_START = blockIdx.x * BLOCK_SIZE_M;\n\tint N_BLOCK_START = blockIdx.y * BLOCK_SIZE_N;\n\n\tconst int A_THREADS_PER_ROW = BLOCK_SIZE_K / 4;\n\tconst int B_THREADS_PER_ROW = BLOCK_SIZE_N / 4;\n\n\tconst int THREADS_PER_BLOCK = (BLOCK_SIZE_M / THREAD_SIZE_M) * (BLOCK_SIZE_N / THREAD_SIZE_N);\n\n\tconst int A_STRIDES = THREADS_PER_BLOCK / A_THREADS_PER_ROW;\n\tconst int B_STRIDES = THREADS_PER_BLOCK / B_THREADS_PER_ROW;\n\n\t__shared__ float A_shared[BLOCK_SIZE_M * BLOCK_SIZE_K];\n\t__shared__ float B_shared[BLOCK_SIZE_N * BLOCK_SIZE_K_SPARSE];\n\t__shared__ int B_index_shared[BLOCK_SIZE_N * BLOCK_SIZE_K_SPARSE];\n\n\tfloat A_reg[THREAD_SIZE_M];\n\tfloat B_reg[THREAD_SIZE_N];\n\tint B_reg_index;\n\tfloat C_reg[THREAD_SIZE_M][THREAD_SIZE_N] = {0};\n\n\tint tid = threadIdx.x;\n\n\tint t_N = tid % (BLOCK_SIZE_N / THREAD_SIZE_N);\n\tint t_M = tid / (BLOCK_SIZE_N / THREAD_SIZE_N);\n\n\tint A_BLOCK_ROW_START = tid / A_THREADS_PER_ROW;\n\tint B_BLOCK_ROW_START = tid / B_THREADS_PER_ROW;\n\n\tint A_BLOCK_COL_START = tid % A_THREADS_PER_ROW * 4;\n\tint B_BLOCK_COL_START = tid % B_THREADS_PER_ROW * 4;\n\n\tfor(int K_BLOCK_START = 0, K_SPARSE_BLOCK_START = 0; K_BLOCK_START < K; K_BLOCK_START += BLOCK_SIZE_K, K_SPARSE_BLOCK_START += BLOCK_SIZE_K_SPARSE){\n\t\tfloat *A_global_ptr = g_vec + M_BLOCK_START * K + K_BLOCK_START;\n\t\tfloat *B_global_ptr = g_mat_data + K_SPARSE_BLOCK_START * N + N_BLOCK_START;\n\t\tint *B_index_global_ptr = g_mat_index + K_SPARSE_BLOCK_START * N + N_BLOCK_START;\n\n\t\t__syncthreads();\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_M; i += A_STRIDES){\n\t\t\t*(float4 *)(A_shared + (i + A_BLOCK_ROW_START) * BLOCK_SIZE_K + A_BLOCK_COL_START) = \n\t\t\t\t*(float4 *)(A_global_ptr + (i + A_BLOCK_ROW_START) * K + A_BLOCK_COL_START);\n\t\t}\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_K_SPARSE; i += B_STRIDES){\n\t\t\t*(float4 *)(B_shared + (i + B_BLOCK_ROW_START) * BLOCK_SIZE_N + B_BLOCK_COL_START) =\n\t\t\t\t*(float4 *)(B_global_ptr + (i + B_BLOCK_ROW_START) * N + B_BLOCK_COL_START);\n\n\t\t\t*(float4 *)(B_index_shared + (i + B_BLOCK_ROW_START) * BLOCK_SIZE_N + B_BLOCK_COL_START) =\n\t\t\t\t*(float4 *)(B_index_global_ptr + (i + B_BLOCK_ROW_START) * N + B_BLOCK_COL_START);\n\t\t}\n\n\t\t__syncthreads();\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_K_SPARSE;i += 1){\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_N; k += 1){\n\t\t\t\tB_reg[k] = B_shared[i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k];\n\t\t\t\t//*(float4 *)(B_reg + k) = *(float4 *)(B_shared + i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k);\n\t\t\t\t//*(float4 *)(B_reg_index + k) = *(float4 *)(B_index_shared + i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k);\n\t\t\t}\n\t\t\tint bank_idx = i / LEN_OF_BANK_PER_SPARSE_BLOCK;\n\t\t\tB_reg_index = B_index_shared[i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N] % BANK_VAL+bank_idx * BANK_VAL;\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_M; k += 1){\n\t\t\t\tA_reg[k] = A_shared[(t_M * THREAD_SIZE_M+k) * BLOCK_SIZE_K + B_reg_index];\n\t\t\t}\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_N; k += 1){\n\t\t\t\t#pragma unroll\n\t\t\t\tfor(int j = 0; j < THREAD_SIZE_M; j += 1){\n\t\t\t\t\tC_reg[j][k] += B_reg[k] * A_reg[j];\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n    float bias_local[THREAD_SIZE_N];\n    for(int thread_x = 0; thread_x < THREAD_SIZE_N; thread_x++){\n        bias_local[thread_x] = bias[BLOCK_SIZE_N * blockIdx.y + THREAD_SIZE_N * t_N + thread_x];\n    }\n\n\t#pragma unroll\n\tfor(int i = 0; i < THREAD_SIZE_M; i += 1){\n\t\t#pragma unroll\n\t\tfor(int j = 0; j < THREAD_SIZE_N; j += 1){\n\t\t\tg_data[(BLOCK_SIZE_M * blockIdx.x + THREAD_SIZE_M * t_M + i) * N + BLOCK_SIZE_N * blockIdx.y + THREAD_SIZE_N * t_N + j] = C_reg[i][j] + bias_local[j];\n\t\t}\n\t}\n}", "launch_config": {"dimBlock": [128, 1], "dimGrid": [64, 6]}}, "bert.encoder.layer.9.attention.self.key": {"code": "__global__ void MatMul_TILE_THREAD_GENERAL(float *input0, float *input1, float *input2, float *input3, float *output0) {\n    /*\n    g_vec: A [M, K]\n    g_mat_data: condense B [K_sparse, N]\n    g_mat_index: condense B index [K_sparse, N]\n    g_data: C [M, N]\n    */\n\tfloat *g_vec = input0;\n\tfloat *g_mat_data = input1;\n\tint *g_mat_index = (int*)input2;\n\tfloat *bias = input3;\n\tfloat *g_data = output0;\n\n\tconst int M = 4096;\n\tconst int K = 768;\n\tconst int N = 768;\n\n    const int BLOCK_SIZE_M = 64;\n    const int BLOCK_SIZE_N = 128;\n    const int BLOCK_SIZE_K = 32;\n    const int THREAD_SIZE_M = 8;\n    const int THREAD_SIZE_N = 8;\n\n    const float SPARSITY = 0.875;\n    const int BANK_VAL = 32;\n    const int ALIGN_N = THREAD_SIZE_N;\n\n    const int BANK_NUM_PER_BLOCK = BLOCK_SIZE_K / BANK_VAL;\n    const int BLOCK_SIZE_K_SPARSE = int(BLOCK_SIZE_K * (1-SPARSITY));\n    const int LEN_OF_BANK_PER_SPARSE_BLOCK = BLOCK_SIZE_K_SPARSE / BANK_NUM_PER_BLOCK;\n\n\tconst int K_SPARSE = int(K * SPARSITY);\n\tint M_BLOCK_START = blockIdx.x * BLOCK_SIZE_M;\n\tint N_BLOCK_START = blockIdx.y * BLOCK_SIZE_N;\n\n\tconst int A_THREADS_PER_ROW = BLOCK_SIZE_K / 4;\n\tconst int B_THREADS_PER_ROW = BLOCK_SIZE_N / 4;\n\n\tconst int THREADS_PER_BLOCK = (BLOCK_SIZE_M / THREAD_SIZE_M) * (BLOCK_SIZE_N / THREAD_SIZE_N);\n\n\tconst int A_STRIDES = THREADS_PER_BLOCK / A_THREADS_PER_ROW;\n\tconst int B_STRIDES = THREADS_PER_BLOCK / B_THREADS_PER_ROW;\n\n\t__shared__ float A_shared[BLOCK_SIZE_M * BLOCK_SIZE_K];\n\t__shared__ float B_shared[BLOCK_SIZE_N * BLOCK_SIZE_K_SPARSE];\n\t__shared__ int B_index_shared[BLOCK_SIZE_N * BLOCK_SIZE_K_SPARSE];\n\n\tfloat A_reg[THREAD_SIZE_M];\n\tfloat B_reg[THREAD_SIZE_N];\n\tint B_reg_index;\n\tfloat C_reg[THREAD_SIZE_M][THREAD_SIZE_N] = {0};\n\n\tint tid = threadIdx.x;\n\n\tint t_N = tid % (BLOCK_SIZE_N / THREAD_SIZE_N);\n\tint t_M = tid / (BLOCK_SIZE_N / THREAD_SIZE_N);\n\n\tint A_BLOCK_ROW_START = tid / A_THREADS_PER_ROW;\n\tint B_BLOCK_ROW_START = tid / B_THREADS_PER_ROW;\n\n\tint A_BLOCK_COL_START = tid % A_THREADS_PER_ROW * 4;\n\tint B_BLOCK_COL_START = tid % B_THREADS_PER_ROW * 4;\n\n\tfor(int K_BLOCK_START = 0, K_SPARSE_BLOCK_START = 0; K_BLOCK_START < K; K_BLOCK_START += BLOCK_SIZE_K, K_SPARSE_BLOCK_START += BLOCK_SIZE_K_SPARSE){\n\t\tfloat *A_global_ptr = g_vec + M_BLOCK_START * K + K_BLOCK_START;\n\t\tfloat *B_global_ptr = g_mat_data + K_SPARSE_BLOCK_START * N + N_BLOCK_START;\n\t\tint *B_index_global_ptr = g_mat_index + K_SPARSE_BLOCK_START * N + N_BLOCK_START;\n\n\t\t__syncthreads();\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_M; i += A_STRIDES){\n\t\t\t*(float4 *)(A_shared + (i + A_BLOCK_ROW_START) * BLOCK_SIZE_K + A_BLOCK_COL_START) = \n\t\t\t\t*(float4 *)(A_global_ptr + (i + A_BLOCK_ROW_START) * K + A_BLOCK_COL_START);\n\t\t}\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_K_SPARSE; i += B_STRIDES){\n\t\t\t*(float4 *)(B_shared + (i + B_BLOCK_ROW_START) * BLOCK_SIZE_N + B_BLOCK_COL_START) =\n\t\t\t\t*(float4 *)(B_global_ptr + (i + B_BLOCK_ROW_START) * N + B_BLOCK_COL_START);\n\n\t\t\t*(float4 *)(B_index_shared + (i + B_BLOCK_ROW_START) * BLOCK_SIZE_N + B_BLOCK_COL_START) =\n\t\t\t\t*(float4 *)(B_index_global_ptr + (i + B_BLOCK_ROW_START) * N + B_BLOCK_COL_START);\n\t\t}\n\n\t\t__syncthreads();\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_K_SPARSE;i += 1){\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_N; k += 1){\n\t\t\t\tB_reg[k] = B_shared[i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k];\n\t\t\t\t//*(float4 *)(B_reg + k) = *(float4 *)(B_shared + i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k);\n\t\t\t\t//*(float4 *)(B_reg_index + k) = *(float4 *)(B_index_shared + i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k);\n\t\t\t}\n\t\t\tint bank_idx = i / LEN_OF_BANK_PER_SPARSE_BLOCK;\n\t\t\tB_reg_index = B_index_shared[i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N] % BANK_VAL+bank_idx * BANK_VAL;\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_M; k += 1){\n\t\t\t\tA_reg[k] = A_shared[(t_M * THREAD_SIZE_M+k) * BLOCK_SIZE_K + B_reg_index];\n\t\t\t}\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_N; k += 1){\n\t\t\t\t#pragma unroll\n\t\t\t\tfor(int j = 0; j < THREAD_SIZE_M; j += 1){\n\t\t\t\t\tC_reg[j][k] += B_reg[k] * A_reg[j];\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n    float bias_local[THREAD_SIZE_N];\n    for(int thread_x = 0; thread_x < THREAD_SIZE_N; thread_x++){\n        bias_local[thread_x] = bias[BLOCK_SIZE_N * blockIdx.y + THREAD_SIZE_N * t_N + thread_x];\n    }\n\n\t#pragma unroll\n\tfor(int i = 0; i < THREAD_SIZE_M; i += 1){\n\t\t#pragma unroll\n\t\tfor(int j = 0; j < THREAD_SIZE_N; j += 1){\n\t\t\tg_data[(BLOCK_SIZE_M * blockIdx.x + THREAD_SIZE_M * t_M + i) * N + BLOCK_SIZE_N * blockIdx.y + THREAD_SIZE_N * t_N + j] = C_reg[i][j] + bias_local[j];\n\t\t}\n\t}\n}", "launch_config": {"dimBlock": [128, 1], "dimGrid": [64, 6]}}, "bert.encoder.layer.9.attention.self.value": {"code": "__global__ void MatMul_TILE_THREAD_GENERAL(float *input0, float *input1, float *input2, float *input3, float *output0) {\n    /*\n    g_vec: A [M, K]\n    g_mat_data: condense B [K_sparse, N]\n    g_mat_index: condense B index [K_sparse, N]\n    g_data: C [M, N]\n    */\n\tfloat *g_vec = input0;\n\tfloat *g_mat_data = input1;\n\tint *g_mat_index = (int*)input2;\n\tfloat *bias = input3;\n\tfloat *g_data = output0;\n\n\tconst int M = 4096;\n\tconst int K = 768;\n\tconst int N = 768;\n\n    const int BLOCK_SIZE_M = 64;\n    const int BLOCK_SIZE_N = 128;\n    const int BLOCK_SIZE_K = 32;\n    const int THREAD_SIZE_M = 8;\n    const int THREAD_SIZE_N = 8;\n\n    const float SPARSITY = 0.875;\n    const int BANK_VAL = 32;\n    const int ALIGN_N = THREAD_SIZE_N;\n\n    const int BANK_NUM_PER_BLOCK = BLOCK_SIZE_K / BANK_VAL;\n    const int BLOCK_SIZE_K_SPARSE = int(BLOCK_SIZE_K * (1-SPARSITY));\n    const int LEN_OF_BANK_PER_SPARSE_BLOCK = BLOCK_SIZE_K_SPARSE / BANK_NUM_PER_BLOCK;\n\n\tconst int K_SPARSE = int(K * SPARSITY);\n\tint M_BLOCK_START = blockIdx.x * BLOCK_SIZE_M;\n\tint N_BLOCK_START = blockIdx.y * BLOCK_SIZE_N;\n\n\tconst int A_THREADS_PER_ROW = BLOCK_SIZE_K / 4;\n\tconst int B_THREADS_PER_ROW = BLOCK_SIZE_N / 4;\n\n\tconst int THREADS_PER_BLOCK = (BLOCK_SIZE_M / THREAD_SIZE_M) * (BLOCK_SIZE_N / THREAD_SIZE_N);\n\n\tconst int A_STRIDES = THREADS_PER_BLOCK / A_THREADS_PER_ROW;\n\tconst int B_STRIDES = THREADS_PER_BLOCK / B_THREADS_PER_ROW;\n\n\t__shared__ float A_shared[BLOCK_SIZE_M * BLOCK_SIZE_K];\n\t__shared__ float B_shared[BLOCK_SIZE_N * BLOCK_SIZE_K_SPARSE];\n\t__shared__ int B_index_shared[BLOCK_SIZE_N * BLOCK_SIZE_K_SPARSE];\n\n\tfloat A_reg[THREAD_SIZE_M];\n\tfloat B_reg[THREAD_SIZE_N];\n\tint B_reg_index;\n\tfloat C_reg[THREAD_SIZE_M][THREAD_SIZE_N] = {0};\n\n\tint tid = threadIdx.x;\n\n\tint t_N = tid % (BLOCK_SIZE_N / THREAD_SIZE_N);\n\tint t_M = tid / (BLOCK_SIZE_N / THREAD_SIZE_N);\n\n\tint A_BLOCK_ROW_START = tid / A_THREADS_PER_ROW;\n\tint B_BLOCK_ROW_START = tid / B_THREADS_PER_ROW;\n\n\tint A_BLOCK_COL_START = tid % A_THREADS_PER_ROW * 4;\n\tint B_BLOCK_COL_START = tid % B_THREADS_PER_ROW * 4;\n\n\tfor(int K_BLOCK_START = 0, K_SPARSE_BLOCK_START = 0; K_BLOCK_START < K; K_BLOCK_START += BLOCK_SIZE_K, K_SPARSE_BLOCK_START += BLOCK_SIZE_K_SPARSE){\n\t\tfloat *A_global_ptr = g_vec + M_BLOCK_START * K + K_BLOCK_START;\n\t\tfloat *B_global_ptr = g_mat_data + K_SPARSE_BLOCK_START * N + N_BLOCK_START;\n\t\tint *B_index_global_ptr = g_mat_index + K_SPARSE_BLOCK_START * N + N_BLOCK_START;\n\n\t\t__syncthreads();\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_M; i += A_STRIDES){\n\t\t\t*(float4 *)(A_shared + (i + A_BLOCK_ROW_START) * BLOCK_SIZE_K + A_BLOCK_COL_START) = \n\t\t\t\t*(float4 *)(A_global_ptr + (i + A_BLOCK_ROW_START) * K + A_BLOCK_COL_START);\n\t\t}\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_K_SPARSE; i += B_STRIDES){\n\t\t\t*(float4 *)(B_shared + (i + B_BLOCK_ROW_START) * BLOCK_SIZE_N + B_BLOCK_COL_START) =\n\t\t\t\t*(float4 *)(B_global_ptr + (i + B_BLOCK_ROW_START) * N + B_BLOCK_COL_START);\n\n\t\t\t*(float4 *)(B_index_shared + (i + B_BLOCK_ROW_START) * BLOCK_SIZE_N + B_BLOCK_COL_START) =\n\t\t\t\t*(float4 *)(B_index_global_ptr + (i + B_BLOCK_ROW_START) * N + B_BLOCK_COL_START);\n\t\t}\n\n\t\t__syncthreads();\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_K_SPARSE;i += 1){\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_N; k += 1){\n\t\t\t\tB_reg[k] = B_shared[i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k];\n\t\t\t\t//*(float4 *)(B_reg + k) = *(float4 *)(B_shared + i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k);\n\t\t\t\t//*(float4 *)(B_reg_index + k) = *(float4 *)(B_index_shared + i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k);\n\t\t\t}\n\t\t\tint bank_idx = i / LEN_OF_BANK_PER_SPARSE_BLOCK;\n\t\t\tB_reg_index = B_index_shared[i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N] % BANK_VAL+bank_idx * BANK_VAL;\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_M; k += 1){\n\t\t\t\tA_reg[k] = A_shared[(t_M * THREAD_SIZE_M+k) * BLOCK_SIZE_K + B_reg_index];\n\t\t\t}\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_N; k += 1){\n\t\t\t\t#pragma unroll\n\t\t\t\tfor(int j = 0; j < THREAD_SIZE_M; j += 1){\n\t\t\t\t\tC_reg[j][k] += B_reg[k] * A_reg[j];\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n    float bias_local[THREAD_SIZE_N];\n    for(int thread_x = 0; thread_x < THREAD_SIZE_N; thread_x++){\n        bias_local[thread_x] = bias[BLOCK_SIZE_N * blockIdx.y + THREAD_SIZE_N * t_N + thread_x];\n    }\n\n\t#pragma unroll\n\tfor(int i = 0; i < THREAD_SIZE_M; i += 1){\n\t\t#pragma unroll\n\t\tfor(int j = 0; j < THREAD_SIZE_N; j += 1){\n\t\t\tg_data[(BLOCK_SIZE_M * blockIdx.x + THREAD_SIZE_M * t_M + i) * N + BLOCK_SIZE_N * blockIdx.y + THREAD_SIZE_N * t_N + j] = C_reg[i][j] + bias_local[j];\n\t\t}\n\t}\n}", "launch_config": {"dimBlock": [128, 1], "dimGrid": [64, 6]}}, "bert.encoder.layer.9.attention.output.dense": {"code": "__global__ void MatMul_TILE_THREAD_GENERAL(float *input0, float *input1, float *input2, float *input3, float *output0) {\n    /*\n    g_vec: A [M, K]\n    g_mat_data: condense B [K_sparse, N]\n    g_mat_index: condense B index [K_sparse, N]\n    g_data: C [M, N]\n    */\n\tfloat *g_vec = input0;\n\tfloat *g_mat_data = input1;\n\tint *g_mat_index = (int*)input2;\n\tfloat *bias = input3;\n\tfloat *g_data = output0;\n\n\tconst int M = 4096;\n\tconst int K = 768;\n\tconst int N = 768;\n\n    const int BLOCK_SIZE_M = 64;\n    const int BLOCK_SIZE_N = 128;\n    const int BLOCK_SIZE_K = 32;\n    const int THREAD_SIZE_M = 8;\n    const int THREAD_SIZE_N = 8;\n\n    const float SPARSITY = 0.875;\n    const int BANK_VAL = 32;\n    const int ALIGN_N = THREAD_SIZE_N;\n\n    const int BANK_NUM_PER_BLOCK = BLOCK_SIZE_K / BANK_VAL;\n    const int BLOCK_SIZE_K_SPARSE = int(BLOCK_SIZE_K * (1-SPARSITY));\n    const int LEN_OF_BANK_PER_SPARSE_BLOCK = BLOCK_SIZE_K_SPARSE / BANK_NUM_PER_BLOCK;\n\n\tconst int K_SPARSE = int(K * SPARSITY);\n\tint M_BLOCK_START = blockIdx.x * BLOCK_SIZE_M;\n\tint N_BLOCK_START = blockIdx.y * BLOCK_SIZE_N;\n\n\tconst int A_THREADS_PER_ROW = BLOCK_SIZE_K / 4;\n\tconst int B_THREADS_PER_ROW = BLOCK_SIZE_N / 4;\n\n\tconst int THREADS_PER_BLOCK = (BLOCK_SIZE_M / THREAD_SIZE_M) * (BLOCK_SIZE_N / THREAD_SIZE_N);\n\n\tconst int A_STRIDES = THREADS_PER_BLOCK / A_THREADS_PER_ROW;\n\tconst int B_STRIDES = THREADS_PER_BLOCK / B_THREADS_PER_ROW;\n\n\t__shared__ float A_shared[BLOCK_SIZE_M * BLOCK_SIZE_K];\n\t__shared__ float B_shared[BLOCK_SIZE_N * BLOCK_SIZE_K_SPARSE];\n\t__shared__ int B_index_shared[BLOCK_SIZE_N * BLOCK_SIZE_K_SPARSE];\n\n\tfloat A_reg[THREAD_SIZE_M];\n\tfloat B_reg[THREAD_SIZE_N];\n\tint B_reg_index;\n\tfloat C_reg[THREAD_SIZE_M][THREAD_SIZE_N] = {0};\n\n\tint tid = threadIdx.x;\n\n\tint t_N = tid % (BLOCK_SIZE_N / THREAD_SIZE_N);\n\tint t_M = tid / (BLOCK_SIZE_N / THREAD_SIZE_N);\n\n\tint A_BLOCK_ROW_START = tid / A_THREADS_PER_ROW;\n\tint B_BLOCK_ROW_START = tid / B_THREADS_PER_ROW;\n\n\tint A_BLOCK_COL_START = tid % A_THREADS_PER_ROW * 4;\n\tint B_BLOCK_COL_START = tid % B_THREADS_PER_ROW * 4;\n\n\tfor(int K_BLOCK_START = 0, K_SPARSE_BLOCK_START = 0; K_BLOCK_START < K; K_BLOCK_START += BLOCK_SIZE_K, K_SPARSE_BLOCK_START += BLOCK_SIZE_K_SPARSE){\n\t\tfloat *A_global_ptr = g_vec + M_BLOCK_START * K + K_BLOCK_START;\n\t\tfloat *B_global_ptr = g_mat_data + K_SPARSE_BLOCK_START * N + N_BLOCK_START;\n\t\tint *B_index_global_ptr = g_mat_index + K_SPARSE_BLOCK_START * N + N_BLOCK_START;\n\n\t\t__syncthreads();\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_M; i += A_STRIDES){\n\t\t\t*(float4 *)(A_shared + (i + A_BLOCK_ROW_START) * BLOCK_SIZE_K + A_BLOCK_COL_START) = \n\t\t\t\t*(float4 *)(A_global_ptr + (i + A_BLOCK_ROW_START) * K + A_BLOCK_COL_START);\n\t\t}\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_K_SPARSE; i += B_STRIDES){\n\t\t\t*(float4 *)(B_shared + (i + B_BLOCK_ROW_START) * BLOCK_SIZE_N + B_BLOCK_COL_START) =\n\t\t\t\t*(float4 *)(B_global_ptr + (i + B_BLOCK_ROW_START) * N + B_BLOCK_COL_START);\n\n\t\t\t*(float4 *)(B_index_shared + (i + B_BLOCK_ROW_START) * BLOCK_SIZE_N + B_BLOCK_COL_START) =\n\t\t\t\t*(float4 *)(B_index_global_ptr + (i + B_BLOCK_ROW_START) * N + B_BLOCK_COL_START);\n\t\t}\n\n\t\t__syncthreads();\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_K_SPARSE;i += 1){\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_N; k += 1){\n\t\t\t\tB_reg[k] = B_shared[i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k];\n\t\t\t\t//*(float4 *)(B_reg + k) = *(float4 *)(B_shared + i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k);\n\t\t\t\t//*(float4 *)(B_reg_index + k) = *(float4 *)(B_index_shared + i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k);\n\t\t\t}\n\t\t\tint bank_idx = i / LEN_OF_BANK_PER_SPARSE_BLOCK;\n\t\t\tB_reg_index = B_index_shared[i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N] % BANK_VAL+bank_idx * BANK_VAL;\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_M; k += 1){\n\t\t\t\tA_reg[k] = A_shared[(t_M * THREAD_SIZE_M+k) * BLOCK_SIZE_K + B_reg_index];\n\t\t\t}\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_N; k += 1){\n\t\t\t\t#pragma unroll\n\t\t\t\tfor(int j = 0; j < THREAD_SIZE_M; j += 1){\n\t\t\t\t\tC_reg[j][k] += B_reg[k] * A_reg[j];\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n    float bias_local[THREAD_SIZE_N];\n    for(int thread_x = 0; thread_x < THREAD_SIZE_N; thread_x++){\n        bias_local[thread_x] = bias[BLOCK_SIZE_N * blockIdx.y + THREAD_SIZE_N * t_N + thread_x];\n    }\n\n\t#pragma unroll\n\tfor(int i = 0; i < THREAD_SIZE_M; i += 1){\n\t\t#pragma unroll\n\t\tfor(int j = 0; j < THREAD_SIZE_N; j += 1){\n\t\t\tg_data[(BLOCK_SIZE_M * blockIdx.x + THREAD_SIZE_M * t_M + i) * N + BLOCK_SIZE_N * blockIdx.y + THREAD_SIZE_N * t_N + j] = C_reg[i][j] + bias_local[j];\n\t\t}\n\t}\n}", "launch_config": {"dimBlock": [128, 1], "dimGrid": [64, 6]}}, "bert.encoder.layer.9.intermediate.dense": {"code": "__global__ void MatMul_TILE_THREAD_GENERAL(float *input0, float *input1, float *input2, float *input3, float *output0) {\n    /*\n    g_vec: A [M, K]\n    g_mat_data: condense B [K_sparse, N]\n    g_mat_index: condense B index [K_sparse, N]\n    g_data: C [M, N]\n    */\n\tfloat *g_vec = input0;\n\tfloat *g_mat_data = input1;\n\tint *g_mat_index = (int*)input2;\n\tfloat *bias = input3;\n\tfloat *g_data = output0;\n\n\tconst int M = 4096;\n\tconst int K = 768;\n\tconst int N = 3072;\n\n    const int BLOCK_SIZE_M = 64;\n    const int BLOCK_SIZE_N = 128;\n    const int BLOCK_SIZE_K = 32;\n    const int THREAD_SIZE_M = 8;\n    const int THREAD_SIZE_N = 8;\n\n    const float SPARSITY = 0.875;\n    const int BANK_VAL = 32;\n    const int ALIGN_N = THREAD_SIZE_N;\n\n    const int BANK_NUM_PER_BLOCK = BLOCK_SIZE_K / BANK_VAL;\n    const int BLOCK_SIZE_K_SPARSE = int(BLOCK_SIZE_K * (1-SPARSITY));\n    const int LEN_OF_BANK_PER_SPARSE_BLOCK = BLOCK_SIZE_K_SPARSE / BANK_NUM_PER_BLOCK;\n\n\tconst int K_SPARSE = int(K * SPARSITY);\n\tint M_BLOCK_START = blockIdx.x * BLOCK_SIZE_M;\n\tint N_BLOCK_START = blockIdx.y * BLOCK_SIZE_N;\n\n\tconst int A_THREADS_PER_ROW = BLOCK_SIZE_K / 4;\n\tconst int B_THREADS_PER_ROW = BLOCK_SIZE_N / 4;\n\n\tconst int THREADS_PER_BLOCK = (BLOCK_SIZE_M / THREAD_SIZE_M) * (BLOCK_SIZE_N / THREAD_SIZE_N);\n\n\tconst int A_STRIDES = THREADS_PER_BLOCK / A_THREADS_PER_ROW;\n\tconst int B_STRIDES = THREADS_PER_BLOCK / B_THREADS_PER_ROW;\n\n\t__shared__ float A_shared[BLOCK_SIZE_M * BLOCK_SIZE_K];\n\t__shared__ float B_shared[BLOCK_SIZE_N * BLOCK_SIZE_K_SPARSE];\n\t__shared__ int B_index_shared[BLOCK_SIZE_N * BLOCK_SIZE_K_SPARSE];\n\n\tfloat A_reg[THREAD_SIZE_M];\n\tfloat B_reg[THREAD_SIZE_N];\n\tint B_reg_index;\n\tfloat C_reg[THREAD_SIZE_M][THREAD_SIZE_N] = {0};\n\n\tint tid = threadIdx.x;\n\n\tint t_N = tid % (BLOCK_SIZE_N / THREAD_SIZE_N);\n\tint t_M = tid / (BLOCK_SIZE_N / THREAD_SIZE_N);\n\n\tint A_BLOCK_ROW_START = tid / A_THREADS_PER_ROW;\n\tint B_BLOCK_ROW_START = tid / B_THREADS_PER_ROW;\n\n\tint A_BLOCK_COL_START = tid % A_THREADS_PER_ROW * 4;\n\tint B_BLOCK_COL_START = tid % B_THREADS_PER_ROW * 4;\n\n\tfor(int K_BLOCK_START = 0, K_SPARSE_BLOCK_START = 0; K_BLOCK_START < K; K_BLOCK_START += BLOCK_SIZE_K, K_SPARSE_BLOCK_START += BLOCK_SIZE_K_SPARSE){\n\t\tfloat *A_global_ptr = g_vec + M_BLOCK_START * K + K_BLOCK_START;\n\t\tfloat *B_global_ptr = g_mat_data + K_SPARSE_BLOCK_START * N + N_BLOCK_START;\n\t\tint *B_index_global_ptr = g_mat_index + K_SPARSE_BLOCK_START * N + N_BLOCK_START;\n\n\t\t__syncthreads();\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_M; i += A_STRIDES){\n\t\t\t*(float4 *)(A_shared + (i + A_BLOCK_ROW_START) * BLOCK_SIZE_K + A_BLOCK_COL_START) = \n\t\t\t\t*(float4 *)(A_global_ptr + (i + A_BLOCK_ROW_START) * K + A_BLOCK_COL_START);\n\t\t}\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_K_SPARSE; i += B_STRIDES){\n\t\t\t*(float4 *)(B_shared + (i + B_BLOCK_ROW_START) * BLOCK_SIZE_N + B_BLOCK_COL_START) =\n\t\t\t\t*(float4 *)(B_global_ptr + (i + B_BLOCK_ROW_START) * N + B_BLOCK_COL_START);\n\n\t\t\t*(float4 *)(B_index_shared + (i + B_BLOCK_ROW_START) * BLOCK_SIZE_N + B_BLOCK_COL_START) =\n\t\t\t\t*(float4 *)(B_index_global_ptr + (i + B_BLOCK_ROW_START) * N + B_BLOCK_COL_START);\n\t\t}\n\n\t\t__syncthreads();\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_K_SPARSE;i += 1){\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_N; k += 1){\n\t\t\t\tB_reg[k] = B_shared[i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k];\n\t\t\t\t//*(float4 *)(B_reg + k) = *(float4 *)(B_shared + i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k);\n\t\t\t\t//*(float4 *)(B_reg_index + k) = *(float4 *)(B_index_shared + i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k);\n\t\t\t}\n\t\t\tint bank_idx = i / LEN_OF_BANK_PER_SPARSE_BLOCK;\n\t\t\tB_reg_index = B_index_shared[i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N] % BANK_VAL+bank_idx * BANK_VAL;\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_M; k += 1){\n\t\t\t\tA_reg[k] = A_shared[(t_M * THREAD_SIZE_M+k) * BLOCK_SIZE_K + B_reg_index];\n\t\t\t}\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_N; k += 1){\n\t\t\t\t#pragma unroll\n\t\t\t\tfor(int j = 0; j < THREAD_SIZE_M; j += 1){\n\t\t\t\t\tC_reg[j][k] += B_reg[k] * A_reg[j];\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n    float bias_local[THREAD_SIZE_N];\n    for(int thread_x = 0; thread_x < THREAD_SIZE_N; thread_x++){\n        bias_local[thread_x] = bias[BLOCK_SIZE_N * blockIdx.y + THREAD_SIZE_N * t_N + thread_x];\n    }\n\n\t#pragma unroll\n\tfor(int i = 0; i < THREAD_SIZE_M; i += 1){\n\t\t#pragma unroll\n\t\tfor(int j = 0; j < THREAD_SIZE_N; j += 1){\n\t\t\tg_data[(BLOCK_SIZE_M * blockIdx.x + THREAD_SIZE_M * t_M + i) * N + BLOCK_SIZE_N * blockIdx.y + THREAD_SIZE_N * t_N + j] = C_reg[i][j] + bias_local[j];\n\t\t}\n\t}\n}", "launch_config": {"dimBlock": [128, 1], "dimGrid": [64, 24]}}, "bert.encoder.layer.9.output.dense": {"code": "__global__ void MatMul_TILE_THREAD_GENERAL(float *input0, float *input1, float *input2, float *input3, float *output0) {\n    /*\n    g_vec: A [M, K]\n    g_mat_data: condense B [K_sparse, N]\n    g_mat_index: condense B index [K_sparse, N]\n    g_data: C [M, N]\n    */\n\tfloat *g_vec = input0;\n\tfloat *g_mat_data = input1;\n\tint *g_mat_index = (int*)input2;\n\tfloat *bias = input3;\n\tfloat *g_data = output0;\n\n\tconst int M = 4096;\n\tconst int K = 3072;\n\tconst int N = 768;\n\n    const int BLOCK_SIZE_M = 64;\n    const int BLOCK_SIZE_N = 128;\n    const int BLOCK_SIZE_K = 32;\n    const int THREAD_SIZE_M = 8;\n    const int THREAD_SIZE_N = 8;\n\n    const float SPARSITY = 0.875;\n    const int BANK_VAL = 32;\n    const int ALIGN_N = THREAD_SIZE_N;\n\n    const int BANK_NUM_PER_BLOCK = BLOCK_SIZE_K / BANK_VAL;\n    const int BLOCK_SIZE_K_SPARSE = int(BLOCK_SIZE_K * (1-SPARSITY));\n    const int LEN_OF_BANK_PER_SPARSE_BLOCK = BLOCK_SIZE_K_SPARSE / BANK_NUM_PER_BLOCK;\n\n\tconst int K_SPARSE = int(K * SPARSITY);\n\tint M_BLOCK_START = blockIdx.x * BLOCK_SIZE_M;\n\tint N_BLOCK_START = blockIdx.y * BLOCK_SIZE_N;\n\n\tconst int A_THREADS_PER_ROW = BLOCK_SIZE_K / 4;\n\tconst int B_THREADS_PER_ROW = BLOCK_SIZE_N / 4;\n\n\tconst int THREADS_PER_BLOCK = (BLOCK_SIZE_M / THREAD_SIZE_M) * (BLOCK_SIZE_N / THREAD_SIZE_N);\n\n\tconst int A_STRIDES = THREADS_PER_BLOCK / A_THREADS_PER_ROW;\n\tconst int B_STRIDES = THREADS_PER_BLOCK / B_THREADS_PER_ROW;\n\n\t__shared__ float A_shared[BLOCK_SIZE_M * BLOCK_SIZE_K];\n\t__shared__ float B_shared[BLOCK_SIZE_N * BLOCK_SIZE_K_SPARSE];\n\t__shared__ int B_index_shared[BLOCK_SIZE_N * BLOCK_SIZE_K_SPARSE];\n\n\tfloat A_reg[THREAD_SIZE_M];\n\tfloat B_reg[THREAD_SIZE_N];\n\tint B_reg_index;\n\tfloat C_reg[THREAD_SIZE_M][THREAD_SIZE_N] = {0};\n\n\tint tid = threadIdx.x;\n\n\tint t_N = tid % (BLOCK_SIZE_N / THREAD_SIZE_N);\n\tint t_M = tid / (BLOCK_SIZE_N / THREAD_SIZE_N);\n\n\tint A_BLOCK_ROW_START = tid / A_THREADS_PER_ROW;\n\tint B_BLOCK_ROW_START = tid / B_THREADS_PER_ROW;\n\n\tint A_BLOCK_COL_START = tid % A_THREADS_PER_ROW * 4;\n\tint B_BLOCK_COL_START = tid % B_THREADS_PER_ROW * 4;\n\n\tfor(int K_BLOCK_START = 0, K_SPARSE_BLOCK_START = 0; K_BLOCK_START < K; K_BLOCK_START += BLOCK_SIZE_K, K_SPARSE_BLOCK_START += BLOCK_SIZE_K_SPARSE){\n\t\tfloat *A_global_ptr = g_vec + M_BLOCK_START * K + K_BLOCK_START;\n\t\tfloat *B_global_ptr = g_mat_data + K_SPARSE_BLOCK_START * N + N_BLOCK_START;\n\t\tint *B_index_global_ptr = g_mat_index + K_SPARSE_BLOCK_START * N + N_BLOCK_START;\n\n\t\t__syncthreads();\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_M; i += A_STRIDES){\n\t\t\t*(float4 *)(A_shared + (i + A_BLOCK_ROW_START) * BLOCK_SIZE_K + A_BLOCK_COL_START) = \n\t\t\t\t*(float4 *)(A_global_ptr + (i + A_BLOCK_ROW_START) * K + A_BLOCK_COL_START);\n\t\t}\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_K_SPARSE; i += B_STRIDES){\n\t\t\t*(float4 *)(B_shared + (i + B_BLOCK_ROW_START) * BLOCK_SIZE_N + B_BLOCK_COL_START) =\n\t\t\t\t*(float4 *)(B_global_ptr + (i + B_BLOCK_ROW_START) * N + B_BLOCK_COL_START);\n\n\t\t\t*(float4 *)(B_index_shared + (i + B_BLOCK_ROW_START) * BLOCK_SIZE_N + B_BLOCK_COL_START) =\n\t\t\t\t*(float4 *)(B_index_global_ptr + (i + B_BLOCK_ROW_START) * N + B_BLOCK_COL_START);\n\t\t}\n\n\t\t__syncthreads();\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_K_SPARSE;i += 1){\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_N; k += 1){\n\t\t\t\tB_reg[k] = B_shared[i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k];\n\t\t\t\t//*(float4 *)(B_reg + k) = *(float4 *)(B_shared + i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k);\n\t\t\t\t//*(float4 *)(B_reg_index + k) = *(float4 *)(B_index_shared + i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k);\n\t\t\t}\n\t\t\tint bank_idx = i / LEN_OF_BANK_PER_SPARSE_BLOCK;\n\t\t\tB_reg_index = B_index_shared[i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N] % BANK_VAL+bank_idx * BANK_VAL;\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_M; k += 1){\n\t\t\t\tA_reg[k] = A_shared[(t_M * THREAD_SIZE_M+k) * BLOCK_SIZE_K + B_reg_index];\n\t\t\t}\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_N; k += 1){\n\t\t\t\t#pragma unroll\n\t\t\t\tfor(int j = 0; j < THREAD_SIZE_M; j += 1){\n\t\t\t\t\tC_reg[j][k] += B_reg[k] * A_reg[j];\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n    float bias_local[THREAD_SIZE_N];\n    for(int thread_x = 0; thread_x < THREAD_SIZE_N; thread_x++){\n        bias_local[thread_x] = bias[BLOCK_SIZE_N * blockIdx.y + THREAD_SIZE_N * t_N + thread_x];\n    }\n\n\t#pragma unroll\n\tfor(int i = 0; i < THREAD_SIZE_M; i += 1){\n\t\t#pragma unroll\n\t\tfor(int j = 0; j < THREAD_SIZE_N; j += 1){\n\t\t\tg_data[(BLOCK_SIZE_M * blockIdx.x + THREAD_SIZE_M * t_M + i) * N + BLOCK_SIZE_N * blockIdx.y + THREAD_SIZE_N * t_N + j] = C_reg[i][j] + bias_local[j];\n\t\t}\n\t}\n}", "launch_config": {"dimBlock": [128, 1], "dimGrid": [64, 6]}}, "bert.encoder.layer.10.attention.self.query": {"code": "__global__ void MatMul_TILE_THREAD_GENERAL(float *input0, float *input1, float *input2, float *input3, float *output0) {\n    /*\n    g_vec: A [M, K]\n    g_mat_data: condense B [K_sparse, N]\n    g_mat_index: condense B index [K_sparse, N]\n    g_data: C [M, N]\n    */\n\tfloat *g_vec = input0;\n\tfloat *g_mat_data = input1;\n\tint *g_mat_index = (int*)input2;\n\tfloat *bias = input3;\n\tfloat *g_data = output0;\n\n\tconst int M = 4096;\n\tconst int K = 768;\n\tconst int N = 768;\n\n    const int BLOCK_SIZE_M = 64;\n    const int BLOCK_SIZE_N = 128;\n    const int BLOCK_SIZE_K = 32;\n    const int THREAD_SIZE_M = 8;\n    const int THREAD_SIZE_N = 8;\n\n    const float SPARSITY = 0.875;\n    const int BANK_VAL = 32;\n    const int ALIGN_N = THREAD_SIZE_N;\n\n    const int BANK_NUM_PER_BLOCK = BLOCK_SIZE_K / BANK_VAL;\n    const int BLOCK_SIZE_K_SPARSE = int(BLOCK_SIZE_K * (1-SPARSITY));\n    const int LEN_OF_BANK_PER_SPARSE_BLOCK = BLOCK_SIZE_K_SPARSE / BANK_NUM_PER_BLOCK;\n\n\tconst int K_SPARSE = int(K * SPARSITY);\n\tint M_BLOCK_START = blockIdx.x * BLOCK_SIZE_M;\n\tint N_BLOCK_START = blockIdx.y * BLOCK_SIZE_N;\n\n\tconst int A_THREADS_PER_ROW = BLOCK_SIZE_K / 4;\n\tconst int B_THREADS_PER_ROW = BLOCK_SIZE_N / 4;\n\n\tconst int THREADS_PER_BLOCK = (BLOCK_SIZE_M / THREAD_SIZE_M) * (BLOCK_SIZE_N / THREAD_SIZE_N);\n\n\tconst int A_STRIDES = THREADS_PER_BLOCK / A_THREADS_PER_ROW;\n\tconst int B_STRIDES = THREADS_PER_BLOCK / B_THREADS_PER_ROW;\n\n\t__shared__ float A_shared[BLOCK_SIZE_M * BLOCK_SIZE_K];\n\t__shared__ float B_shared[BLOCK_SIZE_N * BLOCK_SIZE_K_SPARSE];\n\t__shared__ int B_index_shared[BLOCK_SIZE_N * BLOCK_SIZE_K_SPARSE];\n\n\tfloat A_reg[THREAD_SIZE_M];\n\tfloat B_reg[THREAD_SIZE_N];\n\tint B_reg_index;\n\tfloat C_reg[THREAD_SIZE_M][THREAD_SIZE_N] = {0};\n\n\tint tid = threadIdx.x;\n\n\tint t_N = tid % (BLOCK_SIZE_N / THREAD_SIZE_N);\n\tint t_M = tid / (BLOCK_SIZE_N / THREAD_SIZE_N);\n\n\tint A_BLOCK_ROW_START = tid / A_THREADS_PER_ROW;\n\tint B_BLOCK_ROW_START = tid / B_THREADS_PER_ROW;\n\n\tint A_BLOCK_COL_START = tid % A_THREADS_PER_ROW * 4;\n\tint B_BLOCK_COL_START = tid % B_THREADS_PER_ROW * 4;\n\n\tfor(int K_BLOCK_START = 0, K_SPARSE_BLOCK_START = 0; K_BLOCK_START < K; K_BLOCK_START += BLOCK_SIZE_K, K_SPARSE_BLOCK_START += BLOCK_SIZE_K_SPARSE){\n\t\tfloat *A_global_ptr = g_vec + M_BLOCK_START * K + K_BLOCK_START;\n\t\tfloat *B_global_ptr = g_mat_data + K_SPARSE_BLOCK_START * N + N_BLOCK_START;\n\t\tint *B_index_global_ptr = g_mat_index + K_SPARSE_BLOCK_START * N + N_BLOCK_START;\n\n\t\t__syncthreads();\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_M; i += A_STRIDES){\n\t\t\t*(float4 *)(A_shared + (i + A_BLOCK_ROW_START) * BLOCK_SIZE_K + A_BLOCK_COL_START) = \n\t\t\t\t*(float4 *)(A_global_ptr + (i + A_BLOCK_ROW_START) * K + A_BLOCK_COL_START);\n\t\t}\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_K_SPARSE; i += B_STRIDES){\n\t\t\t*(float4 *)(B_shared + (i + B_BLOCK_ROW_START) * BLOCK_SIZE_N + B_BLOCK_COL_START) =\n\t\t\t\t*(float4 *)(B_global_ptr + (i + B_BLOCK_ROW_START) * N + B_BLOCK_COL_START);\n\n\t\t\t*(float4 *)(B_index_shared + (i + B_BLOCK_ROW_START) * BLOCK_SIZE_N + B_BLOCK_COL_START) =\n\t\t\t\t*(float4 *)(B_index_global_ptr + (i + B_BLOCK_ROW_START) * N + B_BLOCK_COL_START);\n\t\t}\n\n\t\t__syncthreads();\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_K_SPARSE;i += 1){\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_N; k += 1){\n\t\t\t\tB_reg[k] = B_shared[i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k];\n\t\t\t\t//*(float4 *)(B_reg + k) = *(float4 *)(B_shared + i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k);\n\t\t\t\t//*(float4 *)(B_reg_index + k) = *(float4 *)(B_index_shared + i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k);\n\t\t\t}\n\t\t\tint bank_idx = i / LEN_OF_BANK_PER_SPARSE_BLOCK;\n\t\t\tB_reg_index = B_index_shared[i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N] % BANK_VAL+bank_idx * BANK_VAL;\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_M; k += 1){\n\t\t\t\tA_reg[k] = A_shared[(t_M * THREAD_SIZE_M+k) * BLOCK_SIZE_K + B_reg_index];\n\t\t\t}\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_N; k += 1){\n\t\t\t\t#pragma unroll\n\t\t\t\tfor(int j = 0; j < THREAD_SIZE_M; j += 1){\n\t\t\t\t\tC_reg[j][k] += B_reg[k] * A_reg[j];\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n    float bias_local[THREAD_SIZE_N];\n    for(int thread_x = 0; thread_x < THREAD_SIZE_N; thread_x++){\n        bias_local[thread_x] = bias[BLOCK_SIZE_N * blockIdx.y + THREAD_SIZE_N * t_N + thread_x];\n    }\n\n\t#pragma unroll\n\tfor(int i = 0; i < THREAD_SIZE_M; i += 1){\n\t\t#pragma unroll\n\t\tfor(int j = 0; j < THREAD_SIZE_N; j += 1){\n\t\t\tg_data[(BLOCK_SIZE_M * blockIdx.x + THREAD_SIZE_M * t_M + i) * N + BLOCK_SIZE_N * blockIdx.y + THREAD_SIZE_N * t_N + j] = C_reg[i][j] + bias_local[j];\n\t\t}\n\t}\n}", "launch_config": {"dimBlock": [128, 1], "dimGrid": [64, 6]}}, "bert.encoder.layer.10.attention.self.key": {"code": "__global__ void MatMul_TILE_THREAD_GENERAL(float *input0, float *input1, float *input2, float *input3, float *output0) {\n    /*\n    g_vec: A [M, K]\n    g_mat_data: condense B [K_sparse, N]\n    g_mat_index: condense B index [K_sparse, N]\n    g_data: C [M, N]\n    */\n\tfloat *g_vec = input0;\n\tfloat *g_mat_data = input1;\n\tint *g_mat_index = (int*)input2;\n\tfloat *bias = input3;\n\tfloat *g_data = output0;\n\n\tconst int M = 4096;\n\tconst int K = 768;\n\tconst int N = 768;\n\n    const int BLOCK_SIZE_M = 64;\n    const int BLOCK_SIZE_N = 128;\n    const int BLOCK_SIZE_K = 32;\n    const int THREAD_SIZE_M = 8;\n    const int THREAD_SIZE_N = 8;\n\n    const float SPARSITY = 0.875;\n    const int BANK_VAL = 32;\n    const int ALIGN_N = THREAD_SIZE_N;\n\n    const int BANK_NUM_PER_BLOCK = BLOCK_SIZE_K / BANK_VAL;\n    const int BLOCK_SIZE_K_SPARSE = int(BLOCK_SIZE_K * (1-SPARSITY));\n    const int LEN_OF_BANK_PER_SPARSE_BLOCK = BLOCK_SIZE_K_SPARSE / BANK_NUM_PER_BLOCK;\n\n\tconst int K_SPARSE = int(K * SPARSITY);\n\tint M_BLOCK_START = blockIdx.x * BLOCK_SIZE_M;\n\tint N_BLOCK_START = blockIdx.y * BLOCK_SIZE_N;\n\n\tconst int A_THREADS_PER_ROW = BLOCK_SIZE_K / 4;\n\tconst int B_THREADS_PER_ROW = BLOCK_SIZE_N / 4;\n\n\tconst int THREADS_PER_BLOCK = (BLOCK_SIZE_M / THREAD_SIZE_M) * (BLOCK_SIZE_N / THREAD_SIZE_N);\n\n\tconst int A_STRIDES = THREADS_PER_BLOCK / A_THREADS_PER_ROW;\n\tconst int B_STRIDES = THREADS_PER_BLOCK / B_THREADS_PER_ROW;\n\n\t__shared__ float A_shared[BLOCK_SIZE_M * BLOCK_SIZE_K];\n\t__shared__ float B_shared[BLOCK_SIZE_N * BLOCK_SIZE_K_SPARSE];\n\t__shared__ int B_index_shared[BLOCK_SIZE_N * BLOCK_SIZE_K_SPARSE];\n\n\tfloat A_reg[THREAD_SIZE_M];\n\tfloat B_reg[THREAD_SIZE_N];\n\tint B_reg_index;\n\tfloat C_reg[THREAD_SIZE_M][THREAD_SIZE_N] = {0};\n\n\tint tid = threadIdx.x;\n\n\tint t_N = tid % (BLOCK_SIZE_N / THREAD_SIZE_N);\n\tint t_M = tid / (BLOCK_SIZE_N / THREAD_SIZE_N);\n\n\tint A_BLOCK_ROW_START = tid / A_THREADS_PER_ROW;\n\tint B_BLOCK_ROW_START = tid / B_THREADS_PER_ROW;\n\n\tint A_BLOCK_COL_START = tid % A_THREADS_PER_ROW * 4;\n\tint B_BLOCK_COL_START = tid % B_THREADS_PER_ROW * 4;\n\n\tfor(int K_BLOCK_START = 0, K_SPARSE_BLOCK_START = 0; K_BLOCK_START < K; K_BLOCK_START += BLOCK_SIZE_K, K_SPARSE_BLOCK_START += BLOCK_SIZE_K_SPARSE){\n\t\tfloat *A_global_ptr = g_vec + M_BLOCK_START * K + K_BLOCK_START;\n\t\tfloat *B_global_ptr = g_mat_data + K_SPARSE_BLOCK_START * N + N_BLOCK_START;\n\t\tint *B_index_global_ptr = g_mat_index + K_SPARSE_BLOCK_START * N + N_BLOCK_START;\n\n\t\t__syncthreads();\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_M; i += A_STRIDES){\n\t\t\t*(float4 *)(A_shared + (i + A_BLOCK_ROW_START) * BLOCK_SIZE_K + A_BLOCK_COL_START) = \n\t\t\t\t*(float4 *)(A_global_ptr + (i + A_BLOCK_ROW_START) * K + A_BLOCK_COL_START);\n\t\t}\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_K_SPARSE; i += B_STRIDES){\n\t\t\t*(float4 *)(B_shared + (i + B_BLOCK_ROW_START) * BLOCK_SIZE_N + B_BLOCK_COL_START) =\n\t\t\t\t*(float4 *)(B_global_ptr + (i + B_BLOCK_ROW_START) * N + B_BLOCK_COL_START);\n\n\t\t\t*(float4 *)(B_index_shared + (i + B_BLOCK_ROW_START) * BLOCK_SIZE_N + B_BLOCK_COL_START) =\n\t\t\t\t*(float4 *)(B_index_global_ptr + (i + B_BLOCK_ROW_START) * N + B_BLOCK_COL_START);\n\t\t}\n\n\t\t__syncthreads();\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_K_SPARSE;i += 1){\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_N; k += 1){\n\t\t\t\tB_reg[k] = B_shared[i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k];\n\t\t\t\t//*(float4 *)(B_reg + k) = *(float4 *)(B_shared + i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k);\n\t\t\t\t//*(float4 *)(B_reg_index + k) = *(float4 *)(B_index_shared + i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k);\n\t\t\t}\n\t\t\tint bank_idx = i / LEN_OF_BANK_PER_SPARSE_BLOCK;\n\t\t\tB_reg_index = B_index_shared[i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N] % BANK_VAL+bank_idx * BANK_VAL;\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_M; k += 1){\n\t\t\t\tA_reg[k] = A_shared[(t_M * THREAD_SIZE_M+k) * BLOCK_SIZE_K + B_reg_index];\n\t\t\t}\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_N; k += 1){\n\t\t\t\t#pragma unroll\n\t\t\t\tfor(int j = 0; j < THREAD_SIZE_M; j += 1){\n\t\t\t\t\tC_reg[j][k] += B_reg[k] * A_reg[j];\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n    float bias_local[THREAD_SIZE_N];\n    for(int thread_x = 0; thread_x < THREAD_SIZE_N; thread_x++){\n        bias_local[thread_x] = bias[BLOCK_SIZE_N * blockIdx.y + THREAD_SIZE_N * t_N + thread_x];\n    }\n\n\t#pragma unroll\n\tfor(int i = 0; i < THREAD_SIZE_M; i += 1){\n\t\t#pragma unroll\n\t\tfor(int j = 0; j < THREAD_SIZE_N; j += 1){\n\t\t\tg_data[(BLOCK_SIZE_M * blockIdx.x + THREAD_SIZE_M * t_M + i) * N + BLOCK_SIZE_N * blockIdx.y + THREAD_SIZE_N * t_N + j] = C_reg[i][j] + bias_local[j];\n\t\t}\n\t}\n}", "launch_config": {"dimBlock": [128, 1], "dimGrid": [64, 6]}}, "bert.encoder.layer.10.attention.self.value": {"code": "__global__ void MatMul_TILE_THREAD_GENERAL(float *input0, float *input1, float *input2, float *input3, float *output0) {\n    /*\n    g_vec: A [M, K]\n    g_mat_data: condense B [K_sparse, N]\n    g_mat_index: condense B index [K_sparse, N]\n    g_data: C [M, N]\n    */\n\tfloat *g_vec = input0;\n\tfloat *g_mat_data = input1;\n\tint *g_mat_index = (int*)input2;\n\tfloat *bias = input3;\n\tfloat *g_data = output0;\n\n\tconst int M = 4096;\n\tconst int K = 768;\n\tconst int N = 768;\n\n    const int BLOCK_SIZE_M = 64;\n    const int BLOCK_SIZE_N = 128;\n    const int BLOCK_SIZE_K = 32;\n    const int THREAD_SIZE_M = 8;\n    const int THREAD_SIZE_N = 8;\n\n    const float SPARSITY = 0.875;\n    const int BANK_VAL = 32;\n    const int ALIGN_N = THREAD_SIZE_N;\n\n    const int BANK_NUM_PER_BLOCK = BLOCK_SIZE_K / BANK_VAL;\n    const int BLOCK_SIZE_K_SPARSE = int(BLOCK_SIZE_K * (1-SPARSITY));\n    const int LEN_OF_BANK_PER_SPARSE_BLOCK = BLOCK_SIZE_K_SPARSE / BANK_NUM_PER_BLOCK;\n\n\tconst int K_SPARSE = int(K * SPARSITY);\n\tint M_BLOCK_START = blockIdx.x * BLOCK_SIZE_M;\n\tint N_BLOCK_START = blockIdx.y * BLOCK_SIZE_N;\n\n\tconst int A_THREADS_PER_ROW = BLOCK_SIZE_K / 4;\n\tconst int B_THREADS_PER_ROW = BLOCK_SIZE_N / 4;\n\n\tconst int THREADS_PER_BLOCK = (BLOCK_SIZE_M / THREAD_SIZE_M) * (BLOCK_SIZE_N / THREAD_SIZE_N);\n\n\tconst int A_STRIDES = THREADS_PER_BLOCK / A_THREADS_PER_ROW;\n\tconst int B_STRIDES = THREADS_PER_BLOCK / B_THREADS_PER_ROW;\n\n\t__shared__ float A_shared[BLOCK_SIZE_M * BLOCK_SIZE_K];\n\t__shared__ float B_shared[BLOCK_SIZE_N * BLOCK_SIZE_K_SPARSE];\n\t__shared__ int B_index_shared[BLOCK_SIZE_N * BLOCK_SIZE_K_SPARSE];\n\n\tfloat A_reg[THREAD_SIZE_M];\n\tfloat B_reg[THREAD_SIZE_N];\n\tint B_reg_index;\n\tfloat C_reg[THREAD_SIZE_M][THREAD_SIZE_N] = {0};\n\n\tint tid = threadIdx.x;\n\n\tint t_N = tid % (BLOCK_SIZE_N / THREAD_SIZE_N);\n\tint t_M = tid / (BLOCK_SIZE_N / THREAD_SIZE_N);\n\n\tint A_BLOCK_ROW_START = tid / A_THREADS_PER_ROW;\n\tint B_BLOCK_ROW_START = tid / B_THREADS_PER_ROW;\n\n\tint A_BLOCK_COL_START = tid % A_THREADS_PER_ROW * 4;\n\tint B_BLOCK_COL_START = tid % B_THREADS_PER_ROW * 4;\n\n\tfor(int K_BLOCK_START = 0, K_SPARSE_BLOCK_START = 0; K_BLOCK_START < K; K_BLOCK_START += BLOCK_SIZE_K, K_SPARSE_BLOCK_START += BLOCK_SIZE_K_SPARSE){\n\t\tfloat *A_global_ptr = g_vec + M_BLOCK_START * K + K_BLOCK_START;\n\t\tfloat *B_global_ptr = g_mat_data + K_SPARSE_BLOCK_START * N + N_BLOCK_START;\n\t\tint *B_index_global_ptr = g_mat_index + K_SPARSE_BLOCK_START * N + N_BLOCK_START;\n\n\t\t__syncthreads();\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_M; i += A_STRIDES){\n\t\t\t*(float4 *)(A_shared + (i + A_BLOCK_ROW_START) * BLOCK_SIZE_K + A_BLOCK_COL_START) = \n\t\t\t\t*(float4 *)(A_global_ptr + (i + A_BLOCK_ROW_START) * K + A_BLOCK_COL_START);\n\t\t}\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_K_SPARSE; i += B_STRIDES){\n\t\t\t*(float4 *)(B_shared + (i + B_BLOCK_ROW_START) * BLOCK_SIZE_N + B_BLOCK_COL_START) =\n\t\t\t\t*(float4 *)(B_global_ptr + (i + B_BLOCK_ROW_START) * N + B_BLOCK_COL_START);\n\n\t\t\t*(float4 *)(B_index_shared + (i + B_BLOCK_ROW_START) * BLOCK_SIZE_N + B_BLOCK_COL_START) =\n\t\t\t\t*(float4 *)(B_index_global_ptr + (i + B_BLOCK_ROW_START) * N + B_BLOCK_COL_START);\n\t\t}\n\n\t\t__syncthreads();\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_K_SPARSE;i += 1){\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_N; k += 1){\n\t\t\t\tB_reg[k] = B_shared[i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k];\n\t\t\t\t//*(float4 *)(B_reg + k) = *(float4 *)(B_shared + i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k);\n\t\t\t\t//*(float4 *)(B_reg_index + k) = *(float4 *)(B_index_shared + i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k);\n\t\t\t}\n\t\t\tint bank_idx = i / LEN_OF_BANK_PER_SPARSE_BLOCK;\n\t\t\tB_reg_index = B_index_shared[i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N] % BANK_VAL+bank_idx * BANK_VAL;\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_M; k += 1){\n\t\t\t\tA_reg[k] = A_shared[(t_M * THREAD_SIZE_M+k) * BLOCK_SIZE_K + B_reg_index];\n\t\t\t}\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_N; k += 1){\n\t\t\t\t#pragma unroll\n\t\t\t\tfor(int j = 0; j < THREAD_SIZE_M; j += 1){\n\t\t\t\t\tC_reg[j][k] += B_reg[k] * A_reg[j];\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n    float bias_local[THREAD_SIZE_N];\n    for(int thread_x = 0; thread_x < THREAD_SIZE_N; thread_x++){\n        bias_local[thread_x] = bias[BLOCK_SIZE_N * blockIdx.y + THREAD_SIZE_N * t_N + thread_x];\n    }\n\n\t#pragma unroll\n\tfor(int i = 0; i < THREAD_SIZE_M; i += 1){\n\t\t#pragma unroll\n\t\tfor(int j = 0; j < THREAD_SIZE_N; j += 1){\n\t\t\tg_data[(BLOCK_SIZE_M * blockIdx.x + THREAD_SIZE_M * t_M + i) * N + BLOCK_SIZE_N * blockIdx.y + THREAD_SIZE_N * t_N + j] = C_reg[i][j] + bias_local[j];\n\t\t}\n\t}\n}", "launch_config": {"dimBlock": [128, 1], "dimGrid": [64, 6]}}, "bert.encoder.layer.10.attention.output.dense": {"code": "__global__ void MatMul_TILE_THREAD_GENERAL(float *input0, float *input1, float *input2, float *input3, float *output0) {\n    /*\n    g_vec: A [M, K]\n    g_mat_data: condense B [K_sparse, N]\n    g_mat_index: condense B index [K_sparse, N]\n    g_data: C [M, N]\n    */\n\tfloat *g_vec = input0;\n\tfloat *g_mat_data = input1;\n\tint *g_mat_index = (int*)input2;\n\tfloat *bias = input3;\n\tfloat *g_data = output0;\n\n\tconst int M = 4096;\n\tconst int K = 768;\n\tconst int N = 768;\n\n    const int BLOCK_SIZE_M = 64;\n    const int BLOCK_SIZE_N = 128;\n    const int BLOCK_SIZE_K = 32;\n    const int THREAD_SIZE_M = 8;\n    const int THREAD_SIZE_N = 8;\n\n    const float SPARSITY = 0.875;\n    const int BANK_VAL = 32;\n    const int ALIGN_N = THREAD_SIZE_N;\n\n    const int BANK_NUM_PER_BLOCK = BLOCK_SIZE_K / BANK_VAL;\n    const int BLOCK_SIZE_K_SPARSE = int(BLOCK_SIZE_K * (1-SPARSITY));\n    const int LEN_OF_BANK_PER_SPARSE_BLOCK = BLOCK_SIZE_K_SPARSE / BANK_NUM_PER_BLOCK;\n\n\tconst int K_SPARSE = int(K * SPARSITY);\n\tint M_BLOCK_START = blockIdx.x * BLOCK_SIZE_M;\n\tint N_BLOCK_START = blockIdx.y * BLOCK_SIZE_N;\n\n\tconst int A_THREADS_PER_ROW = BLOCK_SIZE_K / 4;\n\tconst int B_THREADS_PER_ROW = BLOCK_SIZE_N / 4;\n\n\tconst int THREADS_PER_BLOCK = (BLOCK_SIZE_M / THREAD_SIZE_M) * (BLOCK_SIZE_N / THREAD_SIZE_N);\n\n\tconst int A_STRIDES = THREADS_PER_BLOCK / A_THREADS_PER_ROW;\n\tconst int B_STRIDES = THREADS_PER_BLOCK / B_THREADS_PER_ROW;\n\n\t__shared__ float A_shared[BLOCK_SIZE_M * BLOCK_SIZE_K];\n\t__shared__ float B_shared[BLOCK_SIZE_N * BLOCK_SIZE_K_SPARSE];\n\t__shared__ int B_index_shared[BLOCK_SIZE_N * BLOCK_SIZE_K_SPARSE];\n\n\tfloat A_reg[THREAD_SIZE_M];\n\tfloat B_reg[THREAD_SIZE_N];\n\tint B_reg_index;\n\tfloat C_reg[THREAD_SIZE_M][THREAD_SIZE_N] = {0};\n\n\tint tid = threadIdx.x;\n\n\tint t_N = tid % (BLOCK_SIZE_N / THREAD_SIZE_N);\n\tint t_M = tid / (BLOCK_SIZE_N / THREAD_SIZE_N);\n\n\tint A_BLOCK_ROW_START = tid / A_THREADS_PER_ROW;\n\tint B_BLOCK_ROW_START = tid / B_THREADS_PER_ROW;\n\n\tint A_BLOCK_COL_START = tid % A_THREADS_PER_ROW * 4;\n\tint B_BLOCK_COL_START = tid % B_THREADS_PER_ROW * 4;\n\n\tfor(int K_BLOCK_START = 0, K_SPARSE_BLOCK_START = 0; K_BLOCK_START < K; K_BLOCK_START += BLOCK_SIZE_K, K_SPARSE_BLOCK_START += BLOCK_SIZE_K_SPARSE){\n\t\tfloat *A_global_ptr = g_vec + M_BLOCK_START * K + K_BLOCK_START;\n\t\tfloat *B_global_ptr = g_mat_data + K_SPARSE_BLOCK_START * N + N_BLOCK_START;\n\t\tint *B_index_global_ptr = g_mat_index + K_SPARSE_BLOCK_START * N + N_BLOCK_START;\n\n\t\t__syncthreads();\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_M; i += A_STRIDES){\n\t\t\t*(float4 *)(A_shared + (i + A_BLOCK_ROW_START) * BLOCK_SIZE_K + A_BLOCK_COL_START) = \n\t\t\t\t*(float4 *)(A_global_ptr + (i + A_BLOCK_ROW_START) * K + A_BLOCK_COL_START);\n\t\t}\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_K_SPARSE; i += B_STRIDES){\n\t\t\t*(float4 *)(B_shared + (i + B_BLOCK_ROW_START) * BLOCK_SIZE_N + B_BLOCK_COL_START) =\n\t\t\t\t*(float4 *)(B_global_ptr + (i + B_BLOCK_ROW_START) * N + B_BLOCK_COL_START);\n\n\t\t\t*(float4 *)(B_index_shared + (i + B_BLOCK_ROW_START) * BLOCK_SIZE_N + B_BLOCK_COL_START) =\n\t\t\t\t*(float4 *)(B_index_global_ptr + (i + B_BLOCK_ROW_START) * N + B_BLOCK_COL_START);\n\t\t}\n\n\t\t__syncthreads();\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_K_SPARSE;i += 1){\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_N; k += 1){\n\t\t\t\tB_reg[k] = B_shared[i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k];\n\t\t\t\t//*(float4 *)(B_reg + k) = *(float4 *)(B_shared + i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k);\n\t\t\t\t//*(float4 *)(B_reg_index + k) = *(float4 *)(B_index_shared + i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k);\n\t\t\t}\n\t\t\tint bank_idx = i / LEN_OF_BANK_PER_SPARSE_BLOCK;\n\t\t\tB_reg_index = B_index_shared[i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N] % BANK_VAL+bank_idx * BANK_VAL;\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_M; k += 1){\n\t\t\t\tA_reg[k] = A_shared[(t_M * THREAD_SIZE_M+k) * BLOCK_SIZE_K + B_reg_index];\n\t\t\t}\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_N; k += 1){\n\t\t\t\t#pragma unroll\n\t\t\t\tfor(int j = 0; j < THREAD_SIZE_M; j += 1){\n\t\t\t\t\tC_reg[j][k] += B_reg[k] * A_reg[j];\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n    float bias_local[THREAD_SIZE_N];\n    for(int thread_x = 0; thread_x < THREAD_SIZE_N; thread_x++){\n        bias_local[thread_x] = bias[BLOCK_SIZE_N * blockIdx.y + THREAD_SIZE_N * t_N + thread_x];\n    }\n\n\t#pragma unroll\n\tfor(int i = 0; i < THREAD_SIZE_M; i += 1){\n\t\t#pragma unroll\n\t\tfor(int j = 0; j < THREAD_SIZE_N; j += 1){\n\t\t\tg_data[(BLOCK_SIZE_M * blockIdx.x + THREAD_SIZE_M * t_M + i) * N + BLOCK_SIZE_N * blockIdx.y + THREAD_SIZE_N * t_N + j] = C_reg[i][j] + bias_local[j];\n\t\t}\n\t}\n}", "launch_config": {"dimBlock": [128, 1], "dimGrid": [64, 6]}}, "bert.encoder.layer.10.intermediate.dense": {"code": "__global__ void MatMul_TILE_THREAD_GENERAL(float *input0, float *input1, float *input2, float *input3, float *output0) {\n    /*\n    g_vec: A [M, K]\n    g_mat_data: condense B [K_sparse, N]\n    g_mat_index: condense B index [K_sparse, N]\n    g_data: C [M, N]\n    */\n\tfloat *g_vec = input0;\n\tfloat *g_mat_data = input1;\n\tint *g_mat_index = (int*)input2;\n\tfloat *bias = input3;\n\tfloat *g_data = output0;\n\n\tconst int M = 4096;\n\tconst int K = 768;\n\tconst int N = 3072;\n\n    const int BLOCK_SIZE_M = 64;\n    const int BLOCK_SIZE_N = 128;\n    const int BLOCK_SIZE_K = 32;\n    const int THREAD_SIZE_M = 8;\n    const int THREAD_SIZE_N = 8;\n\n    const float SPARSITY = 0.875;\n    const int BANK_VAL = 32;\n    const int ALIGN_N = THREAD_SIZE_N;\n\n    const int BANK_NUM_PER_BLOCK = BLOCK_SIZE_K / BANK_VAL;\n    const int BLOCK_SIZE_K_SPARSE = int(BLOCK_SIZE_K * (1-SPARSITY));\n    const int LEN_OF_BANK_PER_SPARSE_BLOCK = BLOCK_SIZE_K_SPARSE / BANK_NUM_PER_BLOCK;\n\n\tconst int K_SPARSE = int(K * SPARSITY);\n\tint M_BLOCK_START = blockIdx.x * BLOCK_SIZE_M;\n\tint N_BLOCK_START = blockIdx.y * BLOCK_SIZE_N;\n\n\tconst int A_THREADS_PER_ROW = BLOCK_SIZE_K / 4;\n\tconst int B_THREADS_PER_ROW = BLOCK_SIZE_N / 4;\n\n\tconst int THREADS_PER_BLOCK = (BLOCK_SIZE_M / THREAD_SIZE_M) * (BLOCK_SIZE_N / THREAD_SIZE_N);\n\n\tconst int A_STRIDES = THREADS_PER_BLOCK / A_THREADS_PER_ROW;\n\tconst int B_STRIDES = THREADS_PER_BLOCK / B_THREADS_PER_ROW;\n\n\t__shared__ float A_shared[BLOCK_SIZE_M * BLOCK_SIZE_K];\n\t__shared__ float B_shared[BLOCK_SIZE_N * BLOCK_SIZE_K_SPARSE];\n\t__shared__ int B_index_shared[BLOCK_SIZE_N * BLOCK_SIZE_K_SPARSE];\n\n\tfloat A_reg[THREAD_SIZE_M];\n\tfloat B_reg[THREAD_SIZE_N];\n\tint B_reg_index;\n\tfloat C_reg[THREAD_SIZE_M][THREAD_SIZE_N] = {0};\n\n\tint tid = threadIdx.x;\n\n\tint t_N = tid % (BLOCK_SIZE_N / THREAD_SIZE_N);\n\tint t_M = tid / (BLOCK_SIZE_N / THREAD_SIZE_N);\n\n\tint A_BLOCK_ROW_START = tid / A_THREADS_PER_ROW;\n\tint B_BLOCK_ROW_START = tid / B_THREADS_PER_ROW;\n\n\tint A_BLOCK_COL_START = tid % A_THREADS_PER_ROW * 4;\n\tint B_BLOCK_COL_START = tid % B_THREADS_PER_ROW * 4;\n\n\tfor(int K_BLOCK_START = 0, K_SPARSE_BLOCK_START = 0; K_BLOCK_START < K; K_BLOCK_START += BLOCK_SIZE_K, K_SPARSE_BLOCK_START += BLOCK_SIZE_K_SPARSE){\n\t\tfloat *A_global_ptr = g_vec + M_BLOCK_START * K + K_BLOCK_START;\n\t\tfloat *B_global_ptr = g_mat_data + K_SPARSE_BLOCK_START * N + N_BLOCK_START;\n\t\tint *B_index_global_ptr = g_mat_index + K_SPARSE_BLOCK_START * N + N_BLOCK_START;\n\n\t\t__syncthreads();\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_M; i += A_STRIDES){\n\t\t\t*(float4 *)(A_shared + (i + A_BLOCK_ROW_START) * BLOCK_SIZE_K + A_BLOCK_COL_START) = \n\t\t\t\t*(float4 *)(A_global_ptr + (i + A_BLOCK_ROW_START) * K + A_BLOCK_COL_START);\n\t\t}\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_K_SPARSE; i += B_STRIDES){\n\t\t\t*(float4 *)(B_shared + (i + B_BLOCK_ROW_START) * BLOCK_SIZE_N + B_BLOCK_COL_START) =\n\t\t\t\t*(float4 *)(B_global_ptr + (i + B_BLOCK_ROW_START) * N + B_BLOCK_COL_START);\n\n\t\t\t*(float4 *)(B_index_shared + (i + B_BLOCK_ROW_START) * BLOCK_SIZE_N + B_BLOCK_COL_START) =\n\t\t\t\t*(float4 *)(B_index_global_ptr + (i + B_BLOCK_ROW_START) * N + B_BLOCK_COL_START);\n\t\t}\n\n\t\t__syncthreads();\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_K_SPARSE;i += 1){\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_N; k += 1){\n\t\t\t\tB_reg[k] = B_shared[i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k];\n\t\t\t\t//*(float4 *)(B_reg + k) = *(float4 *)(B_shared + i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k);\n\t\t\t\t//*(float4 *)(B_reg_index + k) = *(float4 *)(B_index_shared + i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k);\n\t\t\t}\n\t\t\tint bank_idx = i / LEN_OF_BANK_PER_SPARSE_BLOCK;\n\t\t\tB_reg_index = B_index_shared[i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N] % BANK_VAL+bank_idx * BANK_VAL;\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_M; k += 1){\n\t\t\t\tA_reg[k] = A_shared[(t_M * THREAD_SIZE_M+k) * BLOCK_SIZE_K + B_reg_index];\n\t\t\t}\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_N; k += 1){\n\t\t\t\t#pragma unroll\n\t\t\t\tfor(int j = 0; j < THREAD_SIZE_M; j += 1){\n\t\t\t\t\tC_reg[j][k] += B_reg[k] * A_reg[j];\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n    float bias_local[THREAD_SIZE_N];\n    for(int thread_x = 0; thread_x < THREAD_SIZE_N; thread_x++){\n        bias_local[thread_x] = bias[BLOCK_SIZE_N * blockIdx.y + THREAD_SIZE_N * t_N + thread_x];\n    }\n\n\t#pragma unroll\n\tfor(int i = 0; i < THREAD_SIZE_M; i += 1){\n\t\t#pragma unroll\n\t\tfor(int j = 0; j < THREAD_SIZE_N; j += 1){\n\t\t\tg_data[(BLOCK_SIZE_M * blockIdx.x + THREAD_SIZE_M * t_M + i) * N + BLOCK_SIZE_N * blockIdx.y + THREAD_SIZE_N * t_N + j] = C_reg[i][j] + bias_local[j];\n\t\t}\n\t}\n}", "launch_config": {"dimBlock": [128, 1], "dimGrid": [64, 24]}}, "bert.encoder.layer.10.output.dense": {"code": "__global__ void MatMul_TILE_THREAD_GENERAL(float *input0, float *input1, float *input2, float *input3, float *output0) {\n    /*\n    g_vec: A [M, K]\n    g_mat_data: condense B [K_sparse, N]\n    g_mat_index: condense B index [K_sparse, N]\n    g_data: C [M, N]\n    */\n\tfloat *g_vec = input0;\n\tfloat *g_mat_data = input1;\n\tint *g_mat_index = (int*)input2;\n\tfloat *bias = input3;\n\tfloat *g_data = output0;\n\n\tconst int M = 4096;\n\tconst int K = 3072;\n\tconst int N = 768;\n\n    const int BLOCK_SIZE_M = 64;\n    const int BLOCK_SIZE_N = 128;\n    const int BLOCK_SIZE_K = 32;\n    const int THREAD_SIZE_M = 8;\n    const int THREAD_SIZE_N = 8;\n\n    const float SPARSITY = 0.875;\n    const int BANK_VAL = 32;\n    const int ALIGN_N = THREAD_SIZE_N;\n\n    const int BANK_NUM_PER_BLOCK = BLOCK_SIZE_K / BANK_VAL;\n    const int BLOCK_SIZE_K_SPARSE = int(BLOCK_SIZE_K * (1-SPARSITY));\n    const int LEN_OF_BANK_PER_SPARSE_BLOCK = BLOCK_SIZE_K_SPARSE / BANK_NUM_PER_BLOCK;\n\n\tconst int K_SPARSE = int(K * SPARSITY);\n\tint M_BLOCK_START = blockIdx.x * BLOCK_SIZE_M;\n\tint N_BLOCK_START = blockIdx.y * BLOCK_SIZE_N;\n\n\tconst int A_THREADS_PER_ROW = BLOCK_SIZE_K / 4;\n\tconst int B_THREADS_PER_ROW = BLOCK_SIZE_N / 4;\n\n\tconst int THREADS_PER_BLOCK = (BLOCK_SIZE_M / THREAD_SIZE_M) * (BLOCK_SIZE_N / THREAD_SIZE_N);\n\n\tconst int A_STRIDES = THREADS_PER_BLOCK / A_THREADS_PER_ROW;\n\tconst int B_STRIDES = THREADS_PER_BLOCK / B_THREADS_PER_ROW;\n\n\t__shared__ float A_shared[BLOCK_SIZE_M * BLOCK_SIZE_K];\n\t__shared__ float B_shared[BLOCK_SIZE_N * BLOCK_SIZE_K_SPARSE];\n\t__shared__ int B_index_shared[BLOCK_SIZE_N * BLOCK_SIZE_K_SPARSE];\n\n\tfloat A_reg[THREAD_SIZE_M];\n\tfloat B_reg[THREAD_SIZE_N];\n\tint B_reg_index;\n\tfloat C_reg[THREAD_SIZE_M][THREAD_SIZE_N] = {0};\n\n\tint tid = threadIdx.x;\n\n\tint t_N = tid % (BLOCK_SIZE_N / THREAD_SIZE_N);\n\tint t_M = tid / (BLOCK_SIZE_N / THREAD_SIZE_N);\n\n\tint A_BLOCK_ROW_START = tid / A_THREADS_PER_ROW;\n\tint B_BLOCK_ROW_START = tid / B_THREADS_PER_ROW;\n\n\tint A_BLOCK_COL_START = tid % A_THREADS_PER_ROW * 4;\n\tint B_BLOCK_COL_START = tid % B_THREADS_PER_ROW * 4;\n\n\tfor(int K_BLOCK_START = 0, K_SPARSE_BLOCK_START = 0; K_BLOCK_START < K; K_BLOCK_START += BLOCK_SIZE_K, K_SPARSE_BLOCK_START += BLOCK_SIZE_K_SPARSE){\n\t\tfloat *A_global_ptr = g_vec + M_BLOCK_START * K + K_BLOCK_START;\n\t\tfloat *B_global_ptr = g_mat_data + K_SPARSE_BLOCK_START * N + N_BLOCK_START;\n\t\tint *B_index_global_ptr = g_mat_index + K_SPARSE_BLOCK_START * N + N_BLOCK_START;\n\n\t\t__syncthreads();\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_M; i += A_STRIDES){\n\t\t\t*(float4 *)(A_shared + (i + A_BLOCK_ROW_START) * BLOCK_SIZE_K + A_BLOCK_COL_START) = \n\t\t\t\t*(float4 *)(A_global_ptr + (i + A_BLOCK_ROW_START) * K + A_BLOCK_COL_START);\n\t\t}\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_K_SPARSE; i += B_STRIDES){\n\t\t\t*(float4 *)(B_shared + (i + B_BLOCK_ROW_START) * BLOCK_SIZE_N + B_BLOCK_COL_START) =\n\t\t\t\t*(float4 *)(B_global_ptr + (i + B_BLOCK_ROW_START) * N + B_BLOCK_COL_START);\n\n\t\t\t*(float4 *)(B_index_shared + (i + B_BLOCK_ROW_START) * BLOCK_SIZE_N + B_BLOCK_COL_START) =\n\t\t\t\t*(float4 *)(B_index_global_ptr + (i + B_BLOCK_ROW_START) * N + B_BLOCK_COL_START);\n\t\t}\n\n\t\t__syncthreads();\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_K_SPARSE;i += 1){\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_N; k += 1){\n\t\t\t\tB_reg[k] = B_shared[i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k];\n\t\t\t\t//*(float4 *)(B_reg + k) = *(float4 *)(B_shared + i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k);\n\t\t\t\t//*(float4 *)(B_reg_index + k) = *(float4 *)(B_index_shared + i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k);\n\t\t\t}\n\t\t\tint bank_idx = i / LEN_OF_BANK_PER_SPARSE_BLOCK;\n\t\t\tB_reg_index = B_index_shared[i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N] % BANK_VAL+bank_idx * BANK_VAL;\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_M; k += 1){\n\t\t\t\tA_reg[k] = A_shared[(t_M * THREAD_SIZE_M+k) * BLOCK_SIZE_K + B_reg_index];\n\t\t\t}\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_N; k += 1){\n\t\t\t\t#pragma unroll\n\t\t\t\tfor(int j = 0; j < THREAD_SIZE_M; j += 1){\n\t\t\t\t\tC_reg[j][k] += B_reg[k] * A_reg[j];\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n    float bias_local[THREAD_SIZE_N];\n    for(int thread_x = 0; thread_x < THREAD_SIZE_N; thread_x++){\n        bias_local[thread_x] = bias[BLOCK_SIZE_N * blockIdx.y + THREAD_SIZE_N * t_N + thread_x];\n    }\n\n\t#pragma unroll\n\tfor(int i = 0; i < THREAD_SIZE_M; i += 1){\n\t\t#pragma unroll\n\t\tfor(int j = 0; j < THREAD_SIZE_N; j += 1){\n\t\t\tg_data[(BLOCK_SIZE_M * blockIdx.x + THREAD_SIZE_M * t_M + i) * N + BLOCK_SIZE_N * blockIdx.y + THREAD_SIZE_N * t_N + j] = C_reg[i][j] + bias_local[j];\n\t\t}\n\t}\n}", "launch_config": {"dimBlock": [128, 1], "dimGrid": [64, 6]}}, "bert.encoder.layer.11.attention.self.query": {"code": "__global__ void MatMul_TILE_THREAD_GENERAL(float *input0, float *input1, float *input2, float *input3, float *output0) {\n    /*\n    g_vec: A [M, K]\n    g_mat_data: condense B [K_sparse, N]\n    g_mat_index: condense B index [K_sparse, N]\n    g_data: C [M, N]\n    */\n\tfloat *g_vec = input0;\n\tfloat *g_mat_data = input1;\n\tint *g_mat_index = (int*)input2;\n\tfloat *bias = input3;\n\tfloat *g_data = output0;\n\n\tconst int M = 4096;\n\tconst int K = 768;\n\tconst int N = 768;\n\n    const int BLOCK_SIZE_M = 64;\n    const int BLOCK_SIZE_N = 128;\n    const int BLOCK_SIZE_K = 32;\n    const int THREAD_SIZE_M = 8;\n    const int THREAD_SIZE_N = 8;\n\n    const float SPARSITY = 0.875;\n    const int BANK_VAL = 32;\n    const int ALIGN_N = THREAD_SIZE_N;\n\n    const int BANK_NUM_PER_BLOCK = BLOCK_SIZE_K / BANK_VAL;\n    const int BLOCK_SIZE_K_SPARSE = int(BLOCK_SIZE_K * (1-SPARSITY));\n    const int LEN_OF_BANK_PER_SPARSE_BLOCK = BLOCK_SIZE_K_SPARSE / BANK_NUM_PER_BLOCK;\n\n\tconst int K_SPARSE = int(K * SPARSITY);\n\tint M_BLOCK_START = blockIdx.x * BLOCK_SIZE_M;\n\tint N_BLOCK_START = blockIdx.y * BLOCK_SIZE_N;\n\n\tconst int A_THREADS_PER_ROW = BLOCK_SIZE_K / 4;\n\tconst int B_THREADS_PER_ROW = BLOCK_SIZE_N / 4;\n\n\tconst int THREADS_PER_BLOCK = (BLOCK_SIZE_M / THREAD_SIZE_M) * (BLOCK_SIZE_N / THREAD_SIZE_N);\n\n\tconst int A_STRIDES = THREADS_PER_BLOCK / A_THREADS_PER_ROW;\n\tconst int B_STRIDES = THREADS_PER_BLOCK / B_THREADS_PER_ROW;\n\n\t__shared__ float A_shared[BLOCK_SIZE_M * BLOCK_SIZE_K];\n\t__shared__ float B_shared[BLOCK_SIZE_N * BLOCK_SIZE_K_SPARSE];\n\t__shared__ int B_index_shared[BLOCK_SIZE_N * BLOCK_SIZE_K_SPARSE];\n\n\tfloat A_reg[THREAD_SIZE_M];\n\tfloat B_reg[THREAD_SIZE_N];\n\tint B_reg_index;\n\tfloat C_reg[THREAD_SIZE_M][THREAD_SIZE_N] = {0};\n\n\tint tid = threadIdx.x;\n\n\tint t_N = tid % (BLOCK_SIZE_N / THREAD_SIZE_N);\n\tint t_M = tid / (BLOCK_SIZE_N / THREAD_SIZE_N);\n\n\tint A_BLOCK_ROW_START = tid / A_THREADS_PER_ROW;\n\tint B_BLOCK_ROW_START = tid / B_THREADS_PER_ROW;\n\n\tint A_BLOCK_COL_START = tid % A_THREADS_PER_ROW * 4;\n\tint B_BLOCK_COL_START = tid % B_THREADS_PER_ROW * 4;\n\n\tfor(int K_BLOCK_START = 0, K_SPARSE_BLOCK_START = 0; K_BLOCK_START < K; K_BLOCK_START += BLOCK_SIZE_K, K_SPARSE_BLOCK_START += BLOCK_SIZE_K_SPARSE){\n\t\tfloat *A_global_ptr = g_vec + M_BLOCK_START * K + K_BLOCK_START;\n\t\tfloat *B_global_ptr = g_mat_data + K_SPARSE_BLOCK_START * N + N_BLOCK_START;\n\t\tint *B_index_global_ptr = g_mat_index + K_SPARSE_BLOCK_START * N + N_BLOCK_START;\n\n\t\t__syncthreads();\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_M; i += A_STRIDES){\n\t\t\t*(float4 *)(A_shared + (i + A_BLOCK_ROW_START) * BLOCK_SIZE_K + A_BLOCK_COL_START) = \n\t\t\t\t*(float4 *)(A_global_ptr + (i + A_BLOCK_ROW_START) * K + A_BLOCK_COL_START);\n\t\t}\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_K_SPARSE; i += B_STRIDES){\n\t\t\t*(float4 *)(B_shared + (i + B_BLOCK_ROW_START) * BLOCK_SIZE_N + B_BLOCK_COL_START) =\n\t\t\t\t*(float4 *)(B_global_ptr + (i + B_BLOCK_ROW_START) * N + B_BLOCK_COL_START);\n\n\t\t\t*(float4 *)(B_index_shared + (i + B_BLOCK_ROW_START) * BLOCK_SIZE_N + B_BLOCK_COL_START) =\n\t\t\t\t*(float4 *)(B_index_global_ptr + (i + B_BLOCK_ROW_START) * N + B_BLOCK_COL_START);\n\t\t}\n\n\t\t__syncthreads();\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_K_SPARSE;i += 1){\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_N; k += 1){\n\t\t\t\tB_reg[k] = B_shared[i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k];\n\t\t\t\t//*(float4 *)(B_reg + k) = *(float4 *)(B_shared + i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k);\n\t\t\t\t//*(float4 *)(B_reg_index + k) = *(float4 *)(B_index_shared + i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k);\n\t\t\t}\n\t\t\tint bank_idx = i / LEN_OF_BANK_PER_SPARSE_BLOCK;\n\t\t\tB_reg_index = B_index_shared[i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N] % BANK_VAL+bank_idx * BANK_VAL;\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_M; k += 1){\n\t\t\t\tA_reg[k] = A_shared[(t_M * THREAD_SIZE_M+k) * BLOCK_SIZE_K + B_reg_index];\n\t\t\t}\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_N; k += 1){\n\t\t\t\t#pragma unroll\n\t\t\t\tfor(int j = 0; j < THREAD_SIZE_M; j += 1){\n\t\t\t\t\tC_reg[j][k] += B_reg[k] * A_reg[j];\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n    float bias_local[THREAD_SIZE_N];\n    for(int thread_x = 0; thread_x < THREAD_SIZE_N; thread_x++){\n        bias_local[thread_x] = bias[BLOCK_SIZE_N * blockIdx.y + THREAD_SIZE_N * t_N + thread_x];\n    }\n\n\t#pragma unroll\n\tfor(int i = 0; i < THREAD_SIZE_M; i += 1){\n\t\t#pragma unroll\n\t\tfor(int j = 0; j < THREAD_SIZE_N; j += 1){\n\t\t\tg_data[(BLOCK_SIZE_M * blockIdx.x + THREAD_SIZE_M * t_M + i) * N + BLOCK_SIZE_N * blockIdx.y + THREAD_SIZE_N * t_N + j] = C_reg[i][j] + bias_local[j];\n\t\t}\n\t}\n}", "launch_config": {"dimBlock": [128, 1], "dimGrid": [64, 6]}}, "bert.encoder.layer.11.attention.self.key": {"code": "__global__ void MatMul_TILE_THREAD_GENERAL(float *input0, float *input1, float *input2, float *input3, float *output0) {\n    /*\n    g_vec: A [M, K]\n    g_mat_data: condense B [K_sparse, N]\n    g_mat_index: condense B index [K_sparse, N]\n    g_data: C [M, N]\n    */\n\tfloat *g_vec = input0;\n\tfloat *g_mat_data = input1;\n\tint *g_mat_index = (int*)input2;\n\tfloat *bias = input3;\n\tfloat *g_data = output0;\n\n\tconst int M = 4096;\n\tconst int K = 768;\n\tconst int N = 768;\n\n    const int BLOCK_SIZE_M = 64;\n    const int BLOCK_SIZE_N = 128;\n    const int BLOCK_SIZE_K = 32;\n    const int THREAD_SIZE_M = 8;\n    const int THREAD_SIZE_N = 8;\n\n    const float SPARSITY = 0.875;\n    const int BANK_VAL = 32;\n    const int ALIGN_N = THREAD_SIZE_N;\n\n    const int BANK_NUM_PER_BLOCK = BLOCK_SIZE_K / BANK_VAL;\n    const int BLOCK_SIZE_K_SPARSE = int(BLOCK_SIZE_K * (1-SPARSITY));\n    const int LEN_OF_BANK_PER_SPARSE_BLOCK = BLOCK_SIZE_K_SPARSE / BANK_NUM_PER_BLOCK;\n\n\tconst int K_SPARSE = int(K * SPARSITY);\n\tint M_BLOCK_START = blockIdx.x * BLOCK_SIZE_M;\n\tint N_BLOCK_START = blockIdx.y * BLOCK_SIZE_N;\n\n\tconst int A_THREADS_PER_ROW = BLOCK_SIZE_K / 4;\n\tconst int B_THREADS_PER_ROW = BLOCK_SIZE_N / 4;\n\n\tconst int THREADS_PER_BLOCK = (BLOCK_SIZE_M / THREAD_SIZE_M) * (BLOCK_SIZE_N / THREAD_SIZE_N);\n\n\tconst int A_STRIDES = THREADS_PER_BLOCK / A_THREADS_PER_ROW;\n\tconst int B_STRIDES = THREADS_PER_BLOCK / B_THREADS_PER_ROW;\n\n\t__shared__ float A_shared[BLOCK_SIZE_M * BLOCK_SIZE_K];\n\t__shared__ float B_shared[BLOCK_SIZE_N * BLOCK_SIZE_K_SPARSE];\n\t__shared__ int B_index_shared[BLOCK_SIZE_N * BLOCK_SIZE_K_SPARSE];\n\n\tfloat A_reg[THREAD_SIZE_M];\n\tfloat B_reg[THREAD_SIZE_N];\n\tint B_reg_index;\n\tfloat C_reg[THREAD_SIZE_M][THREAD_SIZE_N] = {0};\n\n\tint tid = threadIdx.x;\n\n\tint t_N = tid % (BLOCK_SIZE_N / THREAD_SIZE_N);\n\tint t_M = tid / (BLOCK_SIZE_N / THREAD_SIZE_N);\n\n\tint A_BLOCK_ROW_START = tid / A_THREADS_PER_ROW;\n\tint B_BLOCK_ROW_START = tid / B_THREADS_PER_ROW;\n\n\tint A_BLOCK_COL_START = tid % A_THREADS_PER_ROW * 4;\n\tint B_BLOCK_COL_START = tid % B_THREADS_PER_ROW * 4;\n\n\tfor(int K_BLOCK_START = 0, K_SPARSE_BLOCK_START = 0; K_BLOCK_START < K; K_BLOCK_START += BLOCK_SIZE_K, K_SPARSE_BLOCK_START += BLOCK_SIZE_K_SPARSE){\n\t\tfloat *A_global_ptr = g_vec + M_BLOCK_START * K + K_BLOCK_START;\n\t\tfloat *B_global_ptr = g_mat_data + K_SPARSE_BLOCK_START * N + N_BLOCK_START;\n\t\tint *B_index_global_ptr = g_mat_index + K_SPARSE_BLOCK_START * N + N_BLOCK_START;\n\n\t\t__syncthreads();\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_M; i += A_STRIDES){\n\t\t\t*(float4 *)(A_shared + (i + A_BLOCK_ROW_START) * BLOCK_SIZE_K + A_BLOCK_COL_START) = \n\t\t\t\t*(float4 *)(A_global_ptr + (i + A_BLOCK_ROW_START) * K + A_BLOCK_COL_START);\n\t\t}\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_K_SPARSE; i += B_STRIDES){\n\t\t\t*(float4 *)(B_shared + (i + B_BLOCK_ROW_START) * BLOCK_SIZE_N + B_BLOCK_COL_START) =\n\t\t\t\t*(float4 *)(B_global_ptr + (i + B_BLOCK_ROW_START) * N + B_BLOCK_COL_START);\n\n\t\t\t*(float4 *)(B_index_shared + (i + B_BLOCK_ROW_START) * BLOCK_SIZE_N + B_BLOCK_COL_START) =\n\t\t\t\t*(float4 *)(B_index_global_ptr + (i + B_BLOCK_ROW_START) * N + B_BLOCK_COL_START);\n\t\t}\n\n\t\t__syncthreads();\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_K_SPARSE;i += 1){\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_N; k += 1){\n\t\t\t\tB_reg[k] = B_shared[i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k];\n\t\t\t\t//*(float4 *)(B_reg + k) = *(float4 *)(B_shared + i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k);\n\t\t\t\t//*(float4 *)(B_reg_index + k) = *(float4 *)(B_index_shared + i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k);\n\t\t\t}\n\t\t\tint bank_idx = i / LEN_OF_BANK_PER_SPARSE_BLOCK;\n\t\t\tB_reg_index = B_index_shared[i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N] % BANK_VAL+bank_idx * BANK_VAL;\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_M; k += 1){\n\t\t\t\tA_reg[k] = A_shared[(t_M * THREAD_SIZE_M+k) * BLOCK_SIZE_K + B_reg_index];\n\t\t\t}\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_N; k += 1){\n\t\t\t\t#pragma unroll\n\t\t\t\tfor(int j = 0; j < THREAD_SIZE_M; j += 1){\n\t\t\t\t\tC_reg[j][k] += B_reg[k] * A_reg[j];\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n    float bias_local[THREAD_SIZE_N];\n    for(int thread_x = 0; thread_x < THREAD_SIZE_N; thread_x++){\n        bias_local[thread_x] = bias[BLOCK_SIZE_N * blockIdx.y + THREAD_SIZE_N * t_N + thread_x];\n    }\n\n\t#pragma unroll\n\tfor(int i = 0; i < THREAD_SIZE_M; i += 1){\n\t\t#pragma unroll\n\t\tfor(int j = 0; j < THREAD_SIZE_N; j += 1){\n\t\t\tg_data[(BLOCK_SIZE_M * blockIdx.x + THREAD_SIZE_M * t_M + i) * N + BLOCK_SIZE_N * blockIdx.y + THREAD_SIZE_N * t_N + j] = C_reg[i][j] + bias_local[j];\n\t\t}\n\t}\n}", "launch_config": {"dimBlock": [128, 1], "dimGrid": [64, 6]}}, "bert.encoder.layer.11.attention.self.value": {"code": "__global__ void MatMul_TILE_THREAD_GENERAL(float *input0, float *input1, float *input2, float *input3, float *output0) {\n    /*\n    g_vec: A [M, K]\n    g_mat_data: condense B [K_sparse, N]\n    g_mat_index: condense B index [K_sparse, N]\n    g_data: C [M, N]\n    */\n\tfloat *g_vec = input0;\n\tfloat *g_mat_data = input1;\n\tint *g_mat_index = (int*)input2;\n\tfloat *bias = input3;\n\tfloat *g_data = output0;\n\n\tconst int M = 4096;\n\tconst int K = 768;\n\tconst int N = 768;\n\n    const int BLOCK_SIZE_M = 64;\n    const int BLOCK_SIZE_N = 128;\n    const int BLOCK_SIZE_K = 32;\n    const int THREAD_SIZE_M = 8;\n    const int THREAD_SIZE_N = 8;\n\n    const float SPARSITY = 0.875;\n    const int BANK_VAL = 32;\n    const int ALIGN_N = THREAD_SIZE_N;\n\n    const int BANK_NUM_PER_BLOCK = BLOCK_SIZE_K / BANK_VAL;\n    const int BLOCK_SIZE_K_SPARSE = int(BLOCK_SIZE_K * (1-SPARSITY));\n    const int LEN_OF_BANK_PER_SPARSE_BLOCK = BLOCK_SIZE_K_SPARSE / BANK_NUM_PER_BLOCK;\n\n\tconst int K_SPARSE = int(K * SPARSITY);\n\tint M_BLOCK_START = blockIdx.x * BLOCK_SIZE_M;\n\tint N_BLOCK_START = blockIdx.y * BLOCK_SIZE_N;\n\n\tconst int A_THREADS_PER_ROW = BLOCK_SIZE_K / 4;\n\tconst int B_THREADS_PER_ROW = BLOCK_SIZE_N / 4;\n\n\tconst int THREADS_PER_BLOCK = (BLOCK_SIZE_M / THREAD_SIZE_M) * (BLOCK_SIZE_N / THREAD_SIZE_N);\n\n\tconst int A_STRIDES = THREADS_PER_BLOCK / A_THREADS_PER_ROW;\n\tconst int B_STRIDES = THREADS_PER_BLOCK / B_THREADS_PER_ROW;\n\n\t__shared__ float A_shared[BLOCK_SIZE_M * BLOCK_SIZE_K];\n\t__shared__ float B_shared[BLOCK_SIZE_N * BLOCK_SIZE_K_SPARSE];\n\t__shared__ int B_index_shared[BLOCK_SIZE_N * BLOCK_SIZE_K_SPARSE];\n\n\tfloat A_reg[THREAD_SIZE_M];\n\tfloat B_reg[THREAD_SIZE_N];\n\tint B_reg_index;\n\tfloat C_reg[THREAD_SIZE_M][THREAD_SIZE_N] = {0};\n\n\tint tid = threadIdx.x;\n\n\tint t_N = tid % (BLOCK_SIZE_N / THREAD_SIZE_N);\n\tint t_M = tid / (BLOCK_SIZE_N / THREAD_SIZE_N);\n\n\tint A_BLOCK_ROW_START = tid / A_THREADS_PER_ROW;\n\tint B_BLOCK_ROW_START = tid / B_THREADS_PER_ROW;\n\n\tint A_BLOCK_COL_START = tid % A_THREADS_PER_ROW * 4;\n\tint B_BLOCK_COL_START = tid % B_THREADS_PER_ROW * 4;\n\n\tfor(int K_BLOCK_START = 0, K_SPARSE_BLOCK_START = 0; K_BLOCK_START < K; K_BLOCK_START += BLOCK_SIZE_K, K_SPARSE_BLOCK_START += BLOCK_SIZE_K_SPARSE){\n\t\tfloat *A_global_ptr = g_vec + M_BLOCK_START * K + K_BLOCK_START;\n\t\tfloat *B_global_ptr = g_mat_data + K_SPARSE_BLOCK_START * N + N_BLOCK_START;\n\t\tint *B_index_global_ptr = g_mat_index + K_SPARSE_BLOCK_START * N + N_BLOCK_START;\n\n\t\t__syncthreads();\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_M; i += A_STRIDES){\n\t\t\t*(float4 *)(A_shared + (i + A_BLOCK_ROW_START) * BLOCK_SIZE_K + A_BLOCK_COL_START) = \n\t\t\t\t*(float4 *)(A_global_ptr + (i + A_BLOCK_ROW_START) * K + A_BLOCK_COL_START);\n\t\t}\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_K_SPARSE; i += B_STRIDES){\n\t\t\t*(float4 *)(B_shared + (i + B_BLOCK_ROW_START) * BLOCK_SIZE_N + B_BLOCK_COL_START) =\n\t\t\t\t*(float4 *)(B_global_ptr + (i + B_BLOCK_ROW_START) * N + B_BLOCK_COL_START);\n\n\t\t\t*(float4 *)(B_index_shared + (i + B_BLOCK_ROW_START) * BLOCK_SIZE_N + B_BLOCK_COL_START) =\n\t\t\t\t*(float4 *)(B_index_global_ptr + (i + B_BLOCK_ROW_START) * N + B_BLOCK_COL_START);\n\t\t}\n\n\t\t__syncthreads();\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_K_SPARSE;i += 1){\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_N; k += 1){\n\t\t\t\tB_reg[k] = B_shared[i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k];\n\t\t\t\t//*(float4 *)(B_reg + k) = *(float4 *)(B_shared + i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k);\n\t\t\t\t//*(float4 *)(B_reg_index + k) = *(float4 *)(B_index_shared + i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k);\n\t\t\t}\n\t\t\tint bank_idx = i / LEN_OF_BANK_PER_SPARSE_BLOCK;\n\t\t\tB_reg_index = B_index_shared[i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N] % BANK_VAL+bank_idx * BANK_VAL;\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_M; k += 1){\n\t\t\t\tA_reg[k] = A_shared[(t_M * THREAD_SIZE_M+k) * BLOCK_SIZE_K + B_reg_index];\n\t\t\t}\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_N; k += 1){\n\t\t\t\t#pragma unroll\n\t\t\t\tfor(int j = 0; j < THREAD_SIZE_M; j += 1){\n\t\t\t\t\tC_reg[j][k] += B_reg[k] * A_reg[j];\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n    float bias_local[THREAD_SIZE_N];\n    for(int thread_x = 0; thread_x < THREAD_SIZE_N; thread_x++){\n        bias_local[thread_x] = bias[BLOCK_SIZE_N * blockIdx.y + THREAD_SIZE_N * t_N + thread_x];\n    }\n\n\t#pragma unroll\n\tfor(int i = 0; i < THREAD_SIZE_M; i += 1){\n\t\t#pragma unroll\n\t\tfor(int j = 0; j < THREAD_SIZE_N; j += 1){\n\t\t\tg_data[(BLOCK_SIZE_M * blockIdx.x + THREAD_SIZE_M * t_M + i) * N + BLOCK_SIZE_N * blockIdx.y + THREAD_SIZE_N * t_N + j] = C_reg[i][j] + bias_local[j];\n\t\t}\n\t}\n}", "launch_config": {"dimBlock": [128, 1], "dimGrid": [64, 6]}}, "bert.encoder.layer.11.attention.output.dense": {"code": "__global__ void MatMul_TILE_THREAD_GENERAL(float *input0, float *input1, float *input2, float *input3, float *output0) {\n    /*\n    g_vec: A [M, K]\n    g_mat_data: condense B [K_sparse, N]\n    g_mat_index: condense B index [K_sparse, N]\n    g_data: C [M, N]\n    */\n\tfloat *g_vec = input0;\n\tfloat *g_mat_data = input1;\n\tint *g_mat_index = (int*)input2;\n\tfloat *bias = input3;\n\tfloat *g_data = output0;\n\n\tconst int M = 4096;\n\tconst int K = 768;\n\tconst int N = 768;\n\n    const int BLOCK_SIZE_M = 64;\n    const int BLOCK_SIZE_N = 128;\n    const int BLOCK_SIZE_K = 32;\n    const int THREAD_SIZE_M = 8;\n    const int THREAD_SIZE_N = 8;\n\n    const float SPARSITY = 0.875;\n    const int BANK_VAL = 32;\n    const int ALIGN_N = THREAD_SIZE_N;\n\n    const int BANK_NUM_PER_BLOCK = BLOCK_SIZE_K / BANK_VAL;\n    const int BLOCK_SIZE_K_SPARSE = int(BLOCK_SIZE_K * (1-SPARSITY));\n    const int LEN_OF_BANK_PER_SPARSE_BLOCK = BLOCK_SIZE_K_SPARSE / BANK_NUM_PER_BLOCK;\n\n\tconst int K_SPARSE = int(K * SPARSITY);\n\tint M_BLOCK_START = blockIdx.x * BLOCK_SIZE_M;\n\tint N_BLOCK_START = blockIdx.y * BLOCK_SIZE_N;\n\n\tconst int A_THREADS_PER_ROW = BLOCK_SIZE_K / 4;\n\tconst int B_THREADS_PER_ROW = BLOCK_SIZE_N / 4;\n\n\tconst int THREADS_PER_BLOCK = (BLOCK_SIZE_M / THREAD_SIZE_M) * (BLOCK_SIZE_N / THREAD_SIZE_N);\n\n\tconst int A_STRIDES = THREADS_PER_BLOCK / A_THREADS_PER_ROW;\n\tconst int B_STRIDES = THREADS_PER_BLOCK / B_THREADS_PER_ROW;\n\n\t__shared__ float A_shared[BLOCK_SIZE_M * BLOCK_SIZE_K];\n\t__shared__ float B_shared[BLOCK_SIZE_N * BLOCK_SIZE_K_SPARSE];\n\t__shared__ int B_index_shared[BLOCK_SIZE_N * BLOCK_SIZE_K_SPARSE];\n\n\tfloat A_reg[THREAD_SIZE_M];\n\tfloat B_reg[THREAD_SIZE_N];\n\tint B_reg_index;\n\tfloat C_reg[THREAD_SIZE_M][THREAD_SIZE_N] = {0};\n\n\tint tid = threadIdx.x;\n\n\tint t_N = tid % (BLOCK_SIZE_N / THREAD_SIZE_N);\n\tint t_M = tid / (BLOCK_SIZE_N / THREAD_SIZE_N);\n\n\tint A_BLOCK_ROW_START = tid / A_THREADS_PER_ROW;\n\tint B_BLOCK_ROW_START = tid / B_THREADS_PER_ROW;\n\n\tint A_BLOCK_COL_START = tid % A_THREADS_PER_ROW * 4;\n\tint B_BLOCK_COL_START = tid % B_THREADS_PER_ROW * 4;\n\n\tfor(int K_BLOCK_START = 0, K_SPARSE_BLOCK_START = 0; K_BLOCK_START < K; K_BLOCK_START += BLOCK_SIZE_K, K_SPARSE_BLOCK_START += BLOCK_SIZE_K_SPARSE){\n\t\tfloat *A_global_ptr = g_vec + M_BLOCK_START * K + K_BLOCK_START;\n\t\tfloat *B_global_ptr = g_mat_data + K_SPARSE_BLOCK_START * N + N_BLOCK_START;\n\t\tint *B_index_global_ptr = g_mat_index + K_SPARSE_BLOCK_START * N + N_BLOCK_START;\n\n\t\t__syncthreads();\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_M; i += A_STRIDES){\n\t\t\t*(float4 *)(A_shared + (i + A_BLOCK_ROW_START) * BLOCK_SIZE_K + A_BLOCK_COL_START) = \n\t\t\t\t*(float4 *)(A_global_ptr + (i + A_BLOCK_ROW_START) * K + A_BLOCK_COL_START);\n\t\t}\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_K_SPARSE; i += B_STRIDES){\n\t\t\t*(float4 *)(B_shared + (i + B_BLOCK_ROW_START) * BLOCK_SIZE_N + B_BLOCK_COL_START) =\n\t\t\t\t*(float4 *)(B_global_ptr + (i + B_BLOCK_ROW_START) * N + B_BLOCK_COL_START);\n\n\t\t\t*(float4 *)(B_index_shared + (i + B_BLOCK_ROW_START) * BLOCK_SIZE_N + B_BLOCK_COL_START) =\n\t\t\t\t*(float4 *)(B_index_global_ptr + (i + B_BLOCK_ROW_START) * N + B_BLOCK_COL_START);\n\t\t}\n\n\t\t__syncthreads();\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_K_SPARSE;i += 1){\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_N; k += 1){\n\t\t\t\tB_reg[k] = B_shared[i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k];\n\t\t\t\t//*(float4 *)(B_reg + k) = *(float4 *)(B_shared + i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k);\n\t\t\t\t//*(float4 *)(B_reg_index + k) = *(float4 *)(B_index_shared + i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k);\n\t\t\t}\n\t\t\tint bank_idx = i / LEN_OF_BANK_PER_SPARSE_BLOCK;\n\t\t\tB_reg_index = B_index_shared[i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N] % BANK_VAL+bank_idx * BANK_VAL;\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_M; k += 1){\n\t\t\t\tA_reg[k] = A_shared[(t_M * THREAD_SIZE_M+k) * BLOCK_SIZE_K + B_reg_index];\n\t\t\t}\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_N; k += 1){\n\t\t\t\t#pragma unroll\n\t\t\t\tfor(int j = 0; j < THREAD_SIZE_M; j += 1){\n\t\t\t\t\tC_reg[j][k] += B_reg[k] * A_reg[j];\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n    float bias_local[THREAD_SIZE_N];\n    for(int thread_x = 0; thread_x < THREAD_SIZE_N; thread_x++){\n        bias_local[thread_x] = bias[BLOCK_SIZE_N * blockIdx.y + THREAD_SIZE_N * t_N + thread_x];\n    }\n\n\t#pragma unroll\n\tfor(int i = 0; i < THREAD_SIZE_M; i += 1){\n\t\t#pragma unroll\n\t\tfor(int j = 0; j < THREAD_SIZE_N; j += 1){\n\t\t\tg_data[(BLOCK_SIZE_M * blockIdx.x + THREAD_SIZE_M * t_M + i) * N + BLOCK_SIZE_N * blockIdx.y + THREAD_SIZE_N * t_N + j] = C_reg[i][j] + bias_local[j];\n\t\t}\n\t}\n}", "launch_config": {"dimBlock": [128, 1], "dimGrid": [64, 6]}}, "bert.encoder.layer.11.intermediate.dense": {"code": "__global__ void MatMul_TILE_THREAD_GENERAL(float *input0, float *input1, float *input2, float *input3, float *output0) {\n    /*\n    g_vec: A [M, K]\n    g_mat_data: condense B [K_sparse, N]\n    g_mat_index: condense B index [K_sparse, N]\n    g_data: C [M, N]\n    */\n\tfloat *g_vec = input0;\n\tfloat *g_mat_data = input1;\n\tint *g_mat_index = (int*)input2;\n\tfloat *bias = input3;\n\tfloat *g_data = output0;\n\n\tconst int M = 4096;\n\tconst int K = 768;\n\tconst int N = 3072;\n\n    const int BLOCK_SIZE_M = 64;\n    const int BLOCK_SIZE_N = 128;\n    const int BLOCK_SIZE_K = 32;\n    const int THREAD_SIZE_M = 8;\n    const int THREAD_SIZE_N = 8;\n\n    const float SPARSITY = 0.875;\n    const int BANK_VAL = 32;\n    const int ALIGN_N = THREAD_SIZE_N;\n\n    const int BANK_NUM_PER_BLOCK = BLOCK_SIZE_K / BANK_VAL;\n    const int BLOCK_SIZE_K_SPARSE = int(BLOCK_SIZE_K * (1-SPARSITY));\n    const int LEN_OF_BANK_PER_SPARSE_BLOCK = BLOCK_SIZE_K_SPARSE / BANK_NUM_PER_BLOCK;\n\n\tconst int K_SPARSE = int(K * SPARSITY);\n\tint M_BLOCK_START = blockIdx.x * BLOCK_SIZE_M;\n\tint N_BLOCK_START = blockIdx.y * BLOCK_SIZE_N;\n\n\tconst int A_THREADS_PER_ROW = BLOCK_SIZE_K / 4;\n\tconst int B_THREADS_PER_ROW = BLOCK_SIZE_N / 4;\n\n\tconst int THREADS_PER_BLOCK = (BLOCK_SIZE_M / THREAD_SIZE_M) * (BLOCK_SIZE_N / THREAD_SIZE_N);\n\n\tconst int A_STRIDES = THREADS_PER_BLOCK / A_THREADS_PER_ROW;\n\tconst int B_STRIDES = THREADS_PER_BLOCK / B_THREADS_PER_ROW;\n\n\t__shared__ float A_shared[BLOCK_SIZE_M * BLOCK_SIZE_K];\n\t__shared__ float B_shared[BLOCK_SIZE_N * BLOCK_SIZE_K_SPARSE];\n\t__shared__ int B_index_shared[BLOCK_SIZE_N * BLOCK_SIZE_K_SPARSE];\n\n\tfloat A_reg[THREAD_SIZE_M];\n\tfloat B_reg[THREAD_SIZE_N];\n\tint B_reg_index;\n\tfloat C_reg[THREAD_SIZE_M][THREAD_SIZE_N] = {0};\n\n\tint tid = threadIdx.x;\n\n\tint t_N = tid % (BLOCK_SIZE_N / THREAD_SIZE_N);\n\tint t_M = tid / (BLOCK_SIZE_N / THREAD_SIZE_N);\n\n\tint A_BLOCK_ROW_START = tid / A_THREADS_PER_ROW;\n\tint B_BLOCK_ROW_START = tid / B_THREADS_PER_ROW;\n\n\tint A_BLOCK_COL_START = tid % A_THREADS_PER_ROW * 4;\n\tint B_BLOCK_COL_START = tid % B_THREADS_PER_ROW * 4;\n\n\tfor(int K_BLOCK_START = 0, K_SPARSE_BLOCK_START = 0; K_BLOCK_START < K; K_BLOCK_START += BLOCK_SIZE_K, K_SPARSE_BLOCK_START += BLOCK_SIZE_K_SPARSE){\n\t\tfloat *A_global_ptr = g_vec + M_BLOCK_START * K + K_BLOCK_START;\n\t\tfloat *B_global_ptr = g_mat_data + K_SPARSE_BLOCK_START * N + N_BLOCK_START;\n\t\tint *B_index_global_ptr = g_mat_index + K_SPARSE_BLOCK_START * N + N_BLOCK_START;\n\n\t\t__syncthreads();\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_M; i += A_STRIDES){\n\t\t\t*(float4 *)(A_shared + (i + A_BLOCK_ROW_START) * BLOCK_SIZE_K + A_BLOCK_COL_START) = \n\t\t\t\t*(float4 *)(A_global_ptr + (i + A_BLOCK_ROW_START) * K + A_BLOCK_COL_START);\n\t\t}\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_K_SPARSE; i += B_STRIDES){\n\t\t\t*(float4 *)(B_shared + (i + B_BLOCK_ROW_START) * BLOCK_SIZE_N + B_BLOCK_COL_START) =\n\t\t\t\t*(float4 *)(B_global_ptr + (i + B_BLOCK_ROW_START) * N + B_BLOCK_COL_START);\n\n\t\t\t*(float4 *)(B_index_shared + (i + B_BLOCK_ROW_START) * BLOCK_SIZE_N + B_BLOCK_COL_START) =\n\t\t\t\t*(float4 *)(B_index_global_ptr + (i + B_BLOCK_ROW_START) * N + B_BLOCK_COL_START);\n\t\t}\n\n\t\t__syncthreads();\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_K_SPARSE;i += 1){\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_N; k += 1){\n\t\t\t\tB_reg[k] = B_shared[i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k];\n\t\t\t\t//*(float4 *)(B_reg + k) = *(float4 *)(B_shared + i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k);\n\t\t\t\t//*(float4 *)(B_reg_index + k) = *(float4 *)(B_index_shared + i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k);\n\t\t\t}\n\t\t\tint bank_idx = i / LEN_OF_BANK_PER_SPARSE_BLOCK;\n\t\t\tB_reg_index = B_index_shared[i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N] % BANK_VAL+bank_idx * BANK_VAL;\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_M; k += 1){\n\t\t\t\tA_reg[k] = A_shared[(t_M * THREAD_SIZE_M+k) * BLOCK_SIZE_K + B_reg_index];\n\t\t\t}\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_N; k += 1){\n\t\t\t\t#pragma unroll\n\t\t\t\tfor(int j = 0; j < THREAD_SIZE_M; j += 1){\n\t\t\t\t\tC_reg[j][k] += B_reg[k] * A_reg[j];\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n    float bias_local[THREAD_SIZE_N];\n    for(int thread_x = 0; thread_x < THREAD_SIZE_N; thread_x++){\n        bias_local[thread_x] = bias[BLOCK_SIZE_N * blockIdx.y + THREAD_SIZE_N * t_N + thread_x];\n    }\n\n\t#pragma unroll\n\tfor(int i = 0; i < THREAD_SIZE_M; i += 1){\n\t\t#pragma unroll\n\t\tfor(int j = 0; j < THREAD_SIZE_N; j += 1){\n\t\t\tg_data[(BLOCK_SIZE_M * blockIdx.x + THREAD_SIZE_M * t_M + i) * N + BLOCK_SIZE_N * blockIdx.y + THREAD_SIZE_N * t_N + j] = C_reg[i][j] + bias_local[j];\n\t\t}\n\t}\n}", "launch_config": {"dimBlock": [128, 1], "dimGrid": [64, 24]}}, "bert.encoder.layer.11.output.dense": {"code": "__global__ void MatMul_TILE_THREAD_GENERAL(float *input0, float *input1, float *input2, float *input3, float *output0) {\n    /*\n    g_vec: A [M, K]\n    g_mat_data: condense B [K_sparse, N]\n    g_mat_index: condense B index [K_sparse, N]\n    g_data: C [M, N]\n    */\n\tfloat *g_vec = input0;\n\tfloat *g_mat_data = input1;\n\tint *g_mat_index = (int*)input2;\n\tfloat *bias = input3;\n\tfloat *g_data = output0;\n\n\tconst int M = 4096;\n\tconst int K = 3072;\n\tconst int N = 768;\n\n    const int BLOCK_SIZE_M = 64;\n    const int BLOCK_SIZE_N = 128;\n    const int BLOCK_SIZE_K = 32;\n    const int THREAD_SIZE_M = 8;\n    const int THREAD_SIZE_N = 8;\n\n    const float SPARSITY = 0.875;\n    const int BANK_VAL = 32;\n    const int ALIGN_N = THREAD_SIZE_N;\n\n    const int BANK_NUM_PER_BLOCK = BLOCK_SIZE_K / BANK_VAL;\n    const int BLOCK_SIZE_K_SPARSE = int(BLOCK_SIZE_K * (1-SPARSITY));\n    const int LEN_OF_BANK_PER_SPARSE_BLOCK = BLOCK_SIZE_K_SPARSE / BANK_NUM_PER_BLOCK;\n\n\tconst int K_SPARSE = int(K * SPARSITY);\n\tint M_BLOCK_START = blockIdx.x * BLOCK_SIZE_M;\n\tint N_BLOCK_START = blockIdx.y * BLOCK_SIZE_N;\n\n\tconst int A_THREADS_PER_ROW = BLOCK_SIZE_K / 4;\n\tconst int B_THREADS_PER_ROW = BLOCK_SIZE_N / 4;\n\n\tconst int THREADS_PER_BLOCK = (BLOCK_SIZE_M / THREAD_SIZE_M) * (BLOCK_SIZE_N / THREAD_SIZE_N);\n\n\tconst int A_STRIDES = THREADS_PER_BLOCK / A_THREADS_PER_ROW;\n\tconst int B_STRIDES = THREADS_PER_BLOCK / B_THREADS_PER_ROW;\n\n\t__shared__ float A_shared[BLOCK_SIZE_M * BLOCK_SIZE_K];\n\t__shared__ float B_shared[BLOCK_SIZE_N * BLOCK_SIZE_K_SPARSE];\n\t__shared__ int B_index_shared[BLOCK_SIZE_N * BLOCK_SIZE_K_SPARSE];\n\n\tfloat A_reg[THREAD_SIZE_M];\n\tfloat B_reg[THREAD_SIZE_N];\n\tint B_reg_index;\n\tfloat C_reg[THREAD_SIZE_M][THREAD_SIZE_N] = {0};\n\n\tint tid = threadIdx.x;\n\n\tint t_N = tid % (BLOCK_SIZE_N / THREAD_SIZE_N);\n\tint t_M = tid / (BLOCK_SIZE_N / THREAD_SIZE_N);\n\n\tint A_BLOCK_ROW_START = tid / A_THREADS_PER_ROW;\n\tint B_BLOCK_ROW_START = tid / B_THREADS_PER_ROW;\n\n\tint A_BLOCK_COL_START = tid % A_THREADS_PER_ROW * 4;\n\tint B_BLOCK_COL_START = tid % B_THREADS_PER_ROW * 4;\n\n\tfor(int K_BLOCK_START = 0, K_SPARSE_BLOCK_START = 0; K_BLOCK_START < K; K_BLOCK_START += BLOCK_SIZE_K, K_SPARSE_BLOCK_START += BLOCK_SIZE_K_SPARSE){\n\t\tfloat *A_global_ptr = g_vec + M_BLOCK_START * K + K_BLOCK_START;\n\t\tfloat *B_global_ptr = g_mat_data + K_SPARSE_BLOCK_START * N + N_BLOCK_START;\n\t\tint *B_index_global_ptr = g_mat_index + K_SPARSE_BLOCK_START * N + N_BLOCK_START;\n\n\t\t__syncthreads();\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_M; i += A_STRIDES){\n\t\t\t*(float4 *)(A_shared + (i + A_BLOCK_ROW_START) * BLOCK_SIZE_K + A_BLOCK_COL_START) = \n\t\t\t\t*(float4 *)(A_global_ptr + (i + A_BLOCK_ROW_START) * K + A_BLOCK_COL_START);\n\t\t}\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_K_SPARSE; i += B_STRIDES){\n\t\t\t*(float4 *)(B_shared + (i + B_BLOCK_ROW_START) * BLOCK_SIZE_N + B_BLOCK_COL_START) =\n\t\t\t\t*(float4 *)(B_global_ptr + (i + B_BLOCK_ROW_START) * N + B_BLOCK_COL_START);\n\n\t\t\t*(float4 *)(B_index_shared + (i + B_BLOCK_ROW_START) * BLOCK_SIZE_N + B_BLOCK_COL_START) =\n\t\t\t\t*(float4 *)(B_index_global_ptr + (i + B_BLOCK_ROW_START) * N + B_BLOCK_COL_START);\n\t\t}\n\n\t\t__syncthreads();\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_K_SPARSE;i += 1){\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_N; k += 1){\n\t\t\t\tB_reg[k] = B_shared[i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k];\n\t\t\t\t//*(float4 *)(B_reg + k) = *(float4 *)(B_shared + i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k);\n\t\t\t\t//*(float4 *)(B_reg_index + k) = *(float4 *)(B_index_shared + i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k);\n\t\t\t}\n\t\t\tint bank_idx = i / LEN_OF_BANK_PER_SPARSE_BLOCK;\n\t\t\tB_reg_index = B_index_shared[i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N] % BANK_VAL+bank_idx * BANK_VAL;\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_M; k += 1){\n\t\t\t\tA_reg[k] = A_shared[(t_M * THREAD_SIZE_M+k) * BLOCK_SIZE_K + B_reg_index];\n\t\t\t}\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_N; k += 1){\n\t\t\t\t#pragma unroll\n\t\t\t\tfor(int j = 0; j < THREAD_SIZE_M; j += 1){\n\t\t\t\t\tC_reg[j][k] += B_reg[k] * A_reg[j];\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n    float bias_local[THREAD_SIZE_N];\n    for(int thread_x = 0; thread_x < THREAD_SIZE_N; thread_x++){\n        bias_local[thread_x] = bias[BLOCK_SIZE_N * blockIdx.y + THREAD_SIZE_N * t_N + thread_x];\n    }\n\n\t#pragma unroll\n\tfor(int i = 0; i < THREAD_SIZE_M; i += 1){\n\t\t#pragma unroll\n\t\tfor(int j = 0; j < THREAD_SIZE_N; j += 1){\n\t\t\tg_data[(BLOCK_SIZE_M * blockIdx.x + THREAD_SIZE_M * t_M + i) * N + BLOCK_SIZE_N * blockIdx.y + THREAD_SIZE_N * t_N + j] = C_reg[i][j] + bias_local[j];\n\t\t}\n\t}\n}", "launch_config": {"dimBlock": [128, 1], "dimGrid": [64, 6]}}, "bert.pooler.dense": {"code": "__global__ void MatMul_TILE_THREAD_GENERAL(float *input0, float *input1, float *input2, float *input3, float *output0) {\n    /*\n    g_vec: A [M, K]\n    g_mat_data: condense B [K_sparse, N]\n    g_mat_index: condense B index [K_sparse, N]\n    g_data: C [M, N]\n    */\n\tfloat *g_vec = input0;\n\tfloat *g_mat_data = input1;\n\tint *g_mat_index = (int*)input2;\n\tfloat *bias = input3;\n\tfloat *g_data = output0;\n\n\tconst int M = 24576;\n\tconst int K = torch.float32;\n\tconst int N = 768;\n\n    const int BLOCK_SIZE_M = 64;\n    const int BLOCK_SIZE_N = 128;\n    const int BLOCK_SIZE_K = 32;\n    const int THREAD_SIZE_M = 8;\n    const int THREAD_SIZE_N = 8;\n\n    const float SPARSITY = 0.875;\n    const int BANK_VAL = 32;\n    const int ALIGN_N = THREAD_SIZE_N;\n\n    const int BANK_NUM_PER_BLOCK = BLOCK_SIZE_K / BANK_VAL;\n    const int BLOCK_SIZE_K_SPARSE = int(BLOCK_SIZE_K * (1-SPARSITY));\n    const int LEN_OF_BANK_PER_SPARSE_BLOCK = BLOCK_SIZE_K_SPARSE / BANK_NUM_PER_BLOCK;\n\n\tconst int K_SPARSE = int(K * SPARSITY);\n\tint M_BLOCK_START = blockIdx.x * BLOCK_SIZE_M;\n\tint N_BLOCK_START = blockIdx.y * BLOCK_SIZE_N;\n\n\tconst int A_THREADS_PER_ROW = BLOCK_SIZE_K / 4;\n\tconst int B_THREADS_PER_ROW = BLOCK_SIZE_N / 4;\n\n\tconst int THREADS_PER_BLOCK = (BLOCK_SIZE_M / THREAD_SIZE_M) * (BLOCK_SIZE_N / THREAD_SIZE_N);\n\n\tconst int A_STRIDES = THREADS_PER_BLOCK / A_THREADS_PER_ROW;\n\tconst int B_STRIDES = THREADS_PER_BLOCK / B_THREADS_PER_ROW;\n\n\t__shared__ float A_shared[BLOCK_SIZE_M * BLOCK_SIZE_K];\n\t__shared__ float B_shared[BLOCK_SIZE_N * BLOCK_SIZE_K_SPARSE];\n\t__shared__ int B_index_shared[BLOCK_SIZE_N * BLOCK_SIZE_K_SPARSE];\n\n\tfloat A_reg[THREAD_SIZE_M];\n\tfloat B_reg[THREAD_SIZE_N];\n\tint B_reg_index;\n\tfloat C_reg[THREAD_SIZE_M][THREAD_SIZE_N] = {0};\n\n\tint tid = threadIdx.x;\n\n\tint t_N = tid % (BLOCK_SIZE_N / THREAD_SIZE_N);\n\tint t_M = tid / (BLOCK_SIZE_N / THREAD_SIZE_N);\n\n\tint A_BLOCK_ROW_START = tid / A_THREADS_PER_ROW;\n\tint B_BLOCK_ROW_START = tid / B_THREADS_PER_ROW;\n\n\tint A_BLOCK_COL_START = tid % A_THREADS_PER_ROW * 4;\n\tint B_BLOCK_COL_START = tid % B_THREADS_PER_ROW * 4;\n\n\tfor(int K_BLOCK_START = 0, K_SPARSE_BLOCK_START = 0; K_BLOCK_START < K; K_BLOCK_START += BLOCK_SIZE_K, K_SPARSE_BLOCK_START += BLOCK_SIZE_K_SPARSE){\n\t\tfloat *A_global_ptr = g_vec + M_BLOCK_START * K + K_BLOCK_START;\n\t\tfloat *B_global_ptr = g_mat_data + K_SPARSE_BLOCK_START * N + N_BLOCK_START;\n\t\tint *B_index_global_ptr = g_mat_index + K_SPARSE_BLOCK_START * N + N_BLOCK_START;\n\n\t\t__syncthreads();\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_M; i += A_STRIDES){\n\t\t\t*(float4 *)(A_shared + (i + A_BLOCK_ROW_START) * BLOCK_SIZE_K + A_BLOCK_COL_START) = \n\t\t\t\t*(float4 *)(A_global_ptr + (i + A_BLOCK_ROW_START) * K + A_BLOCK_COL_START);\n\t\t}\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_K_SPARSE; i += B_STRIDES){\n\t\t\t*(float4 *)(B_shared + (i + B_BLOCK_ROW_START) * BLOCK_SIZE_N + B_BLOCK_COL_START) =\n\t\t\t\t*(float4 *)(B_global_ptr + (i + B_BLOCK_ROW_START) * N + B_BLOCK_COL_START);\n\n\t\t\t*(float4 *)(B_index_shared + (i + B_BLOCK_ROW_START) * BLOCK_SIZE_N + B_BLOCK_COL_START) =\n\t\t\t\t*(float4 *)(B_index_global_ptr + (i + B_BLOCK_ROW_START) * N + B_BLOCK_COL_START);\n\t\t}\n\n\t\t__syncthreads();\n\n\t\t#pragma unroll\n\t\tfor(int i = 0; i < BLOCK_SIZE_K_SPARSE;i += 1){\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_N; k += 1){\n\t\t\t\tB_reg[k] = B_shared[i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k];\n\t\t\t\t//*(float4 *)(B_reg + k) = *(float4 *)(B_shared + i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k);\n\t\t\t\t//*(float4 *)(B_reg_index + k) = *(float4 *)(B_index_shared + i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N + k);\n\t\t\t}\n\t\t\tint bank_idx = i / LEN_OF_BANK_PER_SPARSE_BLOCK;\n\t\t\tB_reg_index = B_index_shared[i * BLOCK_SIZE_N + t_N * THREAD_SIZE_N] % BANK_VAL+bank_idx * BANK_VAL;\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_M; k += 1){\n\t\t\t\tA_reg[k] = A_shared[(t_M * THREAD_SIZE_M+k) * BLOCK_SIZE_K + B_reg_index];\n\t\t\t}\n\t\t\t#pragma unroll\n\t\t\tfor(int k = 0; k < THREAD_SIZE_N; k += 1){\n\t\t\t\t#pragma unroll\n\t\t\t\tfor(int j = 0; j < THREAD_SIZE_M; j += 1){\n\t\t\t\t\tC_reg[j][k] += B_reg[k] * A_reg[j];\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n    float bias_local[THREAD_SIZE_N];\n    for(int thread_x = 0; thread_x < THREAD_SIZE_N; thread_x++){\n        bias_local[thread_x] = bias[BLOCK_SIZE_N * blockIdx.y + THREAD_SIZE_N * t_N + thread_x];\n    }\n\n\t#pragma unroll\n\tfor(int i = 0; i < THREAD_SIZE_M; i += 1){\n\t\t#pragma unroll\n\t\tfor(int j = 0; j < THREAD_SIZE_N; j += 1){\n\t\t\tg_data[(BLOCK_SIZE_M * blockIdx.x + THREAD_SIZE_M * t_M + i) * N + BLOCK_SIZE_N * blockIdx.y + THREAD_SIZE_N * t_N + j] = C_reg[i][j] + bias_local[j];\n\t\t}\n\t}\n}", "launch_config": {"dimBlock": [128, 1], "dimGrid": [384, 6]}}}