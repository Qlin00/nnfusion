{"1": {"code": "\n__global__ void compute_gemm_imma(float *input0, float *input1, float *input2, float* input3, float* input4, float*input5, float* input6,\n                                  float *output0) {\n    // keys need to be replaced 4096, 1024, K_GLOBA_VALUEL, 0.75, 64\n\n\t//extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B = reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_index = reinterpret_cast<int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n\tconst int WARP_SIZE = 32;\n    const int M = 16;\n    const int K = 16;\n    const int N = 16;\n    const int WMMA_M = 16;\n    const int WMMA_K = 16;\n    const int WMMA_N = 16;\n    const int M_GLOBAL = 4096;\n    const int K_GLOBAL = 1024;\n    const int N_GLOBAL = 1024;\n    const float SPARSITY = 0.75;\n    const int K_GLOBAL_SPARSE = (int(K_GLOBAL * (1-SPARSITY)));\n    const int M_TILES = (M_GLOBAL / M);\n    const int N_TILES = (N_GLOBAL / N);\n    const int K_TILES = (K_GLOBAL / K);\n    const int BLOCK_ROW_WARPS = 2;\n    const int BLOCK_COL_WARPS = 4;\n    const int WARP_ROW_TILES = 4;\n    const int WARP_COL_TILES = 2;\n    const int SKEW_UINT8 = 16;\n    const int BANK_VAL = 32;\n    const int NUM_BANK = (K_GLOBAL / BANK_VAL);\n    /////////// BLOCK_ROW_TILES <= N_TILES ////////////\n    // const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    // ////////// BLOCK_COL_TILES <= M_TILES ////////////\n    // const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    auto C_LAYOUT = wmma::mem_col_major;\n    const int CHUNK_K = 64;\n    const int CHUNK_K_SPARSE = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K_SPARSE = (int((CHUNK_K * K) * (1 - SPARSITY)));\n    const int WARP_COPY_BYTES = (WARP_SIZE * sizeof(int4));\n    const int CHUNK_LINE_BYTES_A (BLOCK_COL_TILES * M * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_A = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_A);\n    const int CHUNK_COPY_LINE_LANES_A = (CHUNK_LINE_BYTES_A / sizeof(int4));\n    const int SHARED_OFFSET_A = (BLOCK_COL_TILES * M + SKEW_UINT8);\n\n    const int CHUNK_LINE_BYTES_B = (BLOCK_SIZE_K_SPARSE * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_B = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_B);\n    const int CHUNK_COPY_LINE_LANES_B = (CHUNK_LINE_BYTES_B / sizeof(int4));\n    const int SHARED_OFFSET_B = (BLOCK_SIZE_K_SPARSE + SKEW_UINT8);\n    const int SHARED_TO_GLOBAL_BYTES_PER_LINE = ((WARP_COL_TILES * M) * sizeof(int));\n    const int SHARED_TO_GLOBAL_BYTES_PER_WARP = (WARP_SIZE * sizeof(int));\n    const int SHARED_TO_GLOBAL_LINES_PER_WARP = (SHARED_TO_GLOBAL_BYTES_PER_WARP / SHARED_TO_GLOBAL_BYTES_PER_LINE);\n    const int SHARED_TO_GLOBAL_LANES_PER_LINE = (WARP_SIZE / SHARED_TO_GLOBAL_LINES_PER_WARP);\n    const int SHARED_TO_GLOBAL_ITERS = ((WARP_ROW_TILES * N) / SHARED_TO_GLOBAL_LINES_PER_WARP);\n\n    const int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const int WARP_STRIDE = (WARP_COL_TILES * M);\n    const int WARPS_PER_BLOCK = (BLOCK_ROW_WARPS * BLOCK_COL_WARPS);\n    const int THREADS_PER_BLOCK = (WARP_SIZE * WARPS_PER_BLOCK);\n    const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    const int GLOBAL_MEM_STRIDE = M_GLOBAL;\n    const int SHMEM_STRIDE = (M * BLOCK_COL_TILES);\n    const int SHMEM_OFFSET = (M * WARP_COL_TILES);\n    const int BLOCK_SIZE_M = (M * BLOCK_COL_TILES);\n    const int BLOCK_SIZE_N = (N * BLOCK_ROW_TILES);\n\n   extern __shared__ uint8_t shmem[];\n\n\t// Warp and lane identification.\n\tconst unsigned int warpId = threadIdx.x / WARP_SIZE;\n\tconst unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n\n\t// Offset in shared memory from which the B matrix is stored.\n\t// const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\tconst size_t shmem_idx_b_off = BLOCK_SIZE_K_SPARSE * SHARED_OFFSET_A;\n\n\n\t// Each CTA slides along the 128 x 128 tiles from the top left corner of the\n\t// matrix to the right and down, and selects the next tile to compute. Once\n\t// there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n\tconst unsigned int block_tile_i =\n\t\t((block_pos * BLOCK_COL_TILES) / M_TILES) * (BLOCK_ROW_TILES);\n\tconst unsigned int block_tile_j = (block_pos * BLOCK_COL_TILES) % M_TILES;\n\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n\twmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_ROW_TILES]\n\t\t\t\t\t\t\t\t\t\t\t\t\t [WARP_COL_TILES];\n\n    // Load the C matrix tiles into fragments from shared memory.\n\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t\twmma::fill_fragment(c[i][j], 0);\n\t\t}\n\t}\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    //for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n    for(int tile_k_idx_sparse = 0, tile_k_idx = 0; tile_k_idx_sparse < K_GLOBAL_SPARSE; tile_k_idx_sparse += BLOCK_SIZE_K_SPARSE, tile_k_idx += BLOCK_SIZE_K){\n\n\t\tsize_t shmem_idx = \n\t\twarpId < (WARPS_PER_BLOCK / 2)\n\t\t\t? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A\n\t\t\t: (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B + shmem_idx_b_off;\n\n\t\tint4 *lane_ptr = NULL;\n\t\tint *lane_ptr_index = NULL;\n\t\tconst uint8_t *warp_ptr = NULL;\n\n\n\t\tif(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\t//warp_ptr = &A[block_tile_j * M] +\n\t\t\t//\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * M_GLOBAL;\n\t\t\twarp_ptr = &A[block_tile_j * M];\n\t\t\t\n\t\t\tconst int *warp_ptr_index = &B_index[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t\t\t\t\t\t((warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A);\n\n\t\t\tlane_ptr_index = (int *)(warp_ptr_index + tile_k_idx_sparse + (laneId / CHUNK_COPY_LINE_LANES_A));\n\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_A) * SHARED_OFFSET_A;\n\t\t}else{\n\t\t\twarp_ptr = &B[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * K_GLOBAL_SPARSE;\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_sparse +\n\t\t\t\t\t\t\t\t(laneId / CHUNK_COPY_LINE_LANES_B) * K_GLOBAL_SPARSE) +\n\t\t\t\t\t\t\t\t(laneId % CHUNK_COPY_LINE_LANES_B);\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_B) * SHARED_OFFSET_B;\n\t\t}\n\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      // shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n\t  int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n\t  \t? BLOCK_SIZE_K_SPARSE / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A)\n\t\t: BLOCK_SIZE_N / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\n\t  /*\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\t  */\n\n\t  /*\n      int tile_k_idx_A;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n          tile_k_idx_A = *(lane_ptr_index);\n      }\n\t  */\n\n\t  #pragma unroll\n\t  for(int i = 0; i < iter_index; i += 1){\n\t\t  if(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\tint tile_k_idx_A = *(lane_ptr_index);\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_A * M_GLOBAL) + (laneId % CHUNK_COPY_LINE_LANES_A);\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_A)) =\n\t\t\t\t*lane_ptr;\n\t\t\t//warp_ptr = (uint8_t *)((uint8_t *)warp_ptr + M_GLOBAL * (WARPS_PER_BLOCK / 2) *CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tlane_ptr_index = (int *)((int *)lane_ptr_index +  (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A;\n\t\t  }else{\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_B)) =\n\t\t\t\t*lane_ptr;\n\t\t\tlane_ptr = (int4 *)((uint8_t *)lane_ptr + K_GLOBAL_SPARSE * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B;\n\t\t  }\n\t  }\n\n      __syncthreads();\n\n\t#pragma unroll\n      for (int k_step = 0; k_step < CHUNK_K_SPARSE; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::col_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n\t#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i += 1) {\n\t\t\tsize_t shmem_idx_a = (warpId % BLOCK_COL_WARPS) * M * WARP_COL_TILES + (i * M);\n\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_a + k_step * K * SHARED_OFFSET_A;\n\n\t\t\twmma::load_matrix_sync(a[i], tile_ptr, SHARED_OFFSET_A);\n\t\t#pragma unroll\n\t\t\tfor(int j = 0; j < WARP_ROW_TILES; j += 1){\n\t\t\t\tif(i == 0){\n\t\t\t\t\tsize_t shmem_idx_b = shmem_idx_b_off +\n\t\t\t\t\t\t\t\t\t\t\t(warpId / BLOCK_COL_WARPS) * (WARP_ROW_TILES * N) * SHARED_OFFSET_B +\n\t\t\t\t\t\t\t\t\t\t\t(j * N) * SHARED_OFFSET_B;\n\t\t\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_b + k_step * K;\n\t\t\t\t\twmma::load_matrix_sync(b[j], tile_ptr, SHARED_OFFSET_B);\n\t\t\t\t}\n\t\t\t\twmma::mma_sync(c[j][i], a[i], b[j], c[j][i]);\n\t\t\t}\n\n        }\n      }\n\n      __syncthreads();\n    }\n\n    // This pointer is used to access the C and D matrix tiles this warp computes.\n\tint *shmem_warp_tile_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * N * WARP_ROW_TILES * SHMEM_STRIDE +\n\t(warpId % BLOCK_COL_WARPS) * SHMEM_OFFSET;\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t#pragma unroll\n\t\t\tfor(int t = 0; t < c[i][j].num_elements; t += 1){\n\t\t\t\tc[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n\t\t\t}\n\t\t\tint *tile_ptr = shmem_warp_tile_ptr + i * N * SHMEM_STRIDE + j * M;\n\t\t\twmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, C_LAYOUT);\n\t\t}\n\t}\n\n    __syncthreads();\n\n\tint *shmem_warp_stream_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N * SHMEM_STRIDE\n\t\t\t\t\t\t\t\t\t+ (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tconst size_t gmem_idx =\n\t\t(block_tile_i * N + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N) * GLOBAL_MEM_STRIDE +\n\t\tblock_tile_j * M + (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tuint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n\tint *shmem_lane_stream_ptr =\n\t\tshmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * SHMEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\t\n\tuint8_t *dst_gmem_lane_stream_ptr =\n\t\tdst_gmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * GLOBAL_MEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\n\tfor(int i = 0; i < WARP_ROW_TILES * N; i += SHARED_TO_GLOBAL_LINES_PER_WARP){\n\t\t*(dst_gmem_lane_stream_ptr + i * GLOBAL_MEM_STRIDE) = (uint8_t)(*(shmem_lane_stream_ptr + i * SHMEM_STRIDE));\n\t}\n\n\t__syncthreads();\n}\n", "launch_config": {"dimBlock": [256, 1], "dimGrid": [256, 1]}}, "2": {"code": "\n__global__ void compute_gemm_imma(float *input0, float *input1, float *input2, float* input3, float* input4, float*input5, float* input6,\n                                  float *output0) {\n    // keys need to be replaced 4096, 1024, K_GLOBA_VALUEL, 0.75, 64\n\n\t//extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B = reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_index = reinterpret_cast<int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n\tconst int WARP_SIZE = 32;\n    const int M = 16;\n    const int K = 16;\n    const int N = 16;\n    const int WMMA_M = 16;\n    const int WMMA_K = 16;\n    const int WMMA_N = 16;\n    const int M_GLOBAL = 4096;\n    const int K_GLOBAL = 1024;\n    const int N_GLOBAL = 1024;\n    const float SPARSITY = 0.75;\n    const int K_GLOBAL_SPARSE = (int(K_GLOBAL * (1-SPARSITY)));\n    const int M_TILES = (M_GLOBAL / M);\n    const int N_TILES = (N_GLOBAL / N);\n    const int K_TILES = (K_GLOBAL / K);\n    const int BLOCK_ROW_WARPS = 2;\n    const int BLOCK_COL_WARPS = 4;\n    const int WARP_ROW_TILES = 4;\n    const int WARP_COL_TILES = 2;\n    const int SKEW_UINT8 = 16;\n    const int BANK_VAL = 32;\n    const int NUM_BANK = (K_GLOBAL / BANK_VAL);\n    /////////// BLOCK_ROW_TILES <= N_TILES ////////////\n    // const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    // ////////// BLOCK_COL_TILES <= M_TILES ////////////\n    // const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    auto C_LAYOUT = wmma::mem_col_major;\n    const int CHUNK_K = 64;\n    const int CHUNK_K_SPARSE = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K_SPARSE = (int((CHUNK_K * K) * (1 - SPARSITY)));\n    const int WARP_COPY_BYTES = (WARP_SIZE * sizeof(int4));\n    const int CHUNK_LINE_BYTES_A (BLOCK_COL_TILES * M * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_A = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_A);\n    const int CHUNK_COPY_LINE_LANES_A = (CHUNK_LINE_BYTES_A / sizeof(int4));\n    const int SHARED_OFFSET_A = (BLOCK_COL_TILES * M + SKEW_UINT8);\n\n    const int CHUNK_LINE_BYTES_B = (BLOCK_SIZE_K_SPARSE * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_B = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_B);\n    const int CHUNK_COPY_LINE_LANES_B = (CHUNK_LINE_BYTES_B / sizeof(int4));\n    const int SHARED_OFFSET_B = (BLOCK_SIZE_K_SPARSE + SKEW_UINT8);\n    const int SHARED_TO_GLOBAL_BYTES_PER_LINE = ((WARP_COL_TILES * M) * sizeof(int));\n    const int SHARED_TO_GLOBAL_BYTES_PER_WARP = (WARP_SIZE * sizeof(int));\n    const int SHARED_TO_GLOBAL_LINES_PER_WARP = (SHARED_TO_GLOBAL_BYTES_PER_WARP / SHARED_TO_GLOBAL_BYTES_PER_LINE);\n    const int SHARED_TO_GLOBAL_LANES_PER_LINE = (WARP_SIZE / SHARED_TO_GLOBAL_LINES_PER_WARP);\n    const int SHARED_TO_GLOBAL_ITERS = ((WARP_ROW_TILES * N) / SHARED_TO_GLOBAL_LINES_PER_WARP);\n\n    const int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const int WARP_STRIDE = (WARP_COL_TILES * M);\n    const int WARPS_PER_BLOCK = (BLOCK_ROW_WARPS * BLOCK_COL_WARPS);\n    const int THREADS_PER_BLOCK = (WARP_SIZE * WARPS_PER_BLOCK);\n    const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    const int GLOBAL_MEM_STRIDE = M_GLOBAL;\n    const int SHMEM_STRIDE = (M * BLOCK_COL_TILES);\n    const int SHMEM_OFFSET = (M * WARP_COL_TILES);\n    const int BLOCK_SIZE_M = (M * BLOCK_COL_TILES);\n    const int BLOCK_SIZE_N = (N * BLOCK_ROW_TILES);\n\n   extern __shared__ uint8_t shmem[];\n\n\t// Warp and lane identification.\n\tconst unsigned int warpId = threadIdx.x / WARP_SIZE;\n\tconst unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n\n\t// Offset in shared memory from which the B matrix is stored.\n\t// const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\tconst size_t shmem_idx_b_off = BLOCK_SIZE_K_SPARSE * SHARED_OFFSET_A;\n\n\n\t// Each CTA slides along the 128 x 128 tiles from the top left corner of the\n\t// matrix to the right and down, and selects the next tile to compute. Once\n\t// there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n\tconst unsigned int block_tile_i =\n\t\t((block_pos * BLOCK_COL_TILES) / M_TILES) * (BLOCK_ROW_TILES);\n\tconst unsigned int block_tile_j = (block_pos * BLOCK_COL_TILES) % M_TILES;\n\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n\twmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_ROW_TILES]\n\t\t\t\t\t\t\t\t\t\t\t\t\t [WARP_COL_TILES];\n\n    // Load the C matrix tiles into fragments from shared memory.\n\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t\twmma::fill_fragment(c[i][j], 0);\n\t\t}\n\t}\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    //for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n    for(int tile_k_idx_sparse = 0, tile_k_idx = 0; tile_k_idx_sparse < K_GLOBAL_SPARSE; tile_k_idx_sparse += BLOCK_SIZE_K_SPARSE, tile_k_idx += BLOCK_SIZE_K){\n\n\t\tsize_t shmem_idx = \n\t\twarpId < (WARPS_PER_BLOCK / 2)\n\t\t\t? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A\n\t\t\t: (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B + shmem_idx_b_off;\n\n\t\tint4 *lane_ptr = NULL;\n\t\tint *lane_ptr_index = NULL;\n\t\tconst uint8_t *warp_ptr = NULL;\n\n\n\t\tif(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\t//warp_ptr = &A[block_tile_j * M] +\n\t\t\t//\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * M_GLOBAL;\n\t\t\twarp_ptr = &A[block_tile_j * M];\n\t\t\t\n\t\t\tconst int *warp_ptr_index = &B_index[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t\t\t\t\t\t((warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A);\n\n\t\t\tlane_ptr_index = (int *)(warp_ptr_index + tile_k_idx_sparse + (laneId / CHUNK_COPY_LINE_LANES_A));\n\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_A) * SHARED_OFFSET_A;\n\t\t}else{\n\t\t\twarp_ptr = &B[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * K_GLOBAL_SPARSE;\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_sparse +\n\t\t\t\t\t\t\t\t(laneId / CHUNK_COPY_LINE_LANES_B) * K_GLOBAL_SPARSE) +\n\t\t\t\t\t\t\t\t(laneId % CHUNK_COPY_LINE_LANES_B);\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_B) * SHARED_OFFSET_B;\n\t\t}\n\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      // shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n\t  int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n\t  \t? BLOCK_SIZE_K_SPARSE / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A)\n\t\t: BLOCK_SIZE_N / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\n\t  /*\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\t  */\n\n\t  /*\n      int tile_k_idx_A;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n          tile_k_idx_A = *(lane_ptr_index);\n      }\n\t  */\n\n\t  #pragma unroll\n\t  for(int i = 0; i < iter_index; i += 1){\n\t\t  if(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\tint tile_k_idx_A = *(lane_ptr_index);\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_A * M_GLOBAL) + (laneId % CHUNK_COPY_LINE_LANES_A);\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_A)) =\n\t\t\t\t*lane_ptr;\n\t\t\t//warp_ptr = (uint8_t *)((uint8_t *)warp_ptr + M_GLOBAL * (WARPS_PER_BLOCK / 2) *CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tlane_ptr_index = (int *)((int *)lane_ptr_index +  (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A;\n\t\t  }else{\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_B)) =\n\t\t\t\t*lane_ptr;\n\t\t\tlane_ptr = (int4 *)((uint8_t *)lane_ptr + K_GLOBAL_SPARSE * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B;\n\t\t  }\n\t  }\n\n      __syncthreads();\n\n\t#pragma unroll\n      for (int k_step = 0; k_step < CHUNK_K_SPARSE; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::col_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n\t#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i += 1) {\n\t\t\tsize_t shmem_idx_a = (warpId % BLOCK_COL_WARPS) * M * WARP_COL_TILES + (i * M);\n\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_a + k_step * K * SHARED_OFFSET_A;\n\n\t\t\twmma::load_matrix_sync(a[i], tile_ptr, SHARED_OFFSET_A);\n\t\t#pragma unroll\n\t\t\tfor(int j = 0; j < WARP_ROW_TILES; j += 1){\n\t\t\t\tif(i == 0){\n\t\t\t\t\tsize_t shmem_idx_b = shmem_idx_b_off +\n\t\t\t\t\t\t\t\t\t\t\t(warpId / BLOCK_COL_WARPS) * (WARP_ROW_TILES * N) * SHARED_OFFSET_B +\n\t\t\t\t\t\t\t\t\t\t\t(j * N) * SHARED_OFFSET_B;\n\t\t\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_b + k_step * K;\n\t\t\t\t\twmma::load_matrix_sync(b[j], tile_ptr, SHARED_OFFSET_B);\n\t\t\t\t}\n\t\t\t\twmma::mma_sync(c[j][i], a[i], b[j], c[j][i]);\n\t\t\t}\n\n        }\n      }\n\n      __syncthreads();\n    }\n\n    // This pointer is used to access the C and D matrix tiles this warp computes.\n\tint *shmem_warp_tile_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * N * WARP_ROW_TILES * SHMEM_STRIDE +\n\t(warpId % BLOCK_COL_WARPS) * SHMEM_OFFSET;\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t#pragma unroll\n\t\t\tfor(int t = 0; t < c[i][j].num_elements; t += 1){\n\t\t\t\tc[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n\t\t\t}\n\t\t\tint *tile_ptr = shmem_warp_tile_ptr + i * N * SHMEM_STRIDE + j * M;\n\t\t\twmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, C_LAYOUT);\n\t\t}\n\t}\n\n    __syncthreads();\n\n\tint *shmem_warp_stream_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N * SHMEM_STRIDE\n\t\t\t\t\t\t\t\t\t+ (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tconst size_t gmem_idx =\n\t\t(block_tile_i * N + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N) * GLOBAL_MEM_STRIDE +\n\t\tblock_tile_j * M + (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tuint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n\tint *shmem_lane_stream_ptr =\n\t\tshmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * SHMEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\t\n\tuint8_t *dst_gmem_lane_stream_ptr =\n\t\tdst_gmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * GLOBAL_MEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\n\tfor(int i = 0; i < WARP_ROW_TILES * N; i += SHARED_TO_GLOBAL_LINES_PER_WARP){\n\t\t*(dst_gmem_lane_stream_ptr + i * GLOBAL_MEM_STRIDE) = (uint8_t)(*(shmem_lane_stream_ptr + i * SHMEM_STRIDE));\n\t}\n\n\t__syncthreads();\n}\n", "launch_config": {"dimBlock": [256, 1], "dimGrid": [256, 1]}}, "3": {"code": "\n__global__ void compute_gemm_imma(float *input0, float *input1, float *input2, float* input3, float* input4, float*input5, float* input6,\n                                  float *output0) {\n    // keys need to be replaced 4096, 1024, K_GLOBA_VALUEL, 0.75, 64\n\n\t//extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B = reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_index = reinterpret_cast<int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n\tconst int WARP_SIZE = 32;\n    const int M = 16;\n    const int K = 16;\n    const int N = 16;\n    const int WMMA_M = 16;\n    const int WMMA_K = 16;\n    const int WMMA_N = 16;\n    const int M_GLOBAL = 4096;\n    const int K_GLOBAL = 1024;\n    const int N_GLOBAL = 1024;\n    const float SPARSITY = 0.75;\n    const int K_GLOBAL_SPARSE = (int(K_GLOBAL * (1-SPARSITY)));\n    const int M_TILES = (M_GLOBAL / M);\n    const int N_TILES = (N_GLOBAL / N);\n    const int K_TILES = (K_GLOBAL / K);\n    const int BLOCK_ROW_WARPS = 2;\n    const int BLOCK_COL_WARPS = 4;\n    const int WARP_ROW_TILES = 4;\n    const int WARP_COL_TILES = 2;\n    const int SKEW_UINT8 = 16;\n    const int BANK_VAL = 32;\n    const int NUM_BANK = (K_GLOBAL / BANK_VAL);\n    /////////// BLOCK_ROW_TILES <= N_TILES ////////////\n    // const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    // ////////// BLOCK_COL_TILES <= M_TILES ////////////\n    // const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    auto C_LAYOUT = wmma::mem_col_major;\n    const int CHUNK_K = 64;\n    const int CHUNK_K_SPARSE = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K_SPARSE = (int((CHUNK_K * K) * (1 - SPARSITY)));\n    const int WARP_COPY_BYTES = (WARP_SIZE * sizeof(int4));\n    const int CHUNK_LINE_BYTES_A (BLOCK_COL_TILES * M * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_A = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_A);\n    const int CHUNK_COPY_LINE_LANES_A = (CHUNK_LINE_BYTES_A / sizeof(int4));\n    const int SHARED_OFFSET_A = (BLOCK_COL_TILES * M + SKEW_UINT8);\n\n    const int CHUNK_LINE_BYTES_B = (BLOCK_SIZE_K_SPARSE * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_B = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_B);\n    const int CHUNK_COPY_LINE_LANES_B = (CHUNK_LINE_BYTES_B / sizeof(int4));\n    const int SHARED_OFFSET_B = (BLOCK_SIZE_K_SPARSE + SKEW_UINT8);\n    const int SHARED_TO_GLOBAL_BYTES_PER_LINE = ((WARP_COL_TILES * M) * sizeof(int));\n    const int SHARED_TO_GLOBAL_BYTES_PER_WARP = (WARP_SIZE * sizeof(int));\n    const int SHARED_TO_GLOBAL_LINES_PER_WARP = (SHARED_TO_GLOBAL_BYTES_PER_WARP / SHARED_TO_GLOBAL_BYTES_PER_LINE);\n    const int SHARED_TO_GLOBAL_LANES_PER_LINE = (WARP_SIZE / SHARED_TO_GLOBAL_LINES_PER_WARP);\n    const int SHARED_TO_GLOBAL_ITERS = ((WARP_ROW_TILES * N) / SHARED_TO_GLOBAL_LINES_PER_WARP);\n\n    const int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const int WARP_STRIDE = (WARP_COL_TILES * M);\n    const int WARPS_PER_BLOCK = (BLOCK_ROW_WARPS * BLOCK_COL_WARPS);\n    const int THREADS_PER_BLOCK = (WARP_SIZE * WARPS_PER_BLOCK);\n    const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    const int GLOBAL_MEM_STRIDE = M_GLOBAL;\n    const int SHMEM_STRIDE = (M * BLOCK_COL_TILES);\n    const int SHMEM_OFFSET = (M * WARP_COL_TILES);\n    const int BLOCK_SIZE_M = (M * BLOCK_COL_TILES);\n    const int BLOCK_SIZE_N = (N * BLOCK_ROW_TILES);\n\n   extern __shared__ uint8_t shmem[];\n\n\t// Warp and lane identification.\n\tconst unsigned int warpId = threadIdx.x / WARP_SIZE;\n\tconst unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n\n\t// Offset in shared memory from which the B matrix is stored.\n\t// const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\tconst size_t shmem_idx_b_off = BLOCK_SIZE_K_SPARSE * SHARED_OFFSET_A;\n\n\n\t// Each CTA slides along the 128 x 128 tiles from the top left corner of the\n\t// matrix to the right and down, and selects the next tile to compute. Once\n\t// there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n\tconst unsigned int block_tile_i =\n\t\t((block_pos * BLOCK_COL_TILES) / M_TILES) * (BLOCK_ROW_TILES);\n\tconst unsigned int block_tile_j = (block_pos * BLOCK_COL_TILES) % M_TILES;\n\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n\twmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_ROW_TILES]\n\t\t\t\t\t\t\t\t\t\t\t\t\t [WARP_COL_TILES];\n\n    // Load the C matrix tiles into fragments from shared memory.\n\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t\twmma::fill_fragment(c[i][j], 0);\n\t\t}\n\t}\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    //for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n    for(int tile_k_idx_sparse = 0, tile_k_idx = 0; tile_k_idx_sparse < K_GLOBAL_SPARSE; tile_k_idx_sparse += BLOCK_SIZE_K_SPARSE, tile_k_idx += BLOCK_SIZE_K){\n\n\t\tsize_t shmem_idx = \n\t\twarpId < (WARPS_PER_BLOCK / 2)\n\t\t\t? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A\n\t\t\t: (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B + shmem_idx_b_off;\n\n\t\tint4 *lane_ptr = NULL;\n\t\tint *lane_ptr_index = NULL;\n\t\tconst uint8_t *warp_ptr = NULL;\n\n\n\t\tif(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\t//warp_ptr = &A[block_tile_j * M] +\n\t\t\t//\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * M_GLOBAL;\n\t\t\twarp_ptr = &A[block_tile_j * M];\n\t\t\t\n\t\t\tconst int *warp_ptr_index = &B_index[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t\t\t\t\t\t((warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A);\n\n\t\t\tlane_ptr_index = (int *)(warp_ptr_index + tile_k_idx_sparse + (laneId / CHUNK_COPY_LINE_LANES_A));\n\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_A) * SHARED_OFFSET_A;\n\t\t}else{\n\t\t\twarp_ptr = &B[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * K_GLOBAL_SPARSE;\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_sparse +\n\t\t\t\t\t\t\t\t(laneId / CHUNK_COPY_LINE_LANES_B) * K_GLOBAL_SPARSE) +\n\t\t\t\t\t\t\t\t(laneId % CHUNK_COPY_LINE_LANES_B);\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_B) * SHARED_OFFSET_B;\n\t\t}\n\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      // shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n\t  int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n\t  \t? BLOCK_SIZE_K_SPARSE / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A)\n\t\t: BLOCK_SIZE_N / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\n\t  /*\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\t  */\n\n\t  /*\n      int tile_k_idx_A;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n          tile_k_idx_A = *(lane_ptr_index);\n      }\n\t  */\n\n\t  #pragma unroll\n\t  for(int i = 0; i < iter_index; i += 1){\n\t\t  if(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\tint tile_k_idx_A = *(lane_ptr_index);\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_A * M_GLOBAL) + (laneId % CHUNK_COPY_LINE_LANES_A);\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_A)) =\n\t\t\t\t*lane_ptr;\n\t\t\t//warp_ptr = (uint8_t *)((uint8_t *)warp_ptr + M_GLOBAL * (WARPS_PER_BLOCK / 2) *CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tlane_ptr_index = (int *)((int *)lane_ptr_index +  (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A;\n\t\t  }else{\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_B)) =\n\t\t\t\t*lane_ptr;\n\t\t\tlane_ptr = (int4 *)((uint8_t *)lane_ptr + K_GLOBAL_SPARSE * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B;\n\t\t  }\n\t  }\n\n      __syncthreads();\n\n\t#pragma unroll\n      for (int k_step = 0; k_step < CHUNK_K_SPARSE; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::col_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n\t#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i += 1) {\n\t\t\tsize_t shmem_idx_a = (warpId % BLOCK_COL_WARPS) * M * WARP_COL_TILES + (i * M);\n\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_a + k_step * K * SHARED_OFFSET_A;\n\n\t\t\twmma::load_matrix_sync(a[i], tile_ptr, SHARED_OFFSET_A);\n\t\t#pragma unroll\n\t\t\tfor(int j = 0; j < WARP_ROW_TILES; j += 1){\n\t\t\t\tif(i == 0){\n\t\t\t\t\tsize_t shmem_idx_b = shmem_idx_b_off +\n\t\t\t\t\t\t\t\t\t\t\t(warpId / BLOCK_COL_WARPS) * (WARP_ROW_TILES * N) * SHARED_OFFSET_B +\n\t\t\t\t\t\t\t\t\t\t\t(j * N) * SHARED_OFFSET_B;\n\t\t\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_b + k_step * K;\n\t\t\t\t\twmma::load_matrix_sync(b[j], tile_ptr, SHARED_OFFSET_B);\n\t\t\t\t}\n\t\t\t\twmma::mma_sync(c[j][i], a[i], b[j], c[j][i]);\n\t\t\t}\n\n        }\n      }\n\n      __syncthreads();\n    }\n\n    // This pointer is used to access the C and D matrix tiles this warp computes.\n\tint *shmem_warp_tile_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * N * WARP_ROW_TILES * SHMEM_STRIDE +\n\t(warpId % BLOCK_COL_WARPS) * SHMEM_OFFSET;\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t#pragma unroll\n\t\t\tfor(int t = 0; t < c[i][j].num_elements; t += 1){\n\t\t\t\tc[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n\t\t\t}\n\t\t\tint *tile_ptr = shmem_warp_tile_ptr + i * N * SHMEM_STRIDE + j * M;\n\t\t\twmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, C_LAYOUT);\n\t\t}\n\t}\n\n    __syncthreads();\n\n\tint *shmem_warp_stream_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N * SHMEM_STRIDE\n\t\t\t\t\t\t\t\t\t+ (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tconst size_t gmem_idx =\n\t\t(block_tile_i * N + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N) * GLOBAL_MEM_STRIDE +\n\t\tblock_tile_j * M + (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tuint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n\tint *shmem_lane_stream_ptr =\n\t\tshmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * SHMEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\t\n\tuint8_t *dst_gmem_lane_stream_ptr =\n\t\tdst_gmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * GLOBAL_MEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\n\tfor(int i = 0; i < WARP_ROW_TILES * N; i += SHARED_TO_GLOBAL_LINES_PER_WARP){\n\t\t*(dst_gmem_lane_stream_ptr + i * GLOBAL_MEM_STRIDE) = (uint8_t)(*(shmem_lane_stream_ptr + i * SHMEM_STRIDE));\n\t}\n\n\t__syncthreads();\n}\n", "launch_config": {"dimBlock": [256, 1], "dimGrid": [256, 1]}}, "4": {"code": "\n__global__ void compute_gemm_imma(float *input0, float *input1, float *input2, float* input3, float* input4, float*input5, float* input6,\n                                  float *output0) {\n    // keys need to be replaced 4096, 1024, K_GLOBA_VALUEL, 0.75, 64\n\n\t//extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B = reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_index = reinterpret_cast<int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n\tconst int WARP_SIZE = 32;\n    const int M = 16;\n    const int K = 16;\n    const int N = 16;\n    const int WMMA_M = 16;\n    const int WMMA_K = 16;\n    const int WMMA_N = 16;\n    const int M_GLOBAL = 4096;\n    const int K_GLOBAL = 1024;\n    const int N_GLOBAL = 1024;\n    const float SPARSITY = 0.75;\n    const int K_GLOBAL_SPARSE = (int(K_GLOBAL * (1-SPARSITY)));\n    const int M_TILES = (M_GLOBAL / M);\n    const int N_TILES = (N_GLOBAL / N);\n    const int K_TILES = (K_GLOBAL / K);\n    const int BLOCK_ROW_WARPS = 2;\n    const int BLOCK_COL_WARPS = 4;\n    const int WARP_ROW_TILES = 4;\n    const int WARP_COL_TILES = 2;\n    const int SKEW_UINT8 = 16;\n    const int BANK_VAL = 32;\n    const int NUM_BANK = (K_GLOBAL / BANK_VAL);\n    /////////// BLOCK_ROW_TILES <= N_TILES ////////////\n    // const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    // ////////// BLOCK_COL_TILES <= M_TILES ////////////\n    // const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    auto C_LAYOUT = wmma::mem_col_major;\n    const int CHUNK_K = 64;\n    const int CHUNK_K_SPARSE = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K_SPARSE = (int((CHUNK_K * K) * (1 - SPARSITY)));\n    const int WARP_COPY_BYTES = (WARP_SIZE * sizeof(int4));\n    const int CHUNK_LINE_BYTES_A (BLOCK_COL_TILES * M * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_A = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_A);\n    const int CHUNK_COPY_LINE_LANES_A = (CHUNK_LINE_BYTES_A / sizeof(int4));\n    const int SHARED_OFFSET_A = (BLOCK_COL_TILES * M + SKEW_UINT8);\n\n    const int CHUNK_LINE_BYTES_B = (BLOCK_SIZE_K_SPARSE * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_B = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_B);\n    const int CHUNK_COPY_LINE_LANES_B = (CHUNK_LINE_BYTES_B / sizeof(int4));\n    const int SHARED_OFFSET_B = (BLOCK_SIZE_K_SPARSE + SKEW_UINT8);\n    const int SHARED_TO_GLOBAL_BYTES_PER_LINE = ((WARP_COL_TILES * M) * sizeof(int));\n    const int SHARED_TO_GLOBAL_BYTES_PER_WARP = (WARP_SIZE * sizeof(int));\n    const int SHARED_TO_GLOBAL_LINES_PER_WARP = (SHARED_TO_GLOBAL_BYTES_PER_WARP / SHARED_TO_GLOBAL_BYTES_PER_LINE);\n    const int SHARED_TO_GLOBAL_LANES_PER_LINE = (WARP_SIZE / SHARED_TO_GLOBAL_LINES_PER_WARP);\n    const int SHARED_TO_GLOBAL_ITERS = ((WARP_ROW_TILES * N) / SHARED_TO_GLOBAL_LINES_PER_WARP);\n\n    const int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const int WARP_STRIDE = (WARP_COL_TILES * M);\n    const int WARPS_PER_BLOCK = (BLOCK_ROW_WARPS * BLOCK_COL_WARPS);\n    const int THREADS_PER_BLOCK = (WARP_SIZE * WARPS_PER_BLOCK);\n    const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    const int GLOBAL_MEM_STRIDE = M_GLOBAL;\n    const int SHMEM_STRIDE = (M * BLOCK_COL_TILES);\n    const int SHMEM_OFFSET = (M * WARP_COL_TILES);\n    const int BLOCK_SIZE_M = (M * BLOCK_COL_TILES);\n    const int BLOCK_SIZE_N = (N * BLOCK_ROW_TILES);\n\n   extern __shared__ uint8_t shmem[];\n\n\t// Warp and lane identification.\n\tconst unsigned int warpId = threadIdx.x / WARP_SIZE;\n\tconst unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n\n\t// Offset in shared memory from which the B matrix is stored.\n\t// const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\tconst size_t shmem_idx_b_off = BLOCK_SIZE_K_SPARSE * SHARED_OFFSET_A;\n\n\n\t// Each CTA slides along the 128 x 128 tiles from the top left corner of the\n\t// matrix to the right and down, and selects the next tile to compute. Once\n\t// there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n\tconst unsigned int block_tile_i =\n\t\t((block_pos * BLOCK_COL_TILES) / M_TILES) * (BLOCK_ROW_TILES);\n\tconst unsigned int block_tile_j = (block_pos * BLOCK_COL_TILES) % M_TILES;\n\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n\twmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_ROW_TILES]\n\t\t\t\t\t\t\t\t\t\t\t\t\t [WARP_COL_TILES];\n\n    // Load the C matrix tiles into fragments from shared memory.\n\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t\twmma::fill_fragment(c[i][j], 0);\n\t\t}\n\t}\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    //for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n    for(int tile_k_idx_sparse = 0, tile_k_idx = 0; tile_k_idx_sparse < K_GLOBAL_SPARSE; tile_k_idx_sparse += BLOCK_SIZE_K_SPARSE, tile_k_idx += BLOCK_SIZE_K){\n\n\t\tsize_t shmem_idx = \n\t\twarpId < (WARPS_PER_BLOCK / 2)\n\t\t\t? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A\n\t\t\t: (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B + shmem_idx_b_off;\n\n\t\tint4 *lane_ptr = NULL;\n\t\tint *lane_ptr_index = NULL;\n\t\tconst uint8_t *warp_ptr = NULL;\n\n\n\t\tif(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\t//warp_ptr = &A[block_tile_j * M] +\n\t\t\t//\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * M_GLOBAL;\n\t\t\twarp_ptr = &A[block_tile_j * M];\n\t\t\t\n\t\t\tconst int *warp_ptr_index = &B_index[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t\t\t\t\t\t((warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A);\n\n\t\t\tlane_ptr_index = (int *)(warp_ptr_index + tile_k_idx_sparse + (laneId / CHUNK_COPY_LINE_LANES_A));\n\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_A) * SHARED_OFFSET_A;\n\t\t}else{\n\t\t\twarp_ptr = &B[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * K_GLOBAL_SPARSE;\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_sparse +\n\t\t\t\t\t\t\t\t(laneId / CHUNK_COPY_LINE_LANES_B) * K_GLOBAL_SPARSE) +\n\t\t\t\t\t\t\t\t(laneId % CHUNK_COPY_LINE_LANES_B);\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_B) * SHARED_OFFSET_B;\n\t\t}\n\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      // shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n\t  int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n\t  \t? BLOCK_SIZE_K_SPARSE / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A)\n\t\t: BLOCK_SIZE_N / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\n\t  /*\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\t  */\n\n\t  /*\n      int tile_k_idx_A;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n          tile_k_idx_A = *(lane_ptr_index);\n      }\n\t  */\n\n\t  #pragma unroll\n\t  for(int i = 0; i < iter_index; i += 1){\n\t\t  if(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\tint tile_k_idx_A = *(lane_ptr_index);\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_A * M_GLOBAL) + (laneId % CHUNK_COPY_LINE_LANES_A);\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_A)) =\n\t\t\t\t*lane_ptr;\n\t\t\t//warp_ptr = (uint8_t *)((uint8_t *)warp_ptr + M_GLOBAL * (WARPS_PER_BLOCK / 2) *CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tlane_ptr_index = (int *)((int *)lane_ptr_index +  (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A;\n\t\t  }else{\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_B)) =\n\t\t\t\t*lane_ptr;\n\t\t\tlane_ptr = (int4 *)((uint8_t *)lane_ptr + K_GLOBAL_SPARSE * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B;\n\t\t  }\n\t  }\n\n      __syncthreads();\n\n\t#pragma unroll\n      for (int k_step = 0; k_step < CHUNK_K_SPARSE; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::col_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n\t#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i += 1) {\n\t\t\tsize_t shmem_idx_a = (warpId % BLOCK_COL_WARPS) * M * WARP_COL_TILES + (i * M);\n\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_a + k_step * K * SHARED_OFFSET_A;\n\n\t\t\twmma::load_matrix_sync(a[i], tile_ptr, SHARED_OFFSET_A);\n\t\t#pragma unroll\n\t\t\tfor(int j = 0; j < WARP_ROW_TILES; j += 1){\n\t\t\t\tif(i == 0){\n\t\t\t\t\tsize_t shmem_idx_b = shmem_idx_b_off +\n\t\t\t\t\t\t\t\t\t\t\t(warpId / BLOCK_COL_WARPS) * (WARP_ROW_TILES * N) * SHARED_OFFSET_B +\n\t\t\t\t\t\t\t\t\t\t\t(j * N) * SHARED_OFFSET_B;\n\t\t\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_b + k_step * K;\n\t\t\t\t\twmma::load_matrix_sync(b[j], tile_ptr, SHARED_OFFSET_B);\n\t\t\t\t}\n\t\t\t\twmma::mma_sync(c[j][i], a[i], b[j], c[j][i]);\n\t\t\t}\n\n        }\n      }\n\n      __syncthreads();\n    }\n\n    // This pointer is used to access the C and D matrix tiles this warp computes.\n\tint *shmem_warp_tile_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * N * WARP_ROW_TILES * SHMEM_STRIDE +\n\t(warpId % BLOCK_COL_WARPS) * SHMEM_OFFSET;\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t#pragma unroll\n\t\t\tfor(int t = 0; t < c[i][j].num_elements; t += 1){\n\t\t\t\tc[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n\t\t\t}\n\t\t\tint *tile_ptr = shmem_warp_tile_ptr + i * N * SHMEM_STRIDE + j * M;\n\t\t\twmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, C_LAYOUT);\n\t\t}\n\t}\n\n    __syncthreads();\n\n\tint *shmem_warp_stream_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N * SHMEM_STRIDE\n\t\t\t\t\t\t\t\t\t+ (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tconst size_t gmem_idx =\n\t\t(block_tile_i * N + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N) * GLOBAL_MEM_STRIDE +\n\t\tblock_tile_j * M + (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tuint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n\tint *shmem_lane_stream_ptr =\n\t\tshmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * SHMEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\t\n\tuint8_t *dst_gmem_lane_stream_ptr =\n\t\tdst_gmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * GLOBAL_MEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\n\tfor(int i = 0; i < WARP_ROW_TILES * N; i += SHARED_TO_GLOBAL_LINES_PER_WARP){\n\t\t*(dst_gmem_lane_stream_ptr + i * GLOBAL_MEM_STRIDE) = (uint8_t)(*(shmem_lane_stream_ptr + i * SHMEM_STRIDE));\n\t}\n\n\t__syncthreads();\n}\n", "launch_config": {"dimBlock": [256, 1], "dimGrid": [256, 1]}}, "5": {"code": "\n__global__ void compute_gemm_imma(float *input0, float *input1, float *input2, float* input3, float* input4, float*input5, float* input6,\n                                  float *output0) {\n    // keys need to be replaced 4096, 4096, K_GLOBA_VALUEL, 0.75, 64\n\n\t//extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B = reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_index = reinterpret_cast<int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n\tconst int WARP_SIZE = 32;\n    const int M = 16;\n    const int K = 16;\n    const int N = 16;\n    const int WMMA_M = 16;\n    const int WMMA_K = 16;\n    const int WMMA_N = 16;\n    const int M_GLOBAL = 4096;\n    const int K_GLOBAL = 1024;\n    const int N_GLOBAL = 4096;\n    const float SPARSITY = 0.75;\n    const int K_GLOBAL_SPARSE = (int(K_GLOBAL * (1-SPARSITY)));\n    const int M_TILES = (M_GLOBAL / M);\n    const int N_TILES = (N_GLOBAL / N);\n    const int K_TILES = (K_GLOBAL / K);\n    const int BLOCK_ROW_WARPS = 2;\n    const int BLOCK_COL_WARPS = 4;\n    const int WARP_ROW_TILES = 4;\n    const int WARP_COL_TILES = 2;\n    const int SKEW_UINT8 = 16;\n    const int BANK_VAL = 32;\n    const int NUM_BANK = (K_GLOBAL / BANK_VAL);\n    /////////// BLOCK_ROW_TILES <= N_TILES ////////////\n    // const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    // ////////// BLOCK_COL_TILES <= M_TILES ////////////\n    // const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    auto C_LAYOUT = wmma::mem_col_major;\n    const int CHUNK_K = 64;\n    const int CHUNK_K_SPARSE = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K_SPARSE = (int((CHUNK_K * K) * (1 - SPARSITY)));\n    const int WARP_COPY_BYTES = (WARP_SIZE * sizeof(int4));\n    const int CHUNK_LINE_BYTES_A (BLOCK_COL_TILES * M * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_A = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_A);\n    const int CHUNK_COPY_LINE_LANES_A = (CHUNK_LINE_BYTES_A / sizeof(int4));\n    const int SHARED_OFFSET_A = (BLOCK_COL_TILES * M + SKEW_UINT8);\n\n    const int CHUNK_LINE_BYTES_B = (BLOCK_SIZE_K_SPARSE * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_B = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_B);\n    const int CHUNK_COPY_LINE_LANES_B = (CHUNK_LINE_BYTES_B / sizeof(int4));\n    const int SHARED_OFFSET_B = (BLOCK_SIZE_K_SPARSE + SKEW_UINT8);\n    const int SHARED_TO_GLOBAL_BYTES_PER_LINE = ((WARP_COL_TILES * M) * sizeof(int));\n    const int SHARED_TO_GLOBAL_BYTES_PER_WARP = (WARP_SIZE * sizeof(int));\n    const int SHARED_TO_GLOBAL_LINES_PER_WARP = (SHARED_TO_GLOBAL_BYTES_PER_WARP / SHARED_TO_GLOBAL_BYTES_PER_LINE);\n    const int SHARED_TO_GLOBAL_LANES_PER_LINE = (WARP_SIZE / SHARED_TO_GLOBAL_LINES_PER_WARP);\n    const int SHARED_TO_GLOBAL_ITERS = ((WARP_ROW_TILES * N) / SHARED_TO_GLOBAL_LINES_PER_WARP);\n\n    const int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const int WARP_STRIDE = (WARP_COL_TILES * M);\n    const int WARPS_PER_BLOCK = (BLOCK_ROW_WARPS * BLOCK_COL_WARPS);\n    const int THREADS_PER_BLOCK = (WARP_SIZE * WARPS_PER_BLOCK);\n    const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    const int GLOBAL_MEM_STRIDE = M_GLOBAL;\n    const int SHMEM_STRIDE = (M * BLOCK_COL_TILES);\n    const int SHMEM_OFFSET = (M * WARP_COL_TILES);\n    const int BLOCK_SIZE_M = (M * BLOCK_COL_TILES);\n    const int BLOCK_SIZE_N = (N * BLOCK_ROW_TILES);\n\n   extern __shared__ uint8_t shmem[];\n\n\t// Warp and lane identification.\n\tconst unsigned int warpId = threadIdx.x / WARP_SIZE;\n\tconst unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n\n\t// Offset in shared memory from which the B matrix is stored.\n\t// const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\tconst size_t shmem_idx_b_off = BLOCK_SIZE_K_SPARSE * SHARED_OFFSET_A;\n\n\n\t// Each CTA slides along the 128 x 128 tiles from the top left corner of the\n\t// matrix to the right and down, and selects the next tile to compute. Once\n\t// there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n\tconst unsigned int block_tile_i =\n\t\t((block_pos * BLOCK_COL_TILES) / M_TILES) * (BLOCK_ROW_TILES);\n\tconst unsigned int block_tile_j = (block_pos * BLOCK_COL_TILES) % M_TILES;\n\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n\twmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_ROW_TILES]\n\t\t\t\t\t\t\t\t\t\t\t\t\t [WARP_COL_TILES];\n\n    // Load the C matrix tiles into fragments from shared memory.\n\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t\twmma::fill_fragment(c[i][j], 0);\n\t\t}\n\t}\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    //for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n    for(int tile_k_idx_sparse = 0, tile_k_idx = 0; tile_k_idx_sparse < K_GLOBAL_SPARSE; tile_k_idx_sparse += BLOCK_SIZE_K_SPARSE, tile_k_idx += BLOCK_SIZE_K){\n\n\t\tsize_t shmem_idx = \n\t\twarpId < (WARPS_PER_BLOCK / 2)\n\t\t\t? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A\n\t\t\t: (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B + shmem_idx_b_off;\n\n\t\tint4 *lane_ptr = NULL;\n\t\tint *lane_ptr_index = NULL;\n\t\tconst uint8_t *warp_ptr = NULL;\n\n\n\t\tif(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\t//warp_ptr = &A[block_tile_j * M] +\n\t\t\t//\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * M_GLOBAL;\n\t\t\twarp_ptr = &A[block_tile_j * M];\n\t\t\t\n\t\t\tconst int *warp_ptr_index = &B_index[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t\t\t\t\t\t((warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A);\n\n\t\t\tlane_ptr_index = (int *)(warp_ptr_index + tile_k_idx_sparse + (laneId / CHUNK_COPY_LINE_LANES_A));\n\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_A) * SHARED_OFFSET_A;\n\t\t}else{\n\t\t\twarp_ptr = &B[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * K_GLOBAL_SPARSE;\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_sparse +\n\t\t\t\t\t\t\t\t(laneId / CHUNK_COPY_LINE_LANES_B) * K_GLOBAL_SPARSE) +\n\t\t\t\t\t\t\t\t(laneId % CHUNK_COPY_LINE_LANES_B);\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_B) * SHARED_OFFSET_B;\n\t\t}\n\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      // shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n\t  int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n\t  \t? BLOCK_SIZE_K_SPARSE / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A)\n\t\t: BLOCK_SIZE_N / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\n\t  /*\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\t  */\n\n\t  /*\n      int tile_k_idx_A;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n          tile_k_idx_A = *(lane_ptr_index);\n      }\n\t  */\n\n\t  #pragma unroll\n\t  for(int i = 0; i < iter_index; i += 1){\n\t\t  if(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\tint tile_k_idx_A = *(lane_ptr_index);\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_A * M_GLOBAL) + (laneId % CHUNK_COPY_LINE_LANES_A);\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_A)) =\n\t\t\t\t*lane_ptr;\n\t\t\t//warp_ptr = (uint8_t *)((uint8_t *)warp_ptr + M_GLOBAL * (WARPS_PER_BLOCK / 2) *CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tlane_ptr_index = (int *)((int *)lane_ptr_index +  (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A;\n\t\t  }else{\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_B)) =\n\t\t\t\t*lane_ptr;\n\t\t\tlane_ptr = (int4 *)((uint8_t *)lane_ptr + K_GLOBAL_SPARSE * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B;\n\t\t  }\n\t  }\n\n      __syncthreads();\n\n\t#pragma unroll\n      for (int k_step = 0; k_step < CHUNK_K_SPARSE; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::col_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n\t#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i += 1) {\n\t\t\tsize_t shmem_idx_a = (warpId % BLOCK_COL_WARPS) * M * WARP_COL_TILES + (i * M);\n\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_a + k_step * K * SHARED_OFFSET_A;\n\n\t\t\twmma::load_matrix_sync(a[i], tile_ptr, SHARED_OFFSET_A);\n\t\t#pragma unroll\n\t\t\tfor(int j = 0; j < WARP_ROW_TILES; j += 1){\n\t\t\t\tif(i == 0){\n\t\t\t\t\tsize_t shmem_idx_b = shmem_idx_b_off +\n\t\t\t\t\t\t\t\t\t\t\t(warpId / BLOCK_COL_WARPS) * (WARP_ROW_TILES * N) * SHARED_OFFSET_B +\n\t\t\t\t\t\t\t\t\t\t\t(j * N) * SHARED_OFFSET_B;\n\t\t\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_b + k_step * K;\n\t\t\t\t\twmma::load_matrix_sync(b[j], tile_ptr, SHARED_OFFSET_B);\n\t\t\t\t}\n\t\t\t\twmma::mma_sync(c[j][i], a[i], b[j], c[j][i]);\n\t\t\t}\n\n        }\n      }\n\n      __syncthreads();\n    }\n\n    // This pointer is used to access the C and D matrix tiles this warp computes.\n\tint *shmem_warp_tile_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * N * WARP_ROW_TILES * SHMEM_STRIDE +\n\t(warpId % BLOCK_COL_WARPS) * SHMEM_OFFSET;\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t#pragma unroll\n\t\t\tfor(int t = 0; t < c[i][j].num_elements; t += 1){\n\t\t\t\tc[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n\t\t\t}\n\t\t\tint *tile_ptr = shmem_warp_tile_ptr + i * N * SHMEM_STRIDE + j * M;\n\t\t\twmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, C_LAYOUT);\n\t\t}\n\t}\n\n    __syncthreads();\n\n\tint *shmem_warp_stream_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N * SHMEM_STRIDE\n\t\t\t\t\t\t\t\t\t+ (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tconst size_t gmem_idx =\n\t\t(block_tile_i * N + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N) * GLOBAL_MEM_STRIDE +\n\t\tblock_tile_j * M + (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tuint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n\tint *shmem_lane_stream_ptr =\n\t\tshmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * SHMEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\t\n\tuint8_t *dst_gmem_lane_stream_ptr =\n\t\tdst_gmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * GLOBAL_MEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\n\tfor(int i = 0; i < WARP_ROW_TILES * N; i += SHARED_TO_GLOBAL_LINES_PER_WARP){\n\t\t*(dst_gmem_lane_stream_ptr + i * GLOBAL_MEM_STRIDE) = (uint8_t)(*(shmem_lane_stream_ptr + i * SHMEM_STRIDE));\n\t}\n\n\t__syncthreads();\n}\n", "launch_config": {"dimBlock": [1024, 1], "dimGrid": [256, 1]}}, "6": {"code": "\n__global__ void compute_gemm_imma(float *input0, float *input1, float *input2, float* input3, float* input4, float*input5, float* input6,\n                                  float *output0) {\n    // keys need to be replaced 4096, 1024, K_GLOBA_VALUEL, 0.75, 64\n\n\t//extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B = reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_index = reinterpret_cast<int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n\tconst int WARP_SIZE = 32;\n    const int M = 16;\n    const int K = 16;\n    const int N = 16;\n    const int WMMA_M = 16;\n    const int WMMA_K = 16;\n    const int WMMA_N = 16;\n    const int M_GLOBAL = 4096;\n    const int K_GLOBAL = 4096;\n    const int N_GLOBAL = 1024;\n    const float SPARSITY = 0.75;\n    const int K_GLOBAL_SPARSE = (int(K_GLOBAL * (1-SPARSITY)));\n    const int M_TILES = (M_GLOBAL / M);\n    const int N_TILES = (N_GLOBAL / N);\n    const int K_TILES = (K_GLOBAL / K);\n    const int BLOCK_ROW_WARPS = 2;\n    const int BLOCK_COL_WARPS = 4;\n    const int WARP_ROW_TILES = 4;\n    const int WARP_COL_TILES = 2;\n    const int SKEW_UINT8 = 16;\n    const int BANK_VAL = 32;\n    const int NUM_BANK = (K_GLOBAL / BANK_VAL);\n    /////////// BLOCK_ROW_TILES <= N_TILES ////////////\n    // const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    // ////////// BLOCK_COL_TILES <= M_TILES ////////////\n    // const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    auto C_LAYOUT = wmma::mem_col_major;\n    const int CHUNK_K = 64;\n    const int CHUNK_K_SPARSE = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K_SPARSE = (int((CHUNK_K * K) * (1 - SPARSITY)));\n    const int WARP_COPY_BYTES = (WARP_SIZE * sizeof(int4));\n    const int CHUNK_LINE_BYTES_A (BLOCK_COL_TILES * M * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_A = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_A);\n    const int CHUNK_COPY_LINE_LANES_A = (CHUNK_LINE_BYTES_A / sizeof(int4));\n    const int SHARED_OFFSET_A = (BLOCK_COL_TILES * M + SKEW_UINT8);\n\n    const int CHUNK_LINE_BYTES_B = (BLOCK_SIZE_K_SPARSE * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_B = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_B);\n    const int CHUNK_COPY_LINE_LANES_B = (CHUNK_LINE_BYTES_B / sizeof(int4));\n    const int SHARED_OFFSET_B = (BLOCK_SIZE_K_SPARSE + SKEW_UINT8);\n    const int SHARED_TO_GLOBAL_BYTES_PER_LINE = ((WARP_COL_TILES * M) * sizeof(int));\n    const int SHARED_TO_GLOBAL_BYTES_PER_WARP = (WARP_SIZE * sizeof(int));\n    const int SHARED_TO_GLOBAL_LINES_PER_WARP = (SHARED_TO_GLOBAL_BYTES_PER_WARP / SHARED_TO_GLOBAL_BYTES_PER_LINE);\n    const int SHARED_TO_GLOBAL_LANES_PER_LINE = (WARP_SIZE / SHARED_TO_GLOBAL_LINES_PER_WARP);\n    const int SHARED_TO_GLOBAL_ITERS = ((WARP_ROW_TILES * N) / SHARED_TO_GLOBAL_LINES_PER_WARP);\n\n    const int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const int WARP_STRIDE = (WARP_COL_TILES * M);\n    const int WARPS_PER_BLOCK = (BLOCK_ROW_WARPS * BLOCK_COL_WARPS);\n    const int THREADS_PER_BLOCK = (WARP_SIZE * WARPS_PER_BLOCK);\n    const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    const int GLOBAL_MEM_STRIDE = M_GLOBAL;\n    const int SHMEM_STRIDE = (M * BLOCK_COL_TILES);\n    const int SHMEM_OFFSET = (M * WARP_COL_TILES);\n    const int BLOCK_SIZE_M = (M * BLOCK_COL_TILES);\n    const int BLOCK_SIZE_N = (N * BLOCK_ROW_TILES);\n\n   extern __shared__ uint8_t shmem[];\n\n\t// Warp and lane identification.\n\tconst unsigned int warpId = threadIdx.x / WARP_SIZE;\n\tconst unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n\n\t// Offset in shared memory from which the B matrix is stored.\n\t// const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\tconst size_t shmem_idx_b_off = BLOCK_SIZE_K_SPARSE * SHARED_OFFSET_A;\n\n\n\t// Each CTA slides along the 128 x 128 tiles from the top left corner of the\n\t// matrix to the right and down, and selects the next tile to compute. Once\n\t// there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n\tconst unsigned int block_tile_i =\n\t\t((block_pos * BLOCK_COL_TILES) / M_TILES) * (BLOCK_ROW_TILES);\n\tconst unsigned int block_tile_j = (block_pos * BLOCK_COL_TILES) % M_TILES;\n\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n\twmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_ROW_TILES]\n\t\t\t\t\t\t\t\t\t\t\t\t\t [WARP_COL_TILES];\n\n    // Load the C matrix tiles into fragments from shared memory.\n\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t\twmma::fill_fragment(c[i][j], 0);\n\t\t}\n\t}\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    //for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n    for(int tile_k_idx_sparse = 0, tile_k_idx = 0; tile_k_idx_sparse < K_GLOBAL_SPARSE; tile_k_idx_sparse += BLOCK_SIZE_K_SPARSE, tile_k_idx += BLOCK_SIZE_K){\n\n\t\tsize_t shmem_idx = \n\t\twarpId < (WARPS_PER_BLOCK / 2)\n\t\t\t? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A\n\t\t\t: (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B + shmem_idx_b_off;\n\n\t\tint4 *lane_ptr = NULL;\n\t\tint *lane_ptr_index = NULL;\n\t\tconst uint8_t *warp_ptr = NULL;\n\n\n\t\tif(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\t//warp_ptr = &A[block_tile_j * M] +\n\t\t\t//\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * M_GLOBAL;\n\t\t\twarp_ptr = &A[block_tile_j * M];\n\t\t\t\n\t\t\tconst int *warp_ptr_index = &B_index[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t\t\t\t\t\t((warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A);\n\n\t\t\tlane_ptr_index = (int *)(warp_ptr_index + tile_k_idx_sparse + (laneId / CHUNK_COPY_LINE_LANES_A));\n\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_A) * SHARED_OFFSET_A;\n\t\t}else{\n\t\t\twarp_ptr = &B[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * K_GLOBAL_SPARSE;\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_sparse +\n\t\t\t\t\t\t\t\t(laneId / CHUNK_COPY_LINE_LANES_B) * K_GLOBAL_SPARSE) +\n\t\t\t\t\t\t\t\t(laneId % CHUNK_COPY_LINE_LANES_B);\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_B) * SHARED_OFFSET_B;\n\t\t}\n\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      // shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n\t  int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n\t  \t? BLOCK_SIZE_K_SPARSE / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A)\n\t\t: BLOCK_SIZE_N / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\n\t  /*\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\t  */\n\n\t  /*\n      int tile_k_idx_A;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n          tile_k_idx_A = *(lane_ptr_index);\n      }\n\t  */\n\n\t  #pragma unroll\n\t  for(int i = 0; i < iter_index; i += 1){\n\t\t  if(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\tint tile_k_idx_A = *(lane_ptr_index);\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_A * M_GLOBAL) + (laneId % CHUNK_COPY_LINE_LANES_A);\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_A)) =\n\t\t\t\t*lane_ptr;\n\t\t\t//warp_ptr = (uint8_t *)((uint8_t *)warp_ptr + M_GLOBAL * (WARPS_PER_BLOCK / 2) *CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tlane_ptr_index = (int *)((int *)lane_ptr_index +  (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A;\n\t\t  }else{\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_B)) =\n\t\t\t\t*lane_ptr;\n\t\t\tlane_ptr = (int4 *)((uint8_t *)lane_ptr + K_GLOBAL_SPARSE * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B;\n\t\t  }\n\t  }\n\n      __syncthreads();\n\n\t#pragma unroll\n      for (int k_step = 0; k_step < CHUNK_K_SPARSE; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::col_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n\t#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i += 1) {\n\t\t\tsize_t shmem_idx_a = (warpId % BLOCK_COL_WARPS) * M * WARP_COL_TILES + (i * M);\n\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_a + k_step * K * SHARED_OFFSET_A;\n\n\t\t\twmma::load_matrix_sync(a[i], tile_ptr, SHARED_OFFSET_A);\n\t\t#pragma unroll\n\t\t\tfor(int j = 0; j < WARP_ROW_TILES; j += 1){\n\t\t\t\tif(i == 0){\n\t\t\t\t\tsize_t shmem_idx_b = shmem_idx_b_off +\n\t\t\t\t\t\t\t\t\t\t\t(warpId / BLOCK_COL_WARPS) * (WARP_ROW_TILES * N) * SHARED_OFFSET_B +\n\t\t\t\t\t\t\t\t\t\t\t(j * N) * SHARED_OFFSET_B;\n\t\t\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_b + k_step * K;\n\t\t\t\t\twmma::load_matrix_sync(b[j], tile_ptr, SHARED_OFFSET_B);\n\t\t\t\t}\n\t\t\t\twmma::mma_sync(c[j][i], a[i], b[j], c[j][i]);\n\t\t\t}\n\n        }\n      }\n\n      __syncthreads();\n    }\n\n    // This pointer is used to access the C and D matrix tiles this warp computes.\n\tint *shmem_warp_tile_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * N * WARP_ROW_TILES * SHMEM_STRIDE +\n\t(warpId % BLOCK_COL_WARPS) * SHMEM_OFFSET;\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t#pragma unroll\n\t\t\tfor(int t = 0; t < c[i][j].num_elements; t += 1){\n\t\t\t\tc[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n\t\t\t}\n\t\t\tint *tile_ptr = shmem_warp_tile_ptr + i * N * SHMEM_STRIDE + j * M;\n\t\t\twmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, C_LAYOUT);\n\t\t}\n\t}\n\n    __syncthreads();\n\n\tint *shmem_warp_stream_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N * SHMEM_STRIDE\n\t\t\t\t\t\t\t\t\t+ (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tconst size_t gmem_idx =\n\t\t(block_tile_i * N + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N) * GLOBAL_MEM_STRIDE +\n\t\tblock_tile_j * M + (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tuint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n\tint *shmem_lane_stream_ptr =\n\t\tshmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * SHMEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\t\n\tuint8_t *dst_gmem_lane_stream_ptr =\n\t\tdst_gmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * GLOBAL_MEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\n\tfor(int i = 0; i < WARP_ROW_TILES * N; i += SHARED_TO_GLOBAL_LINES_PER_WARP){\n\t\t*(dst_gmem_lane_stream_ptr + i * GLOBAL_MEM_STRIDE) = (uint8_t)(*(shmem_lane_stream_ptr + i * SHMEM_STRIDE));\n\t}\n\n\t__syncthreads();\n}\n", "launch_config": {"dimBlock": [256, 1], "dimGrid": [256, 1]}}, "7": {"code": "\n__global__ void compute_gemm_imma(float *input0, float *input1, float *input2, float* input3, float* input4, float*input5, float* input6,\n                                  float *output0) {\n    // keys need to be replaced 4096, 1024, K_GLOBA_VALUEL, 0.75, 64\n\n\t//extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B = reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_index = reinterpret_cast<int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n\tconst int WARP_SIZE = 32;\n    const int M = 16;\n    const int K = 16;\n    const int N = 16;\n    const int WMMA_M = 16;\n    const int WMMA_K = 16;\n    const int WMMA_N = 16;\n    const int M_GLOBAL = 4096;\n    const int K_GLOBAL = 1024;\n    const int N_GLOBAL = 1024;\n    const float SPARSITY = 0.75;\n    const int K_GLOBAL_SPARSE = (int(K_GLOBAL * (1-SPARSITY)));\n    const int M_TILES = (M_GLOBAL / M);\n    const int N_TILES = (N_GLOBAL / N);\n    const int K_TILES = (K_GLOBAL / K);\n    const int BLOCK_ROW_WARPS = 2;\n    const int BLOCK_COL_WARPS = 4;\n    const int WARP_ROW_TILES = 4;\n    const int WARP_COL_TILES = 2;\n    const int SKEW_UINT8 = 16;\n    const int BANK_VAL = 32;\n    const int NUM_BANK = (K_GLOBAL / BANK_VAL);\n    /////////// BLOCK_ROW_TILES <= N_TILES ////////////\n    // const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    // ////////// BLOCK_COL_TILES <= M_TILES ////////////\n    // const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    auto C_LAYOUT = wmma::mem_col_major;\n    const int CHUNK_K = 64;\n    const int CHUNK_K_SPARSE = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K_SPARSE = (int((CHUNK_K * K) * (1 - SPARSITY)));\n    const int WARP_COPY_BYTES = (WARP_SIZE * sizeof(int4));\n    const int CHUNK_LINE_BYTES_A (BLOCK_COL_TILES * M * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_A = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_A);\n    const int CHUNK_COPY_LINE_LANES_A = (CHUNK_LINE_BYTES_A / sizeof(int4));\n    const int SHARED_OFFSET_A = (BLOCK_COL_TILES * M + SKEW_UINT8);\n\n    const int CHUNK_LINE_BYTES_B = (BLOCK_SIZE_K_SPARSE * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_B = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_B);\n    const int CHUNK_COPY_LINE_LANES_B = (CHUNK_LINE_BYTES_B / sizeof(int4));\n    const int SHARED_OFFSET_B = (BLOCK_SIZE_K_SPARSE + SKEW_UINT8);\n    const int SHARED_TO_GLOBAL_BYTES_PER_LINE = ((WARP_COL_TILES * M) * sizeof(int));\n    const int SHARED_TO_GLOBAL_BYTES_PER_WARP = (WARP_SIZE * sizeof(int));\n    const int SHARED_TO_GLOBAL_LINES_PER_WARP = (SHARED_TO_GLOBAL_BYTES_PER_WARP / SHARED_TO_GLOBAL_BYTES_PER_LINE);\n    const int SHARED_TO_GLOBAL_LANES_PER_LINE = (WARP_SIZE / SHARED_TO_GLOBAL_LINES_PER_WARP);\n    const int SHARED_TO_GLOBAL_ITERS = ((WARP_ROW_TILES * N) / SHARED_TO_GLOBAL_LINES_PER_WARP);\n\n    const int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const int WARP_STRIDE = (WARP_COL_TILES * M);\n    const int WARPS_PER_BLOCK = (BLOCK_ROW_WARPS * BLOCK_COL_WARPS);\n    const int THREADS_PER_BLOCK = (WARP_SIZE * WARPS_PER_BLOCK);\n    const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    const int GLOBAL_MEM_STRIDE = M_GLOBAL;\n    const int SHMEM_STRIDE = (M * BLOCK_COL_TILES);\n    const int SHMEM_OFFSET = (M * WARP_COL_TILES);\n    const int BLOCK_SIZE_M = (M * BLOCK_COL_TILES);\n    const int BLOCK_SIZE_N = (N * BLOCK_ROW_TILES);\n\n   extern __shared__ uint8_t shmem[];\n\n\t// Warp and lane identification.\n\tconst unsigned int warpId = threadIdx.x / WARP_SIZE;\n\tconst unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n\n\t// Offset in shared memory from which the B matrix is stored.\n\t// const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\tconst size_t shmem_idx_b_off = BLOCK_SIZE_K_SPARSE * SHARED_OFFSET_A;\n\n\n\t// Each CTA slides along the 128 x 128 tiles from the top left corner of the\n\t// matrix to the right and down, and selects the next tile to compute. Once\n\t// there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n\tconst unsigned int block_tile_i =\n\t\t((block_pos * BLOCK_COL_TILES) / M_TILES) * (BLOCK_ROW_TILES);\n\tconst unsigned int block_tile_j = (block_pos * BLOCK_COL_TILES) % M_TILES;\n\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n\twmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_ROW_TILES]\n\t\t\t\t\t\t\t\t\t\t\t\t\t [WARP_COL_TILES];\n\n    // Load the C matrix tiles into fragments from shared memory.\n\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t\twmma::fill_fragment(c[i][j], 0);\n\t\t}\n\t}\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    //for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n    for(int tile_k_idx_sparse = 0, tile_k_idx = 0; tile_k_idx_sparse < K_GLOBAL_SPARSE; tile_k_idx_sparse += BLOCK_SIZE_K_SPARSE, tile_k_idx += BLOCK_SIZE_K){\n\n\t\tsize_t shmem_idx = \n\t\twarpId < (WARPS_PER_BLOCK / 2)\n\t\t\t? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A\n\t\t\t: (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B + shmem_idx_b_off;\n\n\t\tint4 *lane_ptr = NULL;\n\t\tint *lane_ptr_index = NULL;\n\t\tconst uint8_t *warp_ptr = NULL;\n\n\n\t\tif(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\t//warp_ptr = &A[block_tile_j * M] +\n\t\t\t//\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * M_GLOBAL;\n\t\t\twarp_ptr = &A[block_tile_j * M];\n\t\t\t\n\t\t\tconst int *warp_ptr_index = &B_index[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t\t\t\t\t\t((warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A);\n\n\t\t\tlane_ptr_index = (int *)(warp_ptr_index + tile_k_idx_sparse + (laneId / CHUNK_COPY_LINE_LANES_A));\n\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_A) * SHARED_OFFSET_A;\n\t\t}else{\n\t\t\twarp_ptr = &B[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * K_GLOBAL_SPARSE;\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_sparse +\n\t\t\t\t\t\t\t\t(laneId / CHUNK_COPY_LINE_LANES_B) * K_GLOBAL_SPARSE) +\n\t\t\t\t\t\t\t\t(laneId % CHUNK_COPY_LINE_LANES_B);\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_B) * SHARED_OFFSET_B;\n\t\t}\n\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      // shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n\t  int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n\t  \t? BLOCK_SIZE_K_SPARSE / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A)\n\t\t: BLOCK_SIZE_N / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\n\t  /*\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\t  */\n\n\t  /*\n      int tile_k_idx_A;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n          tile_k_idx_A = *(lane_ptr_index);\n      }\n\t  */\n\n\t  #pragma unroll\n\t  for(int i = 0; i < iter_index; i += 1){\n\t\t  if(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\tint tile_k_idx_A = *(lane_ptr_index);\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_A * M_GLOBAL) + (laneId % CHUNK_COPY_LINE_LANES_A);\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_A)) =\n\t\t\t\t*lane_ptr;\n\t\t\t//warp_ptr = (uint8_t *)((uint8_t *)warp_ptr + M_GLOBAL * (WARPS_PER_BLOCK / 2) *CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tlane_ptr_index = (int *)((int *)lane_ptr_index +  (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A;\n\t\t  }else{\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_B)) =\n\t\t\t\t*lane_ptr;\n\t\t\tlane_ptr = (int4 *)((uint8_t *)lane_ptr + K_GLOBAL_SPARSE * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B;\n\t\t  }\n\t  }\n\n      __syncthreads();\n\n\t#pragma unroll\n      for (int k_step = 0; k_step < CHUNK_K_SPARSE; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::col_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n\t#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i += 1) {\n\t\t\tsize_t shmem_idx_a = (warpId % BLOCK_COL_WARPS) * M * WARP_COL_TILES + (i * M);\n\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_a + k_step * K * SHARED_OFFSET_A;\n\n\t\t\twmma::load_matrix_sync(a[i], tile_ptr, SHARED_OFFSET_A);\n\t\t#pragma unroll\n\t\t\tfor(int j = 0; j < WARP_ROW_TILES; j += 1){\n\t\t\t\tif(i == 0){\n\t\t\t\t\tsize_t shmem_idx_b = shmem_idx_b_off +\n\t\t\t\t\t\t\t\t\t\t\t(warpId / BLOCK_COL_WARPS) * (WARP_ROW_TILES * N) * SHARED_OFFSET_B +\n\t\t\t\t\t\t\t\t\t\t\t(j * N) * SHARED_OFFSET_B;\n\t\t\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_b + k_step * K;\n\t\t\t\t\twmma::load_matrix_sync(b[j], tile_ptr, SHARED_OFFSET_B);\n\t\t\t\t}\n\t\t\t\twmma::mma_sync(c[j][i], a[i], b[j], c[j][i]);\n\t\t\t}\n\n        }\n      }\n\n      __syncthreads();\n    }\n\n    // This pointer is used to access the C and D matrix tiles this warp computes.\n\tint *shmem_warp_tile_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * N * WARP_ROW_TILES * SHMEM_STRIDE +\n\t(warpId % BLOCK_COL_WARPS) * SHMEM_OFFSET;\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t#pragma unroll\n\t\t\tfor(int t = 0; t < c[i][j].num_elements; t += 1){\n\t\t\t\tc[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n\t\t\t}\n\t\t\tint *tile_ptr = shmem_warp_tile_ptr + i * N * SHMEM_STRIDE + j * M;\n\t\t\twmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, C_LAYOUT);\n\t\t}\n\t}\n\n    __syncthreads();\n\n\tint *shmem_warp_stream_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N * SHMEM_STRIDE\n\t\t\t\t\t\t\t\t\t+ (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tconst size_t gmem_idx =\n\t\t(block_tile_i * N + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N) * GLOBAL_MEM_STRIDE +\n\t\tblock_tile_j * M + (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tuint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n\tint *shmem_lane_stream_ptr =\n\t\tshmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * SHMEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\t\n\tuint8_t *dst_gmem_lane_stream_ptr =\n\t\tdst_gmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * GLOBAL_MEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\n\tfor(int i = 0; i < WARP_ROW_TILES * N; i += SHARED_TO_GLOBAL_LINES_PER_WARP){\n\t\t*(dst_gmem_lane_stream_ptr + i * GLOBAL_MEM_STRIDE) = (uint8_t)(*(shmem_lane_stream_ptr + i * SHMEM_STRIDE));\n\t}\n\n\t__syncthreads();\n}\n", "launch_config": {"dimBlock": [256, 1], "dimGrid": [256, 1]}}, "8": {"code": "\n__global__ void compute_gemm_imma(float *input0, float *input1, float *input2, float* input3, float* input4, float*input5, float* input6,\n                                  float *output0) {\n    // keys need to be replaced 4096, 1024, K_GLOBA_VALUEL, 0.75, 64\n\n\t//extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B = reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_index = reinterpret_cast<int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n\tconst int WARP_SIZE = 32;\n    const int M = 16;\n    const int K = 16;\n    const int N = 16;\n    const int WMMA_M = 16;\n    const int WMMA_K = 16;\n    const int WMMA_N = 16;\n    const int M_GLOBAL = 4096;\n    const int K_GLOBAL = 1024;\n    const int N_GLOBAL = 1024;\n    const float SPARSITY = 0.75;\n    const int K_GLOBAL_SPARSE = (int(K_GLOBAL * (1-SPARSITY)));\n    const int M_TILES = (M_GLOBAL / M);\n    const int N_TILES = (N_GLOBAL / N);\n    const int K_TILES = (K_GLOBAL / K);\n    const int BLOCK_ROW_WARPS = 2;\n    const int BLOCK_COL_WARPS = 4;\n    const int WARP_ROW_TILES = 4;\n    const int WARP_COL_TILES = 2;\n    const int SKEW_UINT8 = 16;\n    const int BANK_VAL = 32;\n    const int NUM_BANK = (K_GLOBAL / BANK_VAL);\n    /////////// BLOCK_ROW_TILES <= N_TILES ////////////\n    // const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    // ////////// BLOCK_COL_TILES <= M_TILES ////////////\n    // const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    auto C_LAYOUT = wmma::mem_col_major;\n    const int CHUNK_K = 64;\n    const int CHUNK_K_SPARSE = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K_SPARSE = (int((CHUNK_K * K) * (1 - SPARSITY)));\n    const int WARP_COPY_BYTES = (WARP_SIZE * sizeof(int4));\n    const int CHUNK_LINE_BYTES_A (BLOCK_COL_TILES * M * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_A = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_A);\n    const int CHUNK_COPY_LINE_LANES_A = (CHUNK_LINE_BYTES_A / sizeof(int4));\n    const int SHARED_OFFSET_A = (BLOCK_COL_TILES * M + SKEW_UINT8);\n\n    const int CHUNK_LINE_BYTES_B = (BLOCK_SIZE_K_SPARSE * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_B = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_B);\n    const int CHUNK_COPY_LINE_LANES_B = (CHUNK_LINE_BYTES_B / sizeof(int4));\n    const int SHARED_OFFSET_B = (BLOCK_SIZE_K_SPARSE + SKEW_UINT8);\n    const int SHARED_TO_GLOBAL_BYTES_PER_LINE = ((WARP_COL_TILES * M) * sizeof(int));\n    const int SHARED_TO_GLOBAL_BYTES_PER_WARP = (WARP_SIZE * sizeof(int));\n    const int SHARED_TO_GLOBAL_LINES_PER_WARP = (SHARED_TO_GLOBAL_BYTES_PER_WARP / SHARED_TO_GLOBAL_BYTES_PER_LINE);\n    const int SHARED_TO_GLOBAL_LANES_PER_LINE = (WARP_SIZE / SHARED_TO_GLOBAL_LINES_PER_WARP);\n    const int SHARED_TO_GLOBAL_ITERS = ((WARP_ROW_TILES * N) / SHARED_TO_GLOBAL_LINES_PER_WARP);\n\n    const int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const int WARP_STRIDE = (WARP_COL_TILES * M);\n    const int WARPS_PER_BLOCK = (BLOCK_ROW_WARPS * BLOCK_COL_WARPS);\n    const int THREADS_PER_BLOCK = (WARP_SIZE * WARPS_PER_BLOCK);\n    const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    const int GLOBAL_MEM_STRIDE = M_GLOBAL;\n    const int SHMEM_STRIDE = (M * BLOCK_COL_TILES);\n    const int SHMEM_OFFSET = (M * WARP_COL_TILES);\n    const int BLOCK_SIZE_M = (M * BLOCK_COL_TILES);\n    const int BLOCK_SIZE_N = (N * BLOCK_ROW_TILES);\n\n   extern __shared__ uint8_t shmem[];\n\n\t// Warp and lane identification.\n\tconst unsigned int warpId = threadIdx.x / WARP_SIZE;\n\tconst unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n\n\t// Offset in shared memory from which the B matrix is stored.\n\t// const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\tconst size_t shmem_idx_b_off = BLOCK_SIZE_K_SPARSE * SHARED_OFFSET_A;\n\n\n\t// Each CTA slides along the 128 x 128 tiles from the top left corner of the\n\t// matrix to the right and down, and selects the next tile to compute. Once\n\t// there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n\tconst unsigned int block_tile_i =\n\t\t((block_pos * BLOCK_COL_TILES) / M_TILES) * (BLOCK_ROW_TILES);\n\tconst unsigned int block_tile_j = (block_pos * BLOCK_COL_TILES) % M_TILES;\n\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n\twmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_ROW_TILES]\n\t\t\t\t\t\t\t\t\t\t\t\t\t [WARP_COL_TILES];\n\n    // Load the C matrix tiles into fragments from shared memory.\n\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t\twmma::fill_fragment(c[i][j], 0);\n\t\t}\n\t}\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    //for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n    for(int tile_k_idx_sparse = 0, tile_k_idx = 0; tile_k_idx_sparse < K_GLOBAL_SPARSE; tile_k_idx_sparse += BLOCK_SIZE_K_SPARSE, tile_k_idx += BLOCK_SIZE_K){\n\n\t\tsize_t shmem_idx = \n\t\twarpId < (WARPS_PER_BLOCK / 2)\n\t\t\t? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A\n\t\t\t: (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B + shmem_idx_b_off;\n\n\t\tint4 *lane_ptr = NULL;\n\t\tint *lane_ptr_index = NULL;\n\t\tconst uint8_t *warp_ptr = NULL;\n\n\n\t\tif(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\t//warp_ptr = &A[block_tile_j * M] +\n\t\t\t//\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * M_GLOBAL;\n\t\t\twarp_ptr = &A[block_tile_j * M];\n\t\t\t\n\t\t\tconst int *warp_ptr_index = &B_index[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t\t\t\t\t\t((warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A);\n\n\t\t\tlane_ptr_index = (int *)(warp_ptr_index + tile_k_idx_sparse + (laneId / CHUNK_COPY_LINE_LANES_A));\n\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_A) * SHARED_OFFSET_A;\n\t\t}else{\n\t\t\twarp_ptr = &B[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * K_GLOBAL_SPARSE;\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_sparse +\n\t\t\t\t\t\t\t\t(laneId / CHUNK_COPY_LINE_LANES_B) * K_GLOBAL_SPARSE) +\n\t\t\t\t\t\t\t\t(laneId % CHUNK_COPY_LINE_LANES_B);\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_B) * SHARED_OFFSET_B;\n\t\t}\n\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      // shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n\t  int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n\t  \t? BLOCK_SIZE_K_SPARSE / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A)\n\t\t: BLOCK_SIZE_N / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\n\t  /*\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\t  */\n\n\t  /*\n      int tile_k_idx_A;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n          tile_k_idx_A = *(lane_ptr_index);\n      }\n\t  */\n\n\t  #pragma unroll\n\t  for(int i = 0; i < iter_index; i += 1){\n\t\t  if(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\tint tile_k_idx_A = *(lane_ptr_index);\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_A * M_GLOBAL) + (laneId % CHUNK_COPY_LINE_LANES_A);\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_A)) =\n\t\t\t\t*lane_ptr;\n\t\t\t//warp_ptr = (uint8_t *)((uint8_t *)warp_ptr + M_GLOBAL * (WARPS_PER_BLOCK / 2) *CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tlane_ptr_index = (int *)((int *)lane_ptr_index +  (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A;\n\t\t  }else{\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_B)) =\n\t\t\t\t*lane_ptr;\n\t\t\tlane_ptr = (int4 *)((uint8_t *)lane_ptr + K_GLOBAL_SPARSE * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B;\n\t\t  }\n\t  }\n\n      __syncthreads();\n\n\t#pragma unroll\n      for (int k_step = 0; k_step < CHUNK_K_SPARSE; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::col_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n\t#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i += 1) {\n\t\t\tsize_t shmem_idx_a = (warpId % BLOCK_COL_WARPS) * M * WARP_COL_TILES + (i * M);\n\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_a + k_step * K * SHARED_OFFSET_A;\n\n\t\t\twmma::load_matrix_sync(a[i], tile_ptr, SHARED_OFFSET_A);\n\t\t#pragma unroll\n\t\t\tfor(int j = 0; j < WARP_ROW_TILES; j += 1){\n\t\t\t\tif(i == 0){\n\t\t\t\t\tsize_t shmem_idx_b = shmem_idx_b_off +\n\t\t\t\t\t\t\t\t\t\t\t(warpId / BLOCK_COL_WARPS) * (WARP_ROW_TILES * N) * SHARED_OFFSET_B +\n\t\t\t\t\t\t\t\t\t\t\t(j * N) * SHARED_OFFSET_B;\n\t\t\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_b + k_step * K;\n\t\t\t\t\twmma::load_matrix_sync(b[j], tile_ptr, SHARED_OFFSET_B);\n\t\t\t\t}\n\t\t\t\twmma::mma_sync(c[j][i], a[i], b[j], c[j][i]);\n\t\t\t}\n\n        }\n      }\n\n      __syncthreads();\n    }\n\n    // This pointer is used to access the C and D matrix tiles this warp computes.\n\tint *shmem_warp_tile_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * N * WARP_ROW_TILES * SHMEM_STRIDE +\n\t(warpId % BLOCK_COL_WARPS) * SHMEM_OFFSET;\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t#pragma unroll\n\t\t\tfor(int t = 0; t < c[i][j].num_elements; t += 1){\n\t\t\t\tc[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n\t\t\t}\n\t\t\tint *tile_ptr = shmem_warp_tile_ptr + i * N * SHMEM_STRIDE + j * M;\n\t\t\twmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, C_LAYOUT);\n\t\t}\n\t}\n\n    __syncthreads();\n\n\tint *shmem_warp_stream_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N * SHMEM_STRIDE\n\t\t\t\t\t\t\t\t\t+ (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tconst size_t gmem_idx =\n\t\t(block_tile_i * N + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N) * GLOBAL_MEM_STRIDE +\n\t\tblock_tile_j * M + (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tuint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n\tint *shmem_lane_stream_ptr =\n\t\tshmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * SHMEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\t\n\tuint8_t *dst_gmem_lane_stream_ptr =\n\t\tdst_gmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * GLOBAL_MEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\n\tfor(int i = 0; i < WARP_ROW_TILES * N; i += SHARED_TO_GLOBAL_LINES_PER_WARP){\n\t\t*(dst_gmem_lane_stream_ptr + i * GLOBAL_MEM_STRIDE) = (uint8_t)(*(shmem_lane_stream_ptr + i * SHMEM_STRIDE));\n\t}\n\n\t__syncthreads();\n}\n", "launch_config": {"dimBlock": [256, 1], "dimGrid": [256, 1]}}, "9": {"code": "\n__global__ void compute_gemm_imma(float *input0, float *input1, float *input2, float* input3, float* input4, float*input5, float* input6,\n                                  float *output0) {\n    // keys need to be replaced 4096, 1024, K_GLOBA_VALUEL, 0.75, 64\n\n\t//extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B = reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_index = reinterpret_cast<int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n\tconst int WARP_SIZE = 32;\n    const int M = 16;\n    const int K = 16;\n    const int N = 16;\n    const int WMMA_M = 16;\n    const int WMMA_K = 16;\n    const int WMMA_N = 16;\n    const int M_GLOBAL = 4096;\n    const int K_GLOBAL = 1024;\n    const int N_GLOBAL = 1024;\n    const float SPARSITY = 0.75;\n    const int K_GLOBAL_SPARSE = (int(K_GLOBAL * (1-SPARSITY)));\n    const int M_TILES = (M_GLOBAL / M);\n    const int N_TILES = (N_GLOBAL / N);\n    const int K_TILES = (K_GLOBAL / K);\n    const int BLOCK_ROW_WARPS = 2;\n    const int BLOCK_COL_WARPS = 4;\n    const int WARP_ROW_TILES = 4;\n    const int WARP_COL_TILES = 2;\n    const int SKEW_UINT8 = 16;\n    const int BANK_VAL = 32;\n    const int NUM_BANK = (K_GLOBAL / BANK_VAL);\n    /////////// BLOCK_ROW_TILES <= N_TILES ////////////\n    // const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    // ////////// BLOCK_COL_TILES <= M_TILES ////////////\n    // const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    auto C_LAYOUT = wmma::mem_col_major;\n    const int CHUNK_K = 64;\n    const int CHUNK_K_SPARSE = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K_SPARSE = (int((CHUNK_K * K) * (1 - SPARSITY)));\n    const int WARP_COPY_BYTES = (WARP_SIZE * sizeof(int4));\n    const int CHUNK_LINE_BYTES_A (BLOCK_COL_TILES * M * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_A = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_A);\n    const int CHUNK_COPY_LINE_LANES_A = (CHUNK_LINE_BYTES_A / sizeof(int4));\n    const int SHARED_OFFSET_A = (BLOCK_COL_TILES * M + SKEW_UINT8);\n\n    const int CHUNK_LINE_BYTES_B = (BLOCK_SIZE_K_SPARSE * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_B = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_B);\n    const int CHUNK_COPY_LINE_LANES_B = (CHUNK_LINE_BYTES_B / sizeof(int4));\n    const int SHARED_OFFSET_B = (BLOCK_SIZE_K_SPARSE + SKEW_UINT8);\n    const int SHARED_TO_GLOBAL_BYTES_PER_LINE = ((WARP_COL_TILES * M) * sizeof(int));\n    const int SHARED_TO_GLOBAL_BYTES_PER_WARP = (WARP_SIZE * sizeof(int));\n    const int SHARED_TO_GLOBAL_LINES_PER_WARP = (SHARED_TO_GLOBAL_BYTES_PER_WARP / SHARED_TO_GLOBAL_BYTES_PER_LINE);\n    const int SHARED_TO_GLOBAL_LANES_PER_LINE = (WARP_SIZE / SHARED_TO_GLOBAL_LINES_PER_WARP);\n    const int SHARED_TO_GLOBAL_ITERS = ((WARP_ROW_TILES * N) / SHARED_TO_GLOBAL_LINES_PER_WARP);\n\n    const int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const int WARP_STRIDE = (WARP_COL_TILES * M);\n    const int WARPS_PER_BLOCK = (BLOCK_ROW_WARPS * BLOCK_COL_WARPS);\n    const int THREADS_PER_BLOCK = (WARP_SIZE * WARPS_PER_BLOCK);\n    const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    const int GLOBAL_MEM_STRIDE = M_GLOBAL;\n    const int SHMEM_STRIDE = (M * BLOCK_COL_TILES);\n    const int SHMEM_OFFSET = (M * WARP_COL_TILES);\n    const int BLOCK_SIZE_M = (M * BLOCK_COL_TILES);\n    const int BLOCK_SIZE_N = (N * BLOCK_ROW_TILES);\n\n   extern __shared__ uint8_t shmem[];\n\n\t// Warp and lane identification.\n\tconst unsigned int warpId = threadIdx.x / WARP_SIZE;\n\tconst unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n\n\t// Offset in shared memory from which the B matrix is stored.\n\t// const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\tconst size_t shmem_idx_b_off = BLOCK_SIZE_K_SPARSE * SHARED_OFFSET_A;\n\n\n\t// Each CTA slides along the 128 x 128 tiles from the top left corner of the\n\t// matrix to the right and down, and selects the next tile to compute. Once\n\t// there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n\tconst unsigned int block_tile_i =\n\t\t((block_pos * BLOCK_COL_TILES) / M_TILES) * (BLOCK_ROW_TILES);\n\tconst unsigned int block_tile_j = (block_pos * BLOCK_COL_TILES) % M_TILES;\n\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n\twmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_ROW_TILES]\n\t\t\t\t\t\t\t\t\t\t\t\t\t [WARP_COL_TILES];\n\n    // Load the C matrix tiles into fragments from shared memory.\n\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t\twmma::fill_fragment(c[i][j], 0);\n\t\t}\n\t}\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    //for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n    for(int tile_k_idx_sparse = 0, tile_k_idx = 0; tile_k_idx_sparse < K_GLOBAL_SPARSE; tile_k_idx_sparse += BLOCK_SIZE_K_SPARSE, tile_k_idx += BLOCK_SIZE_K){\n\n\t\tsize_t shmem_idx = \n\t\twarpId < (WARPS_PER_BLOCK / 2)\n\t\t\t? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A\n\t\t\t: (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B + shmem_idx_b_off;\n\n\t\tint4 *lane_ptr = NULL;\n\t\tint *lane_ptr_index = NULL;\n\t\tconst uint8_t *warp_ptr = NULL;\n\n\n\t\tif(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\t//warp_ptr = &A[block_tile_j * M] +\n\t\t\t//\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * M_GLOBAL;\n\t\t\twarp_ptr = &A[block_tile_j * M];\n\t\t\t\n\t\t\tconst int *warp_ptr_index = &B_index[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t\t\t\t\t\t((warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A);\n\n\t\t\tlane_ptr_index = (int *)(warp_ptr_index + tile_k_idx_sparse + (laneId / CHUNK_COPY_LINE_LANES_A));\n\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_A) * SHARED_OFFSET_A;\n\t\t}else{\n\t\t\twarp_ptr = &B[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * K_GLOBAL_SPARSE;\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_sparse +\n\t\t\t\t\t\t\t\t(laneId / CHUNK_COPY_LINE_LANES_B) * K_GLOBAL_SPARSE) +\n\t\t\t\t\t\t\t\t(laneId % CHUNK_COPY_LINE_LANES_B);\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_B) * SHARED_OFFSET_B;\n\t\t}\n\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      // shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n\t  int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n\t  \t? BLOCK_SIZE_K_SPARSE / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A)\n\t\t: BLOCK_SIZE_N / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\n\t  /*\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\t  */\n\n\t  /*\n      int tile_k_idx_A;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n          tile_k_idx_A = *(lane_ptr_index);\n      }\n\t  */\n\n\t  #pragma unroll\n\t  for(int i = 0; i < iter_index; i += 1){\n\t\t  if(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\tint tile_k_idx_A = *(lane_ptr_index);\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_A * M_GLOBAL) + (laneId % CHUNK_COPY_LINE_LANES_A);\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_A)) =\n\t\t\t\t*lane_ptr;\n\t\t\t//warp_ptr = (uint8_t *)((uint8_t *)warp_ptr + M_GLOBAL * (WARPS_PER_BLOCK / 2) *CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tlane_ptr_index = (int *)((int *)lane_ptr_index +  (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A;\n\t\t  }else{\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_B)) =\n\t\t\t\t*lane_ptr;\n\t\t\tlane_ptr = (int4 *)((uint8_t *)lane_ptr + K_GLOBAL_SPARSE * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B;\n\t\t  }\n\t  }\n\n      __syncthreads();\n\n\t#pragma unroll\n      for (int k_step = 0; k_step < CHUNK_K_SPARSE; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::col_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n\t#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i += 1) {\n\t\t\tsize_t shmem_idx_a = (warpId % BLOCK_COL_WARPS) * M * WARP_COL_TILES + (i * M);\n\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_a + k_step * K * SHARED_OFFSET_A;\n\n\t\t\twmma::load_matrix_sync(a[i], tile_ptr, SHARED_OFFSET_A);\n\t\t#pragma unroll\n\t\t\tfor(int j = 0; j < WARP_ROW_TILES; j += 1){\n\t\t\t\tif(i == 0){\n\t\t\t\t\tsize_t shmem_idx_b = shmem_idx_b_off +\n\t\t\t\t\t\t\t\t\t\t\t(warpId / BLOCK_COL_WARPS) * (WARP_ROW_TILES * N) * SHARED_OFFSET_B +\n\t\t\t\t\t\t\t\t\t\t\t(j * N) * SHARED_OFFSET_B;\n\t\t\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_b + k_step * K;\n\t\t\t\t\twmma::load_matrix_sync(b[j], tile_ptr, SHARED_OFFSET_B);\n\t\t\t\t}\n\t\t\t\twmma::mma_sync(c[j][i], a[i], b[j], c[j][i]);\n\t\t\t}\n\n        }\n      }\n\n      __syncthreads();\n    }\n\n    // This pointer is used to access the C and D matrix tiles this warp computes.\n\tint *shmem_warp_tile_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * N * WARP_ROW_TILES * SHMEM_STRIDE +\n\t(warpId % BLOCK_COL_WARPS) * SHMEM_OFFSET;\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t#pragma unroll\n\t\t\tfor(int t = 0; t < c[i][j].num_elements; t += 1){\n\t\t\t\tc[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n\t\t\t}\n\t\t\tint *tile_ptr = shmem_warp_tile_ptr + i * N * SHMEM_STRIDE + j * M;\n\t\t\twmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, C_LAYOUT);\n\t\t}\n\t}\n\n    __syncthreads();\n\n\tint *shmem_warp_stream_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N * SHMEM_STRIDE\n\t\t\t\t\t\t\t\t\t+ (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tconst size_t gmem_idx =\n\t\t(block_tile_i * N + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N) * GLOBAL_MEM_STRIDE +\n\t\tblock_tile_j * M + (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tuint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n\tint *shmem_lane_stream_ptr =\n\t\tshmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * SHMEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\t\n\tuint8_t *dst_gmem_lane_stream_ptr =\n\t\tdst_gmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * GLOBAL_MEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\n\tfor(int i = 0; i < WARP_ROW_TILES * N; i += SHARED_TO_GLOBAL_LINES_PER_WARP){\n\t\t*(dst_gmem_lane_stream_ptr + i * GLOBAL_MEM_STRIDE) = (uint8_t)(*(shmem_lane_stream_ptr + i * SHMEM_STRIDE));\n\t}\n\n\t__syncthreads();\n}\n", "launch_config": {"dimBlock": [256, 1], "dimGrid": [256, 1]}}, "10": {"code": "\n__global__ void compute_gemm_imma(float *input0, float *input1, float *input2, float* input3, float* input4, float*input5, float* input6,\n                                  float *output0) {\n    // keys need to be replaced 4096, 1024, K_GLOBA_VALUEL, 0.75, 64\n\n\t//extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B = reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_index = reinterpret_cast<int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n\tconst int WARP_SIZE = 32;\n    const int M = 16;\n    const int K = 16;\n    const int N = 16;\n    const int WMMA_M = 16;\n    const int WMMA_K = 16;\n    const int WMMA_N = 16;\n    const int M_GLOBAL = 4096;\n    const int K_GLOBAL = 1024;\n    const int N_GLOBAL = 1024;\n    const float SPARSITY = 0.75;\n    const int K_GLOBAL_SPARSE = (int(K_GLOBAL * (1-SPARSITY)));\n    const int M_TILES = (M_GLOBAL / M);\n    const int N_TILES = (N_GLOBAL / N);\n    const int K_TILES = (K_GLOBAL / K);\n    const int BLOCK_ROW_WARPS = 2;\n    const int BLOCK_COL_WARPS = 4;\n    const int WARP_ROW_TILES = 4;\n    const int WARP_COL_TILES = 2;\n    const int SKEW_UINT8 = 16;\n    const int BANK_VAL = 32;\n    const int NUM_BANK = (K_GLOBAL / BANK_VAL);\n    /////////// BLOCK_ROW_TILES <= N_TILES ////////////\n    // const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    // ////////// BLOCK_COL_TILES <= M_TILES ////////////\n    // const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    auto C_LAYOUT = wmma::mem_col_major;\n    const int CHUNK_K = 64;\n    const int CHUNK_K_SPARSE = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K_SPARSE = (int((CHUNK_K * K) * (1 - SPARSITY)));\n    const int WARP_COPY_BYTES = (WARP_SIZE * sizeof(int4));\n    const int CHUNK_LINE_BYTES_A (BLOCK_COL_TILES * M * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_A = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_A);\n    const int CHUNK_COPY_LINE_LANES_A = (CHUNK_LINE_BYTES_A / sizeof(int4));\n    const int SHARED_OFFSET_A = (BLOCK_COL_TILES * M + SKEW_UINT8);\n\n    const int CHUNK_LINE_BYTES_B = (BLOCK_SIZE_K_SPARSE * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_B = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_B);\n    const int CHUNK_COPY_LINE_LANES_B = (CHUNK_LINE_BYTES_B / sizeof(int4));\n    const int SHARED_OFFSET_B = (BLOCK_SIZE_K_SPARSE + SKEW_UINT8);\n    const int SHARED_TO_GLOBAL_BYTES_PER_LINE = ((WARP_COL_TILES * M) * sizeof(int));\n    const int SHARED_TO_GLOBAL_BYTES_PER_WARP = (WARP_SIZE * sizeof(int));\n    const int SHARED_TO_GLOBAL_LINES_PER_WARP = (SHARED_TO_GLOBAL_BYTES_PER_WARP / SHARED_TO_GLOBAL_BYTES_PER_LINE);\n    const int SHARED_TO_GLOBAL_LANES_PER_LINE = (WARP_SIZE / SHARED_TO_GLOBAL_LINES_PER_WARP);\n    const int SHARED_TO_GLOBAL_ITERS = ((WARP_ROW_TILES * N) / SHARED_TO_GLOBAL_LINES_PER_WARP);\n\n    const int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const int WARP_STRIDE = (WARP_COL_TILES * M);\n    const int WARPS_PER_BLOCK = (BLOCK_ROW_WARPS * BLOCK_COL_WARPS);\n    const int THREADS_PER_BLOCK = (WARP_SIZE * WARPS_PER_BLOCK);\n    const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    const int GLOBAL_MEM_STRIDE = M_GLOBAL;\n    const int SHMEM_STRIDE = (M * BLOCK_COL_TILES);\n    const int SHMEM_OFFSET = (M * WARP_COL_TILES);\n    const int BLOCK_SIZE_M = (M * BLOCK_COL_TILES);\n    const int BLOCK_SIZE_N = (N * BLOCK_ROW_TILES);\n\n   extern __shared__ uint8_t shmem[];\n\n\t// Warp and lane identification.\n\tconst unsigned int warpId = threadIdx.x / WARP_SIZE;\n\tconst unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n\n\t// Offset in shared memory from which the B matrix is stored.\n\t// const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\tconst size_t shmem_idx_b_off = BLOCK_SIZE_K_SPARSE * SHARED_OFFSET_A;\n\n\n\t// Each CTA slides along the 128 x 128 tiles from the top left corner of the\n\t// matrix to the right and down, and selects the next tile to compute. Once\n\t// there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n\tconst unsigned int block_tile_i =\n\t\t((block_pos * BLOCK_COL_TILES) / M_TILES) * (BLOCK_ROW_TILES);\n\tconst unsigned int block_tile_j = (block_pos * BLOCK_COL_TILES) % M_TILES;\n\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n\twmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_ROW_TILES]\n\t\t\t\t\t\t\t\t\t\t\t\t\t [WARP_COL_TILES];\n\n    // Load the C matrix tiles into fragments from shared memory.\n\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t\twmma::fill_fragment(c[i][j], 0);\n\t\t}\n\t}\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    //for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n    for(int tile_k_idx_sparse = 0, tile_k_idx = 0; tile_k_idx_sparse < K_GLOBAL_SPARSE; tile_k_idx_sparse += BLOCK_SIZE_K_SPARSE, tile_k_idx += BLOCK_SIZE_K){\n\n\t\tsize_t shmem_idx = \n\t\twarpId < (WARPS_PER_BLOCK / 2)\n\t\t\t? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A\n\t\t\t: (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B + shmem_idx_b_off;\n\n\t\tint4 *lane_ptr = NULL;\n\t\tint *lane_ptr_index = NULL;\n\t\tconst uint8_t *warp_ptr = NULL;\n\n\n\t\tif(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\t//warp_ptr = &A[block_tile_j * M] +\n\t\t\t//\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * M_GLOBAL;\n\t\t\twarp_ptr = &A[block_tile_j * M];\n\t\t\t\n\t\t\tconst int *warp_ptr_index = &B_index[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t\t\t\t\t\t((warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A);\n\n\t\t\tlane_ptr_index = (int *)(warp_ptr_index + tile_k_idx_sparse + (laneId / CHUNK_COPY_LINE_LANES_A));\n\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_A) * SHARED_OFFSET_A;\n\t\t}else{\n\t\t\twarp_ptr = &B[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * K_GLOBAL_SPARSE;\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_sparse +\n\t\t\t\t\t\t\t\t(laneId / CHUNK_COPY_LINE_LANES_B) * K_GLOBAL_SPARSE) +\n\t\t\t\t\t\t\t\t(laneId % CHUNK_COPY_LINE_LANES_B);\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_B) * SHARED_OFFSET_B;\n\t\t}\n\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      // shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n\t  int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n\t  \t? BLOCK_SIZE_K_SPARSE / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A)\n\t\t: BLOCK_SIZE_N / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\n\t  /*\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\t  */\n\n\t  /*\n      int tile_k_idx_A;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n          tile_k_idx_A = *(lane_ptr_index);\n      }\n\t  */\n\n\t  #pragma unroll\n\t  for(int i = 0; i < iter_index; i += 1){\n\t\t  if(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\tint tile_k_idx_A = *(lane_ptr_index);\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_A * M_GLOBAL) + (laneId % CHUNK_COPY_LINE_LANES_A);\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_A)) =\n\t\t\t\t*lane_ptr;\n\t\t\t//warp_ptr = (uint8_t *)((uint8_t *)warp_ptr + M_GLOBAL * (WARPS_PER_BLOCK / 2) *CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tlane_ptr_index = (int *)((int *)lane_ptr_index +  (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A;\n\t\t  }else{\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_B)) =\n\t\t\t\t*lane_ptr;\n\t\t\tlane_ptr = (int4 *)((uint8_t *)lane_ptr + K_GLOBAL_SPARSE * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B;\n\t\t  }\n\t  }\n\n      __syncthreads();\n\n\t#pragma unroll\n      for (int k_step = 0; k_step < CHUNK_K_SPARSE; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::col_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n\t#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i += 1) {\n\t\t\tsize_t shmem_idx_a = (warpId % BLOCK_COL_WARPS) * M * WARP_COL_TILES + (i * M);\n\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_a + k_step * K * SHARED_OFFSET_A;\n\n\t\t\twmma::load_matrix_sync(a[i], tile_ptr, SHARED_OFFSET_A);\n\t\t#pragma unroll\n\t\t\tfor(int j = 0; j < WARP_ROW_TILES; j += 1){\n\t\t\t\tif(i == 0){\n\t\t\t\t\tsize_t shmem_idx_b = shmem_idx_b_off +\n\t\t\t\t\t\t\t\t\t\t\t(warpId / BLOCK_COL_WARPS) * (WARP_ROW_TILES * N) * SHARED_OFFSET_B +\n\t\t\t\t\t\t\t\t\t\t\t(j * N) * SHARED_OFFSET_B;\n\t\t\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_b + k_step * K;\n\t\t\t\t\twmma::load_matrix_sync(b[j], tile_ptr, SHARED_OFFSET_B);\n\t\t\t\t}\n\t\t\t\twmma::mma_sync(c[j][i], a[i], b[j], c[j][i]);\n\t\t\t}\n\n        }\n      }\n\n      __syncthreads();\n    }\n\n    // This pointer is used to access the C and D matrix tiles this warp computes.\n\tint *shmem_warp_tile_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * N * WARP_ROW_TILES * SHMEM_STRIDE +\n\t(warpId % BLOCK_COL_WARPS) * SHMEM_OFFSET;\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t#pragma unroll\n\t\t\tfor(int t = 0; t < c[i][j].num_elements; t += 1){\n\t\t\t\tc[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n\t\t\t}\n\t\t\tint *tile_ptr = shmem_warp_tile_ptr + i * N * SHMEM_STRIDE + j * M;\n\t\t\twmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, C_LAYOUT);\n\t\t}\n\t}\n\n    __syncthreads();\n\n\tint *shmem_warp_stream_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N * SHMEM_STRIDE\n\t\t\t\t\t\t\t\t\t+ (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tconst size_t gmem_idx =\n\t\t(block_tile_i * N + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N) * GLOBAL_MEM_STRIDE +\n\t\tblock_tile_j * M + (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tuint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n\tint *shmem_lane_stream_ptr =\n\t\tshmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * SHMEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\t\n\tuint8_t *dst_gmem_lane_stream_ptr =\n\t\tdst_gmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * GLOBAL_MEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\n\tfor(int i = 0; i < WARP_ROW_TILES * N; i += SHARED_TO_GLOBAL_LINES_PER_WARP){\n\t\t*(dst_gmem_lane_stream_ptr + i * GLOBAL_MEM_STRIDE) = (uint8_t)(*(shmem_lane_stream_ptr + i * SHMEM_STRIDE));\n\t}\n\n\t__syncthreads();\n}\n", "launch_config": {"dimBlock": [256, 1], "dimGrid": [256, 1]}}, "11": {"code": "\n__global__ void compute_gemm_imma(float *input0, float *input1, float *input2, float* input3, float* input4, float*input5, float* input6,\n                                  float *output0) {\n    // keys need to be replaced 4096, 4096, K_GLOBA_VALUEL, 0.75, 64\n\n\t//extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B = reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_index = reinterpret_cast<int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n\tconst int WARP_SIZE = 32;\n    const int M = 16;\n    const int K = 16;\n    const int N = 16;\n    const int WMMA_M = 16;\n    const int WMMA_K = 16;\n    const int WMMA_N = 16;\n    const int M_GLOBAL = 4096;\n    const int K_GLOBAL = 1024;\n    const int N_GLOBAL = 4096;\n    const float SPARSITY = 0.75;\n    const int K_GLOBAL_SPARSE = (int(K_GLOBAL * (1-SPARSITY)));\n    const int M_TILES = (M_GLOBAL / M);\n    const int N_TILES = (N_GLOBAL / N);\n    const int K_TILES = (K_GLOBAL / K);\n    const int BLOCK_ROW_WARPS = 2;\n    const int BLOCK_COL_WARPS = 4;\n    const int WARP_ROW_TILES = 4;\n    const int WARP_COL_TILES = 2;\n    const int SKEW_UINT8 = 16;\n    const int BANK_VAL = 32;\n    const int NUM_BANK = (K_GLOBAL / BANK_VAL);\n    /////////// BLOCK_ROW_TILES <= N_TILES ////////////\n    // const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    // ////////// BLOCK_COL_TILES <= M_TILES ////////////\n    // const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    auto C_LAYOUT = wmma::mem_col_major;\n    const int CHUNK_K = 64;\n    const int CHUNK_K_SPARSE = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K_SPARSE = (int((CHUNK_K * K) * (1 - SPARSITY)));\n    const int WARP_COPY_BYTES = (WARP_SIZE * sizeof(int4));\n    const int CHUNK_LINE_BYTES_A (BLOCK_COL_TILES * M * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_A = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_A);\n    const int CHUNK_COPY_LINE_LANES_A = (CHUNK_LINE_BYTES_A / sizeof(int4));\n    const int SHARED_OFFSET_A = (BLOCK_COL_TILES * M + SKEW_UINT8);\n\n    const int CHUNK_LINE_BYTES_B = (BLOCK_SIZE_K_SPARSE * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_B = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_B);\n    const int CHUNK_COPY_LINE_LANES_B = (CHUNK_LINE_BYTES_B / sizeof(int4));\n    const int SHARED_OFFSET_B = (BLOCK_SIZE_K_SPARSE + SKEW_UINT8);\n    const int SHARED_TO_GLOBAL_BYTES_PER_LINE = ((WARP_COL_TILES * M) * sizeof(int));\n    const int SHARED_TO_GLOBAL_BYTES_PER_WARP = (WARP_SIZE * sizeof(int));\n    const int SHARED_TO_GLOBAL_LINES_PER_WARP = (SHARED_TO_GLOBAL_BYTES_PER_WARP / SHARED_TO_GLOBAL_BYTES_PER_LINE);\n    const int SHARED_TO_GLOBAL_LANES_PER_LINE = (WARP_SIZE / SHARED_TO_GLOBAL_LINES_PER_WARP);\n    const int SHARED_TO_GLOBAL_ITERS = ((WARP_ROW_TILES * N) / SHARED_TO_GLOBAL_LINES_PER_WARP);\n\n    const int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const int WARP_STRIDE = (WARP_COL_TILES * M);\n    const int WARPS_PER_BLOCK = (BLOCK_ROW_WARPS * BLOCK_COL_WARPS);\n    const int THREADS_PER_BLOCK = (WARP_SIZE * WARPS_PER_BLOCK);\n    const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    const int GLOBAL_MEM_STRIDE = M_GLOBAL;\n    const int SHMEM_STRIDE = (M * BLOCK_COL_TILES);\n    const int SHMEM_OFFSET = (M * WARP_COL_TILES);\n    const int BLOCK_SIZE_M = (M * BLOCK_COL_TILES);\n    const int BLOCK_SIZE_N = (N * BLOCK_ROW_TILES);\n\n   extern __shared__ uint8_t shmem[];\n\n\t// Warp and lane identification.\n\tconst unsigned int warpId = threadIdx.x / WARP_SIZE;\n\tconst unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n\n\t// Offset in shared memory from which the B matrix is stored.\n\t// const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\tconst size_t shmem_idx_b_off = BLOCK_SIZE_K_SPARSE * SHARED_OFFSET_A;\n\n\n\t// Each CTA slides along the 128 x 128 tiles from the top left corner of the\n\t// matrix to the right and down, and selects the next tile to compute. Once\n\t// there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n\tconst unsigned int block_tile_i =\n\t\t((block_pos * BLOCK_COL_TILES) / M_TILES) * (BLOCK_ROW_TILES);\n\tconst unsigned int block_tile_j = (block_pos * BLOCK_COL_TILES) % M_TILES;\n\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n\twmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_ROW_TILES]\n\t\t\t\t\t\t\t\t\t\t\t\t\t [WARP_COL_TILES];\n\n    // Load the C matrix tiles into fragments from shared memory.\n\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t\twmma::fill_fragment(c[i][j], 0);\n\t\t}\n\t}\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    //for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n    for(int tile_k_idx_sparse = 0, tile_k_idx = 0; tile_k_idx_sparse < K_GLOBAL_SPARSE; tile_k_idx_sparse += BLOCK_SIZE_K_SPARSE, tile_k_idx += BLOCK_SIZE_K){\n\n\t\tsize_t shmem_idx = \n\t\twarpId < (WARPS_PER_BLOCK / 2)\n\t\t\t? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A\n\t\t\t: (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B + shmem_idx_b_off;\n\n\t\tint4 *lane_ptr = NULL;\n\t\tint *lane_ptr_index = NULL;\n\t\tconst uint8_t *warp_ptr = NULL;\n\n\n\t\tif(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\t//warp_ptr = &A[block_tile_j * M] +\n\t\t\t//\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * M_GLOBAL;\n\t\t\twarp_ptr = &A[block_tile_j * M];\n\t\t\t\n\t\t\tconst int *warp_ptr_index = &B_index[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t\t\t\t\t\t((warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A);\n\n\t\t\tlane_ptr_index = (int *)(warp_ptr_index + tile_k_idx_sparse + (laneId / CHUNK_COPY_LINE_LANES_A));\n\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_A) * SHARED_OFFSET_A;\n\t\t}else{\n\t\t\twarp_ptr = &B[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * K_GLOBAL_SPARSE;\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_sparse +\n\t\t\t\t\t\t\t\t(laneId / CHUNK_COPY_LINE_LANES_B) * K_GLOBAL_SPARSE) +\n\t\t\t\t\t\t\t\t(laneId % CHUNK_COPY_LINE_LANES_B);\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_B) * SHARED_OFFSET_B;\n\t\t}\n\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      // shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n\t  int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n\t  \t? BLOCK_SIZE_K_SPARSE / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A)\n\t\t: BLOCK_SIZE_N / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\n\t  /*\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\t  */\n\n\t  /*\n      int tile_k_idx_A;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n          tile_k_idx_A = *(lane_ptr_index);\n      }\n\t  */\n\n\t  #pragma unroll\n\t  for(int i = 0; i < iter_index; i += 1){\n\t\t  if(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\tint tile_k_idx_A = *(lane_ptr_index);\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_A * M_GLOBAL) + (laneId % CHUNK_COPY_LINE_LANES_A);\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_A)) =\n\t\t\t\t*lane_ptr;\n\t\t\t//warp_ptr = (uint8_t *)((uint8_t *)warp_ptr + M_GLOBAL * (WARPS_PER_BLOCK / 2) *CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tlane_ptr_index = (int *)((int *)lane_ptr_index +  (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A;\n\t\t  }else{\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_B)) =\n\t\t\t\t*lane_ptr;\n\t\t\tlane_ptr = (int4 *)((uint8_t *)lane_ptr + K_GLOBAL_SPARSE * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B;\n\t\t  }\n\t  }\n\n      __syncthreads();\n\n\t#pragma unroll\n      for (int k_step = 0; k_step < CHUNK_K_SPARSE; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::col_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n\t#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i += 1) {\n\t\t\tsize_t shmem_idx_a = (warpId % BLOCK_COL_WARPS) * M * WARP_COL_TILES + (i * M);\n\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_a + k_step * K * SHARED_OFFSET_A;\n\n\t\t\twmma::load_matrix_sync(a[i], tile_ptr, SHARED_OFFSET_A);\n\t\t#pragma unroll\n\t\t\tfor(int j = 0; j < WARP_ROW_TILES; j += 1){\n\t\t\t\tif(i == 0){\n\t\t\t\t\tsize_t shmem_idx_b = shmem_idx_b_off +\n\t\t\t\t\t\t\t\t\t\t\t(warpId / BLOCK_COL_WARPS) * (WARP_ROW_TILES * N) * SHARED_OFFSET_B +\n\t\t\t\t\t\t\t\t\t\t\t(j * N) * SHARED_OFFSET_B;\n\t\t\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_b + k_step * K;\n\t\t\t\t\twmma::load_matrix_sync(b[j], tile_ptr, SHARED_OFFSET_B);\n\t\t\t\t}\n\t\t\t\twmma::mma_sync(c[j][i], a[i], b[j], c[j][i]);\n\t\t\t}\n\n        }\n      }\n\n      __syncthreads();\n    }\n\n    // This pointer is used to access the C and D matrix tiles this warp computes.\n\tint *shmem_warp_tile_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * N * WARP_ROW_TILES * SHMEM_STRIDE +\n\t(warpId % BLOCK_COL_WARPS) * SHMEM_OFFSET;\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t#pragma unroll\n\t\t\tfor(int t = 0; t < c[i][j].num_elements; t += 1){\n\t\t\t\tc[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n\t\t\t}\n\t\t\tint *tile_ptr = shmem_warp_tile_ptr + i * N * SHMEM_STRIDE + j * M;\n\t\t\twmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, C_LAYOUT);\n\t\t}\n\t}\n\n    __syncthreads();\n\n\tint *shmem_warp_stream_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N * SHMEM_STRIDE\n\t\t\t\t\t\t\t\t\t+ (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tconst size_t gmem_idx =\n\t\t(block_tile_i * N + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N) * GLOBAL_MEM_STRIDE +\n\t\tblock_tile_j * M + (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tuint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n\tint *shmem_lane_stream_ptr =\n\t\tshmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * SHMEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\t\n\tuint8_t *dst_gmem_lane_stream_ptr =\n\t\tdst_gmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * GLOBAL_MEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\n\tfor(int i = 0; i < WARP_ROW_TILES * N; i += SHARED_TO_GLOBAL_LINES_PER_WARP){\n\t\t*(dst_gmem_lane_stream_ptr + i * GLOBAL_MEM_STRIDE) = (uint8_t)(*(shmem_lane_stream_ptr + i * SHMEM_STRIDE));\n\t}\n\n\t__syncthreads();\n}\n", "launch_config": {"dimBlock": [1024, 1], "dimGrid": [256, 1]}}, "12": {"code": "\n__global__ void compute_gemm_imma(float *input0, float *input1, float *input2, float* input3, float* input4, float*input5, float* input6,\n                                  float *output0) {\n    // keys need to be replaced 4096, 1024, K_GLOBA_VALUEL, 0.75, 64\n\n\t//extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B = reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_index = reinterpret_cast<int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n\tconst int WARP_SIZE = 32;\n    const int M = 16;\n    const int K = 16;\n    const int N = 16;\n    const int WMMA_M = 16;\n    const int WMMA_K = 16;\n    const int WMMA_N = 16;\n    const int M_GLOBAL = 4096;\n    const int K_GLOBAL = 4096;\n    const int N_GLOBAL = 1024;\n    const float SPARSITY = 0.75;\n    const int K_GLOBAL_SPARSE = (int(K_GLOBAL * (1-SPARSITY)));\n    const int M_TILES = (M_GLOBAL / M);\n    const int N_TILES = (N_GLOBAL / N);\n    const int K_TILES = (K_GLOBAL / K);\n    const int BLOCK_ROW_WARPS = 2;\n    const int BLOCK_COL_WARPS = 4;\n    const int WARP_ROW_TILES = 4;\n    const int WARP_COL_TILES = 2;\n    const int SKEW_UINT8 = 16;\n    const int BANK_VAL = 32;\n    const int NUM_BANK = (K_GLOBAL / BANK_VAL);\n    /////////// BLOCK_ROW_TILES <= N_TILES ////////////\n    // const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    // ////////// BLOCK_COL_TILES <= M_TILES ////////////\n    // const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    auto C_LAYOUT = wmma::mem_col_major;\n    const int CHUNK_K = 64;\n    const int CHUNK_K_SPARSE = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K_SPARSE = (int((CHUNK_K * K) * (1 - SPARSITY)));\n    const int WARP_COPY_BYTES = (WARP_SIZE * sizeof(int4));\n    const int CHUNK_LINE_BYTES_A (BLOCK_COL_TILES * M * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_A = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_A);\n    const int CHUNK_COPY_LINE_LANES_A = (CHUNK_LINE_BYTES_A / sizeof(int4));\n    const int SHARED_OFFSET_A = (BLOCK_COL_TILES * M + SKEW_UINT8);\n\n    const int CHUNK_LINE_BYTES_B = (BLOCK_SIZE_K_SPARSE * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_B = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_B);\n    const int CHUNK_COPY_LINE_LANES_B = (CHUNK_LINE_BYTES_B / sizeof(int4));\n    const int SHARED_OFFSET_B = (BLOCK_SIZE_K_SPARSE + SKEW_UINT8);\n    const int SHARED_TO_GLOBAL_BYTES_PER_LINE = ((WARP_COL_TILES * M) * sizeof(int));\n    const int SHARED_TO_GLOBAL_BYTES_PER_WARP = (WARP_SIZE * sizeof(int));\n    const int SHARED_TO_GLOBAL_LINES_PER_WARP = (SHARED_TO_GLOBAL_BYTES_PER_WARP / SHARED_TO_GLOBAL_BYTES_PER_LINE);\n    const int SHARED_TO_GLOBAL_LANES_PER_LINE = (WARP_SIZE / SHARED_TO_GLOBAL_LINES_PER_WARP);\n    const int SHARED_TO_GLOBAL_ITERS = ((WARP_ROW_TILES * N) / SHARED_TO_GLOBAL_LINES_PER_WARP);\n\n    const int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const int WARP_STRIDE = (WARP_COL_TILES * M);\n    const int WARPS_PER_BLOCK = (BLOCK_ROW_WARPS * BLOCK_COL_WARPS);\n    const int THREADS_PER_BLOCK = (WARP_SIZE * WARPS_PER_BLOCK);\n    const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    const int GLOBAL_MEM_STRIDE = M_GLOBAL;\n    const int SHMEM_STRIDE = (M * BLOCK_COL_TILES);\n    const int SHMEM_OFFSET = (M * WARP_COL_TILES);\n    const int BLOCK_SIZE_M = (M * BLOCK_COL_TILES);\n    const int BLOCK_SIZE_N = (N * BLOCK_ROW_TILES);\n\n   extern __shared__ uint8_t shmem[];\n\n\t// Warp and lane identification.\n\tconst unsigned int warpId = threadIdx.x / WARP_SIZE;\n\tconst unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n\n\t// Offset in shared memory from which the B matrix is stored.\n\t// const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\tconst size_t shmem_idx_b_off = BLOCK_SIZE_K_SPARSE * SHARED_OFFSET_A;\n\n\n\t// Each CTA slides along the 128 x 128 tiles from the top left corner of the\n\t// matrix to the right and down, and selects the next tile to compute. Once\n\t// there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n\tconst unsigned int block_tile_i =\n\t\t((block_pos * BLOCK_COL_TILES) / M_TILES) * (BLOCK_ROW_TILES);\n\tconst unsigned int block_tile_j = (block_pos * BLOCK_COL_TILES) % M_TILES;\n\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n\twmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_ROW_TILES]\n\t\t\t\t\t\t\t\t\t\t\t\t\t [WARP_COL_TILES];\n\n    // Load the C matrix tiles into fragments from shared memory.\n\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t\twmma::fill_fragment(c[i][j], 0);\n\t\t}\n\t}\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    //for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n    for(int tile_k_idx_sparse = 0, tile_k_idx = 0; tile_k_idx_sparse < K_GLOBAL_SPARSE; tile_k_idx_sparse += BLOCK_SIZE_K_SPARSE, tile_k_idx += BLOCK_SIZE_K){\n\n\t\tsize_t shmem_idx = \n\t\twarpId < (WARPS_PER_BLOCK / 2)\n\t\t\t? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A\n\t\t\t: (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B + shmem_idx_b_off;\n\n\t\tint4 *lane_ptr = NULL;\n\t\tint *lane_ptr_index = NULL;\n\t\tconst uint8_t *warp_ptr = NULL;\n\n\n\t\tif(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\t//warp_ptr = &A[block_tile_j * M] +\n\t\t\t//\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * M_GLOBAL;\n\t\t\twarp_ptr = &A[block_tile_j * M];\n\t\t\t\n\t\t\tconst int *warp_ptr_index = &B_index[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t\t\t\t\t\t((warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A);\n\n\t\t\tlane_ptr_index = (int *)(warp_ptr_index + tile_k_idx_sparse + (laneId / CHUNK_COPY_LINE_LANES_A));\n\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_A) * SHARED_OFFSET_A;\n\t\t}else{\n\t\t\twarp_ptr = &B[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * K_GLOBAL_SPARSE;\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_sparse +\n\t\t\t\t\t\t\t\t(laneId / CHUNK_COPY_LINE_LANES_B) * K_GLOBAL_SPARSE) +\n\t\t\t\t\t\t\t\t(laneId % CHUNK_COPY_LINE_LANES_B);\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_B) * SHARED_OFFSET_B;\n\t\t}\n\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      // shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n\t  int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n\t  \t? BLOCK_SIZE_K_SPARSE / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A)\n\t\t: BLOCK_SIZE_N / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\n\t  /*\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\t  */\n\n\t  /*\n      int tile_k_idx_A;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n          tile_k_idx_A = *(lane_ptr_index);\n      }\n\t  */\n\n\t  #pragma unroll\n\t  for(int i = 0; i < iter_index; i += 1){\n\t\t  if(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\tint tile_k_idx_A = *(lane_ptr_index);\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_A * M_GLOBAL) + (laneId % CHUNK_COPY_LINE_LANES_A);\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_A)) =\n\t\t\t\t*lane_ptr;\n\t\t\t//warp_ptr = (uint8_t *)((uint8_t *)warp_ptr + M_GLOBAL * (WARPS_PER_BLOCK / 2) *CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tlane_ptr_index = (int *)((int *)lane_ptr_index +  (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A;\n\t\t  }else{\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_B)) =\n\t\t\t\t*lane_ptr;\n\t\t\tlane_ptr = (int4 *)((uint8_t *)lane_ptr + K_GLOBAL_SPARSE * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B;\n\t\t  }\n\t  }\n\n      __syncthreads();\n\n\t#pragma unroll\n      for (int k_step = 0; k_step < CHUNK_K_SPARSE; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::col_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n\t#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i += 1) {\n\t\t\tsize_t shmem_idx_a = (warpId % BLOCK_COL_WARPS) * M * WARP_COL_TILES + (i * M);\n\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_a + k_step * K * SHARED_OFFSET_A;\n\n\t\t\twmma::load_matrix_sync(a[i], tile_ptr, SHARED_OFFSET_A);\n\t\t#pragma unroll\n\t\t\tfor(int j = 0; j < WARP_ROW_TILES; j += 1){\n\t\t\t\tif(i == 0){\n\t\t\t\t\tsize_t shmem_idx_b = shmem_idx_b_off +\n\t\t\t\t\t\t\t\t\t\t\t(warpId / BLOCK_COL_WARPS) * (WARP_ROW_TILES * N) * SHARED_OFFSET_B +\n\t\t\t\t\t\t\t\t\t\t\t(j * N) * SHARED_OFFSET_B;\n\t\t\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_b + k_step * K;\n\t\t\t\t\twmma::load_matrix_sync(b[j], tile_ptr, SHARED_OFFSET_B);\n\t\t\t\t}\n\t\t\t\twmma::mma_sync(c[j][i], a[i], b[j], c[j][i]);\n\t\t\t}\n\n        }\n      }\n\n      __syncthreads();\n    }\n\n    // This pointer is used to access the C and D matrix tiles this warp computes.\n\tint *shmem_warp_tile_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * N * WARP_ROW_TILES * SHMEM_STRIDE +\n\t(warpId % BLOCK_COL_WARPS) * SHMEM_OFFSET;\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t#pragma unroll\n\t\t\tfor(int t = 0; t < c[i][j].num_elements; t += 1){\n\t\t\t\tc[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n\t\t\t}\n\t\t\tint *tile_ptr = shmem_warp_tile_ptr + i * N * SHMEM_STRIDE + j * M;\n\t\t\twmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, C_LAYOUT);\n\t\t}\n\t}\n\n    __syncthreads();\n\n\tint *shmem_warp_stream_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N * SHMEM_STRIDE\n\t\t\t\t\t\t\t\t\t+ (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tconst size_t gmem_idx =\n\t\t(block_tile_i * N + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N) * GLOBAL_MEM_STRIDE +\n\t\tblock_tile_j * M + (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tuint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n\tint *shmem_lane_stream_ptr =\n\t\tshmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * SHMEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\t\n\tuint8_t *dst_gmem_lane_stream_ptr =\n\t\tdst_gmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * GLOBAL_MEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\n\tfor(int i = 0; i < WARP_ROW_TILES * N; i += SHARED_TO_GLOBAL_LINES_PER_WARP){\n\t\t*(dst_gmem_lane_stream_ptr + i * GLOBAL_MEM_STRIDE) = (uint8_t)(*(shmem_lane_stream_ptr + i * SHMEM_STRIDE));\n\t}\n\n\t__syncthreads();\n}\n", "launch_config": {"dimBlock": [256, 1], "dimGrid": [256, 1]}}, "13": {"code": "\n__global__ void compute_gemm_imma(float *input0, float *input1, float *input2, float* input3, float* input4, float*input5, float* input6,\n                                  float *output0) {\n    // keys need to be replaced 4096, 1024, K_GLOBA_VALUEL, 0.75, 64\n\n\t//extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B = reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_index = reinterpret_cast<int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n\tconst int WARP_SIZE = 32;\n    const int M = 16;\n    const int K = 16;\n    const int N = 16;\n    const int WMMA_M = 16;\n    const int WMMA_K = 16;\n    const int WMMA_N = 16;\n    const int M_GLOBAL = 4096;\n    const int K_GLOBAL = 1024;\n    const int N_GLOBAL = 1024;\n    const float SPARSITY = 0.75;\n    const int K_GLOBAL_SPARSE = (int(K_GLOBAL * (1-SPARSITY)));\n    const int M_TILES = (M_GLOBAL / M);\n    const int N_TILES = (N_GLOBAL / N);\n    const int K_TILES = (K_GLOBAL / K);\n    const int BLOCK_ROW_WARPS = 2;\n    const int BLOCK_COL_WARPS = 4;\n    const int WARP_ROW_TILES = 4;\n    const int WARP_COL_TILES = 2;\n    const int SKEW_UINT8 = 16;\n    const int BANK_VAL = 32;\n    const int NUM_BANK = (K_GLOBAL / BANK_VAL);\n    /////////// BLOCK_ROW_TILES <= N_TILES ////////////\n    // const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    // ////////// BLOCK_COL_TILES <= M_TILES ////////////\n    // const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    auto C_LAYOUT = wmma::mem_col_major;\n    const int CHUNK_K = 64;\n    const int CHUNK_K_SPARSE = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K_SPARSE = (int((CHUNK_K * K) * (1 - SPARSITY)));\n    const int WARP_COPY_BYTES = (WARP_SIZE * sizeof(int4));\n    const int CHUNK_LINE_BYTES_A (BLOCK_COL_TILES * M * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_A = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_A);\n    const int CHUNK_COPY_LINE_LANES_A = (CHUNK_LINE_BYTES_A / sizeof(int4));\n    const int SHARED_OFFSET_A = (BLOCK_COL_TILES * M + SKEW_UINT8);\n\n    const int CHUNK_LINE_BYTES_B = (BLOCK_SIZE_K_SPARSE * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_B = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_B);\n    const int CHUNK_COPY_LINE_LANES_B = (CHUNK_LINE_BYTES_B / sizeof(int4));\n    const int SHARED_OFFSET_B = (BLOCK_SIZE_K_SPARSE + SKEW_UINT8);\n    const int SHARED_TO_GLOBAL_BYTES_PER_LINE = ((WARP_COL_TILES * M) * sizeof(int));\n    const int SHARED_TO_GLOBAL_BYTES_PER_WARP = (WARP_SIZE * sizeof(int));\n    const int SHARED_TO_GLOBAL_LINES_PER_WARP = (SHARED_TO_GLOBAL_BYTES_PER_WARP / SHARED_TO_GLOBAL_BYTES_PER_LINE);\n    const int SHARED_TO_GLOBAL_LANES_PER_LINE = (WARP_SIZE / SHARED_TO_GLOBAL_LINES_PER_WARP);\n    const int SHARED_TO_GLOBAL_ITERS = ((WARP_ROW_TILES * N) / SHARED_TO_GLOBAL_LINES_PER_WARP);\n\n    const int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const int WARP_STRIDE = (WARP_COL_TILES * M);\n    const int WARPS_PER_BLOCK = (BLOCK_ROW_WARPS * BLOCK_COL_WARPS);\n    const int THREADS_PER_BLOCK = (WARP_SIZE * WARPS_PER_BLOCK);\n    const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    const int GLOBAL_MEM_STRIDE = M_GLOBAL;\n    const int SHMEM_STRIDE = (M * BLOCK_COL_TILES);\n    const int SHMEM_OFFSET = (M * WARP_COL_TILES);\n    const int BLOCK_SIZE_M = (M * BLOCK_COL_TILES);\n    const int BLOCK_SIZE_N = (N * BLOCK_ROW_TILES);\n\n   extern __shared__ uint8_t shmem[];\n\n\t// Warp and lane identification.\n\tconst unsigned int warpId = threadIdx.x / WARP_SIZE;\n\tconst unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n\n\t// Offset in shared memory from which the B matrix is stored.\n\t// const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\tconst size_t shmem_idx_b_off = BLOCK_SIZE_K_SPARSE * SHARED_OFFSET_A;\n\n\n\t// Each CTA slides along the 128 x 128 tiles from the top left corner of the\n\t// matrix to the right and down, and selects the next tile to compute. Once\n\t// there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n\tconst unsigned int block_tile_i =\n\t\t((block_pos * BLOCK_COL_TILES) / M_TILES) * (BLOCK_ROW_TILES);\n\tconst unsigned int block_tile_j = (block_pos * BLOCK_COL_TILES) % M_TILES;\n\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n\twmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_ROW_TILES]\n\t\t\t\t\t\t\t\t\t\t\t\t\t [WARP_COL_TILES];\n\n    // Load the C matrix tiles into fragments from shared memory.\n\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t\twmma::fill_fragment(c[i][j], 0);\n\t\t}\n\t}\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    //for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n    for(int tile_k_idx_sparse = 0, tile_k_idx = 0; tile_k_idx_sparse < K_GLOBAL_SPARSE; tile_k_idx_sparse += BLOCK_SIZE_K_SPARSE, tile_k_idx += BLOCK_SIZE_K){\n\n\t\tsize_t shmem_idx = \n\t\twarpId < (WARPS_PER_BLOCK / 2)\n\t\t\t? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A\n\t\t\t: (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B + shmem_idx_b_off;\n\n\t\tint4 *lane_ptr = NULL;\n\t\tint *lane_ptr_index = NULL;\n\t\tconst uint8_t *warp_ptr = NULL;\n\n\n\t\tif(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\t//warp_ptr = &A[block_tile_j * M] +\n\t\t\t//\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * M_GLOBAL;\n\t\t\twarp_ptr = &A[block_tile_j * M];\n\t\t\t\n\t\t\tconst int *warp_ptr_index = &B_index[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t\t\t\t\t\t((warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A);\n\n\t\t\tlane_ptr_index = (int *)(warp_ptr_index + tile_k_idx_sparse + (laneId / CHUNK_COPY_LINE_LANES_A));\n\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_A) * SHARED_OFFSET_A;\n\t\t}else{\n\t\t\twarp_ptr = &B[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * K_GLOBAL_SPARSE;\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_sparse +\n\t\t\t\t\t\t\t\t(laneId / CHUNK_COPY_LINE_LANES_B) * K_GLOBAL_SPARSE) +\n\t\t\t\t\t\t\t\t(laneId % CHUNK_COPY_LINE_LANES_B);\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_B) * SHARED_OFFSET_B;\n\t\t}\n\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      // shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n\t  int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n\t  \t? BLOCK_SIZE_K_SPARSE / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A)\n\t\t: BLOCK_SIZE_N / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\n\t  /*\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\t  */\n\n\t  /*\n      int tile_k_idx_A;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n          tile_k_idx_A = *(lane_ptr_index);\n      }\n\t  */\n\n\t  #pragma unroll\n\t  for(int i = 0; i < iter_index; i += 1){\n\t\t  if(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\tint tile_k_idx_A = *(lane_ptr_index);\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_A * M_GLOBAL) + (laneId % CHUNK_COPY_LINE_LANES_A);\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_A)) =\n\t\t\t\t*lane_ptr;\n\t\t\t//warp_ptr = (uint8_t *)((uint8_t *)warp_ptr + M_GLOBAL * (WARPS_PER_BLOCK / 2) *CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tlane_ptr_index = (int *)((int *)lane_ptr_index +  (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A;\n\t\t  }else{\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_B)) =\n\t\t\t\t*lane_ptr;\n\t\t\tlane_ptr = (int4 *)((uint8_t *)lane_ptr + K_GLOBAL_SPARSE * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B;\n\t\t  }\n\t  }\n\n      __syncthreads();\n\n\t#pragma unroll\n      for (int k_step = 0; k_step < CHUNK_K_SPARSE; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::col_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n\t#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i += 1) {\n\t\t\tsize_t shmem_idx_a = (warpId % BLOCK_COL_WARPS) * M * WARP_COL_TILES + (i * M);\n\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_a + k_step * K * SHARED_OFFSET_A;\n\n\t\t\twmma::load_matrix_sync(a[i], tile_ptr, SHARED_OFFSET_A);\n\t\t#pragma unroll\n\t\t\tfor(int j = 0; j < WARP_ROW_TILES; j += 1){\n\t\t\t\tif(i == 0){\n\t\t\t\t\tsize_t shmem_idx_b = shmem_idx_b_off +\n\t\t\t\t\t\t\t\t\t\t\t(warpId / BLOCK_COL_WARPS) * (WARP_ROW_TILES * N) * SHARED_OFFSET_B +\n\t\t\t\t\t\t\t\t\t\t\t(j * N) * SHARED_OFFSET_B;\n\t\t\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_b + k_step * K;\n\t\t\t\t\twmma::load_matrix_sync(b[j], tile_ptr, SHARED_OFFSET_B);\n\t\t\t\t}\n\t\t\t\twmma::mma_sync(c[j][i], a[i], b[j], c[j][i]);\n\t\t\t}\n\n        }\n      }\n\n      __syncthreads();\n    }\n\n    // This pointer is used to access the C and D matrix tiles this warp computes.\n\tint *shmem_warp_tile_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * N * WARP_ROW_TILES * SHMEM_STRIDE +\n\t(warpId % BLOCK_COL_WARPS) * SHMEM_OFFSET;\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t#pragma unroll\n\t\t\tfor(int t = 0; t < c[i][j].num_elements; t += 1){\n\t\t\t\tc[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n\t\t\t}\n\t\t\tint *tile_ptr = shmem_warp_tile_ptr + i * N * SHMEM_STRIDE + j * M;\n\t\t\twmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, C_LAYOUT);\n\t\t}\n\t}\n\n    __syncthreads();\n\n\tint *shmem_warp_stream_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N * SHMEM_STRIDE\n\t\t\t\t\t\t\t\t\t+ (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tconst size_t gmem_idx =\n\t\t(block_tile_i * N + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N) * GLOBAL_MEM_STRIDE +\n\t\tblock_tile_j * M + (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tuint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n\tint *shmem_lane_stream_ptr =\n\t\tshmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * SHMEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\t\n\tuint8_t *dst_gmem_lane_stream_ptr =\n\t\tdst_gmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * GLOBAL_MEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\n\tfor(int i = 0; i < WARP_ROW_TILES * N; i += SHARED_TO_GLOBAL_LINES_PER_WARP){\n\t\t*(dst_gmem_lane_stream_ptr + i * GLOBAL_MEM_STRIDE) = (uint8_t)(*(shmem_lane_stream_ptr + i * SHMEM_STRIDE));\n\t}\n\n\t__syncthreads();\n}\n", "launch_config": {"dimBlock": [256, 1], "dimGrid": [256, 1]}}, "14": {"code": "\n__global__ void compute_gemm_imma(float *input0, float *input1, float *input2, float* input3, float* input4, float*input5, float* input6,\n                                  float *output0) {\n    // keys need to be replaced 4096, 1024, K_GLOBA_VALUEL, 0.75, 64\n\n\t//extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B = reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_index = reinterpret_cast<int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n\tconst int WARP_SIZE = 32;\n    const int M = 16;\n    const int K = 16;\n    const int N = 16;\n    const int WMMA_M = 16;\n    const int WMMA_K = 16;\n    const int WMMA_N = 16;\n    const int M_GLOBAL = 4096;\n    const int K_GLOBAL = 1024;\n    const int N_GLOBAL = 1024;\n    const float SPARSITY = 0.75;\n    const int K_GLOBAL_SPARSE = (int(K_GLOBAL * (1-SPARSITY)));\n    const int M_TILES = (M_GLOBAL / M);\n    const int N_TILES = (N_GLOBAL / N);\n    const int K_TILES = (K_GLOBAL / K);\n    const int BLOCK_ROW_WARPS = 2;\n    const int BLOCK_COL_WARPS = 4;\n    const int WARP_ROW_TILES = 4;\n    const int WARP_COL_TILES = 2;\n    const int SKEW_UINT8 = 16;\n    const int BANK_VAL = 32;\n    const int NUM_BANK = (K_GLOBAL / BANK_VAL);\n    /////////// BLOCK_ROW_TILES <= N_TILES ////////////\n    // const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    // ////////// BLOCK_COL_TILES <= M_TILES ////////////\n    // const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    auto C_LAYOUT = wmma::mem_col_major;\n    const int CHUNK_K = 64;\n    const int CHUNK_K_SPARSE = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K_SPARSE = (int((CHUNK_K * K) * (1 - SPARSITY)));\n    const int WARP_COPY_BYTES = (WARP_SIZE * sizeof(int4));\n    const int CHUNK_LINE_BYTES_A (BLOCK_COL_TILES * M * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_A = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_A);\n    const int CHUNK_COPY_LINE_LANES_A = (CHUNK_LINE_BYTES_A / sizeof(int4));\n    const int SHARED_OFFSET_A = (BLOCK_COL_TILES * M + SKEW_UINT8);\n\n    const int CHUNK_LINE_BYTES_B = (BLOCK_SIZE_K_SPARSE * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_B = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_B);\n    const int CHUNK_COPY_LINE_LANES_B = (CHUNK_LINE_BYTES_B / sizeof(int4));\n    const int SHARED_OFFSET_B = (BLOCK_SIZE_K_SPARSE + SKEW_UINT8);\n    const int SHARED_TO_GLOBAL_BYTES_PER_LINE = ((WARP_COL_TILES * M) * sizeof(int));\n    const int SHARED_TO_GLOBAL_BYTES_PER_WARP = (WARP_SIZE * sizeof(int));\n    const int SHARED_TO_GLOBAL_LINES_PER_WARP = (SHARED_TO_GLOBAL_BYTES_PER_WARP / SHARED_TO_GLOBAL_BYTES_PER_LINE);\n    const int SHARED_TO_GLOBAL_LANES_PER_LINE = (WARP_SIZE / SHARED_TO_GLOBAL_LINES_PER_WARP);\n    const int SHARED_TO_GLOBAL_ITERS = ((WARP_ROW_TILES * N) / SHARED_TO_GLOBAL_LINES_PER_WARP);\n\n    const int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const int WARP_STRIDE = (WARP_COL_TILES * M);\n    const int WARPS_PER_BLOCK = (BLOCK_ROW_WARPS * BLOCK_COL_WARPS);\n    const int THREADS_PER_BLOCK = (WARP_SIZE * WARPS_PER_BLOCK);\n    const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    const int GLOBAL_MEM_STRIDE = M_GLOBAL;\n    const int SHMEM_STRIDE = (M * BLOCK_COL_TILES);\n    const int SHMEM_OFFSET = (M * WARP_COL_TILES);\n    const int BLOCK_SIZE_M = (M * BLOCK_COL_TILES);\n    const int BLOCK_SIZE_N = (N * BLOCK_ROW_TILES);\n\n   extern __shared__ uint8_t shmem[];\n\n\t// Warp and lane identification.\n\tconst unsigned int warpId = threadIdx.x / WARP_SIZE;\n\tconst unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n\n\t// Offset in shared memory from which the B matrix is stored.\n\t// const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\tconst size_t shmem_idx_b_off = BLOCK_SIZE_K_SPARSE * SHARED_OFFSET_A;\n\n\n\t// Each CTA slides along the 128 x 128 tiles from the top left corner of the\n\t// matrix to the right and down, and selects the next tile to compute. Once\n\t// there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n\tconst unsigned int block_tile_i =\n\t\t((block_pos * BLOCK_COL_TILES) / M_TILES) * (BLOCK_ROW_TILES);\n\tconst unsigned int block_tile_j = (block_pos * BLOCK_COL_TILES) % M_TILES;\n\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n\twmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_ROW_TILES]\n\t\t\t\t\t\t\t\t\t\t\t\t\t [WARP_COL_TILES];\n\n    // Load the C matrix tiles into fragments from shared memory.\n\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t\twmma::fill_fragment(c[i][j], 0);\n\t\t}\n\t}\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    //for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n    for(int tile_k_idx_sparse = 0, tile_k_idx = 0; tile_k_idx_sparse < K_GLOBAL_SPARSE; tile_k_idx_sparse += BLOCK_SIZE_K_SPARSE, tile_k_idx += BLOCK_SIZE_K){\n\n\t\tsize_t shmem_idx = \n\t\twarpId < (WARPS_PER_BLOCK / 2)\n\t\t\t? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A\n\t\t\t: (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B + shmem_idx_b_off;\n\n\t\tint4 *lane_ptr = NULL;\n\t\tint *lane_ptr_index = NULL;\n\t\tconst uint8_t *warp_ptr = NULL;\n\n\n\t\tif(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\t//warp_ptr = &A[block_tile_j * M] +\n\t\t\t//\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * M_GLOBAL;\n\t\t\twarp_ptr = &A[block_tile_j * M];\n\t\t\t\n\t\t\tconst int *warp_ptr_index = &B_index[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t\t\t\t\t\t((warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A);\n\n\t\t\tlane_ptr_index = (int *)(warp_ptr_index + tile_k_idx_sparse + (laneId / CHUNK_COPY_LINE_LANES_A));\n\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_A) * SHARED_OFFSET_A;\n\t\t}else{\n\t\t\twarp_ptr = &B[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * K_GLOBAL_SPARSE;\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_sparse +\n\t\t\t\t\t\t\t\t(laneId / CHUNK_COPY_LINE_LANES_B) * K_GLOBAL_SPARSE) +\n\t\t\t\t\t\t\t\t(laneId % CHUNK_COPY_LINE_LANES_B);\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_B) * SHARED_OFFSET_B;\n\t\t}\n\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      // shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n\t  int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n\t  \t? BLOCK_SIZE_K_SPARSE / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A)\n\t\t: BLOCK_SIZE_N / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\n\t  /*\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\t  */\n\n\t  /*\n      int tile_k_idx_A;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n          tile_k_idx_A = *(lane_ptr_index);\n      }\n\t  */\n\n\t  #pragma unroll\n\t  for(int i = 0; i < iter_index; i += 1){\n\t\t  if(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\tint tile_k_idx_A = *(lane_ptr_index);\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_A * M_GLOBAL) + (laneId % CHUNK_COPY_LINE_LANES_A);\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_A)) =\n\t\t\t\t*lane_ptr;\n\t\t\t//warp_ptr = (uint8_t *)((uint8_t *)warp_ptr + M_GLOBAL * (WARPS_PER_BLOCK / 2) *CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tlane_ptr_index = (int *)((int *)lane_ptr_index +  (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A;\n\t\t  }else{\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_B)) =\n\t\t\t\t*lane_ptr;\n\t\t\tlane_ptr = (int4 *)((uint8_t *)lane_ptr + K_GLOBAL_SPARSE * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B;\n\t\t  }\n\t  }\n\n      __syncthreads();\n\n\t#pragma unroll\n      for (int k_step = 0; k_step < CHUNK_K_SPARSE; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::col_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n\t#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i += 1) {\n\t\t\tsize_t shmem_idx_a = (warpId % BLOCK_COL_WARPS) * M * WARP_COL_TILES + (i * M);\n\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_a + k_step * K * SHARED_OFFSET_A;\n\n\t\t\twmma::load_matrix_sync(a[i], tile_ptr, SHARED_OFFSET_A);\n\t\t#pragma unroll\n\t\t\tfor(int j = 0; j < WARP_ROW_TILES; j += 1){\n\t\t\t\tif(i == 0){\n\t\t\t\t\tsize_t shmem_idx_b = shmem_idx_b_off +\n\t\t\t\t\t\t\t\t\t\t\t(warpId / BLOCK_COL_WARPS) * (WARP_ROW_TILES * N) * SHARED_OFFSET_B +\n\t\t\t\t\t\t\t\t\t\t\t(j * N) * SHARED_OFFSET_B;\n\t\t\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_b + k_step * K;\n\t\t\t\t\twmma::load_matrix_sync(b[j], tile_ptr, SHARED_OFFSET_B);\n\t\t\t\t}\n\t\t\t\twmma::mma_sync(c[j][i], a[i], b[j], c[j][i]);\n\t\t\t}\n\n        }\n      }\n\n      __syncthreads();\n    }\n\n    // This pointer is used to access the C and D matrix tiles this warp computes.\n\tint *shmem_warp_tile_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * N * WARP_ROW_TILES * SHMEM_STRIDE +\n\t(warpId % BLOCK_COL_WARPS) * SHMEM_OFFSET;\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t#pragma unroll\n\t\t\tfor(int t = 0; t < c[i][j].num_elements; t += 1){\n\t\t\t\tc[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n\t\t\t}\n\t\t\tint *tile_ptr = shmem_warp_tile_ptr + i * N * SHMEM_STRIDE + j * M;\n\t\t\twmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, C_LAYOUT);\n\t\t}\n\t}\n\n    __syncthreads();\n\n\tint *shmem_warp_stream_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N * SHMEM_STRIDE\n\t\t\t\t\t\t\t\t\t+ (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tconst size_t gmem_idx =\n\t\t(block_tile_i * N + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N) * GLOBAL_MEM_STRIDE +\n\t\tblock_tile_j * M + (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tuint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n\tint *shmem_lane_stream_ptr =\n\t\tshmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * SHMEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\t\n\tuint8_t *dst_gmem_lane_stream_ptr =\n\t\tdst_gmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * GLOBAL_MEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\n\tfor(int i = 0; i < WARP_ROW_TILES * N; i += SHARED_TO_GLOBAL_LINES_PER_WARP){\n\t\t*(dst_gmem_lane_stream_ptr + i * GLOBAL_MEM_STRIDE) = (uint8_t)(*(shmem_lane_stream_ptr + i * SHMEM_STRIDE));\n\t}\n\n\t__syncthreads();\n}\n", "launch_config": {"dimBlock": [256, 1], "dimGrid": [256, 1]}}, "15": {"code": "\n__global__ void compute_gemm_imma(float *input0, float *input1, float *input2, float* input3, float* input4, float*input5, float* input6,\n                                  float *output0) {\n    // keys need to be replaced 4096, 1024, K_GLOBA_VALUEL, 0.75, 64\n\n\t//extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B = reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_index = reinterpret_cast<int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n\tconst int WARP_SIZE = 32;\n    const int M = 16;\n    const int K = 16;\n    const int N = 16;\n    const int WMMA_M = 16;\n    const int WMMA_K = 16;\n    const int WMMA_N = 16;\n    const int M_GLOBAL = 4096;\n    const int K_GLOBAL = 1024;\n    const int N_GLOBAL = 1024;\n    const float SPARSITY = 0.75;\n    const int K_GLOBAL_SPARSE = (int(K_GLOBAL * (1-SPARSITY)));\n    const int M_TILES = (M_GLOBAL / M);\n    const int N_TILES = (N_GLOBAL / N);\n    const int K_TILES = (K_GLOBAL / K);\n    const int BLOCK_ROW_WARPS = 2;\n    const int BLOCK_COL_WARPS = 4;\n    const int WARP_ROW_TILES = 4;\n    const int WARP_COL_TILES = 2;\n    const int SKEW_UINT8 = 16;\n    const int BANK_VAL = 32;\n    const int NUM_BANK = (K_GLOBAL / BANK_VAL);\n    /////////// BLOCK_ROW_TILES <= N_TILES ////////////\n    // const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    // ////////// BLOCK_COL_TILES <= M_TILES ////////////\n    // const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    auto C_LAYOUT = wmma::mem_col_major;\n    const int CHUNK_K = 64;\n    const int CHUNK_K_SPARSE = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K_SPARSE = (int((CHUNK_K * K) * (1 - SPARSITY)));\n    const int WARP_COPY_BYTES = (WARP_SIZE * sizeof(int4));\n    const int CHUNK_LINE_BYTES_A (BLOCK_COL_TILES * M * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_A = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_A);\n    const int CHUNK_COPY_LINE_LANES_A = (CHUNK_LINE_BYTES_A / sizeof(int4));\n    const int SHARED_OFFSET_A = (BLOCK_COL_TILES * M + SKEW_UINT8);\n\n    const int CHUNK_LINE_BYTES_B = (BLOCK_SIZE_K_SPARSE * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_B = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_B);\n    const int CHUNK_COPY_LINE_LANES_B = (CHUNK_LINE_BYTES_B / sizeof(int4));\n    const int SHARED_OFFSET_B = (BLOCK_SIZE_K_SPARSE + SKEW_UINT8);\n    const int SHARED_TO_GLOBAL_BYTES_PER_LINE = ((WARP_COL_TILES * M) * sizeof(int));\n    const int SHARED_TO_GLOBAL_BYTES_PER_WARP = (WARP_SIZE * sizeof(int));\n    const int SHARED_TO_GLOBAL_LINES_PER_WARP = (SHARED_TO_GLOBAL_BYTES_PER_WARP / SHARED_TO_GLOBAL_BYTES_PER_LINE);\n    const int SHARED_TO_GLOBAL_LANES_PER_LINE = (WARP_SIZE / SHARED_TO_GLOBAL_LINES_PER_WARP);\n    const int SHARED_TO_GLOBAL_ITERS = ((WARP_ROW_TILES * N) / SHARED_TO_GLOBAL_LINES_PER_WARP);\n\n    const int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const int WARP_STRIDE = (WARP_COL_TILES * M);\n    const int WARPS_PER_BLOCK = (BLOCK_ROW_WARPS * BLOCK_COL_WARPS);\n    const int THREADS_PER_BLOCK = (WARP_SIZE * WARPS_PER_BLOCK);\n    const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    const int GLOBAL_MEM_STRIDE = M_GLOBAL;\n    const int SHMEM_STRIDE = (M * BLOCK_COL_TILES);\n    const int SHMEM_OFFSET = (M * WARP_COL_TILES);\n    const int BLOCK_SIZE_M = (M * BLOCK_COL_TILES);\n    const int BLOCK_SIZE_N = (N * BLOCK_ROW_TILES);\n\n   extern __shared__ uint8_t shmem[];\n\n\t// Warp and lane identification.\n\tconst unsigned int warpId = threadIdx.x / WARP_SIZE;\n\tconst unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n\n\t// Offset in shared memory from which the B matrix is stored.\n\t// const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\tconst size_t shmem_idx_b_off = BLOCK_SIZE_K_SPARSE * SHARED_OFFSET_A;\n\n\n\t// Each CTA slides along the 128 x 128 tiles from the top left corner of the\n\t// matrix to the right and down, and selects the next tile to compute. Once\n\t// there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n\tconst unsigned int block_tile_i =\n\t\t((block_pos * BLOCK_COL_TILES) / M_TILES) * (BLOCK_ROW_TILES);\n\tconst unsigned int block_tile_j = (block_pos * BLOCK_COL_TILES) % M_TILES;\n\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n\twmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_ROW_TILES]\n\t\t\t\t\t\t\t\t\t\t\t\t\t [WARP_COL_TILES];\n\n    // Load the C matrix tiles into fragments from shared memory.\n\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t\twmma::fill_fragment(c[i][j], 0);\n\t\t}\n\t}\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    //for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n    for(int tile_k_idx_sparse = 0, tile_k_idx = 0; tile_k_idx_sparse < K_GLOBAL_SPARSE; tile_k_idx_sparse += BLOCK_SIZE_K_SPARSE, tile_k_idx += BLOCK_SIZE_K){\n\n\t\tsize_t shmem_idx = \n\t\twarpId < (WARPS_PER_BLOCK / 2)\n\t\t\t? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A\n\t\t\t: (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B + shmem_idx_b_off;\n\n\t\tint4 *lane_ptr = NULL;\n\t\tint *lane_ptr_index = NULL;\n\t\tconst uint8_t *warp_ptr = NULL;\n\n\n\t\tif(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\t//warp_ptr = &A[block_tile_j * M] +\n\t\t\t//\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * M_GLOBAL;\n\t\t\twarp_ptr = &A[block_tile_j * M];\n\t\t\t\n\t\t\tconst int *warp_ptr_index = &B_index[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t\t\t\t\t\t((warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A);\n\n\t\t\tlane_ptr_index = (int *)(warp_ptr_index + tile_k_idx_sparse + (laneId / CHUNK_COPY_LINE_LANES_A));\n\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_A) * SHARED_OFFSET_A;\n\t\t}else{\n\t\t\twarp_ptr = &B[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * K_GLOBAL_SPARSE;\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_sparse +\n\t\t\t\t\t\t\t\t(laneId / CHUNK_COPY_LINE_LANES_B) * K_GLOBAL_SPARSE) +\n\t\t\t\t\t\t\t\t(laneId % CHUNK_COPY_LINE_LANES_B);\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_B) * SHARED_OFFSET_B;\n\t\t}\n\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      // shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n\t  int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n\t  \t? BLOCK_SIZE_K_SPARSE / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A)\n\t\t: BLOCK_SIZE_N / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\n\t  /*\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\t  */\n\n\t  /*\n      int tile_k_idx_A;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n          tile_k_idx_A = *(lane_ptr_index);\n      }\n\t  */\n\n\t  #pragma unroll\n\t  for(int i = 0; i < iter_index; i += 1){\n\t\t  if(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\tint tile_k_idx_A = *(lane_ptr_index);\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_A * M_GLOBAL) + (laneId % CHUNK_COPY_LINE_LANES_A);\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_A)) =\n\t\t\t\t*lane_ptr;\n\t\t\t//warp_ptr = (uint8_t *)((uint8_t *)warp_ptr + M_GLOBAL * (WARPS_PER_BLOCK / 2) *CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tlane_ptr_index = (int *)((int *)lane_ptr_index +  (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A;\n\t\t  }else{\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_B)) =\n\t\t\t\t*lane_ptr;\n\t\t\tlane_ptr = (int4 *)((uint8_t *)lane_ptr + K_GLOBAL_SPARSE * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B;\n\t\t  }\n\t  }\n\n      __syncthreads();\n\n\t#pragma unroll\n      for (int k_step = 0; k_step < CHUNK_K_SPARSE; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::col_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n\t#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i += 1) {\n\t\t\tsize_t shmem_idx_a = (warpId % BLOCK_COL_WARPS) * M * WARP_COL_TILES + (i * M);\n\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_a + k_step * K * SHARED_OFFSET_A;\n\n\t\t\twmma::load_matrix_sync(a[i], tile_ptr, SHARED_OFFSET_A);\n\t\t#pragma unroll\n\t\t\tfor(int j = 0; j < WARP_ROW_TILES; j += 1){\n\t\t\t\tif(i == 0){\n\t\t\t\t\tsize_t shmem_idx_b = shmem_idx_b_off +\n\t\t\t\t\t\t\t\t\t\t\t(warpId / BLOCK_COL_WARPS) * (WARP_ROW_TILES * N) * SHARED_OFFSET_B +\n\t\t\t\t\t\t\t\t\t\t\t(j * N) * SHARED_OFFSET_B;\n\t\t\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_b + k_step * K;\n\t\t\t\t\twmma::load_matrix_sync(b[j], tile_ptr, SHARED_OFFSET_B);\n\t\t\t\t}\n\t\t\t\twmma::mma_sync(c[j][i], a[i], b[j], c[j][i]);\n\t\t\t}\n\n        }\n      }\n\n      __syncthreads();\n    }\n\n    // This pointer is used to access the C and D matrix tiles this warp computes.\n\tint *shmem_warp_tile_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * N * WARP_ROW_TILES * SHMEM_STRIDE +\n\t(warpId % BLOCK_COL_WARPS) * SHMEM_OFFSET;\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t#pragma unroll\n\t\t\tfor(int t = 0; t < c[i][j].num_elements; t += 1){\n\t\t\t\tc[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n\t\t\t}\n\t\t\tint *tile_ptr = shmem_warp_tile_ptr + i * N * SHMEM_STRIDE + j * M;\n\t\t\twmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, C_LAYOUT);\n\t\t}\n\t}\n\n    __syncthreads();\n\n\tint *shmem_warp_stream_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N * SHMEM_STRIDE\n\t\t\t\t\t\t\t\t\t+ (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tconst size_t gmem_idx =\n\t\t(block_tile_i * N + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N) * GLOBAL_MEM_STRIDE +\n\t\tblock_tile_j * M + (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tuint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n\tint *shmem_lane_stream_ptr =\n\t\tshmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * SHMEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\t\n\tuint8_t *dst_gmem_lane_stream_ptr =\n\t\tdst_gmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * GLOBAL_MEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\n\tfor(int i = 0; i < WARP_ROW_TILES * N; i += SHARED_TO_GLOBAL_LINES_PER_WARP){\n\t\t*(dst_gmem_lane_stream_ptr + i * GLOBAL_MEM_STRIDE) = (uint8_t)(*(shmem_lane_stream_ptr + i * SHMEM_STRIDE));\n\t}\n\n\t__syncthreads();\n}\n", "launch_config": {"dimBlock": [256, 1], "dimGrid": [256, 1]}}, "16": {"code": "\n__global__ void compute_gemm_imma(float *input0, float *input1, float *input2, float* input3, float* input4, float*input5, float* input6,\n                                  float *output0) {\n    // keys need to be replaced 4096, 1024, K_GLOBA_VALUEL, 0.75, 64\n\n\t//extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B = reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_index = reinterpret_cast<int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n\tconst int WARP_SIZE = 32;\n    const int M = 16;\n    const int K = 16;\n    const int N = 16;\n    const int WMMA_M = 16;\n    const int WMMA_K = 16;\n    const int WMMA_N = 16;\n    const int M_GLOBAL = 4096;\n    const int K_GLOBAL = 1024;\n    const int N_GLOBAL = 1024;\n    const float SPARSITY = 0.75;\n    const int K_GLOBAL_SPARSE = (int(K_GLOBAL * (1-SPARSITY)));\n    const int M_TILES = (M_GLOBAL / M);\n    const int N_TILES = (N_GLOBAL / N);\n    const int K_TILES = (K_GLOBAL / K);\n    const int BLOCK_ROW_WARPS = 2;\n    const int BLOCK_COL_WARPS = 4;\n    const int WARP_ROW_TILES = 4;\n    const int WARP_COL_TILES = 2;\n    const int SKEW_UINT8 = 16;\n    const int BANK_VAL = 32;\n    const int NUM_BANK = (K_GLOBAL / BANK_VAL);\n    /////////// BLOCK_ROW_TILES <= N_TILES ////////////\n    // const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    // ////////// BLOCK_COL_TILES <= M_TILES ////////////\n    // const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    auto C_LAYOUT = wmma::mem_col_major;\n    const int CHUNK_K = 64;\n    const int CHUNK_K_SPARSE = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K_SPARSE = (int((CHUNK_K * K) * (1 - SPARSITY)));\n    const int WARP_COPY_BYTES = (WARP_SIZE * sizeof(int4));\n    const int CHUNK_LINE_BYTES_A (BLOCK_COL_TILES * M * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_A = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_A);\n    const int CHUNK_COPY_LINE_LANES_A = (CHUNK_LINE_BYTES_A / sizeof(int4));\n    const int SHARED_OFFSET_A = (BLOCK_COL_TILES * M + SKEW_UINT8);\n\n    const int CHUNK_LINE_BYTES_B = (BLOCK_SIZE_K_SPARSE * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_B = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_B);\n    const int CHUNK_COPY_LINE_LANES_B = (CHUNK_LINE_BYTES_B / sizeof(int4));\n    const int SHARED_OFFSET_B = (BLOCK_SIZE_K_SPARSE + SKEW_UINT8);\n    const int SHARED_TO_GLOBAL_BYTES_PER_LINE = ((WARP_COL_TILES * M) * sizeof(int));\n    const int SHARED_TO_GLOBAL_BYTES_PER_WARP = (WARP_SIZE * sizeof(int));\n    const int SHARED_TO_GLOBAL_LINES_PER_WARP = (SHARED_TO_GLOBAL_BYTES_PER_WARP / SHARED_TO_GLOBAL_BYTES_PER_LINE);\n    const int SHARED_TO_GLOBAL_LANES_PER_LINE = (WARP_SIZE / SHARED_TO_GLOBAL_LINES_PER_WARP);\n    const int SHARED_TO_GLOBAL_ITERS = ((WARP_ROW_TILES * N) / SHARED_TO_GLOBAL_LINES_PER_WARP);\n\n    const int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const int WARP_STRIDE = (WARP_COL_TILES * M);\n    const int WARPS_PER_BLOCK = (BLOCK_ROW_WARPS * BLOCK_COL_WARPS);\n    const int THREADS_PER_BLOCK = (WARP_SIZE * WARPS_PER_BLOCK);\n    const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    const int GLOBAL_MEM_STRIDE = M_GLOBAL;\n    const int SHMEM_STRIDE = (M * BLOCK_COL_TILES);\n    const int SHMEM_OFFSET = (M * WARP_COL_TILES);\n    const int BLOCK_SIZE_M = (M * BLOCK_COL_TILES);\n    const int BLOCK_SIZE_N = (N * BLOCK_ROW_TILES);\n\n   extern __shared__ uint8_t shmem[];\n\n\t// Warp and lane identification.\n\tconst unsigned int warpId = threadIdx.x / WARP_SIZE;\n\tconst unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n\n\t// Offset in shared memory from which the B matrix is stored.\n\t// const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\tconst size_t shmem_idx_b_off = BLOCK_SIZE_K_SPARSE * SHARED_OFFSET_A;\n\n\n\t// Each CTA slides along the 128 x 128 tiles from the top left corner of the\n\t// matrix to the right and down, and selects the next tile to compute. Once\n\t// there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n\tconst unsigned int block_tile_i =\n\t\t((block_pos * BLOCK_COL_TILES) / M_TILES) * (BLOCK_ROW_TILES);\n\tconst unsigned int block_tile_j = (block_pos * BLOCK_COL_TILES) % M_TILES;\n\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n\twmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_ROW_TILES]\n\t\t\t\t\t\t\t\t\t\t\t\t\t [WARP_COL_TILES];\n\n    // Load the C matrix tiles into fragments from shared memory.\n\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t\twmma::fill_fragment(c[i][j], 0);\n\t\t}\n\t}\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    //for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n    for(int tile_k_idx_sparse = 0, tile_k_idx = 0; tile_k_idx_sparse < K_GLOBAL_SPARSE; tile_k_idx_sparse += BLOCK_SIZE_K_SPARSE, tile_k_idx += BLOCK_SIZE_K){\n\n\t\tsize_t shmem_idx = \n\t\twarpId < (WARPS_PER_BLOCK / 2)\n\t\t\t? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A\n\t\t\t: (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B + shmem_idx_b_off;\n\n\t\tint4 *lane_ptr = NULL;\n\t\tint *lane_ptr_index = NULL;\n\t\tconst uint8_t *warp_ptr = NULL;\n\n\n\t\tif(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\t//warp_ptr = &A[block_tile_j * M] +\n\t\t\t//\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * M_GLOBAL;\n\t\t\twarp_ptr = &A[block_tile_j * M];\n\t\t\t\n\t\t\tconst int *warp_ptr_index = &B_index[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t\t\t\t\t\t((warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A);\n\n\t\t\tlane_ptr_index = (int *)(warp_ptr_index + tile_k_idx_sparse + (laneId / CHUNK_COPY_LINE_LANES_A));\n\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_A) * SHARED_OFFSET_A;\n\t\t}else{\n\t\t\twarp_ptr = &B[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * K_GLOBAL_SPARSE;\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_sparse +\n\t\t\t\t\t\t\t\t(laneId / CHUNK_COPY_LINE_LANES_B) * K_GLOBAL_SPARSE) +\n\t\t\t\t\t\t\t\t(laneId % CHUNK_COPY_LINE_LANES_B);\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_B) * SHARED_OFFSET_B;\n\t\t}\n\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      // shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n\t  int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n\t  \t? BLOCK_SIZE_K_SPARSE / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A)\n\t\t: BLOCK_SIZE_N / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\n\t  /*\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\t  */\n\n\t  /*\n      int tile_k_idx_A;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n          tile_k_idx_A = *(lane_ptr_index);\n      }\n\t  */\n\n\t  #pragma unroll\n\t  for(int i = 0; i < iter_index; i += 1){\n\t\t  if(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\tint tile_k_idx_A = *(lane_ptr_index);\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_A * M_GLOBAL) + (laneId % CHUNK_COPY_LINE_LANES_A);\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_A)) =\n\t\t\t\t*lane_ptr;\n\t\t\t//warp_ptr = (uint8_t *)((uint8_t *)warp_ptr + M_GLOBAL * (WARPS_PER_BLOCK / 2) *CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tlane_ptr_index = (int *)((int *)lane_ptr_index +  (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A;\n\t\t  }else{\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_B)) =\n\t\t\t\t*lane_ptr;\n\t\t\tlane_ptr = (int4 *)((uint8_t *)lane_ptr + K_GLOBAL_SPARSE * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B;\n\t\t  }\n\t  }\n\n      __syncthreads();\n\n\t#pragma unroll\n      for (int k_step = 0; k_step < CHUNK_K_SPARSE; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::col_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n\t#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i += 1) {\n\t\t\tsize_t shmem_idx_a = (warpId % BLOCK_COL_WARPS) * M * WARP_COL_TILES + (i * M);\n\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_a + k_step * K * SHARED_OFFSET_A;\n\n\t\t\twmma::load_matrix_sync(a[i], tile_ptr, SHARED_OFFSET_A);\n\t\t#pragma unroll\n\t\t\tfor(int j = 0; j < WARP_ROW_TILES; j += 1){\n\t\t\t\tif(i == 0){\n\t\t\t\t\tsize_t shmem_idx_b = shmem_idx_b_off +\n\t\t\t\t\t\t\t\t\t\t\t(warpId / BLOCK_COL_WARPS) * (WARP_ROW_TILES * N) * SHARED_OFFSET_B +\n\t\t\t\t\t\t\t\t\t\t\t(j * N) * SHARED_OFFSET_B;\n\t\t\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_b + k_step * K;\n\t\t\t\t\twmma::load_matrix_sync(b[j], tile_ptr, SHARED_OFFSET_B);\n\t\t\t\t}\n\t\t\t\twmma::mma_sync(c[j][i], a[i], b[j], c[j][i]);\n\t\t\t}\n\n        }\n      }\n\n      __syncthreads();\n    }\n\n    // This pointer is used to access the C and D matrix tiles this warp computes.\n\tint *shmem_warp_tile_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * N * WARP_ROW_TILES * SHMEM_STRIDE +\n\t(warpId % BLOCK_COL_WARPS) * SHMEM_OFFSET;\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t#pragma unroll\n\t\t\tfor(int t = 0; t < c[i][j].num_elements; t += 1){\n\t\t\t\tc[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n\t\t\t}\n\t\t\tint *tile_ptr = shmem_warp_tile_ptr + i * N * SHMEM_STRIDE + j * M;\n\t\t\twmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, C_LAYOUT);\n\t\t}\n\t}\n\n    __syncthreads();\n\n\tint *shmem_warp_stream_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N * SHMEM_STRIDE\n\t\t\t\t\t\t\t\t\t+ (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tconst size_t gmem_idx =\n\t\t(block_tile_i * N + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N) * GLOBAL_MEM_STRIDE +\n\t\tblock_tile_j * M + (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tuint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n\tint *shmem_lane_stream_ptr =\n\t\tshmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * SHMEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\t\n\tuint8_t *dst_gmem_lane_stream_ptr =\n\t\tdst_gmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * GLOBAL_MEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\n\tfor(int i = 0; i < WARP_ROW_TILES * N; i += SHARED_TO_GLOBAL_LINES_PER_WARP){\n\t\t*(dst_gmem_lane_stream_ptr + i * GLOBAL_MEM_STRIDE) = (uint8_t)(*(shmem_lane_stream_ptr + i * SHMEM_STRIDE));\n\t}\n\n\t__syncthreads();\n}\n", "launch_config": {"dimBlock": [256, 1], "dimGrid": [256, 1]}}, "17": {"code": "\n__global__ void compute_gemm_imma(float *input0, float *input1, float *input2, float* input3, float* input4, float*input5, float* input6,\n                                  float *output0) {\n    // keys need to be replaced 4096, 4096, K_GLOBA_VALUEL, 0.75, 64\n\n\t//extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B = reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_index = reinterpret_cast<int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n\tconst int WARP_SIZE = 32;\n    const int M = 16;\n    const int K = 16;\n    const int N = 16;\n    const int WMMA_M = 16;\n    const int WMMA_K = 16;\n    const int WMMA_N = 16;\n    const int M_GLOBAL = 4096;\n    const int K_GLOBAL = 1024;\n    const int N_GLOBAL = 4096;\n    const float SPARSITY = 0.75;\n    const int K_GLOBAL_SPARSE = (int(K_GLOBAL * (1-SPARSITY)));\n    const int M_TILES = (M_GLOBAL / M);\n    const int N_TILES = (N_GLOBAL / N);\n    const int K_TILES = (K_GLOBAL / K);\n    const int BLOCK_ROW_WARPS = 2;\n    const int BLOCK_COL_WARPS = 4;\n    const int WARP_ROW_TILES = 4;\n    const int WARP_COL_TILES = 2;\n    const int SKEW_UINT8 = 16;\n    const int BANK_VAL = 32;\n    const int NUM_BANK = (K_GLOBAL / BANK_VAL);\n    /////////// BLOCK_ROW_TILES <= N_TILES ////////////\n    // const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    // ////////// BLOCK_COL_TILES <= M_TILES ////////////\n    // const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    auto C_LAYOUT = wmma::mem_col_major;\n    const int CHUNK_K = 64;\n    const int CHUNK_K_SPARSE = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K_SPARSE = (int((CHUNK_K * K) * (1 - SPARSITY)));\n    const int WARP_COPY_BYTES = (WARP_SIZE * sizeof(int4));\n    const int CHUNK_LINE_BYTES_A (BLOCK_COL_TILES * M * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_A = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_A);\n    const int CHUNK_COPY_LINE_LANES_A = (CHUNK_LINE_BYTES_A / sizeof(int4));\n    const int SHARED_OFFSET_A = (BLOCK_COL_TILES * M + SKEW_UINT8);\n\n    const int CHUNK_LINE_BYTES_B = (BLOCK_SIZE_K_SPARSE * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_B = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_B);\n    const int CHUNK_COPY_LINE_LANES_B = (CHUNK_LINE_BYTES_B / sizeof(int4));\n    const int SHARED_OFFSET_B = (BLOCK_SIZE_K_SPARSE + SKEW_UINT8);\n    const int SHARED_TO_GLOBAL_BYTES_PER_LINE = ((WARP_COL_TILES * M) * sizeof(int));\n    const int SHARED_TO_GLOBAL_BYTES_PER_WARP = (WARP_SIZE * sizeof(int));\n    const int SHARED_TO_GLOBAL_LINES_PER_WARP = (SHARED_TO_GLOBAL_BYTES_PER_WARP / SHARED_TO_GLOBAL_BYTES_PER_LINE);\n    const int SHARED_TO_GLOBAL_LANES_PER_LINE = (WARP_SIZE / SHARED_TO_GLOBAL_LINES_PER_WARP);\n    const int SHARED_TO_GLOBAL_ITERS = ((WARP_ROW_TILES * N) / SHARED_TO_GLOBAL_LINES_PER_WARP);\n\n    const int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const int WARP_STRIDE = (WARP_COL_TILES * M);\n    const int WARPS_PER_BLOCK = (BLOCK_ROW_WARPS * BLOCK_COL_WARPS);\n    const int THREADS_PER_BLOCK = (WARP_SIZE * WARPS_PER_BLOCK);\n    const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    const int GLOBAL_MEM_STRIDE = M_GLOBAL;\n    const int SHMEM_STRIDE = (M * BLOCK_COL_TILES);\n    const int SHMEM_OFFSET = (M * WARP_COL_TILES);\n    const int BLOCK_SIZE_M = (M * BLOCK_COL_TILES);\n    const int BLOCK_SIZE_N = (N * BLOCK_ROW_TILES);\n\n   extern __shared__ uint8_t shmem[];\n\n\t// Warp and lane identification.\n\tconst unsigned int warpId = threadIdx.x / WARP_SIZE;\n\tconst unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n\n\t// Offset in shared memory from which the B matrix is stored.\n\t// const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\tconst size_t shmem_idx_b_off = BLOCK_SIZE_K_SPARSE * SHARED_OFFSET_A;\n\n\n\t// Each CTA slides along the 128 x 128 tiles from the top left corner of the\n\t// matrix to the right and down, and selects the next tile to compute. Once\n\t// there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n\tconst unsigned int block_tile_i =\n\t\t((block_pos * BLOCK_COL_TILES) / M_TILES) * (BLOCK_ROW_TILES);\n\tconst unsigned int block_tile_j = (block_pos * BLOCK_COL_TILES) % M_TILES;\n\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n\twmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_ROW_TILES]\n\t\t\t\t\t\t\t\t\t\t\t\t\t [WARP_COL_TILES];\n\n    // Load the C matrix tiles into fragments from shared memory.\n\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t\twmma::fill_fragment(c[i][j], 0);\n\t\t}\n\t}\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    //for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n    for(int tile_k_idx_sparse = 0, tile_k_idx = 0; tile_k_idx_sparse < K_GLOBAL_SPARSE; tile_k_idx_sparse += BLOCK_SIZE_K_SPARSE, tile_k_idx += BLOCK_SIZE_K){\n\n\t\tsize_t shmem_idx = \n\t\twarpId < (WARPS_PER_BLOCK / 2)\n\t\t\t? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A\n\t\t\t: (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B + shmem_idx_b_off;\n\n\t\tint4 *lane_ptr = NULL;\n\t\tint *lane_ptr_index = NULL;\n\t\tconst uint8_t *warp_ptr = NULL;\n\n\n\t\tif(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\t//warp_ptr = &A[block_tile_j * M] +\n\t\t\t//\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * M_GLOBAL;\n\t\t\twarp_ptr = &A[block_tile_j * M];\n\t\t\t\n\t\t\tconst int *warp_ptr_index = &B_index[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t\t\t\t\t\t((warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A);\n\n\t\t\tlane_ptr_index = (int *)(warp_ptr_index + tile_k_idx_sparse + (laneId / CHUNK_COPY_LINE_LANES_A));\n\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_A) * SHARED_OFFSET_A;\n\t\t}else{\n\t\t\twarp_ptr = &B[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * K_GLOBAL_SPARSE;\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_sparse +\n\t\t\t\t\t\t\t\t(laneId / CHUNK_COPY_LINE_LANES_B) * K_GLOBAL_SPARSE) +\n\t\t\t\t\t\t\t\t(laneId % CHUNK_COPY_LINE_LANES_B);\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_B) * SHARED_OFFSET_B;\n\t\t}\n\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      // shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n\t  int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n\t  \t? BLOCK_SIZE_K_SPARSE / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A)\n\t\t: BLOCK_SIZE_N / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\n\t  /*\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\t  */\n\n\t  /*\n      int tile_k_idx_A;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n          tile_k_idx_A = *(lane_ptr_index);\n      }\n\t  */\n\n\t  #pragma unroll\n\t  for(int i = 0; i < iter_index; i += 1){\n\t\t  if(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\tint tile_k_idx_A = *(lane_ptr_index);\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_A * M_GLOBAL) + (laneId % CHUNK_COPY_LINE_LANES_A);\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_A)) =\n\t\t\t\t*lane_ptr;\n\t\t\t//warp_ptr = (uint8_t *)((uint8_t *)warp_ptr + M_GLOBAL * (WARPS_PER_BLOCK / 2) *CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tlane_ptr_index = (int *)((int *)lane_ptr_index +  (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A;\n\t\t  }else{\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_B)) =\n\t\t\t\t*lane_ptr;\n\t\t\tlane_ptr = (int4 *)((uint8_t *)lane_ptr + K_GLOBAL_SPARSE * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B;\n\t\t  }\n\t  }\n\n      __syncthreads();\n\n\t#pragma unroll\n      for (int k_step = 0; k_step < CHUNK_K_SPARSE; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::col_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n\t#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i += 1) {\n\t\t\tsize_t shmem_idx_a = (warpId % BLOCK_COL_WARPS) * M * WARP_COL_TILES + (i * M);\n\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_a + k_step * K * SHARED_OFFSET_A;\n\n\t\t\twmma::load_matrix_sync(a[i], tile_ptr, SHARED_OFFSET_A);\n\t\t#pragma unroll\n\t\t\tfor(int j = 0; j < WARP_ROW_TILES; j += 1){\n\t\t\t\tif(i == 0){\n\t\t\t\t\tsize_t shmem_idx_b = shmem_idx_b_off +\n\t\t\t\t\t\t\t\t\t\t\t(warpId / BLOCK_COL_WARPS) * (WARP_ROW_TILES * N) * SHARED_OFFSET_B +\n\t\t\t\t\t\t\t\t\t\t\t(j * N) * SHARED_OFFSET_B;\n\t\t\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_b + k_step * K;\n\t\t\t\t\twmma::load_matrix_sync(b[j], tile_ptr, SHARED_OFFSET_B);\n\t\t\t\t}\n\t\t\t\twmma::mma_sync(c[j][i], a[i], b[j], c[j][i]);\n\t\t\t}\n\n        }\n      }\n\n      __syncthreads();\n    }\n\n    // This pointer is used to access the C and D matrix tiles this warp computes.\n\tint *shmem_warp_tile_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * N * WARP_ROW_TILES * SHMEM_STRIDE +\n\t(warpId % BLOCK_COL_WARPS) * SHMEM_OFFSET;\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t#pragma unroll\n\t\t\tfor(int t = 0; t < c[i][j].num_elements; t += 1){\n\t\t\t\tc[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n\t\t\t}\n\t\t\tint *tile_ptr = shmem_warp_tile_ptr + i * N * SHMEM_STRIDE + j * M;\n\t\t\twmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, C_LAYOUT);\n\t\t}\n\t}\n\n    __syncthreads();\n\n\tint *shmem_warp_stream_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N * SHMEM_STRIDE\n\t\t\t\t\t\t\t\t\t+ (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tconst size_t gmem_idx =\n\t\t(block_tile_i * N + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N) * GLOBAL_MEM_STRIDE +\n\t\tblock_tile_j * M + (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tuint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n\tint *shmem_lane_stream_ptr =\n\t\tshmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * SHMEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\t\n\tuint8_t *dst_gmem_lane_stream_ptr =\n\t\tdst_gmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * GLOBAL_MEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\n\tfor(int i = 0; i < WARP_ROW_TILES * N; i += SHARED_TO_GLOBAL_LINES_PER_WARP){\n\t\t*(dst_gmem_lane_stream_ptr + i * GLOBAL_MEM_STRIDE) = (uint8_t)(*(shmem_lane_stream_ptr + i * SHMEM_STRIDE));\n\t}\n\n\t__syncthreads();\n}\n", "launch_config": {"dimBlock": [1024, 1], "dimGrid": [256, 1]}}, "18": {"code": "\n__global__ void compute_gemm_imma(float *input0, float *input1, float *input2, float* input3, float* input4, float*input5, float* input6,\n                                  float *output0) {\n    // keys need to be replaced 4096, 1024, K_GLOBA_VALUEL, 0.75, 64\n\n\t//extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B = reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_index = reinterpret_cast<int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n\tconst int WARP_SIZE = 32;\n    const int M = 16;\n    const int K = 16;\n    const int N = 16;\n    const int WMMA_M = 16;\n    const int WMMA_K = 16;\n    const int WMMA_N = 16;\n    const int M_GLOBAL = 4096;\n    const int K_GLOBAL = 4096;\n    const int N_GLOBAL = 1024;\n    const float SPARSITY = 0.75;\n    const int K_GLOBAL_SPARSE = (int(K_GLOBAL * (1-SPARSITY)));\n    const int M_TILES = (M_GLOBAL / M);\n    const int N_TILES = (N_GLOBAL / N);\n    const int K_TILES = (K_GLOBAL / K);\n    const int BLOCK_ROW_WARPS = 2;\n    const int BLOCK_COL_WARPS = 4;\n    const int WARP_ROW_TILES = 4;\n    const int WARP_COL_TILES = 2;\n    const int SKEW_UINT8 = 16;\n    const int BANK_VAL = 32;\n    const int NUM_BANK = (K_GLOBAL / BANK_VAL);\n    /////////// BLOCK_ROW_TILES <= N_TILES ////////////\n    // const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    // ////////// BLOCK_COL_TILES <= M_TILES ////////////\n    // const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    auto C_LAYOUT = wmma::mem_col_major;\n    const int CHUNK_K = 64;\n    const int CHUNK_K_SPARSE = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K_SPARSE = (int((CHUNK_K * K) * (1 - SPARSITY)));\n    const int WARP_COPY_BYTES = (WARP_SIZE * sizeof(int4));\n    const int CHUNK_LINE_BYTES_A (BLOCK_COL_TILES * M * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_A = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_A);\n    const int CHUNK_COPY_LINE_LANES_A = (CHUNK_LINE_BYTES_A / sizeof(int4));\n    const int SHARED_OFFSET_A = (BLOCK_COL_TILES * M + SKEW_UINT8);\n\n    const int CHUNK_LINE_BYTES_B = (BLOCK_SIZE_K_SPARSE * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_B = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_B);\n    const int CHUNK_COPY_LINE_LANES_B = (CHUNK_LINE_BYTES_B / sizeof(int4));\n    const int SHARED_OFFSET_B = (BLOCK_SIZE_K_SPARSE + SKEW_UINT8);\n    const int SHARED_TO_GLOBAL_BYTES_PER_LINE = ((WARP_COL_TILES * M) * sizeof(int));\n    const int SHARED_TO_GLOBAL_BYTES_PER_WARP = (WARP_SIZE * sizeof(int));\n    const int SHARED_TO_GLOBAL_LINES_PER_WARP = (SHARED_TO_GLOBAL_BYTES_PER_WARP / SHARED_TO_GLOBAL_BYTES_PER_LINE);\n    const int SHARED_TO_GLOBAL_LANES_PER_LINE = (WARP_SIZE / SHARED_TO_GLOBAL_LINES_PER_WARP);\n    const int SHARED_TO_GLOBAL_ITERS = ((WARP_ROW_TILES * N) / SHARED_TO_GLOBAL_LINES_PER_WARP);\n\n    const int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const int WARP_STRIDE = (WARP_COL_TILES * M);\n    const int WARPS_PER_BLOCK = (BLOCK_ROW_WARPS * BLOCK_COL_WARPS);\n    const int THREADS_PER_BLOCK = (WARP_SIZE * WARPS_PER_BLOCK);\n    const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    const int GLOBAL_MEM_STRIDE = M_GLOBAL;\n    const int SHMEM_STRIDE = (M * BLOCK_COL_TILES);\n    const int SHMEM_OFFSET = (M * WARP_COL_TILES);\n    const int BLOCK_SIZE_M = (M * BLOCK_COL_TILES);\n    const int BLOCK_SIZE_N = (N * BLOCK_ROW_TILES);\n\n   extern __shared__ uint8_t shmem[];\n\n\t// Warp and lane identification.\n\tconst unsigned int warpId = threadIdx.x / WARP_SIZE;\n\tconst unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n\n\t// Offset in shared memory from which the B matrix is stored.\n\t// const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\tconst size_t shmem_idx_b_off = BLOCK_SIZE_K_SPARSE * SHARED_OFFSET_A;\n\n\n\t// Each CTA slides along the 128 x 128 tiles from the top left corner of the\n\t// matrix to the right and down, and selects the next tile to compute. Once\n\t// there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n\tconst unsigned int block_tile_i =\n\t\t((block_pos * BLOCK_COL_TILES) / M_TILES) * (BLOCK_ROW_TILES);\n\tconst unsigned int block_tile_j = (block_pos * BLOCK_COL_TILES) % M_TILES;\n\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n\twmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_ROW_TILES]\n\t\t\t\t\t\t\t\t\t\t\t\t\t [WARP_COL_TILES];\n\n    // Load the C matrix tiles into fragments from shared memory.\n\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t\twmma::fill_fragment(c[i][j], 0);\n\t\t}\n\t}\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    //for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n    for(int tile_k_idx_sparse = 0, tile_k_idx = 0; tile_k_idx_sparse < K_GLOBAL_SPARSE; tile_k_idx_sparse += BLOCK_SIZE_K_SPARSE, tile_k_idx += BLOCK_SIZE_K){\n\n\t\tsize_t shmem_idx = \n\t\twarpId < (WARPS_PER_BLOCK / 2)\n\t\t\t? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A\n\t\t\t: (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B + shmem_idx_b_off;\n\n\t\tint4 *lane_ptr = NULL;\n\t\tint *lane_ptr_index = NULL;\n\t\tconst uint8_t *warp_ptr = NULL;\n\n\n\t\tif(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\t//warp_ptr = &A[block_tile_j * M] +\n\t\t\t//\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * M_GLOBAL;\n\t\t\twarp_ptr = &A[block_tile_j * M];\n\t\t\t\n\t\t\tconst int *warp_ptr_index = &B_index[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t\t\t\t\t\t((warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A);\n\n\t\t\tlane_ptr_index = (int *)(warp_ptr_index + tile_k_idx_sparse + (laneId / CHUNK_COPY_LINE_LANES_A));\n\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_A) * SHARED_OFFSET_A;\n\t\t}else{\n\t\t\twarp_ptr = &B[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * K_GLOBAL_SPARSE;\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_sparse +\n\t\t\t\t\t\t\t\t(laneId / CHUNK_COPY_LINE_LANES_B) * K_GLOBAL_SPARSE) +\n\t\t\t\t\t\t\t\t(laneId % CHUNK_COPY_LINE_LANES_B);\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_B) * SHARED_OFFSET_B;\n\t\t}\n\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      // shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n\t  int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n\t  \t? BLOCK_SIZE_K_SPARSE / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A)\n\t\t: BLOCK_SIZE_N / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\n\t  /*\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\t  */\n\n\t  /*\n      int tile_k_idx_A;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n          tile_k_idx_A = *(lane_ptr_index);\n      }\n\t  */\n\n\t  #pragma unroll\n\t  for(int i = 0; i < iter_index; i += 1){\n\t\t  if(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\tint tile_k_idx_A = *(lane_ptr_index);\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_A * M_GLOBAL) + (laneId % CHUNK_COPY_LINE_LANES_A);\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_A)) =\n\t\t\t\t*lane_ptr;\n\t\t\t//warp_ptr = (uint8_t *)((uint8_t *)warp_ptr + M_GLOBAL * (WARPS_PER_BLOCK / 2) *CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tlane_ptr_index = (int *)((int *)lane_ptr_index +  (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A;\n\t\t  }else{\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_B)) =\n\t\t\t\t*lane_ptr;\n\t\t\tlane_ptr = (int4 *)((uint8_t *)lane_ptr + K_GLOBAL_SPARSE * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B;\n\t\t  }\n\t  }\n\n      __syncthreads();\n\n\t#pragma unroll\n      for (int k_step = 0; k_step < CHUNK_K_SPARSE; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::col_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n\t#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i += 1) {\n\t\t\tsize_t shmem_idx_a = (warpId % BLOCK_COL_WARPS) * M * WARP_COL_TILES + (i * M);\n\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_a + k_step * K * SHARED_OFFSET_A;\n\n\t\t\twmma::load_matrix_sync(a[i], tile_ptr, SHARED_OFFSET_A);\n\t\t#pragma unroll\n\t\t\tfor(int j = 0; j < WARP_ROW_TILES; j += 1){\n\t\t\t\tif(i == 0){\n\t\t\t\t\tsize_t shmem_idx_b = shmem_idx_b_off +\n\t\t\t\t\t\t\t\t\t\t\t(warpId / BLOCK_COL_WARPS) * (WARP_ROW_TILES * N) * SHARED_OFFSET_B +\n\t\t\t\t\t\t\t\t\t\t\t(j * N) * SHARED_OFFSET_B;\n\t\t\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_b + k_step * K;\n\t\t\t\t\twmma::load_matrix_sync(b[j], tile_ptr, SHARED_OFFSET_B);\n\t\t\t\t}\n\t\t\t\twmma::mma_sync(c[j][i], a[i], b[j], c[j][i]);\n\t\t\t}\n\n        }\n      }\n\n      __syncthreads();\n    }\n\n    // This pointer is used to access the C and D matrix tiles this warp computes.\n\tint *shmem_warp_tile_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * N * WARP_ROW_TILES * SHMEM_STRIDE +\n\t(warpId % BLOCK_COL_WARPS) * SHMEM_OFFSET;\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t#pragma unroll\n\t\t\tfor(int t = 0; t < c[i][j].num_elements; t += 1){\n\t\t\t\tc[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n\t\t\t}\n\t\t\tint *tile_ptr = shmem_warp_tile_ptr + i * N * SHMEM_STRIDE + j * M;\n\t\t\twmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, C_LAYOUT);\n\t\t}\n\t}\n\n    __syncthreads();\n\n\tint *shmem_warp_stream_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N * SHMEM_STRIDE\n\t\t\t\t\t\t\t\t\t+ (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tconst size_t gmem_idx =\n\t\t(block_tile_i * N + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N) * GLOBAL_MEM_STRIDE +\n\t\tblock_tile_j * M + (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tuint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n\tint *shmem_lane_stream_ptr =\n\t\tshmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * SHMEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\t\n\tuint8_t *dst_gmem_lane_stream_ptr =\n\t\tdst_gmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * GLOBAL_MEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\n\tfor(int i = 0; i < WARP_ROW_TILES * N; i += SHARED_TO_GLOBAL_LINES_PER_WARP){\n\t\t*(dst_gmem_lane_stream_ptr + i * GLOBAL_MEM_STRIDE) = (uint8_t)(*(shmem_lane_stream_ptr + i * SHMEM_STRIDE));\n\t}\n\n\t__syncthreads();\n}\n", "launch_config": {"dimBlock": [256, 1], "dimGrid": [256, 1]}}, "19": {"code": "\n__global__ void compute_gemm_imma(float *input0, float *input1, float *input2, float* input3, float* input4, float*input5, float* input6,\n                                  float *output0) {\n    // keys need to be replaced 4096, 1024, K_GLOBA_VALUEL, 0.75, 64\n\n\t//extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B = reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_index = reinterpret_cast<int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n\tconst int WARP_SIZE = 32;\n    const int M = 16;\n    const int K = 16;\n    const int N = 16;\n    const int WMMA_M = 16;\n    const int WMMA_K = 16;\n    const int WMMA_N = 16;\n    const int M_GLOBAL = 4096;\n    const int K_GLOBAL = 1024;\n    const int N_GLOBAL = 1024;\n    const float SPARSITY = 0.75;\n    const int K_GLOBAL_SPARSE = (int(K_GLOBAL * (1-SPARSITY)));\n    const int M_TILES = (M_GLOBAL / M);\n    const int N_TILES = (N_GLOBAL / N);\n    const int K_TILES = (K_GLOBAL / K);\n    const int BLOCK_ROW_WARPS = 2;\n    const int BLOCK_COL_WARPS = 4;\n    const int WARP_ROW_TILES = 4;\n    const int WARP_COL_TILES = 2;\n    const int SKEW_UINT8 = 16;\n    const int BANK_VAL = 32;\n    const int NUM_BANK = (K_GLOBAL / BANK_VAL);\n    /////////// BLOCK_ROW_TILES <= N_TILES ////////////\n    // const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    // ////////// BLOCK_COL_TILES <= M_TILES ////////////\n    // const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    auto C_LAYOUT = wmma::mem_col_major;\n    const int CHUNK_K = 64;\n    const int CHUNK_K_SPARSE = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K_SPARSE = (int((CHUNK_K * K) * (1 - SPARSITY)));\n    const int WARP_COPY_BYTES = (WARP_SIZE * sizeof(int4));\n    const int CHUNK_LINE_BYTES_A (BLOCK_COL_TILES * M * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_A = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_A);\n    const int CHUNK_COPY_LINE_LANES_A = (CHUNK_LINE_BYTES_A / sizeof(int4));\n    const int SHARED_OFFSET_A = (BLOCK_COL_TILES * M + SKEW_UINT8);\n\n    const int CHUNK_LINE_BYTES_B = (BLOCK_SIZE_K_SPARSE * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_B = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_B);\n    const int CHUNK_COPY_LINE_LANES_B = (CHUNK_LINE_BYTES_B / sizeof(int4));\n    const int SHARED_OFFSET_B = (BLOCK_SIZE_K_SPARSE + SKEW_UINT8);\n    const int SHARED_TO_GLOBAL_BYTES_PER_LINE = ((WARP_COL_TILES * M) * sizeof(int));\n    const int SHARED_TO_GLOBAL_BYTES_PER_WARP = (WARP_SIZE * sizeof(int));\n    const int SHARED_TO_GLOBAL_LINES_PER_WARP = (SHARED_TO_GLOBAL_BYTES_PER_WARP / SHARED_TO_GLOBAL_BYTES_PER_LINE);\n    const int SHARED_TO_GLOBAL_LANES_PER_LINE = (WARP_SIZE / SHARED_TO_GLOBAL_LINES_PER_WARP);\n    const int SHARED_TO_GLOBAL_ITERS = ((WARP_ROW_TILES * N) / SHARED_TO_GLOBAL_LINES_PER_WARP);\n\n    const int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const int WARP_STRIDE = (WARP_COL_TILES * M);\n    const int WARPS_PER_BLOCK = (BLOCK_ROW_WARPS * BLOCK_COL_WARPS);\n    const int THREADS_PER_BLOCK = (WARP_SIZE * WARPS_PER_BLOCK);\n    const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    const int GLOBAL_MEM_STRIDE = M_GLOBAL;\n    const int SHMEM_STRIDE = (M * BLOCK_COL_TILES);\n    const int SHMEM_OFFSET = (M * WARP_COL_TILES);\n    const int BLOCK_SIZE_M = (M * BLOCK_COL_TILES);\n    const int BLOCK_SIZE_N = (N * BLOCK_ROW_TILES);\n\n   extern __shared__ uint8_t shmem[];\n\n\t// Warp and lane identification.\n\tconst unsigned int warpId = threadIdx.x / WARP_SIZE;\n\tconst unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n\n\t// Offset in shared memory from which the B matrix is stored.\n\t// const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\tconst size_t shmem_idx_b_off = BLOCK_SIZE_K_SPARSE * SHARED_OFFSET_A;\n\n\n\t// Each CTA slides along the 128 x 128 tiles from the top left corner of the\n\t// matrix to the right and down, and selects the next tile to compute. Once\n\t// there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n\tconst unsigned int block_tile_i =\n\t\t((block_pos * BLOCK_COL_TILES) / M_TILES) * (BLOCK_ROW_TILES);\n\tconst unsigned int block_tile_j = (block_pos * BLOCK_COL_TILES) % M_TILES;\n\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n\twmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_ROW_TILES]\n\t\t\t\t\t\t\t\t\t\t\t\t\t [WARP_COL_TILES];\n\n    // Load the C matrix tiles into fragments from shared memory.\n\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t\twmma::fill_fragment(c[i][j], 0);\n\t\t}\n\t}\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    //for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n    for(int tile_k_idx_sparse = 0, tile_k_idx = 0; tile_k_idx_sparse < K_GLOBAL_SPARSE; tile_k_idx_sparse += BLOCK_SIZE_K_SPARSE, tile_k_idx += BLOCK_SIZE_K){\n\n\t\tsize_t shmem_idx = \n\t\twarpId < (WARPS_PER_BLOCK / 2)\n\t\t\t? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A\n\t\t\t: (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B + shmem_idx_b_off;\n\n\t\tint4 *lane_ptr = NULL;\n\t\tint *lane_ptr_index = NULL;\n\t\tconst uint8_t *warp_ptr = NULL;\n\n\n\t\tif(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\t//warp_ptr = &A[block_tile_j * M] +\n\t\t\t//\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * M_GLOBAL;\n\t\t\twarp_ptr = &A[block_tile_j * M];\n\t\t\t\n\t\t\tconst int *warp_ptr_index = &B_index[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t\t\t\t\t\t((warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A);\n\n\t\t\tlane_ptr_index = (int *)(warp_ptr_index + tile_k_idx_sparse + (laneId / CHUNK_COPY_LINE_LANES_A));\n\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_A) * SHARED_OFFSET_A;\n\t\t}else{\n\t\t\twarp_ptr = &B[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * K_GLOBAL_SPARSE;\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_sparse +\n\t\t\t\t\t\t\t\t(laneId / CHUNK_COPY_LINE_LANES_B) * K_GLOBAL_SPARSE) +\n\t\t\t\t\t\t\t\t(laneId % CHUNK_COPY_LINE_LANES_B);\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_B) * SHARED_OFFSET_B;\n\t\t}\n\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      // shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n\t  int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n\t  \t? BLOCK_SIZE_K_SPARSE / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A)\n\t\t: BLOCK_SIZE_N / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\n\t  /*\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\t  */\n\n\t  /*\n      int tile_k_idx_A;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n          tile_k_idx_A = *(lane_ptr_index);\n      }\n\t  */\n\n\t  #pragma unroll\n\t  for(int i = 0; i < iter_index; i += 1){\n\t\t  if(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\tint tile_k_idx_A = *(lane_ptr_index);\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_A * M_GLOBAL) + (laneId % CHUNK_COPY_LINE_LANES_A);\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_A)) =\n\t\t\t\t*lane_ptr;\n\t\t\t//warp_ptr = (uint8_t *)((uint8_t *)warp_ptr + M_GLOBAL * (WARPS_PER_BLOCK / 2) *CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tlane_ptr_index = (int *)((int *)lane_ptr_index +  (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A;\n\t\t  }else{\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_B)) =\n\t\t\t\t*lane_ptr;\n\t\t\tlane_ptr = (int4 *)((uint8_t *)lane_ptr + K_GLOBAL_SPARSE * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B;\n\t\t  }\n\t  }\n\n      __syncthreads();\n\n\t#pragma unroll\n      for (int k_step = 0; k_step < CHUNK_K_SPARSE; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::col_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n\t#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i += 1) {\n\t\t\tsize_t shmem_idx_a = (warpId % BLOCK_COL_WARPS) * M * WARP_COL_TILES + (i * M);\n\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_a + k_step * K * SHARED_OFFSET_A;\n\n\t\t\twmma::load_matrix_sync(a[i], tile_ptr, SHARED_OFFSET_A);\n\t\t#pragma unroll\n\t\t\tfor(int j = 0; j < WARP_ROW_TILES; j += 1){\n\t\t\t\tif(i == 0){\n\t\t\t\t\tsize_t shmem_idx_b = shmem_idx_b_off +\n\t\t\t\t\t\t\t\t\t\t\t(warpId / BLOCK_COL_WARPS) * (WARP_ROW_TILES * N) * SHARED_OFFSET_B +\n\t\t\t\t\t\t\t\t\t\t\t(j * N) * SHARED_OFFSET_B;\n\t\t\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_b + k_step * K;\n\t\t\t\t\twmma::load_matrix_sync(b[j], tile_ptr, SHARED_OFFSET_B);\n\t\t\t\t}\n\t\t\t\twmma::mma_sync(c[j][i], a[i], b[j], c[j][i]);\n\t\t\t}\n\n        }\n      }\n\n      __syncthreads();\n    }\n\n    // This pointer is used to access the C and D matrix tiles this warp computes.\n\tint *shmem_warp_tile_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * N * WARP_ROW_TILES * SHMEM_STRIDE +\n\t(warpId % BLOCK_COL_WARPS) * SHMEM_OFFSET;\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t#pragma unroll\n\t\t\tfor(int t = 0; t < c[i][j].num_elements; t += 1){\n\t\t\t\tc[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n\t\t\t}\n\t\t\tint *tile_ptr = shmem_warp_tile_ptr + i * N * SHMEM_STRIDE + j * M;\n\t\t\twmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, C_LAYOUT);\n\t\t}\n\t}\n\n    __syncthreads();\n\n\tint *shmem_warp_stream_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N * SHMEM_STRIDE\n\t\t\t\t\t\t\t\t\t+ (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tconst size_t gmem_idx =\n\t\t(block_tile_i * N + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N) * GLOBAL_MEM_STRIDE +\n\t\tblock_tile_j * M + (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tuint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n\tint *shmem_lane_stream_ptr =\n\t\tshmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * SHMEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\t\n\tuint8_t *dst_gmem_lane_stream_ptr =\n\t\tdst_gmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * GLOBAL_MEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\n\tfor(int i = 0; i < WARP_ROW_TILES * N; i += SHARED_TO_GLOBAL_LINES_PER_WARP){\n\t\t*(dst_gmem_lane_stream_ptr + i * GLOBAL_MEM_STRIDE) = (uint8_t)(*(shmem_lane_stream_ptr + i * SHMEM_STRIDE));\n\t}\n\n\t__syncthreads();\n}\n", "launch_config": {"dimBlock": [256, 1], "dimGrid": [256, 1]}}, "20": {"code": "\n__global__ void compute_gemm_imma(float *input0, float *input1, float *input2, float* input3, float* input4, float*input5, float* input6,\n                                  float *output0) {\n    // keys need to be replaced 4096, 1024, K_GLOBA_VALUEL, 0.75, 64\n\n\t//extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B = reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_index = reinterpret_cast<int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n\tconst int WARP_SIZE = 32;\n    const int M = 16;\n    const int K = 16;\n    const int N = 16;\n    const int WMMA_M = 16;\n    const int WMMA_K = 16;\n    const int WMMA_N = 16;\n    const int M_GLOBAL = 4096;\n    const int K_GLOBAL = 1024;\n    const int N_GLOBAL = 1024;\n    const float SPARSITY = 0.75;\n    const int K_GLOBAL_SPARSE = (int(K_GLOBAL * (1-SPARSITY)));\n    const int M_TILES = (M_GLOBAL / M);\n    const int N_TILES = (N_GLOBAL / N);\n    const int K_TILES = (K_GLOBAL / K);\n    const int BLOCK_ROW_WARPS = 2;\n    const int BLOCK_COL_WARPS = 4;\n    const int WARP_ROW_TILES = 4;\n    const int WARP_COL_TILES = 2;\n    const int SKEW_UINT8 = 16;\n    const int BANK_VAL = 32;\n    const int NUM_BANK = (K_GLOBAL / BANK_VAL);\n    /////////// BLOCK_ROW_TILES <= N_TILES ////////////\n    // const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    // ////////// BLOCK_COL_TILES <= M_TILES ////////////\n    // const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    auto C_LAYOUT = wmma::mem_col_major;\n    const int CHUNK_K = 64;\n    const int CHUNK_K_SPARSE = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K_SPARSE = (int((CHUNK_K * K) * (1 - SPARSITY)));\n    const int WARP_COPY_BYTES = (WARP_SIZE * sizeof(int4));\n    const int CHUNK_LINE_BYTES_A (BLOCK_COL_TILES * M * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_A = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_A);\n    const int CHUNK_COPY_LINE_LANES_A = (CHUNK_LINE_BYTES_A / sizeof(int4));\n    const int SHARED_OFFSET_A = (BLOCK_COL_TILES * M + SKEW_UINT8);\n\n    const int CHUNK_LINE_BYTES_B = (BLOCK_SIZE_K_SPARSE * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_B = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_B);\n    const int CHUNK_COPY_LINE_LANES_B = (CHUNK_LINE_BYTES_B / sizeof(int4));\n    const int SHARED_OFFSET_B = (BLOCK_SIZE_K_SPARSE + SKEW_UINT8);\n    const int SHARED_TO_GLOBAL_BYTES_PER_LINE = ((WARP_COL_TILES * M) * sizeof(int));\n    const int SHARED_TO_GLOBAL_BYTES_PER_WARP = (WARP_SIZE * sizeof(int));\n    const int SHARED_TO_GLOBAL_LINES_PER_WARP = (SHARED_TO_GLOBAL_BYTES_PER_WARP / SHARED_TO_GLOBAL_BYTES_PER_LINE);\n    const int SHARED_TO_GLOBAL_LANES_PER_LINE = (WARP_SIZE / SHARED_TO_GLOBAL_LINES_PER_WARP);\n    const int SHARED_TO_GLOBAL_ITERS = ((WARP_ROW_TILES * N) / SHARED_TO_GLOBAL_LINES_PER_WARP);\n\n    const int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const int WARP_STRIDE = (WARP_COL_TILES * M);\n    const int WARPS_PER_BLOCK = (BLOCK_ROW_WARPS * BLOCK_COL_WARPS);\n    const int THREADS_PER_BLOCK = (WARP_SIZE * WARPS_PER_BLOCK);\n    const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    const int GLOBAL_MEM_STRIDE = M_GLOBAL;\n    const int SHMEM_STRIDE = (M * BLOCK_COL_TILES);\n    const int SHMEM_OFFSET = (M * WARP_COL_TILES);\n    const int BLOCK_SIZE_M = (M * BLOCK_COL_TILES);\n    const int BLOCK_SIZE_N = (N * BLOCK_ROW_TILES);\n\n   extern __shared__ uint8_t shmem[];\n\n\t// Warp and lane identification.\n\tconst unsigned int warpId = threadIdx.x / WARP_SIZE;\n\tconst unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n\n\t// Offset in shared memory from which the B matrix is stored.\n\t// const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\tconst size_t shmem_idx_b_off = BLOCK_SIZE_K_SPARSE * SHARED_OFFSET_A;\n\n\n\t// Each CTA slides along the 128 x 128 tiles from the top left corner of the\n\t// matrix to the right and down, and selects the next tile to compute. Once\n\t// there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n\tconst unsigned int block_tile_i =\n\t\t((block_pos * BLOCK_COL_TILES) / M_TILES) * (BLOCK_ROW_TILES);\n\tconst unsigned int block_tile_j = (block_pos * BLOCK_COL_TILES) % M_TILES;\n\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n\twmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_ROW_TILES]\n\t\t\t\t\t\t\t\t\t\t\t\t\t [WARP_COL_TILES];\n\n    // Load the C matrix tiles into fragments from shared memory.\n\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t\twmma::fill_fragment(c[i][j], 0);\n\t\t}\n\t}\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    //for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n    for(int tile_k_idx_sparse = 0, tile_k_idx = 0; tile_k_idx_sparse < K_GLOBAL_SPARSE; tile_k_idx_sparse += BLOCK_SIZE_K_SPARSE, tile_k_idx += BLOCK_SIZE_K){\n\n\t\tsize_t shmem_idx = \n\t\twarpId < (WARPS_PER_BLOCK / 2)\n\t\t\t? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A\n\t\t\t: (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B + shmem_idx_b_off;\n\n\t\tint4 *lane_ptr = NULL;\n\t\tint *lane_ptr_index = NULL;\n\t\tconst uint8_t *warp_ptr = NULL;\n\n\n\t\tif(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\t//warp_ptr = &A[block_tile_j * M] +\n\t\t\t//\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * M_GLOBAL;\n\t\t\twarp_ptr = &A[block_tile_j * M];\n\t\t\t\n\t\t\tconst int *warp_ptr_index = &B_index[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t\t\t\t\t\t((warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A);\n\n\t\t\tlane_ptr_index = (int *)(warp_ptr_index + tile_k_idx_sparse + (laneId / CHUNK_COPY_LINE_LANES_A));\n\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_A) * SHARED_OFFSET_A;\n\t\t}else{\n\t\t\twarp_ptr = &B[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * K_GLOBAL_SPARSE;\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_sparse +\n\t\t\t\t\t\t\t\t(laneId / CHUNK_COPY_LINE_LANES_B) * K_GLOBAL_SPARSE) +\n\t\t\t\t\t\t\t\t(laneId % CHUNK_COPY_LINE_LANES_B);\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_B) * SHARED_OFFSET_B;\n\t\t}\n\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      // shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n\t  int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n\t  \t? BLOCK_SIZE_K_SPARSE / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A)\n\t\t: BLOCK_SIZE_N / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\n\t  /*\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\t  */\n\n\t  /*\n      int tile_k_idx_A;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n          tile_k_idx_A = *(lane_ptr_index);\n      }\n\t  */\n\n\t  #pragma unroll\n\t  for(int i = 0; i < iter_index; i += 1){\n\t\t  if(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\tint tile_k_idx_A = *(lane_ptr_index);\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_A * M_GLOBAL) + (laneId % CHUNK_COPY_LINE_LANES_A);\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_A)) =\n\t\t\t\t*lane_ptr;\n\t\t\t//warp_ptr = (uint8_t *)((uint8_t *)warp_ptr + M_GLOBAL * (WARPS_PER_BLOCK / 2) *CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tlane_ptr_index = (int *)((int *)lane_ptr_index +  (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A;\n\t\t  }else{\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_B)) =\n\t\t\t\t*lane_ptr;\n\t\t\tlane_ptr = (int4 *)((uint8_t *)lane_ptr + K_GLOBAL_SPARSE * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B;\n\t\t  }\n\t  }\n\n      __syncthreads();\n\n\t#pragma unroll\n      for (int k_step = 0; k_step < CHUNK_K_SPARSE; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::col_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n\t#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i += 1) {\n\t\t\tsize_t shmem_idx_a = (warpId % BLOCK_COL_WARPS) * M * WARP_COL_TILES + (i * M);\n\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_a + k_step * K * SHARED_OFFSET_A;\n\n\t\t\twmma::load_matrix_sync(a[i], tile_ptr, SHARED_OFFSET_A);\n\t\t#pragma unroll\n\t\t\tfor(int j = 0; j < WARP_ROW_TILES; j += 1){\n\t\t\t\tif(i == 0){\n\t\t\t\t\tsize_t shmem_idx_b = shmem_idx_b_off +\n\t\t\t\t\t\t\t\t\t\t\t(warpId / BLOCK_COL_WARPS) * (WARP_ROW_TILES * N) * SHARED_OFFSET_B +\n\t\t\t\t\t\t\t\t\t\t\t(j * N) * SHARED_OFFSET_B;\n\t\t\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_b + k_step * K;\n\t\t\t\t\twmma::load_matrix_sync(b[j], tile_ptr, SHARED_OFFSET_B);\n\t\t\t\t}\n\t\t\t\twmma::mma_sync(c[j][i], a[i], b[j], c[j][i]);\n\t\t\t}\n\n        }\n      }\n\n      __syncthreads();\n    }\n\n    // This pointer is used to access the C and D matrix tiles this warp computes.\n\tint *shmem_warp_tile_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * N * WARP_ROW_TILES * SHMEM_STRIDE +\n\t(warpId % BLOCK_COL_WARPS) * SHMEM_OFFSET;\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t#pragma unroll\n\t\t\tfor(int t = 0; t < c[i][j].num_elements; t += 1){\n\t\t\t\tc[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n\t\t\t}\n\t\t\tint *tile_ptr = shmem_warp_tile_ptr + i * N * SHMEM_STRIDE + j * M;\n\t\t\twmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, C_LAYOUT);\n\t\t}\n\t}\n\n    __syncthreads();\n\n\tint *shmem_warp_stream_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N * SHMEM_STRIDE\n\t\t\t\t\t\t\t\t\t+ (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tconst size_t gmem_idx =\n\t\t(block_tile_i * N + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N) * GLOBAL_MEM_STRIDE +\n\t\tblock_tile_j * M + (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tuint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n\tint *shmem_lane_stream_ptr =\n\t\tshmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * SHMEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\t\n\tuint8_t *dst_gmem_lane_stream_ptr =\n\t\tdst_gmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * GLOBAL_MEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\n\tfor(int i = 0; i < WARP_ROW_TILES * N; i += SHARED_TO_GLOBAL_LINES_PER_WARP){\n\t\t*(dst_gmem_lane_stream_ptr + i * GLOBAL_MEM_STRIDE) = (uint8_t)(*(shmem_lane_stream_ptr + i * SHMEM_STRIDE));\n\t}\n\n\t__syncthreads();\n}\n", "launch_config": {"dimBlock": [256, 1], "dimGrid": [256, 1]}}, "21": {"code": "\n__global__ void compute_gemm_imma(float *input0, float *input1, float *input2, float* input3, float* input4, float*input5, float* input6,\n                                  float *output0) {\n    // keys need to be replaced 4096, 1024, K_GLOBA_VALUEL, 0.75, 64\n\n\t//extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B = reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_index = reinterpret_cast<int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n\tconst int WARP_SIZE = 32;\n    const int M = 16;\n    const int K = 16;\n    const int N = 16;\n    const int WMMA_M = 16;\n    const int WMMA_K = 16;\n    const int WMMA_N = 16;\n    const int M_GLOBAL = 4096;\n    const int K_GLOBAL = 1024;\n    const int N_GLOBAL = 1024;\n    const float SPARSITY = 0.75;\n    const int K_GLOBAL_SPARSE = (int(K_GLOBAL * (1-SPARSITY)));\n    const int M_TILES = (M_GLOBAL / M);\n    const int N_TILES = (N_GLOBAL / N);\n    const int K_TILES = (K_GLOBAL / K);\n    const int BLOCK_ROW_WARPS = 2;\n    const int BLOCK_COL_WARPS = 4;\n    const int WARP_ROW_TILES = 4;\n    const int WARP_COL_TILES = 2;\n    const int SKEW_UINT8 = 16;\n    const int BANK_VAL = 32;\n    const int NUM_BANK = (K_GLOBAL / BANK_VAL);\n    /////////// BLOCK_ROW_TILES <= N_TILES ////////////\n    // const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    // ////////// BLOCK_COL_TILES <= M_TILES ////////////\n    // const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    auto C_LAYOUT = wmma::mem_col_major;\n    const int CHUNK_K = 64;\n    const int CHUNK_K_SPARSE = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K_SPARSE = (int((CHUNK_K * K) * (1 - SPARSITY)));\n    const int WARP_COPY_BYTES = (WARP_SIZE * sizeof(int4));\n    const int CHUNK_LINE_BYTES_A (BLOCK_COL_TILES * M * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_A = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_A);\n    const int CHUNK_COPY_LINE_LANES_A = (CHUNK_LINE_BYTES_A / sizeof(int4));\n    const int SHARED_OFFSET_A = (BLOCK_COL_TILES * M + SKEW_UINT8);\n\n    const int CHUNK_LINE_BYTES_B = (BLOCK_SIZE_K_SPARSE * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_B = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_B);\n    const int CHUNK_COPY_LINE_LANES_B = (CHUNK_LINE_BYTES_B / sizeof(int4));\n    const int SHARED_OFFSET_B = (BLOCK_SIZE_K_SPARSE + SKEW_UINT8);\n    const int SHARED_TO_GLOBAL_BYTES_PER_LINE = ((WARP_COL_TILES * M) * sizeof(int));\n    const int SHARED_TO_GLOBAL_BYTES_PER_WARP = (WARP_SIZE * sizeof(int));\n    const int SHARED_TO_GLOBAL_LINES_PER_WARP = (SHARED_TO_GLOBAL_BYTES_PER_WARP / SHARED_TO_GLOBAL_BYTES_PER_LINE);\n    const int SHARED_TO_GLOBAL_LANES_PER_LINE = (WARP_SIZE / SHARED_TO_GLOBAL_LINES_PER_WARP);\n    const int SHARED_TO_GLOBAL_ITERS = ((WARP_ROW_TILES * N) / SHARED_TO_GLOBAL_LINES_PER_WARP);\n\n    const int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const int WARP_STRIDE = (WARP_COL_TILES * M);\n    const int WARPS_PER_BLOCK = (BLOCK_ROW_WARPS * BLOCK_COL_WARPS);\n    const int THREADS_PER_BLOCK = (WARP_SIZE * WARPS_PER_BLOCK);\n    const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    const int GLOBAL_MEM_STRIDE = M_GLOBAL;\n    const int SHMEM_STRIDE = (M * BLOCK_COL_TILES);\n    const int SHMEM_OFFSET = (M * WARP_COL_TILES);\n    const int BLOCK_SIZE_M = (M * BLOCK_COL_TILES);\n    const int BLOCK_SIZE_N = (N * BLOCK_ROW_TILES);\n\n   extern __shared__ uint8_t shmem[];\n\n\t// Warp and lane identification.\n\tconst unsigned int warpId = threadIdx.x / WARP_SIZE;\n\tconst unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n\n\t// Offset in shared memory from which the B matrix is stored.\n\t// const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\tconst size_t shmem_idx_b_off = BLOCK_SIZE_K_SPARSE * SHARED_OFFSET_A;\n\n\n\t// Each CTA slides along the 128 x 128 tiles from the top left corner of the\n\t// matrix to the right and down, and selects the next tile to compute. Once\n\t// there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n\tconst unsigned int block_tile_i =\n\t\t((block_pos * BLOCK_COL_TILES) / M_TILES) * (BLOCK_ROW_TILES);\n\tconst unsigned int block_tile_j = (block_pos * BLOCK_COL_TILES) % M_TILES;\n\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n\twmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_ROW_TILES]\n\t\t\t\t\t\t\t\t\t\t\t\t\t [WARP_COL_TILES];\n\n    // Load the C matrix tiles into fragments from shared memory.\n\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t\twmma::fill_fragment(c[i][j], 0);\n\t\t}\n\t}\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    //for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n    for(int tile_k_idx_sparse = 0, tile_k_idx = 0; tile_k_idx_sparse < K_GLOBAL_SPARSE; tile_k_idx_sparse += BLOCK_SIZE_K_SPARSE, tile_k_idx += BLOCK_SIZE_K){\n\n\t\tsize_t shmem_idx = \n\t\twarpId < (WARPS_PER_BLOCK / 2)\n\t\t\t? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A\n\t\t\t: (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B + shmem_idx_b_off;\n\n\t\tint4 *lane_ptr = NULL;\n\t\tint *lane_ptr_index = NULL;\n\t\tconst uint8_t *warp_ptr = NULL;\n\n\n\t\tif(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\t//warp_ptr = &A[block_tile_j * M] +\n\t\t\t//\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * M_GLOBAL;\n\t\t\twarp_ptr = &A[block_tile_j * M];\n\t\t\t\n\t\t\tconst int *warp_ptr_index = &B_index[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t\t\t\t\t\t((warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A);\n\n\t\t\tlane_ptr_index = (int *)(warp_ptr_index + tile_k_idx_sparse + (laneId / CHUNK_COPY_LINE_LANES_A));\n\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_A) * SHARED_OFFSET_A;\n\t\t}else{\n\t\t\twarp_ptr = &B[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * K_GLOBAL_SPARSE;\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_sparse +\n\t\t\t\t\t\t\t\t(laneId / CHUNK_COPY_LINE_LANES_B) * K_GLOBAL_SPARSE) +\n\t\t\t\t\t\t\t\t(laneId % CHUNK_COPY_LINE_LANES_B);\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_B) * SHARED_OFFSET_B;\n\t\t}\n\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      // shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n\t  int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n\t  \t? BLOCK_SIZE_K_SPARSE / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A)\n\t\t: BLOCK_SIZE_N / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\n\t  /*\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\t  */\n\n\t  /*\n      int tile_k_idx_A;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n          tile_k_idx_A = *(lane_ptr_index);\n      }\n\t  */\n\n\t  #pragma unroll\n\t  for(int i = 0; i < iter_index; i += 1){\n\t\t  if(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\tint tile_k_idx_A = *(lane_ptr_index);\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_A * M_GLOBAL) + (laneId % CHUNK_COPY_LINE_LANES_A);\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_A)) =\n\t\t\t\t*lane_ptr;\n\t\t\t//warp_ptr = (uint8_t *)((uint8_t *)warp_ptr + M_GLOBAL * (WARPS_PER_BLOCK / 2) *CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tlane_ptr_index = (int *)((int *)lane_ptr_index +  (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A;\n\t\t  }else{\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_B)) =\n\t\t\t\t*lane_ptr;\n\t\t\tlane_ptr = (int4 *)((uint8_t *)lane_ptr + K_GLOBAL_SPARSE * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B;\n\t\t  }\n\t  }\n\n      __syncthreads();\n\n\t#pragma unroll\n      for (int k_step = 0; k_step < CHUNK_K_SPARSE; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::col_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n\t#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i += 1) {\n\t\t\tsize_t shmem_idx_a = (warpId % BLOCK_COL_WARPS) * M * WARP_COL_TILES + (i * M);\n\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_a + k_step * K * SHARED_OFFSET_A;\n\n\t\t\twmma::load_matrix_sync(a[i], tile_ptr, SHARED_OFFSET_A);\n\t\t#pragma unroll\n\t\t\tfor(int j = 0; j < WARP_ROW_TILES; j += 1){\n\t\t\t\tif(i == 0){\n\t\t\t\t\tsize_t shmem_idx_b = shmem_idx_b_off +\n\t\t\t\t\t\t\t\t\t\t\t(warpId / BLOCK_COL_WARPS) * (WARP_ROW_TILES * N) * SHARED_OFFSET_B +\n\t\t\t\t\t\t\t\t\t\t\t(j * N) * SHARED_OFFSET_B;\n\t\t\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_b + k_step * K;\n\t\t\t\t\twmma::load_matrix_sync(b[j], tile_ptr, SHARED_OFFSET_B);\n\t\t\t\t}\n\t\t\t\twmma::mma_sync(c[j][i], a[i], b[j], c[j][i]);\n\t\t\t}\n\n        }\n      }\n\n      __syncthreads();\n    }\n\n    // This pointer is used to access the C and D matrix tiles this warp computes.\n\tint *shmem_warp_tile_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * N * WARP_ROW_TILES * SHMEM_STRIDE +\n\t(warpId % BLOCK_COL_WARPS) * SHMEM_OFFSET;\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t#pragma unroll\n\t\t\tfor(int t = 0; t < c[i][j].num_elements; t += 1){\n\t\t\t\tc[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n\t\t\t}\n\t\t\tint *tile_ptr = shmem_warp_tile_ptr + i * N * SHMEM_STRIDE + j * M;\n\t\t\twmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, C_LAYOUT);\n\t\t}\n\t}\n\n    __syncthreads();\n\n\tint *shmem_warp_stream_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N * SHMEM_STRIDE\n\t\t\t\t\t\t\t\t\t+ (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tconst size_t gmem_idx =\n\t\t(block_tile_i * N + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N) * GLOBAL_MEM_STRIDE +\n\t\tblock_tile_j * M + (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tuint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n\tint *shmem_lane_stream_ptr =\n\t\tshmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * SHMEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\t\n\tuint8_t *dst_gmem_lane_stream_ptr =\n\t\tdst_gmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * GLOBAL_MEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\n\tfor(int i = 0; i < WARP_ROW_TILES * N; i += SHARED_TO_GLOBAL_LINES_PER_WARP){\n\t\t*(dst_gmem_lane_stream_ptr + i * GLOBAL_MEM_STRIDE) = (uint8_t)(*(shmem_lane_stream_ptr + i * SHMEM_STRIDE));\n\t}\n\n\t__syncthreads();\n}\n", "launch_config": {"dimBlock": [256, 1], "dimGrid": [256, 1]}}, "22": {"code": "\n__global__ void compute_gemm_imma(float *input0, float *input1, float *input2, float* input3, float* input4, float*input5, float* input6,\n                                  float *output0) {\n    // keys need to be replaced 4096, 1024, K_GLOBA_VALUEL, 0.75, 64\n\n\t//extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B = reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_index = reinterpret_cast<int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n\tconst int WARP_SIZE = 32;\n    const int M = 16;\n    const int K = 16;\n    const int N = 16;\n    const int WMMA_M = 16;\n    const int WMMA_K = 16;\n    const int WMMA_N = 16;\n    const int M_GLOBAL = 4096;\n    const int K_GLOBAL = 1024;\n    const int N_GLOBAL = 1024;\n    const float SPARSITY = 0.75;\n    const int K_GLOBAL_SPARSE = (int(K_GLOBAL * (1-SPARSITY)));\n    const int M_TILES = (M_GLOBAL / M);\n    const int N_TILES = (N_GLOBAL / N);\n    const int K_TILES = (K_GLOBAL / K);\n    const int BLOCK_ROW_WARPS = 2;\n    const int BLOCK_COL_WARPS = 4;\n    const int WARP_ROW_TILES = 4;\n    const int WARP_COL_TILES = 2;\n    const int SKEW_UINT8 = 16;\n    const int BANK_VAL = 32;\n    const int NUM_BANK = (K_GLOBAL / BANK_VAL);\n    /////////// BLOCK_ROW_TILES <= N_TILES ////////////\n    // const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    // ////////// BLOCK_COL_TILES <= M_TILES ////////////\n    // const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    auto C_LAYOUT = wmma::mem_col_major;\n    const int CHUNK_K = 64;\n    const int CHUNK_K_SPARSE = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K_SPARSE = (int((CHUNK_K * K) * (1 - SPARSITY)));\n    const int WARP_COPY_BYTES = (WARP_SIZE * sizeof(int4));\n    const int CHUNK_LINE_BYTES_A (BLOCK_COL_TILES * M * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_A = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_A);\n    const int CHUNK_COPY_LINE_LANES_A = (CHUNK_LINE_BYTES_A / sizeof(int4));\n    const int SHARED_OFFSET_A = (BLOCK_COL_TILES * M + SKEW_UINT8);\n\n    const int CHUNK_LINE_BYTES_B = (BLOCK_SIZE_K_SPARSE * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_B = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_B);\n    const int CHUNK_COPY_LINE_LANES_B = (CHUNK_LINE_BYTES_B / sizeof(int4));\n    const int SHARED_OFFSET_B = (BLOCK_SIZE_K_SPARSE + SKEW_UINT8);\n    const int SHARED_TO_GLOBAL_BYTES_PER_LINE = ((WARP_COL_TILES * M) * sizeof(int));\n    const int SHARED_TO_GLOBAL_BYTES_PER_WARP = (WARP_SIZE * sizeof(int));\n    const int SHARED_TO_GLOBAL_LINES_PER_WARP = (SHARED_TO_GLOBAL_BYTES_PER_WARP / SHARED_TO_GLOBAL_BYTES_PER_LINE);\n    const int SHARED_TO_GLOBAL_LANES_PER_LINE = (WARP_SIZE / SHARED_TO_GLOBAL_LINES_PER_WARP);\n    const int SHARED_TO_GLOBAL_ITERS = ((WARP_ROW_TILES * N) / SHARED_TO_GLOBAL_LINES_PER_WARP);\n\n    const int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const int WARP_STRIDE = (WARP_COL_TILES * M);\n    const int WARPS_PER_BLOCK = (BLOCK_ROW_WARPS * BLOCK_COL_WARPS);\n    const int THREADS_PER_BLOCK = (WARP_SIZE * WARPS_PER_BLOCK);\n    const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    const int GLOBAL_MEM_STRIDE = M_GLOBAL;\n    const int SHMEM_STRIDE = (M * BLOCK_COL_TILES);\n    const int SHMEM_OFFSET = (M * WARP_COL_TILES);\n    const int BLOCK_SIZE_M = (M * BLOCK_COL_TILES);\n    const int BLOCK_SIZE_N = (N * BLOCK_ROW_TILES);\n\n   extern __shared__ uint8_t shmem[];\n\n\t// Warp and lane identification.\n\tconst unsigned int warpId = threadIdx.x / WARP_SIZE;\n\tconst unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n\n\t// Offset in shared memory from which the B matrix is stored.\n\t// const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\tconst size_t shmem_idx_b_off = BLOCK_SIZE_K_SPARSE * SHARED_OFFSET_A;\n\n\n\t// Each CTA slides along the 128 x 128 tiles from the top left corner of the\n\t// matrix to the right and down, and selects the next tile to compute. Once\n\t// there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n\tconst unsigned int block_tile_i =\n\t\t((block_pos * BLOCK_COL_TILES) / M_TILES) * (BLOCK_ROW_TILES);\n\tconst unsigned int block_tile_j = (block_pos * BLOCK_COL_TILES) % M_TILES;\n\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n\twmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_ROW_TILES]\n\t\t\t\t\t\t\t\t\t\t\t\t\t [WARP_COL_TILES];\n\n    // Load the C matrix tiles into fragments from shared memory.\n\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t\twmma::fill_fragment(c[i][j], 0);\n\t\t}\n\t}\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    //for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n    for(int tile_k_idx_sparse = 0, tile_k_idx = 0; tile_k_idx_sparse < K_GLOBAL_SPARSE; tile_k_idx_sparse += BLOCK_SIZE_K_SPARSE, tile_k_idx += BLOCK_SIZE_K){\n\n\t\tsize_t shmem_idx = \n\t\twarpId < (WARPS_PER_BLOCK / 2)\n\t\t\t? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A\n\t\t\t: (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B + shmem_idx_b_off;\n\n\t\tint4 *lane_ptr = NULL;\n\t\tint *lane_ptr_index = NULL;\n\t\tconst uint8_t *warp_ptr = NULL;\n\n\n\t\tif(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\t//warp_ptr = &A[block_tile_j * M] +\n\t\t\t//\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * M_GLOBAL;\n\t\t\twarp_ptr = &A[block_tile_j * M];\n\t\t\t\n\t\t\tconst int *warp_ptr_index = &B_index[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t\t\t\t\t\t((warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A);\n\n\t\t\tlane_ptr_index = (int *)(warp_ptr_index + tile_k_idx_sparse + (laneId / CHUNK_COPY_LINE_LANES_A));\n\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_A) * SHARED_OFFSET_A;\n\t\t}else{\n\t\t\twarp_ptr = &B[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * K_GLOBAL_SPARSE;\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_sparse +\n\t\t\t\t\t\t\t\t(laneId / CHUNK_COPY_LINE_LANES_B) * K_GLOBAL_SPARSE) +\n\t\t\t\t\t\t\t\t(laneId % CHUNK_COPY_LINE_LANES_B);\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_B) * SHARED_OFFSET_B;\n\t\t}\n\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      // shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n\t  int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n\t  \t? BLOCK_SIZE_K_SPARSE / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A)\n\t\t: BLOCK_SIZE_N / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\n\t  /*\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\t  */\n\n\t  /*\n      int tile_k_idx_A;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n          tile_k_idx_A = *(lane_ptr_index);\n      }\n\t  */\n\n\t  #pragma unroll\n\t  for(int i = 0; i < iter_index; i += 1){\n\t\t  if(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\tint tile_k_idx_A = *(lane_ptr_index);\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_A * M_GLOBAL) + (laneId % CHUNK_COPY_LINE_LANES_A);\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_A)) =\n\t\t\t\t*lane_ptr;\n\t\t\t//warp_ptr = (uint8_t *)((uint8_t *)warp_ptr + M_GLOBAL * (WARPS_PER_BLOCK / 2) *CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tlane_ptr_index = (int *)((int *)lane_ptr_index +  (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A;\n\t\t  }else{\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_B)) =\n\t\t\t\t*lane_ptr;\n\t\t\tlane_ptr = (int4 *)((uint8_t *)lane_ptr + K_GLOBAL_SPARSE * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B;\n\t\t  }\n\t  }\n\n      __syncthreads();\n\n\t#pragma unroll\n      for (int k_step = 0; k_step < CHUNK_K_SPARSE; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::col_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n\t#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i += 1) {\n\t\t\tsize_t shmem_idx_a = (warpId % BLOCK_COL_WARPS) * M * WARP_COL_TILES + (i * M);\n\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_a + k_step * K * SHARED_OFFSET_A;\n\n\t\t\twmma::load_matrix_sync(a[i], tile_ptr, SHARED_OFFSET_A);\n\t\t#pragma unroll\n\t\t\tfor(int j = 0; j < WARP_ROW_TILES; j += 1){\n\t\t\t\tif(i == 0){\n\t\t\t\t\tsize_t shmem_idx_b = shmem_idx_b_off +\n\t\t\t\t\t\t\t\t\t\t\t(warpId / BLOCK_COL_WARPS) * (WARP_ROW_TILES * N) * SHARED_OFFSET_B +\n\t\t\t\t\t\t\t\t\t\t\t(j * N) * SHARED_OFFSET_B;\n\t\t\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_b + k_step * K;\n\t\t\t\t\twmma::load_matrix_sync(b[j], tile_ptr, SHARED_OFFSET_B);\n\t\t\t\t}\n\t\t\t\twmma::mma_sync(c[j][i], a[i], b[j], c[j][i]);\n\t\t\t}\n\n        }\n      }\n\n      __syncthreads();\n    }\n\n    // This pointer is used to access the C and D matrix tiles this warp computes.\n\tint *shmem_warp_tile_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * N * WARP_ROW_TILES * SHMEM_STRIDE +\n\t(warpId % BLOCK_COL_WARPS) * SHMEM_OFFSET;\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t#pragma unroll\n\t\t\tfor(int t = 0; t < c[i][j].num_elements; t += 1){\n\t\t\t\tc[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n\t\t\t}\n\t\t\tint *tile_ptr = shmem_warp_tile_ptr + i * N * SHMEM_STRIDE + j * M;\n\t\t\twmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, C_LAYOUT);\n\t\t}\n\t}\n\n    __syncthreads();\n\n\tint *shmem_warp_stream_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N * SHMEM_STRIDE\n\t\t\t\t\t\t\t\t\t+ (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tconst size_t gmem_idx =\n\t\t(block_tile_i * N + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N) * GLOBAL_MEM_STRIDE +\n\t\tblock_tile_j * M + (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tuint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n\tint *shmem_lane_stream_ptr =\n\t\tshmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * SHMEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\t\n\tuint8_t *dst_gmem_lane_stream_ptr =\n\t\tdst_gmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * GLOBAL_MEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\n\tfor(int i = 0; i < WARP_ROW_TILES * N; i += SHARED_TO_GLOBAL_LINES_PER_WARP){\n\t\t*(dst_gmem_lane_stream_ptr + i * GLOBAL_MEM_STRIDE) = (uint8_t)(*(shmem_lane_stream_ptr + i * SHMEM_STRIDE));\n\t}\n\n\t__syncthreads();\n}\n", "launch_config": {"dimBlock": [256, 1], "dimGrid": [256, 1]}}, "23": {"code": "\n__global__ void compute_gemm_imma(float *input0, float *input1, float *input2, float* input3, float* input4, float*input5, float* input6,\n                                  float *output0) {\n    // keys need to be replaced 4096, 4096, K_GLOBA_VALUEL, 0.75, 64\n\n\t//extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B = reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_index = reinterpret_cast<int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n\tconst int WARP_SIZE = 32;\n    const int M = 16;\n    const int K = 16;\n    const int N = 16;\n    const int WMMA_M = 16;\n    const int WMMA_K = 16;\n    const int WMMA_N = 16;\n    const int M_GLOBAL = 4096;\n    const int K_GLOBAL = 1024;\n    const int N_GLOBAL = 4096;\n    const float SPARSITY = 0.75;\n    const int K_GLOBAL_SPARSE = (int(K_GLOBAL * (1-SPARSITY)));\n    const int M_TILES = (M_GLOBAL / M);\n    const int N_TILES = (N_GLOBAL / N);\n    const int K_TILES = (K_GLOBAL / K);\n    const int BLOCK_ROW_WARPS = 2;\n    const int BLOCK_COL_WARPS = 4;\n    const int WARP_ROW_TILES = 4;\n    const int WARP_COL_TILES = 2;\n    const int SKEW_UINT8 = 16;\n    const int BANK_VAL = 32;\n    const int NUM_BANK = (K_GLOBAL / BANK_VAL);\n    /////////// BLOCK_ROW_TILES <= N_TILES ////////////\n    // const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    // ////////// BLOCK_COL_TILES <= M_TILES ////////////\n    // const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    auto C_LAYOUT = wmma::mem_col_major;\n    const int CHUNK_K = 64;\n    const int CHUNK_K_SPARSE = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K_SPARSE = (int((CHUNK_K * K) * (1 - SPARSITY)));\n    const int WARP_COPY_BYTES = (WARP_SIZE * sizeof(int4));\n    const int CHUNK_LINE_BYTES_A (BLOCK_COL_TILES * M * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_A = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_A);\n    const int CHUNK_COPY_LINE_LANES_A = (CHUNK_LINE_BYTES_A / sizeof(int4));\n    const int SHARED_OFFSET_A = (BLOCK_COL_TILES * M + SKEW_UINT8);\n\n    const int CHUNK_LINE_BYTES_B = (BLOCK_SIZE_K_SPARSE * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_B = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_B);\n    const int CHUNK_COPY_LINE_LANES_B = (CHUNK_LINE_BYTES_B / sizeof(int4));\n    const int SHARED_OFFSET_B = (BLOCK_SIZE_K_SPARSE + SKEW_UINT8);\n    const int SHARED_TO_GLOBAL_BYTES_PER_LINE = ((WARP_COL_TILES * M) * sizeof(int));\n    const int SHARED_TO_GLOBAL_BYTES_PER_WARP = (WARP_SIZE * sizeof(int));\n    const int SHARED_TO_GLOBAL_LINES_PER_WARP = (SHARED_TO_GLOBAL_BYTES_PER_WARP / SHARED_TO_GLOBAL_BYTES_PER_LINE);\n    const int SHARED_TO_GLOBAL_LANES_PER_LINE = (WARP_SIZE / SHARED_TO_GLOBAL_LINES_PER_WARP);\n    const int SHARED_TO_GLOBAL_ITERS = ((WARP_ROW_TILES * N) / SHARED_TO_GLOBAL_LINES_PER_WARP);\n\n    const int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const int WARP_STRIDE = (WARP_COL_TILES * M);\n    const int WARPS_PER_BLOCK = (BLOCK_ROW_WARPS * BLOCK_COL_WARPS);\n    const int THREADS_PER_BLOCK = (WARP_SIZE * WARPS_PER_BLOCK);\n    const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    const int GLOBAL_MEM_STRIDE = M_GLOBAL;\n    const int SHMEM_STRIDE = (M * BLOCK_COL_TILES);\n    const int SHMEM_OFFSET = (M * WARP_COL_TILES);\n    const int BLOCK_SIZE_M = (M * BLOCK_COL_TILES);\n    const int BLOCK_SIZE_N = (N * BLOCK_ROW_TILES);\n\n   extern __shared__ uint8_t shmem[];\n\n\t// Warp and lane identification.\n\tconst unsigned int warpId = threadIdx.x / WARP_SIZE;\n\tconst unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n\n\t// Offset in shared memory from which the B matrix is stored.\n\t// const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\tconst size_t shmem_idx_b_off = BLOCK_SIZE_K_SPARSE * SHARED_OFFSET_A;\n\n\n\t// Each CTA slides along the 128 x 128 tiles from the top left corner of the\n\t// matrix to the right and down, and selects the next tile to compute. Once\n\t// there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n\tconst unsigned int block_tile_i =\n\t\t((block_pos * BLOCK_COL_TILES) / M_TILES) * (BLOCK_ROW_TILES);\n\tconst unsigned int block_tile_j = (block_pos * BLOCK_COL_TILES) % M_TILES;\n\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n\twmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_ROW_TILES]\n\t\t\t\t\t\t\t\t\t\t\t\t\t [WARP_COL_TILES];\n\n    // Load the C matrix tiles into fragments from shared memory.\n\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t\twmma::fill_fragment(c[i][j], 0);\n\t\t}\n\t}\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    //for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n    for(int tile_k_idx_sparse = 0, tile_k_idx = 0; tile_k_idx_sparse < K_GLOBAL_SPARSE; tile_k_idx_sparse += BLOCK_SIZE_K_SPARSE, tile_k_idx += BLOCK_SIZE_K){\n\n\t\tsize_t shmem_idx = \n\t\twarpId < (WARPS_PER_BLOCK / 2)\n\t\t\t? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A\n\t\t\t: (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B + shmem_idx_b_off;\n\n\t\tint4 *lane_ptr = NULL;\n\t\tint *lane_ptr_index = NULL;\n\t\tconst uint8_t *warp_ptr = NULL;\n\n\n\t\tif(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\t//warp_ptr = &A[block_tile_j * M] +\n\t\t\t//\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * M_GLOBAL;\n\t\t\twarp_ptr = &A[block_tile_j * M];\n\t\t\t\n\t\t\tconst int *warp_ptr_index = &B_index[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t\t\t\t\t\t((warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A);\n\n\t\t\tlane_ptr_index = (int *)(warp_ptr_index + tile_k_idx_sparse + (laneId / CHUNK_COPY_LINE_LANES_A));\n\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_A) * SHARED_OFFSET_A;\n\t\t}else{\n\t\t\twarp_ptr = &B[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * K_GLOBAL_SPARSE;\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_sparse +\n\t\t\t\t\t\t\t\t(laneId / CHUNK_COPY_LINE_LANES_B) * K_GLOBAL_SPARSE) +\n\t\t\t\t\t\t\t\t(laneId % CHUNK_COPY_LINE_LANES_B);\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_B) * SHARED_OFFSET_B;\n\t\t}\n\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      // shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n\t  int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n\t  \t? BLOCK_SIZE_K_SPARSE / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A)\n\t\t: BLOCK_SIZE_N / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\n\t  /*\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\t  */\n\n\t  /*\n      int tile_k_idx_A;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n          tile_k_idx_A = *(lane_ptr_index);\n      }\n\t  */\n\n\t  #pragma unroll\n\t  for(int i = 0; i < iter_index; i += 1){\n\t\t  if(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\tint tile_k_idx_A = *(lane_ptr_index);\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_A * M_GLOBAL) + (laneId % CHUNK_COPY_LINE_LANES_A);\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_A)) =\n\t\t\t\t*lane_ptr;\n\t\t\t//warp_ptr = (uint8_t *)((uint8_t *)warp_ptr + M_GLOBAL * (WARPS_PER_BLOCK / 2) *CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tlane_ptr_index = (int *)((int *)lane_ptr_index +  (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A;\n\t\t  }else{\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_B)) =\n\t\t\t\t*lane_ptr;\n\t\t\tlane_ptr = (int4 *)((uint8_t *)lane_ptr + K_GLOBAL_SPARSE * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B;\n\t\t  }\n\t  }\n\n      __syncthreads();\n\n\t#pragma unroll\n      for (int k_step = 0; k_step < CHUNK_K_SPARSE; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::col_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n\t#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i += 1) {\n\t\t\tsize_t shmem_idx_a = (warpId % BLOCK_COL_WARPS) * M * WARP_COL_TILES + (i * M);\n\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_a + k_step * K * SHARED_OFFSET_A;\n\n\t\t\twmma::load_matrix_sync(a[i], tile_ptr, SHARED_OFFSET_A);\n\t\t#pragma unroll\n\t\t\tfor(int j = 0; j < WARP_ROW_TILES; j += 1){\n\t\t\t\tif(i == 0){\n\t\t\t\t\tsize_t shmem_idx_b = shmem_idx_b_off +\n\t\t\t\t\t\t\t\t\t\t\t(warpId / BLOCK_COL_WARPS) * (WARP_ROW_TILES * N) * SHARED_OFFSET_B +\n\t\t\t\t\t\t\t\t\t\t\t(j * N) * SHARED_OFFSET_B;\n\t\t\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_b + k_step * K;\n\t\t\t\t\twmma::load_matrix_sync(b[j], tile_ptr, SHARED_OFFSET_B);\n\t\t\t\t}\n\t\t\t\twmma::mma_sync(c[j][i], a[i], b[j], c[j][i]);\n\t\t\t}\n\n        }\n      }\n\n      __syncthreads();\n    }\n\n    // This pointer is used to access the C and D matrix tiles this warp computes.\n\tint *shmem_warp_tile_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * N * WARP_ROW_TILES * SHMEM_STRIDE +\n\t(warpId % BLOCK_COL_WARPS) * SHMEM_OFFSET;\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t#pragma unroll\n\t\t\tfor(int t = 0; t < c[i][j].num_elements; t += 1){\n\t\t\t\tc[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n\t\t\t}\n\t\t\tint *tile_ptr = shmem_warp_tile_ptr + i * N * SHMEM_STRIDE + j * M;\n\t\t\twmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, C_LAYOUT);\n\t\t}\n\t}\n\n    __syncthreads();\n\n\tint *shmem_warp_stream_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N * SHMEM_STRIDE\n\t\t\t\t\t\t\t\t\t+ (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tconst size_t gmem_idx =\n\t\t(block_tile_i * N + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N) * GLOBAL_MEM_STRIDE +\n\t\tblock_tile_j * M + (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tuint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n\tint *shmem_lane_stream_ptr =\n\t\tshmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * SHMEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\t\n\tuint8_t *dst_gmem_lane_stream_ptr =\n\t\tdst_gmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * GLOBAL_MEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\n\tfor(int i = 0; i < WARP_ROW_TILES * N; i += SHARED_TO_GLOBAL_LINES_PER_WARP){\n\t\t*(dst_gmem_lane_stream_ptr + i * GLOBAL_MEM_STRIDE) = (uint8_t)(*(shmem_lane_stream_ptr + i * SHMEM_STRIDE));\n\t}\n\n\t__syncthreads();\n}\n", "launch_config": {"dimBlock": [1024, 1], "dimGrid": [256, 1]}}, "24": {"code": "\n__global__ void compute_gemm_imma(float *input0, float *input1, float *input2, float* input3, float* input4, float*input5, float* input6,\n                                  float *output0) {\n    // keys need to be replaced 4096, 1024, K_GLOBA_VALUEL, 0.75, 64\n\n\t//extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B = reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_index = reinterpret_cast<int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n\tconst int WARP_SIZE = 32;\n    const int M = 16;\n    const int K = 16;\n    const int N = 16;\n    const int WMMA_M = 16;\n    const int WMMA_K = 16;\n    const int WMMA_N = 16;\n    const int M_GLOBAL = 4096;\n    const int K_GLOBAL = 4096;\n    const int N_GLOBAL = 1024;\n    const float SPARSITY = 0.75;\n    const int K_GLOBAL_SPARSE = (int(K_GLOBAL * (1-SPARSITY)));\n    const int M_TILES = (M_GLOBAL / M);\n    const int N_TILES = (N_GLOBAL / N);\n    const int K_TILES = (K_GLOBAL / K);\n    const int BLOCK_ROW_WARPS = 2;\n    const int BLOCK_COL_WARPS = 4;\n    const int WARP_ROW_TILES = 4;\n    const int WARP_COL_TILES = 2;\n    const int SKEW_UINT8 = 16;\n    const int BANK_VAL = 32;\n    const int NUM_BANK = (K_GLOBAL / BANK_VAL);\n    /////////// BLOCK_ROW_TILES <= N_TILES ////////////\n    // const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    // ////////// BLOCK_COL_TILES <= M_TILES ////////////\n    // const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    auto C_LAYOUT = wmma::mem_col_major;\n    const int CHUNK_K = 64;\n    const int CHUNK_K_SPARSE = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K_SPARSE = (int((CHUNK_K * K) * (1 - SPARSITY)));\n    const int WARP_COPY_BYTES = (WARP_SIZE * sizeof(int4));\n    const int CHUNK_LINE_BYTES_A (BLOCK_COL_TILES * M * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_A = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_A);\n    const int CHUNK_COPY_LINE_LANES_A = (CHUNK_LINE_BYTES_A / sizeof(int4));\n    const int SHARED_OFFSET_A = (BLOCK_COL_TILES * M + SKEW_UINT8);\n\n    const int CHUNK_LINE_BYTES_B = (BLOCK_SIZE_K_SPARSE * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_B = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_B);\n    const int CHUNK_COPY_LINE_LANES_B = (CHUNK_LINE_BYTES_B / sizeof(int4));\n    const int SHARED_OFFSET_B = (BLOCK_SIZE_K_SPARSE + SKEW_UINT8);\n    const int SHARED_TO_GLOBAL_BYTES_PER_LINE = ((WARP_COL_TILES * M) * sizeof(int));\n    const int SHARED_TO_GLOBAL_BYTES_PER_WARP = (WARP_SIZE * sizeof(int));\n    const int SHARED_TO_GLOBAL_LINES_PER_WARP = (SHARED_TO_GLOBAL_BYTES_PER_WARP / SHARED_TO_GLOBAL_BYTES_PER_LINE);\n    const int SHARED_TO_GLOBAL_LANES_PER_LINE = (WARP_SIZE / SHARED_TO_GLOBAL_LINES_PER_WARP);\n    const int SHARED_TO_GLOBAL_ITERS = ((WARP_ROW_TILES * N) / SHARED_TO_GLOBAL_LINES_PER_WARP);\n\n    const int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const int WARP_STRIDE = (WARP_COL_TILES * M);\n    const int WARPS_PER_BLOCK = (BLOCK_ROW_WARPS * BLOCK_COL_WARPS);\n    const int THREADS_PER_BLOCK = (WARP_SIZE * WARPS_PER_BLOCK);\n    const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    const int GLOBAL_MEM_STRIDE = M_GLOBAL;\n    const int SHMEM_STRIDE = (M * BLOCK_COL_TILES);\n    const int SHMEM_OFFSET = (M * WARP_COL_TILES);\n    const int BLOCK_SIZE_M = (M * BLOCK_COL_TILES);\n    const int BLOCK_SIZE_N = (N * BLOCK_ROW_TILES);\n\n   extern __shared__ uint8_t shmem[];\n\n\t// Warp and lane identification.\n\tconst unsigned int warpId = threadIdx.x / WARP_SIZE;\n\tconst unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n\n\t// Offset in shared memory from which the B matrix is stored.\n\t// const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\tconst size_t shmem_idx_b_off = BLOCK_SIZE_K_SPARSE * SHARED_OFFSET_A;\n\n\n\t// Each CTA slides along the 128 x 128 tiles from the top left corner of the\n\t// matrix to the right and down, and selects the next tile to compute. Once\n\t// there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n\tconst unsigned int block_tile_i =\n\t\t((block_pos * BLOCK_COL_TILES) / M_TILES) * (BLOCK_ROW_TILES);\n\tconst unsigned int block_tile_j = (block_pos * BLOCK_COL_TILES) % M_TILES;\n\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n\twmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_ROW_TILES]\n\t\t\t\t\t\t\t\t\t\t\t\t\t [WARP_COL_TILES];\n\n    // Load the C matrix tiles into fragments from shared memory.\n\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t\twmma::fill_fragment(c[i][j], 0);\n\t\t}\n\t}\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    //for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n    for(int tile_k_idx_sparse = 0, tile_k_idx = 0; tile_k_idx_sparse < K_GLOBAL_SPARSE; tile_k_idx_sparse += BLOCK_SIZE_K_SPARSE, tile_k_idx += BLOCK_SIZE_K){\n\n\t\tsize_t shmem_idx = \n\t\twarpId < (WARPS_PER_BLOCK / 2)\n\t\t\t? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A\n\t\t\t: (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B + shmem_idx_b_off;\n\n\t\tint4 *lane_ptr = NULL;\n\t\tint *lane_ptr_index = NULL;\n\t\tconst uint8_t *warp_ptr = NULL;\n\n\n\t\tif(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\t//warp_ptr = &A[block_tile_j * M] +\n\t\t\t//\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * M_GLOBAL;\n\t\t\twarp_ptr = &A[block_tile_j * M];\n\t\t\t\n\t\t\tconst int *warp_ptr_index = &B_index[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t\t\t\t\t\t((warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A);\n\n\t\t\tlane_ptr_index = (int *)(warp_ptr_index + tile_k_idx_sparse + (laneId / CHUNK_COPY_LINE_LANES_A));\n\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_A) * SHARED_OFFSET_A;\n\t\t}else{\n\t\t\twarp_ptr = &B[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * K_GLOBAL_SPARSE;\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_sparse +\n\t\t\t\t\t\t\t\t(laneId / CHUNK_COPY_LINE_LANES_B) * K_GLOBAL_SPARSE) +\n\t\t\t\t\t\t\t\t(laneId % CHUNK_COPY_LINE_LANES_B);\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_B) * SHARED_OFFSET_B;\n\t\t}\n\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      // shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n\t  int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n\t  \t? BLOCK_SIZE_K_SPARSE / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A)\n\t\t: BLOCK_SIZE_N / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\n\t  /*\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\t  */\n\n\t  /*\n      int tile_k_idx_A;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n          tile_k_idx_A = *(lane_ptr_index);\n      }\n\t  */\n\n\t  #pragma unroll\n\t  for(int i = 0; i < iter_index; i += 1){\n\t\t  if(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\tint tile_k_idx_A = *(lane_ptr_index);\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_A * M_GLOBAL) + (laneId % CHUNK_COPY_LINE_LANES_A);\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_A)) =\n\t\t\t\t*lane_ptr;\n\t\t\t//warp_ptr = (uint8_t *)((uint8_t *)warp_ptr + M_GLOBAL * (WARPS_PER_BLOCK / 2) *CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tlane_ptr_index = (int *)((int *)lane_ptr_index +  (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A;\n\t\t  }else{\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_B)) =\n\t\t\t\t*lane_ptr;\n\t\t\tlane_ptr = (int4 *)((uint8_t *)lane_ptr + K_GLOBAL_SPARSE * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B;\n\t\t  }\n\t  }\n\n      __syncthreads();\n\n\t#pragma unroll\n      for (int k_step = 0; k_step < CHUNK_K_SPARSE; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::col_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n\t#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i += 1) {\n\t\t\tsize_t shmem_idx_a = (warpId % BLOCK_COL_WARPS) * M * WARP_COL_TILES + (i * M);\n\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_a + k_step * K * SHARED_OFFSET_A;\n\n\t\t\twmma::load_matrix_sync(a[i], tile_ptr, SHARED_OFFSET_A);\n\t\t#pragma unroll\n\t\t\tfor(int j = 0; j < WARP_ROW_TILES; j += 1){\n\t\t\t\tif(i == 0){\n\t\t\t\t\tsize_t shmem_idx_b = shmem_idx_b_off +\n\t\t\t\t\t\t\t\t\t\t\t(warpId / BLOCK_COL_WARPS) * (WARP_ROW_TILES * N) * SHARED_OFFSET_B +\n\t\t\t\t\t\t\t\t\t\t\t(j * N) * SHARED_OFFSET_B;\n\t\t\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_b + k_step * K;\n\t\t\t\t\twmma::load_matrix_sync(b[j], tile_ptr, SHARED_OFFSET_B);\n\t\t\t\t}\n\t\t\t\twmma::mma_sync(c[j][i], a[i], b[j], c[j][i]);\n\t\t\t}\n\n        }\n      }\n\n      __syncthreads();\n    }\n\n    // This pointer is used to access the C and D matrix tiles this warp computes.\n\tint *shmem_warp_tile_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * N * WARP_ROW_TILES * SHMEM_STRIDE +\n\t(warpId % BLOCK_COL_WARPS) * SHMEM_OFFSET;\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t#pragma unroll\n\t\t\tfor(int t = 0; t < c[i][j].num_elements; t += 1){\n\t\t\t\tc[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n\t\t\t}\n\t\t\tint *tile_ptr = shmem_warp_tile_ptr + i * N * SHMEM_STRIDE + j * M;\n\t\t\twmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, C_LAYOUT);\n\t\t}\n\t}\n\n    __syncthreads();\n\n\tint *shmem_warp_stream_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N * SHMEM_STRIDE\n\t\t\t\t\t\t\t\t\t+ (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tconst size_t gmem_idx =\n\t\t(block_tile_i * N + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N) * GLOBAL_MEM_STRIDE +\n\t\tblock_tile_j * M + (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tuint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n\tint *shmem_lane_stream_ptr =\n\t\tshmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * SHMEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\t\n\tuint8_t *dst_gmem_lane_stream_ptr =\n\t\tdst_gmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * GLOBAL_MEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\n\tfor(int i = 0; i < WARP_ROW_TILES * N; i += SHARED_TO_GLOBAL_LINES_PER_WARP){\n\t\t*(dst_gmem_lane_stream_ptr + i * GLOBAL_MEM_STRIDE) = (uint8_t)(*(shmem_lane_stream_ptr + i * SHMEM_STRIDE));\n\t}\n\n\t__syncthreads();\n}\n", "launch_config": {"dimBlock": [256, 1], "dimGrid": [256, 1]}}, "25": {"code": "\n__global__ void compute_gemm_imma(float *input0, float *input1, float *input2, float* input3, float* input4, float*input5, float* input6,\n                                  float *output0) {\n    // keys need to be replaced 4096, 1024, K_GLOBA_VALUEL, 0.75, 64\n\n\t//extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B = reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_index = reinterpret_cast<int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n\tconst int WARP_SIZE = 32;\n    const int M = 16;\n    const int K = 16;\n    const int N = 16;\n    const int WMMA_M = 16;\n    const int WMMA_K = 16;\n    const int WMMA_N = 16;\n    const int M_GLOBAL = 4096;\n    const int K_GLOBAL = 1024;\n    const int N_GLOBAL = 1024;\n    const float SPARSITY = 0.75;\n    const int K_GLOBAL_SPARSE = (int(K_GLOBAL * (1-SPARSITY)));\n    const int M_TILES = (M_GLOBAL / M);\n    const int N_TILES = (N_GLOBAL / N);\n    const int K_TILES = (K_GLOBAL / K);\n    const int BLOCK_ROW_WARPS = 2;\n    const int BLOCK_COL_WARPS = 4;\n    const int WARP_ROW_TILES = 4;\n    const int WARP_COL_TILES = 2;\n    const int SKEW_UINT8 = 16;\n    const int BANK_VAL = 32;\n    const int NUM_BANK = (K_GLOBAL / BANK_VAL);\n    /////////// BLOCK_ROW_TILES <= N_TILES ////////////\n    // const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    // ////////// BLOCK_COL_TILES <= M_TILES ////////////\n    // const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    auto C_LAYOUT = wmma::mem_col_major;\n    const int CHUNK_K = 64;\n    const int CHUNK_K_SPARSE = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K_SPARSE = (int((CHUNK_K * K) * (1 - SPARSITY)));\n    const int WARP_COPY_BYTES = (WARP_SIZE * sizeof(int4));\n    const int CHUNK_LINE_BYTES_A (BLOCK_COL_TILES * M * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_A = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_A);\n    const int CHUNK_COPY_LINE_LANES_A = (CHUNK_LINE_BYTES_A / sizeof(int4));\n    const int SHARED_OFFSET_A = (BLOCK_COL_TILES * M + SKEW_UINT8);\n\n    const int CHUNK_LINE_BYTES_B = (BLOCK_SIZE_K_SPARSE * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_B = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_B);\n    const int CHUNK_COPY_LINE_LANES_B = (CHUNK_LINE_BYTES_B / sizeof(int4));\n    const int SHARED_OFFSET_B = (BLOCK_SIZE_K_SPARSE + SKEW_UINT8);\n    const int SHARED_TO_GLOBAL_BYTES_PER_LINE = ((WARP_COL_TILES * M) * sizeof(int));\n    const int SHARED_TO_GLOBAL_BYTES_PER_WARP = (WARP_SIZE * sizeof(int));\n    const int SHARED_TO_GLOBAL_LINES_PER_WARP = (SHARED_TO_GLOBAL_BYTES_PER_WARP / SHARED_TO_GLOBAL_BYTES_PER_LINE);\n    const int SHARED_TO_GLOBAL_LANES_PER_LINE = (WARP_SIZE / SHARED_TO_GLOBAL_LINES_PER_WARP);\n    const int SHARED_TO_GLOBAL_ITERS = ((WARP_ROW_TILES * N) / SHARED_TO_GLOBAL_LINES_PER_WARP);\n\n    const int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const int WARP_STRIDE = (WARP_COL_TILES * M);\n    const int WARPS_PER_BLOCK = (BLOCK_ROW_WARPS * BLOCK_COL_WARPS);\n    const int THREADS_PER_BLOCK = (WARP_SIZE * WARPS_PER_BLOCK);\n    const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    const int GLOBAL_MEM_STRIDE = M_GLOBAL;\n    const int SHMEM_STRIDE = (M * BLOCK_COL_TILES);\n    const int SHMEM_OFFSET = (M * WARP_COL_TILES);\n    const int BLOCK_SIZE_M = (M * BLOCK_COL_TILES);\n    const int BLOCK_SIZE_N = (N * BLOCK_ROW_TILES);\n\n   extern __shared__ uint8_t shmem[];\n\n\t// Warp and lane identification.\n\tconst unsigned int warpId = threadIdx.x / WARP_SIZE;\n\tconst unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n\n\t// Offset in shared memory from which the B matrix is stored.\n\t// const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\tconst size_t shmem_idx_b_off = BLOCK_SIZE_K_SPARSE * SHARED_OFFSET_A;\n\n\n\t// Each CTA slides along the 128 x 128 tiles from the top left corner of the\n\t// matrix to the right and down, and selects the next tile to compute. Once\n\t// there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n\tconst unsigned int block_tile_i =\n\t\t((block_pos * BLOCK_COL_TILES) / M_TILES) * (BLOCK_ROW_TILES);\n\tconst unsigned int block_tile_j = (block_pos * BLOCK_COL_TILES) % M_TILES;\n\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n\twmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_ROW_TILES]\n\t\t\t\t\t\t\t\t\t\t\t\t\t [WARP_COL_TILES];\n\n    // Load the C matrix tiles into fragments from shared memory.\n\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t\twmma::fill_fragment(c[i][j], 0);\n\t\t}\n\t}\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    //for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n    for(int tile_k_idx_sparse = 0, tile_k_idx = 0; tile_k_idx_sparse < K_GLOBAL_SPARSE; tile_k_idx_sparse += BLOCK_SIZE_K_SPARSE, tile_k_idx += BLOCK_SIZE_K){\n\n\t\tsize_t shmem_idx = \n\t\twarpId < (WARPS_PER_BLOCK / 2)\n\t\t\t? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A\n\t\t\t: (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B + shmem_idx_b_off;\n\n\t\tint4 *lane_ptr = NULL;\n\t\tint *lane_ptr_index = NULL;\n\t\tconst uint8_t *warp_ptr = NULL;\n\n\n\t\tif(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\t//warp_ptr = &A[block_tile_j * M] +\n\t\t\t//\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * M_GLOBAL;\n\t\t\twarp_ptr = &A[block_tile_j * M];\n\t\t\t\n\t\t\tconst int *warp_ptr_index = &B_index[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t\t\t\t\t\t((warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A);\n\n\t\t\tlane_ptr_index = (int *)(warp_ptr_index + tile_k_idx_sparse + (laneId / CHUNK_COPY_LINE_LANES_A));\n\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_A) * SHARED_OFFSET_A;\n\t\t}else{\n\t\t\twarp_ptr = &B[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * K_GLOBAL_SPARSE;\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_sparse +\n\t\t\t\t\t\t\t\t(laneId / CHUNK_COPY_LINE_LANES_B) * K_GLOBAL_SPARSE) +\n\t\t\t\t\t\t\t\t(laneId % CHUNK_COPY_LINE_LANES_B);\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_B) * SHARED_OFFSET_B;\n\t\t}\n\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      // shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n\t  int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n\t  \t? BLOCK_SIZE_K_SPARSE / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A)\n\t\t: BLOCK_SIZE_N / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\n\t  /*\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\t  */\n\n\t  /*\n      int tile_k_idx_A;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n          tile_k_idx_A = *(lane_ptr_index);\n      }\n\t  */\n\n\t  #pragma unroll\n\t  for(int i = 0; i < iter_index; i += 1){\n\t\t  if(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\tint tile_k_idx_A = *(lane_ptr_index);\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_A * M_GLOBAL) + (laneId % CHUNK_COPY_LINE_LANES_A);\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_A)) =\n\t\t\t\t*lane_ptr;\n\t\t\t//warp_ptr = (uint8_t *)((uint8_t *)warp_ptr + M_GLOBAL * (WARPS_PER_BLOCK / 2) *CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tlane_ptr_index = (int *)((int *)lane_ptr_index +  (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A;\n\t\t  }else{\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_B)) =\n\t\t\t\t*lane_ptr;\n\t\t\tlane_ptr = (int4 *)((uint8_t *)lane_ptr + K_GLOBAL_SPARSE * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B;\n\t\t  }\n\t  }\n\n      __syncthreads();\n\n\t#pragma unroll\n      for (int k_step = 0; k_step < CHUNK_K_SPARSE; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::col_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n\t#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i += 1) {\n\t\t\tsize_t shmem_idx_a = (warpId % BLOCK_COL_WARPS) * M * WARP_COL_TILES + (i * M);\n\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_a + k_step * K * SHARED_OFFSET_A;\n\n\t\t\twmma::load_matrix_sync(a[i], tile_ptr, SHARED_OFFSET_A);\n\t\t#pragma unroll\n\t\t\tfor(int j = 0; j < WARP_ROW_TILES; j += 1){\n\t\t\t\tif(i == 0){\n\t\t\t\t\tsize_t shmem_idx_b = shmem_idx_b_off +\n\t\t\t\t\t\t\t\t\t\t\t(warpId / BLOCK_COL_WARPS) * (WARP_ROW_TILES * N) * SHARED_OFFSET_B +\n\t\t\t\t\t\t\t\t\t\t\t(j * N) * SHARED_OFFSET_B;\n\t\t\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_b + k_step * K;\n\t\t\t\t\twmma::load_matrix_sync(b[j], tile_ptr, SHARED_OFFSET_B);\n\t\t\t\t}\n\t\t\t\twmma::mma_sync(c[j][i], a[i], b[j], c[j][i]);\n\t\t\t}\n\n        }\n      }\n\n      __syncthreads();\n    }\n\n    // This pointer is used to access the C and D matrix tiles this warp computes.\n\tint *shmem_warp_tile_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * N * WARP_ROW_TILES * SHMEM_STRIDE +\n\t(warpId % BLOCK_COL_WARPS) * SHMEM_OFFSET;\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t#pragma unroll\n\t\t\tfor(int t = 0; t < c[i][j].num_elements; t += 1){\n\t\t\t\tc[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n\t\t\t}\n\t\t\tint *tile_ptr = shmem_warp_tile_ptr + i * N * SHMEM_STRIDE + j * M;\n\t\t\twmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, C_LAYOUT);\n\t\t}\n\t}\n\n    __syncthreads();\n\n\tint *shmem_warp_stream_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N * SHMEM_STRIDE\n\t\t\t\t\t\t\t\t\t+ (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tconst size_t gmem_idx =\n\t\t(block_tile_i * N + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N) * GLOBAL_MEM_STRIDE +\n\t\tblock_tile_j * M + (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tuint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n\tint *shmem_lane_stream_ptr =\n\t\tshmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * SHMEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\t\n\tuint8_t *dst_gmem_lane_stream_ptr =\n\t\tdst_gmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * GLOBAL_MEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\n\tfor(int i = 0; i < WARP_ROW_TILES * N; i += SHARED_TO_GLOBAL_LINES_PER_WARP){\n\t\t*(dst_gmem_lane_stream_ptr + i * GLOBAL_MEM_STRIDE) = (uint8_t)(*(shmem_lane_stream_ptr + i * SHMEM_STRIDE));\n\t}\n\n\t__syncthreads();\n}\n", "launch_config": {"dimBlock": [256, 1], "dimGrid": [256, 1]}}, "26": {"code": "\n__global__ void compute_gemm_imma(float *input0, float *input1, float *input2, float* input3, float* input4, float*input5, float* input6,\n                                  float *output0) {\n    // keys need to be replaced 4096, 1024, K_GLOBA_VALUEL, 0.75, 64\n\n\t//extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B = reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_index = reinterpret_cast<int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n\tconst int WARP_SIZE = 32;\n    const int M = 16;\n    const int K = 16;\n    const int N = 16;\n    const int WMMA_M = 16;\n    const int WMMA_K = 16;\n    const int WMMA_N = 16;\n    const int M_GLOBAL = 4096;\n    const int K_GLOBAL = 1024;\n    const int N_GLOBAL = 1024;\n    const float SPARSITY = 0.75;\n    const int K_GLOBAL_SPARSE = (int(K_GLOBAL * (1-SPARSITY)));\n    const int M_TILES = (M_GLOBAL / M);\n    const int N_TILES = (N_GLOBAL / N);\n    const int K_TILES = (K_GLOBAL / K);\n    const int BLOCK_ROW_WARPS = 2;\n    const int BLOCK_COL_WARPS = 4;\n    const int WARP_ROW_TILES = 4;\n    const int WARP_COL_TILES = 2;\n    const int SKEW_UINT8 = 16;\n    const int BANK_VAL = 32;\n    const int NUM_BANK = (K_GLOBAL / BANK_VAL);\n    /////////// BLOCK_ROW_TILES <= N_TILES ////////////\n    // const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    // ////////// BLOCK_COL_TILES <= M_TILES ////////////\n    // const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    auto C_LAYOUT = wmma::mem_col_major;\n    const int CHUNK_K = 64;\n    const int CHUNK_K_SPARSE = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K_SPARSE = (int((CHUNK_K * K) * (1 - SPARSITY)));\n    const int WARP_COPY_BYTES = (WARP_SIZE * sizeof(int4));\n    const int CHUNK_LINE_BYTES_A (BLOCK_COL_TILES * M * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_A = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_A);\n    const int CHUNK_COPY_LINE_LANES_A = (CHUNK_LINE_BYTES_A / sizeof(int4));\n    const int SHARED_OFFSET_A = (BLOCK_COL_TILES * M + SKEW_UINT8);\n\n    const int CHUNK_LINE_BYTES_B = (BLOCK_SIZE_K_SPARSE * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_B = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_B);\n    const int CHUNK_COPY_LINE_LANES_B = (CHUNK_LINE_BYTES_B / sizeof(int4));\n    const int SHARED_OFFSET_B = (BLOCK_SIZE_K_SPARSE + SKEW_UINT8);\n    const int SHARED_TO_GLOBAL_BYTES_PER_LINE = ((WARP_COL_TILES * M) * sizeof(int));\n    const int SHARED_TO_GLOBAL_BYTES_PER_WARP = (WARP_SIZE * sizeof(int));\n    const int SHARED_TO_GLOBAL_LINES_PER_WARP = (SHARED_TO_GLOBAL_BYTES_PER_WARP / SHARED_TO_GLOBAL_BYTES_PER_LINE);\n    const int SHARED_TO_GLOBAL_LANES_PER_LINE = (WARP_SIZE / SHARED_TO_GLOBAL_LINES_PER_WARP);\n    const int SHARED_TO_GLOBAL_ITERS = ((WARP_ROW_TILES * N) / SHARED_TO_GLOBAL_LINES_PER_WARP);\n\n    const int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const int WARP_STRIDE = (WARP_COL_TILES * M);\n    const int WARPS_PER_BLOCK = (BLOCK_ROW_WARPS * BLOCK_COL_WARPS);\n    const int THREADS_PER_BLOCK = (WARP_SIZE * WARPS_PER_BLOCK);\n    const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    const int GLOBAL_MEM_STRIDE = M_GLOBAL;\n    const int SHMEM_STRIDE = (M * BLOCK_COL_TILES);\n    const int SHMEM_OFFSET = (M * WARP_COL_TILES);\n    const int BLOCK_SIZE_M = (M * BLOCK_COL_TILES);\n    const int BLOCK_SIZE_N = (N * BLOCK_ROW_TILES);\n\n   extern __shared__ uint8_t shmem[];\n\n\t// Warp and lane identification.\n\tconst unsigned int warpId = threadIdx.x / WARP_SIZE;\n\tconst unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n\n\t// Offset in shared memory from which the B matrix is stored.\n\t// const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\tconst size_t shmem_idx_b_off = BLOCK_SIZE_K_SPARSE * SHARED_OFFSET_A;\n\n\n\t// Each CTA slides along the 128 x 128 tiles from the top left corner of the\n\t// matrix to the right and down, and selects the next tile to compute. Once\n\t// there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n\tconst unsigned int block_tile_i =\n\t\t((block_pos * BLOCK_COL_TILES) / M_TILES) * (BLOCK_ROW_TILES);\n\tconst unsigned int block_tile_j = (block_pos * BLOCK_COL_TILES) % M_TILES;\n\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n\twmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_ROW_TILES]\n\t\t\t\t\t\t\t\t\t\t\t\t\t [WARP_COL_TILES];\n\n    // Load the C matrix tiles into fragments from shared memory.\n\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t\twmma::fill_fragment(c[i][j], 0);\n\t\t}\n\t}\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    //for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n    for(int tile_k_idx_sparse = 0, tile_k_idx = 0; tile_k_idx_sparse < K_GLOBAL_SPARSE; tile_k_idx_sparse += BLOCK_SIZE_K_SPARSE, tile_k_idx += BLOCK_SIZE_K){\n\n\t\tsize_t shmem_idx = \n\t\twarpId < (WARPS_PER_BLOCK / 2)\n\t\t\t? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A\n\t\t\t: (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B + shmem_idx_b_off;\n\n\t\tint4 *lane_ptr = NULL;\n\t\tint *lane_ptr_index = NULL;\n\t\tconst uint8_t *warp_ptr = NULL;\n\n\n\t\tif(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\t//warp_ptr = &A[block_tile_j * M] +\n\t\t\t//\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * M_GLOBAL;\n\t\t\twarp_ptr = &A[block_tile_j * M];\n\t\t\t\n\t\t\tconst int *warp_ptr_index = &B_index[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t\t\t\t\t\t((warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A);\n\n\t\t\tlane_ptr_index = (int *)(warp_ptr_index + tile_k_idx_sparse + (laneId / CHUNK_COPY_LINE_LANES_A));\n\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_A) * SHARED_OFFSET_A;\n\t\t}else{\n\t\t\twarp_ptr = &B[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * K_GLOBAL_SPARSE;\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_sparse +\n\t\t\t\t\t\t\t\t(laneId / CHUNK_COPY_LINE_LANES_B) * K_GLOBAL_SPARSE) +\n\t\t\t\t\t\t\t\t(laneId % CHUNK_COPY_LINE_LANES_B);\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_B) * SHARED_OFFSET_B;\n\t\t}\n\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      // shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n\t  int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n\t  \t? BLOCK_SIZE_K_SPARSE / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A)\n\t\t: BLOCK_SIZE_N / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\n\t  /*\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\t  */\n\n\t  /*\n      int tile_k_idx_A;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n          tile_k_idx_A = *(lane_ptr_index);\n      }\n\t  */\n\n\t  #pragma unroll\n\t  for(int i = 0; i < iter_index; i += 1){\n\t\t  if(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\tint tile_k_idx_A = *(lane_ptr_index);\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_A * M_GLOBAL) + (laneId % CHUNK_COPY_LINE_LANES_A);\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_A)) =\n\t\t\t\t*lane_ptr;\n\t\t\t//warp_ptr = (uint8_t *)((uint8_t *)warp_ptr + M_GLOBAL * (WARPS_PER_BLOCK / 2) *CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tlane_ptr_index = (int *)((int *)lane_ptr_index +  (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A;\n\t\t  }else{\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_B)) =\n\t\t\t\t*lane_ptr;\n\t\t\tlane_ptr = (int4 *)((uint8_t *)lane_ptr + K_GLOBAL_SPARSE * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B;\n\t\t  }\n\t  }\n\n      __syncthreads();\n\n\t#pragma unroll\n      for (int k_step = 0; k_step < CHUNK_K_SPARSE; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::col_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n\t#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i += 1) {\n\t\t\tsize_t shmem_idx_a = (warpId % BLOCK_COL_WARPS) * M * WARP_COL_TILES + (i * M);\n\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_a + k_step * K * SHARED_OFFSET_A;\n\n\t\t\twmma::load_matrix_sync(a[i], tile_ptr, SHARED_OFFSET_A);\n\t\t#pragma unroll\n\t\t\tfor(int j = 0; j < WARP_ROW_TILES; j += 1){\n\t\t\t\tif(i == 0){\n\t\t\t\t\tsize_t shmem_idx_b = shmem_idx_b_off +\n\t\t\t\t\t\t\t\t\t\t\t(warpId / BLOCK_COL_WARPS) * (WARP_ROW_TILES * N) * SHARED_OFFSET_B +\n\t\t\t\t\t\t\t\t\t\t\t(j * N) * SHARED_OFFSET_B;\n\t\t\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_b + k_step * K;\n\t\t\t\t\twmma::load_matrix_sync(b[j], tile_ptr, SHARED_OFFSET_B);\n\t\t\t\t}\n\t\t\t\twmma::mma_sync(c[j][i], a[i], b[j], c[j][i]);\n\t\t\t}\n\n        }\n      }\n\n      __syncthreads();\n    }\n\n    // This pointer is used to access the C and D matrix tiles this warp computes.\n\tint *shmem_warp_tile_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * N * WARP_ROW_TILES * SHMEM_STRIDE +\n\t(warpId % BLOCK_COL_WARPS) * SHMEM_OFFSET;\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t#pragma unroll\n\t\t\tfor(int t = 0; t < c[i][j].num_elements; t += 1){\n\t\t\t\tc[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n\t\t\t}\n\t\t\tint *tile_ptr = shmem_warp_tile_ptr + i * N * SHMEM_STRIDE + j * M;\n\t\t\twmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, C_LAYOUT);\n\t\t}\n\t}\n\n    __syncthreads();\n\n\tint *shmem_warp_stream_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N * SHMEM_STRIDE\n\t\t\t\t\t\t\t\t\t+ (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tconst size_t gmem_idx =\n\t\t(block_tile_i * N + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N) * GLOBAL_MEM_STRIDE +\n\t\tblock_tile_j * M + (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tuint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n\tint *shmem_lane_stream_ptr =\n\t\tshmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * SHMEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\t\n\tuint8_t *dst_gmem_lane_stream_ptr =\n\t\tdst_gmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * GLOBAL_MEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\n\tfor(int i = 0; i < WARP_ROW_TILES * N; i += SHARED_TO_GLOBAL_LINES_PER_WARP){\n\t\t*(dst_gmem_lane_stream_ptr + i * GLOBAL_MEM_STRIDE) = (uint8_t)(*(shmem_lane_stream_ptr + i * SHMEM_STRIDE));\n\t}\n\n\t__syncthreads();\n}\n", "launch_config": {"dimBlock": [256, 1], "dimGrid": [256, 1]}}, "27": {"code": "\n__global__ void compute_gemm_imma(float *input0, float *input1, float *input2, float* input3, float* input4, float*input5, float* input6,\n                                  float *output0) {\n    // keys need to be replaced 4096, 1024, K_GLOBA_VALUEL, 0.75, 64\n\n\t//extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B = reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_index = reinterpret_cast<int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n\tconst int WARP_SIZE = 32;\n    const int M = 16;\n    const int K = 16;\n    const int N = 16;\n    const int WMMA_M = 16;\n    const int WMMA_K = 16;\n    const int WMMA_N = 16;\n    const int M_GLOBAL = 4096;\n    const int K_GLOBAL = 1024;\n    const int N_GLOBAL = 1024;\n    const float SPARSITY = 0.75;\n    const int K_GLOBAL_SPARSE = (int(K_GLOBAL * (1-SPARSITY)));\n    const int M_TILES = (M_GLOBAL / M);\n    const int N_TILES = (N_GLOBAL / N);\n    const int K_TILES = (K_GLOBAL / K);\n    const int BLOCK_ROW_WARPS = 2;\n    const int BLOCK_COL_WARPS = 4;\n    const int WARP_ROW_TILES = 4;\n    const int WARP_COL_TILES = 2;\n    const int SKEW_UINT8 = 16;\n    const int BANK_VAL = 32;\n    const int NUM_BANK = (K_GLOBAL / BANK_VAL);\n    /////////// BLOCK_ROW_TILES <= N_TILES ////////////\n    // const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    // ////////// BLOCK_COL_TILES <= M_TILES ////////////\n    // const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    auto C_LAYOUT = wmma::mem_col_major;\n    const int CHUNK_K = 64;\n    const int CHUNK_K_SPARSE = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K_SPARSE = (int((CHUNK_K * K) * (1 - SPARSITY)));\n    const int WARP_COPY_BYTES = (WARP_SIZE * sizeof(int4));\n    const int CHUNK_LINE_BYTES_A (BLOCK_COL_TILES * M * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_A = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_A);\n    const int CHUNK_COPY_LINE_LANES_A = (CHUNK_LINE_BYTES_A / sizeof(int4));\n    const int SHARED_OFFSET_A = (BLOCK_COL_TILES * M + SKEW_UINT8);\n\n    const int CHUNK_LINE_BYTES_B = (BLOCK_SIZE_K_SPARSE * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_B = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_B);\n    const int CHUNK_COPY_LINE_LANES_B = (CHUNK_LINE_BYTES_B / sizeof(int4));\n    const int SHARED_OFFSET_B = (BLOCK_SIZE_K_SPARSE + SKEW_UINT8);\n    const int SHARED_TO_GLOBAL_BYTES_PER_LINE = ((WARP_COL_TILES * M) * sizeof(int));\n    const int SHARED_TO_GLOBAL_BYTES_PER_WARP = (WARP_SIZE * sizeof(int));\n    const int SHARED_TO_GLOBAL_LINES_PER_WARP = (SHARED_TO_GLOBAL_BYTES_PER_WARP / SHARED_TO_GLOBAL_BYTES_PER_LINE);\n    const int SHARED_TO_GLOBAL_LANES_PER_LINE = (WARP_SIZE / SHARED_TO_GLOBAL_LINES_PER_WARP);\n    const int SHARED_TO_GLOBAL_ITERS = ((WARP_ROW_TILES * N) / SHARED_TO_GLOBAL_LINES_PER_WARP);\n\n    const int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const int WARP_STRIDE = (WARP_COL_TILES * M);\n    const int WARPS_PER_BLOCK = (BLOCK_ROW_WARPS * BLOCK_COL_WARPS);\n    const int THREADS_PER_BLOCK = (WARP_SIZE * WARPS_PER_BLOCK);\n    const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    const int GLOBAL_MEM_STRIDE = M_GLOBAL;\n    const int SHMEM_STRIDE = (M * BLOCK_COL_TILES);\n    const int SHMEM_OFFSET = (M * WARP_COL_TILES);\n    const int BLOCK_SIZE_M = (M * BLOCK_COL_TILES);\n    const int BLOCK_SIZE_N = (N * BLOCK_ROW_TILES);\n\n   extern __shared__ uint8_t shmem[];\n\n\t// Warp and lane identification.\n\tconst unsigned int warpId = threadIdx.x / WARP_SIZE;\n\tconst unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n\n\t// Offset in shared memory from which the B matrix is stored.\n\t// const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\tconst size_t shmem_idx_b_off = BLOCK_SIZE_K_SPARSE * SHARED_OFFSET_A;\n\n\n\t// Each CTA slides along the 128 x 128 tiles from the top left corner of the\n\t// matrix to the right and down, and selects the next tile to compute. Once\n\t// there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n\tconst unsigned int block_tile_i =\n\t\t((block_pos * BLOCK_COL_TILES) / M_TILES) * (BLOCK_ROW_TILES);\n\tconst unsigned int block_tile_j = (block_pos * BLOCK_COL_TILES) % M_TILES;\n\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n\twmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_ROW_TILES]\n\t\t\t\t\t\t\t\t\t\t\t\t\t [WARP_COL_TILES];\n\n    // Load the C matrix tiles into fragments from shared memory.\n\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t\twmma::fill_fragment(c[i][j], 0);\n\t\t}\n\t}\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    //for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n    for(int tile_k_idx_sparse = 0, tile_k_idx = 0; tile_k_idx_sparse < K_GLOBAL_SPARSE; tile_k_idx_sparse += BLOCK_SIZE_K_SPARSE, tile_k_idx += BLOCK_SIZE_K){\n\n\t\tsize_t shmem_idx = \n\t\twarpId < (WARPS_PER_BLOCK / 2)\n\t\t\t? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A\n\t\t\t: (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B + shmem_idx_b_off;\n\n\t\tint4 *lane_ptr = NULL;\n\t\tint *lane_ptr_index = NULL;\n\t\tconst uint8_t *warp_ptr = NULL;\n\n\n\t\tif(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\t//warp_ptr = &A[block_tile_j * M] +\n\t\t\t//\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * M_GLOBAL;\n\t\t\twarp_ptr = &A[block_tile_j * M];\n\t\t\t\n\t\t\tconst int *warp_ptr_index = &B_index[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t\t\t\t\t\t((warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A);\n\n\t\t\tlane_ptr_index = (int *)(warp_ptr_index + tile_k_idx_sparse + (laneId / CHUNK_COPY_LINE_LANES_A));\n\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_A) * SHARED_OFFSET_A;\n\t\t}else{\n\t\t\twarp_ptr = &B[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * K_GLOBAL_SPARSE;\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_sparse +\n\t\t\t\t\t\t\t\t(laneId / CHUNK_COPY_LINE_LANES_B) * K_GLOBAL_SPARSE) +\n\t\t\t\t\t\t\t\t(laneId % CHUNK_COPY_LINE_LANES_B);\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_B) * SHARED_OFFSET_B;\n\t\t}\n\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      // shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n\t  int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n\t  \t? BLOCK_SIZE_K_SPARSE / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A)\n\t\t: BLOCK_SIZE_N / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\n\t  /*\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\t  */\n\n\t  /*\n      int tile_k_idx_A;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n          tile_k_idx_A = *(lane_ptr_index);\n      }\n\t  */\n\n\t  #pragma unroll\n\t  for(int i = 0; i < iter_index; i += 1){\n\t\t  if(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\tint tile_k_idx_A = *(lane_ptr_index);\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_A * M_GLOBAL) + (laneId % CHUNK_COPY_LINE_LANES_A);\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_A)) =\n\t\t\t\t*lane_ptr;\n\t\t\t//warp_ptr = (uint8_t *)((uint8_t *)warp_ptr + M_GLOBAL * (WARPS_PER_BLOCK / 2) *CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tlane_ptr_index = (int *)((int *)lane_ptr_index +  (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A;\n\t\t  }else{\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_B)) =\n\t\t\t\t*lane_ptr;\n\t\t\tlane_ptr = (int4 *)((uint8_t *)lane_ptr + K_GLOBAL_SPARSE * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B;\n\t\t  }\n\t  }\n\n      __syncthreads();\n\n\t#pragma unroll\n      for (int k_step = 0; k_step < CHUNK_K_SPARSE; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::col_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n\t#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i += 1) {\n\t\t\tsize_t shmem_idx_a = (warpId % BLOCK_COL_WARPS) * M * WARP_COL_TILES + (i * M);\n\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_a + k_step * K * SHARED_OFFSET_A;\n\n\t\t\twmma::load_matrix_sync(a[i], tile_ptr, SHARED_OFFSET_A);\n\t\t#pragma unroll\n\t\t\tfor(int j = 0; j < WARP_ROW_TILES; j += 1){\n\t\t\t\tif(i == 0){\n\t\t\t\t\tsize_t shmem_idx_b = shmem_idx_b_off +\n\t\t\t\t\t\t\t\t\t\t\t(warpId / BLOCK_COL_WARPS) * (WARP_ROW_TILES * N) * SHARED_OFFSET_B +\n\t\t\t\t\t\t\t\t\t\t\t(j * N) * SHARED_OFFSET_B;\n\t\t\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_b + k_step * K;\n\t\t\t\t\twmma::load_matrix_sync(b[j], tile_ptr, SHARED_OFFSET_B);\n\t\t\t\t}\n\t\t\t\twmma::mma_sync(c[j][i], a[i], b[j], c[j][i]);\n\t\t\t}\n\n        }\n      }\n\n      __syncthreads();\n    }\n\n    // This pointer is used to access the C and D matrix tiles this warp computes.\n\tint *shmem_warp_tile_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * N * WARP_ROW_TILES * SHMEM_STRIDE +\n\t(warpId % BLOCK_COL_WARPS) * SHMEM_OFFSET;\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t#pragma unroll\n\t\t\tfor(int t = 0; t < c[i][j].num_elements; t += 1){\n\t\t\t\tc[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n\t\t\t}\n\t\t\tint *tile_ptr = shmem_warp_tile_ptr + i * N * SHMEM_STRIDE + j * M;\n\t\t\twmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, C_LAYOUT);\n\t\t}\n\t}\n\n    __syncthreads();\n\n\tint *shmem_warp_stream_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N * SHMEM_STRIDE\n\t\t\t\t\t\t\t\t\t+ (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tconst size_t gmem_idx =\n\t\t(block_tile_i * N + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N) * GLOBAL_MEM_STRIDE +\n\t\tblock_tile_j * M + (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tuint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n\tint *shmem_lane_stream_ptr =\n\t\tshmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * SHMEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\t\n\tuint8_t *dst_gmem_lane_stream_ptr =\n\t\tdst_gmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * GLOBAL_MEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\n\tfor(int i = 0; i < WARP_ROW_TILES * N; i += SHARED_TO_GLOBAL_LINES_PER_WARP){\n\t\t*(dst_gmem_lane_stream_ptr + i * GLOBAL_MEM_STRIDE) = (uint8_t)(*(shmem_lane_stream_ptr + i * SHMEM_STRIDE));\n\t}\n\n\t__syncthreads();\n}\n", "launch_config": {"dimBlock": [256, 1], "dimGrid": [256, 1]}}, "28": {"code": "\n__global__ void compute_gemm_imma(float *input0, float *input1, float *input2, float* input3, float* input4, float*input5, float* input6,\n                                  float *output0) {\n    // keys need to be replaced 4096, 1024, K_GLOBA_VALUEL, 0.75, 64\n\n\t//extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B = reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_index = reinterpret_cast<int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n\tconst int WARP_SIZE = 32;\n    const int M = 16;\n    const int K = 16;\n    const int N = 16;\n    const int WMMA_M = 16;\n    const int WMMA_K = 16;\n    const int WMMA_N = 16;\n    const int M_GLOBAL = 4096;\n    const int K_GLOBAL = 1024;\n    const int N_GLOBAL = 1024;\n    const float SPARSITY = 0.75;\n    const int K_GLOBAL_SPARSE = (int(K_GLOBAL * (1-SPARSITY)));\n    const int M_TILES = (M_GLOBAL / M);\n    const int N_TILES = (N_GLOBAL / N);\n    const int K_TILES = (K_GLOBAL / K);\n    const int BLOCK_ROW_WARPS = 2;\n    const int BLOCK_COL_WARPS = 4;\n    const int WARP_ROW_TILES = 4;\n    const int WARP_COL_TILES = 2;\n    const int SKEW_UINT8 = 16;\n    const int BANK_VAL = 32;\n    const int NUM_BANK = (K_GLOBAL / BANK_VAL);\n    /////////// BLOCK_ROW_TILES <= N_TILES ////////////\n    // const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    // ////////// BLOCK_COL_TILES <= M_TILES ////////////\n    // const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    auto C_LAYOUT = wmma::mem_col_major;\n    const int CHUNK_K = 64;\n    const int CHUNK_K_SPARSE = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K_SPARSE = (int((CHUNK_K * K) * (1 - SPARSITY)));\n    const int WARP_COPY_BYTES = (WARP_SIZE * sizeof(int4));\n    const int CHUNK_LINE_BYTES_A (BLOCK_COL_TILES * M * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_A = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_A);\n    const int CHUNK_COPY_LINE_LANES_A = (CHUNK_LINE_BYTES_A / sizeof(int4));\n    const int SHARED_OFFSET_A = (BLOCK_COL_TILES * M + SKEW_UINT8);\n\n    const int CHUNK_LINE_BYTES_B = (BLOCK_SIZE_K_SPARSE * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_B = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_B);\n    const int CHUNK_COPY_LINE_LANES_B = (CHUNK_LINE_BYTES_B / sizeof(int4));\n    const int SHARED_OFFSET_B = (BLOCK_SIZE_K_SPARSE + SKEW_UINT8);\n    const int SHARED_TO_GLOBAL_BYTES_PER_LINE = ((WARP_COL_TILES * M) * sizeof(int));\n    const int SHARED_TO_GLOBAL_BYTES_PER_WARP = (WARP_SIZE * sizeof(int));\n    const int SHARED_TO_GLOBAL_LINES_PER_WARP = (SHARED_TO_GLOBAL_BYTES_PER_WARP / SHARED_TO_GLOBAL_BYTES_PER_LINE);\n    const int SHARED_TO_GLOBAL_LANES_PER_LINE = (WARP_SIZE / SHARED_TO_GLOBAL_LINES_PER_WARP);\n    const int SHARED_TO_GLOBAL_ITERS = ((WARP_ROW_TILES * N) / SHARED_TO_GLOBAL_LINES_PER_WARP);\n\n    const int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const int WARP_STRIDE = (WARP_COL_TILES * M);\n    const int WARPS_PER_BLOCK = (BLOCK_ROW_WARPS * BLOCK_COL_WARPS);\n    const int THREADS_PER_BLOCK = (WARP_SIZE * WARPS_PER_BLOCK);\n    const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    const int GLOBAL_MEM_STRIDE = M_GLOBAL;\n    const int SHMEM_STRIDE = (M * BLOCK_COL_TILES);\n    const int SHMEM_OFFSET = (M * WARP_COL_TILES);\n    const int BLOCK_SIZE_M = (M * BLOCK_COL_TILES);\n    const int BLOCK_SIZE_N = (N * BLOCK_ROW_TILES);\n\n   extern __shared__ uint8_t shmem[];\n\n\t// Warp and lane identification.\n\tconst unsigned int warpId = threadIdx.x / WARP_SIZE;\n\tconst unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n\n\t// Offset in shared memory from which the B matrix is stored.\n\t// const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\tconst size_t shmem_idx_b_off = BLOCK_SIZE_K_SPARSE * SHARED_OFFSET_A;\n\n\n\t// Each CTA slides along the 128 x 128 tiles from the top left corner of the\n\t// matrix to the right and down, and selects the next tile to compute. Once\n\t// there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n\tconst unsigned int block_tile_i =\n\t\t((block_pos * BLOCK_COL_TILES) / M_TILES) * (BLOCK_ROW_TILES);\n\tconst unsigned int block_tile_j = (block_pos * BLOCK_COL_TILES) % M_TILES;\n\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n\twmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_ROW_TILES]\n\t\t\t\t\t\t\t\t\t\t\t\t\t [WARP_COL_TILES];\n\n    // Load the C matrix tiles into fragments from shared memory.\n\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t\twmma::fill_fragment(c[i][j], 0);\n\t\t}\n\t}\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    //for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n    for(int tile_k_idx_sparse = 0, tile_k_idx = 0; tile_k_idx_sparse < K_GLOBAL_SPARSE; tile_k_idx_sparse += BLOCK_SIZE_K_SPARSE, tile_k_idx += BLOCK_SIZE_K){\n\n\t\tsize_t shmem_idx = \n\t\twarpId < (WARPS_PER_BLOCK / 2)\n\t\t\t? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A\n\t\t\t: (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B + shmem_idx_b_off;\n\n\t\tint4 *lane_ptr = NULL;\n\t\tint *lane_ptr_index = NULL;\n\t\tconst uint8_t *warp_ptr = NULL;\n\n\n\t\tif(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\t//warp_ptr = &A[block_tile_j * M] +\n\t\t\t//\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * M_GLOBAL;\n\t\t\twarp_ptr = &A[block_tile_j * M];\n\t\t\t\n\t\t\tconst int *warp_ptr_index = &B_index[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t\t\t\t\t\t((warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A);\n\n\t\t\tlane_ptr_index = (int *)(warp_ptr_index + tile_k_idx_sparse + (laneId / CHUNK_COPY_LINE_LANES_A));\n\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_A) * SHARED_OFFSET_A;\n\t\t}else{\n\t\t\twarp_ptr = &B[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * K_GLOBAL_SPARSE;\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_sparse +\n\t\t\t\t\t\t\t\t(laneId / CHUNK_COPY_LINE_LANES_B) * K_GLOBAL_SPARSE) +\n\t\t\t\t\t\t\t\t(laneId % CHUNK_COPY_LINE_LANES_B);\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_B) * SHARED_OFFSET_B;\n\t\t}\n\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      // shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n\t  int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n\t  \t? BLOCK_SIZE_K_SPARSE / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A)\n\t\t: BLOCK_SIZE_N / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\n\t  /*\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\t  */\n\n\t  /*\n      int tile_k_idx_A;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n          tile_k_idx_A = *(lane_ptr_index);\n      }\n\t  */\n\n\t  #pragma unroll\n\t  for(int i = 0; i < iter_index; i += 1){\n\t\t  if(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\tint tile_k_idx_A = *(lane_ptr_index);\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_A * M_GLOBAL) + (laneId % CHUNK_COPY_LINE_LANES_A);\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_A)) =\n\t\t\t\t*lane_ptr;\n\t\t\t//warp_ptr = (uint8_t *)((uint8_t *)warp_ptr + M_GLOBAL * (WARPS_PER_BLOCK / 2) *CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tlane_ptr_index = (int *)((int *)lane_ptr_index +  (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A;\n\t\t  }else{\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_B)) =\n\t\t\t\t*lane_ptr;\n\t\t\tlane_ptr = (int4 *)((uint8_t *)lane_ptr + K_GLOBAL_SPARSE * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B;\n\t\t  }\n\t  }\n\n      __syncthreads();\n\n\t#pragma unroll\n      for (int k_step = 0; k_step < CHUNK_K_SPARSE; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::col_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n\t#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i += 1) {\n\t\t\tsize_t shmem_idx_a = (warpId % BLOCK_COL_WARPS) * M * WARP_COL_TILES + (i * M);\n\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_a + k_step * K * SHARED_OFFSET_A;\n\n\t\t\twmma::load_matrix_sync(a[i], tile_ptr, SHARED_OFFSET_A);\n\t\t#pragma unroll\n\t\t\tfor(int j = 0; j < WARP_ROW_TILES; j += 1){\n\t\t\t\tif(i == 0){\n\t\t\t\t\tsize_t shmem_idx_b = shmem_idx_b_off +\n\t\t\t\t\t\t\t\t\t\t\t(warpId / BLOCK_COL_WARPS) * (WARP_ROW_TILES * N) * SHARED_OFFSET_B +\n\t\t\t\t\t\t\t\t\t\t\t(j * N) * SHARED_OFFSET_B;\n\t\t\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_b + k_step * K;\n\t\t\t\t\twmma::load_matrix_sync(b[j], tile_ptr, SHARED_OFFSET_B);\n\t\t\t\t}\n\t\t\t\twmma::mma_sync(c[j][i], a[i], b[j], c[j][i]);\n\t\t\t}\n\n        }\n      }\n\n      __syncthreads();\n    }\n\n    // This pointer is used to access the C and D matrix tiles this warp computes.\n\tint *shmem_warp_tile_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * N * WARP_ROW_TILES * SHMEM_STRIDE +\n\t(warpId % BLOCK_COL_WARPS) * SHMEM_OFFSET;\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t#pragma unroll\n\t\t\tfor(int t = 0; t < c[i][j].num_elements; t += 1){\n\t\t\t\tc[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n\t\t\t}\n\t\t\tint *tile_ptr = shmem_warp_tile_ptr + i * N * SHMEM_STRIDE + j * M;\n\t\t\twmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, C_LAYOUT);\n\t\t}\n\t}\n\n    __syncthreads();\n\n\tint *shmem_warp_stream_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N * SHMEM_STRIDE\n\t\t\t\t\t\t\t\t\t+ (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tconst size_t gmem_idx =\n\t\t(block_tile_i * N + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N) * GLOBAL_MEM_STRIDE +\n\t\tblock_tile_j * M + (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tuint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n\tint *shmem_lane_stream_ptr =\n\t\tshmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * SHMEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\t\n\tuint8_t *dst_gmem_lane_stream_ptr =\n\t\tdst_gmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * GLOBAL_MEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\n\tfor(int i = 0; i < WARP_ROW_TILES * N; i += SHARED_TO_GLOBAL_LINES_PER_WARP){\n\t\t*(dst_gmem_lane_stream_ptr + i * GLOBAL_MEM_STRIDE) = (uint8_t)(*(shmem_lane_stream_ptr + i * SHMEM_STRIDE));\n\t}\n\n\t__syncthreads();\n}\n", "launch_config": {"dimBlock": [256, 1], "dimGrid": [256, 1]}}, "29": {"code": "\n__global__ void compute_gemm_imma(float *input0, float *input1, float *input2, float* input3, float* input4, float*input5, float* input6,\n                                  float *output0) {\n    // keys need to be replaced 4096, 4096, K_GLOBA_VALUEL, 0.75, 64\n\n\t//extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B = reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_index = reinterpret_cast<int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n\tconst int WARP_SIZE = 32;\n    const int M = 16;\n    const int K = 16;\n    const int N = 16;\n    const int WMMA_M = 16;\n    const int WMMA_K = 16;\n    const int WMMA_N = 16;\n    const int M_GLOBAL = 4096;\n    const int K_GLOBAL = 1024;\n    const int N_GLOBAL = 4096;\n    const float SPARSITY = 0.75;\n    const int K_GLOBAL_SPARSE = (int(K_GLOBAL * (1-SPARSITY)));\n    const int M_TILES = (M_GLOBAL / M);\n    const int N_TILES = (N_GLOBAL / N);\n    const int K_TILES = (K_GLOBAL / K);\n    const int BLOCK_ROW_WARPS = 2;\n    const int BLOCK_COL_WARPS = 4;\n    const int WARP_ROW_TILES = 4;\n    const int WARP_COL_TILES = 2;\n    const int SKEW_UINT8 = 16;\n    const int BANK_VAL = 32;\n    const int NUM_BANK = (K_GLOBAL / BANK_VAL);\n    /////////// BLOCK_ROW_TILES <= N_TILES ////////////\n    // const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    // ////////// BLOCK_COL_TILES <= M_TILES ////////////\n    // const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    auto C_LAYOUT = wmma::mem_col_major;\n    const int CHUNK_K = 64;\n    const int CHUNK_K_SPARSE = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K_SPARSE = (int((CHUNK_K * K) * (1 - SPARSITY)));\n    const int WARP_COPY_BYTES = (WARP_SIZE * sizeof(int4));\n    const int CHUNK_LINE_BYTES_A (BLOCK_COL_TILES * M * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_A = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_A);\n    const int CHUNK_COPY_LINE_LANES_A = (CHUNK_LINE_BYTES_A / sizeof(int4));\n    const int SHARED_OFFSET_A = (BLOCK_COL_TILES * M + SKEW_UINT8);\n\n    const int CHUNK_LINE_BYTES_B = (BLOCK_SIZE_K_SPARSE * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_B = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_B);\n    const int CHUNK_COPY_LINE_LANES_B = (CHUNK_LINE_BYTES_B / sizeof(int4));\n    const int SHARED_OFFSET_B = (BLOCK_SIZE_K_SPARSE + SKEW_UINT8);\n    const int SHARED_TO_GLOBAL_BYTES_PER_LINE = ((WARP_COL_TILES * M) * sizeof(int));\n    const int SHARED_TO_GLOBAL_BYTES_PER_WARP = (WARP_SIZE * sizeof(int));\n    const int SHARED_TO_GLOBAL_LINES_PER_WARP = (SHARED_TO_GLOBAL_BYTES_PER_WARP / SHARED_TO_GLOBAL_BYTES_PER_LINE);\n    const int SHARED_TO_GLOBAL_LANES_PER_LINE = (WARP_SIZE / SHARED_TO_GLOBAL_LINES_PER_WARP);\n    const int SHARED_TO_GLOBAL_ITERS = ((WARP_ROW_TILES * N) / SHARED_TO_GLOBAL_LINES_PER_WARP);\n\n    const int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const int WARP_STRIDE = (WARP_COL_TILES * M);\n    const int WARPS_PER_BLOCK = (BLOCK_ROW_WARPS * BLOCK_COL_WARPS);\n    const int THREADS_PER_BLOCK = (WARP_SIZE * WARPS_PER_BLOCK);\n    const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    const int GLOBAL_MEM_STRIDE = M_GLOBAL;\n    const int SHMEM_STRIDE = (M * BLOCK_COL_TILES);\n    const int SHMEM_OFFSET = (M * WARP_COL_TILES);\n    const int BLOCK_SIZE_M = (M * BLOCK_COL_TILES);\n    const int BLOCK_SIZE_N = (N * BLOCK_ROW_TILES);\n\n   extern __shared__ uint8_t shmem[];\n\n\t// Warp and lane identification.\n\tconst unsigned int warpId = threadIdx.x / WARP_SIZE;\n\tconst unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n\n\t// Offset in shared memory from which the B matrix is stored.\n\t// const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\tconst size_t shmem_idx_b_off = BLOCK_SIZE_K_SPARSE * SHARED_OFFSET_A;\n\n\n\t// Each CTA slides along the 128 x 128 tiles from the top left corner of the\n\t// matrix to the right and down, and selects the next tile to compute. Once\n\t// there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n\tconst unsigned int block_tile_i =\n\t\t((block_pos * BLOCK_COL_TILES) / M_TILES) * (BLOCK_ROW_TILES);\n\tconst unsigned int block_tile_j = (block_pos * BLOCK_COL_TILES) % M_TILES;\n\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n\twmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_ROW_TILES]\n\t\t\t\t\t\t\t\t\t\t\t\t\t [WARP_COL_TILES];\n\n    // Load the C matrix tiles into fragments from shared memory.\n\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t\twmma::fill_fragment(c[i][j], 0);\n\t\t}\n\t}\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    //for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n    for(int tile_k_idx_sparse = 0, tile_k_idx = 0; tile_k_idx_sparse < K_GLOBAL_SPARSE; tile_k_idx_sparse += BLOCK_SIZE_K_SPARSE, tile_k_idx += BLOCK_SIZE_K){\n\n\t\tsize_t shmem_idx = \n\t\twarpId < (WARPS_PER_BLOCK / 2)\n\t\t\t? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A\n\t\t\t: (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B + shmem_idx_b_off;\n\n\t\tint4 *lane_ptr = NULL;\n\t\tint *lane_ptr_index = NULL;\n\t\tconst uint8_t *warp_ptr = NULL;\n\n\n\t\tif(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\t//warp_ptr = &A[block_tile_j * M] +\n\t\t\t//\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * M_GLOBAL;\n\t\t\twarp_ptr = &A[block_tile_j * M];\n\t\t\t\n\t\t\tconst int *warp_ptr_index = &B_index[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t\t\t\t\t\t((warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A);\n\n\t\t\tlane_ptr_index = (int *)(warp_ptr_index + tile_k_idx_sparse + (laneId / CHUNK_COPY_LINE_LANES_A));\n\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_A) * SHARED_OFFSET_A;\n\t\t}else{\n\t\t\twarp_ptr = &B[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * K_GLOBAL_SPARSE;\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_sparse +\n\t\t\t\t\t\t\t\t(laneId / CHUNK_COPY_LINE_LANES_B) * K_GLOBAL_SPARSE) +\n\t\t\t\t\t\t\t\t(laneId % CHUNK_COPY_LINE_LANES_B);\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_B) * SHARED_OFFSET_B;\n\t\t}\n\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      // shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n\t  int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n\t  \t? BLOCK_SIZE_K_SPARSE / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A)\n\t\t: BLOCK_SIZE_N / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\n\t  /*\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\t  */\n\n\t  /*\n      int tile_k_idx_A;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n          tile_k_idx_A = *(lane_ptr_index);\n      }\n\t  */\n\n\t  #pragma unroll\n\t  for(int i = 0; i < iter_index; i += 1){\n\t\t  if(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\tint tile_k_idx_A = *(lane_ptr_index);\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_A * M_GLOBAL) + (laneId % CHUNK_COPY_LINE_LANES_A);\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_A)) =\n\t\t\t\t*lane_ptr;\n\t\t\t//warp_ptr = (uint8_t *)((uint8_t *)warp_ptr + M_GLOBAL * (WARPS_PER_BLOCK / 2) *CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tlane_ptr_index = (int *)((int *)lane_ptr_index +  (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A;\n\t\t  }else{\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_B)) =\n\t\t\t\t*lane_ptr;\n\t\t\tlane_ptr = (int4 *)((uint8_t *)lane_ptr + K_GLOBAL_SPARSE * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B;\n\t\t  }\n\t  }\n\n      __syncthreads();\n\n\t#pragma unroll\n      for (int k_step = 0; k_step < CHUNK_K_SPARSE; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::col_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n\t#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i += 1) {\n\t\t\tsize_t shmem_idx_a = (warpId % BLOCK_COL_WARPS) * M * WARP_COL_TILES + (i * M);\n\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_a + k_step * K * SHARED_OFFSET_A;\n\n\t\t\twmma::load_matrix_sync(a[i], tile_ptr, SHARED_OFFSET_A);\n\t\t#pragma unroll\n\t\t\tfor(int j = 0; j < WARP_ROW_TILES; j += 1){\n\t\t\t\tif(i == 0){\n\t\t\t\t\tsize_t shmem_idx_b = shmem_idx_b_off +\n\t\t\t\t\t\t\t\t\t\t\t(warpId / BLOCK_COL_WARPS) * (WARP_ROW_TILES * N) * SHARED_OFFSET_B +\n\t\t\t\t\t\t\t\t\t\t\t(j * N) * SHARED_OFFSET_B;\n\t\t\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_b + k_step * K;\n\t\t\t\t\twmma::load_matrix_sync(b[j], tile_ptr, SHARED_OFFSET_B);\n\t\t\t\t}\n\t\t\t\twmma::mma_sync(c[j][i], a[i], b[j], c[j][i]);\n\t\t\t}\n\n        }\n      }\n\n      __syncthreads();\n    }\n\n    // This pointer is used to access the C and D matrix tiles this warp computes.\n\tint *shmem_warp_tile_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * N * WARP_ROW_TILES * SHMEM_STRIDE +\n\t(warpId % BLOCK_COL_WARPS) * SHMEM_OFFSET;\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t#pragma unroll\n\t\t\tfor(int t = 0; t < c[i][j].num_elements; t += 1){\n\t\t\t\tc[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n\t\t\t}\n\t\t\tint *tile_ptr = shmem_warp_tile_ptr + i * N * SHMEM_STRIDE + j * M;\n\t\t\twmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, C_LAYOUT);\n\t\t}\n\t}\n\n    __syncthreads();\n\n\tint *shmem_warp_stream_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N * SHMEM_STRIDE\n\t\t\t\t\t\t\t\t\t+ (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tconst size_t gmem_idx =\n\t\t(block_tile_i * N + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N) * GLOBAL_MEM_STRIDE +\n\t\tblock_tile_j * M + (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tuint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n\tint *shmem_lane_stream_ptr =\n\t\tshmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * SHMEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\t\n\tuint8_t *dst_gmem_lane_stream_ptr =\n\t\tdst_gmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * GLOBAL_MEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\n\tfor(int i = 0; i < WARP_ROW_TILES * N; i += SHARED_TO_GLOBAL_LINES_PER_WARP){\n\t\t*(dst_gmem_lane_stream_ptr + i * GLOBAL_MEM_STRIDE) = (uint8_t)(*(shmem_lane_stream_ptr + i * SHMEM_STRIDE));\n\t}\n\n\t__syncthreads();\n}\n", "launch_config": {"dimBlock": [1024, 1], "dimGrid": [256, 1]}}, "30": {"code": "\n__global__ void compute_gemm_imma(float *input0, float *input1, float *input2, float* input3, float* input4, float*input5, float* input6,\n                                  float *output0) {\n    // keys need to be replaced 4096, 1024, K_GLOBA_VALUEL, 0.75, 64\n\n\t//extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B = reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_index = reinterpret_cast<int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n\tconst int WARP_SIZE = 32;\n    const int M = 16;\n    const int K = 16;\n    const int N = 16;\n    const int WMMA_M = 16;\n    const int WMMA_K = 16;\n    const int WMMA_N = 16;\n    const int M_GLOBAL = 4096;\n    const int K_GLOBAL = 4096;\n    const int N_GLOBAL = 1024;\n    const float SPARSITY = 0.75;\n    const int K_GLOBAL_SPARSE = (int(K_GLOBAL * (1-SPARSITY)));\n    const int M_TILES = (M_GLOBAL / M);\n    const int N_TILES = (N_GLOBAL / N);\n    const int K_TILES = (K_GLOBAL / K);\n    const int BLOCK_ROW_WARPS = 2;\n    const int BLOCK_COL_WARPS = 4;\n    const int WARP_ROW_TILES = 4;\n    const int WARP_COL_TILES = 2;\n    const int SKEW_UINT8 = 16;\n    const int BANK_VAL = 32;\n    const int NUM_BANK = (K_GLOBAL / BANK_VAL);\n    /////////// BLOCK_ROW_TILES <= N_TILES ////////////\n    // const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    // ////////// BLOCK_COL_TILES <= M_TILES ////////////\n    // const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    auto C_LAYOUT = wmma::mem_col_major;\n    const int CHUNK_K = 64;\n    const int CHUNK_K_SPARSE = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K_SPARSE = (int((CHUNK_K * K) * (1 - SPARSITY)));\n    const int WARP_COPY_BYTES = (WARP_SIZE * sizeof(int4));\n    const int CHUNK_LINE_BYTES_A (BLOCK_COL_TILES * M * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_A = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_A);\n    const int CHUNK_COPY_LINE_LANES_A = (CHUNK_LINE_BYTES_A / sizeof(int4));\n    const int SHARED_OFFSET_A = (BLOCK_COL_TILES * M + SKEW_UINT8);\n\n    const int CHUNK_LINE_BYTES_B = (BLOCK_SIZE_K_SPARSE * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_B = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_B);\n    const int CHUNK_COPY_LINE_LANES_B = (CHUNK_LINE_BYTES_B / sizeof(int4));\n    const int SHARED_OFFSET_B = (BLOCK_SIZE_K_SPARSE + SKEW_UINT8);\n    const int SHARED_TO_GLOBAL_BYTES_PER_LINE = ((WARP_COL_TILES * M) * sizeof(int));\n    const int SHARED_TO_GLOBAL_BYTES_PER_WARP = (WARP_SIZE * sizeof(int));\n    const int SHARED_TO_GLOBAL_LINES_PER_WARP = (SHARED_TO_GLOBAL_BYTES_PER_WARP / SHARED_TO_GLOBAL_BYTES_PER_LINE);\n    const int SHARED_TO_GLOBAL_LANES_PER_LINE = (WARP_SIZE / SHARED_TO_GLOBAL_LINES_PER_WARP);\n    const int SHARED_TO_GLOBAL_ITERS = ((WARP_ROW_TILES * N) / SHARED_TO_GLOBAL_LINES_PER_WARP);\n\n    const int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const int WARP_STRIDE = (WARP_COL_TILES * M);\n    const int WARPS_PER_BLOCK = (BLOCK_ROW_WARPS * BLOCK_COL_WARPS);\n    const int THREADS_PER_BLOCK = (WARP_SIZE * WARPS_PER_BLOCK);\n    const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    const int GLOBAL_MEM_STRIDE = M_GLOBAL;\n    const int SHMEM_STRIDE = (M * BLOCK_COL_TILES);\n    const int SHMEM_OFFSET = (M * WARP_COL_TILES);\n    const int BLOCK_SIZE_M = (M * BLOCK_COL_TILES);\n    const int BLOCK_SIZE_N = (N * BLOCK_ROW_TILES);\n\n   extern __shared__ uint8_t shmem[];\n\n\t// Warp and lane identification.\n\tconst unsigned int warpId = threadIdx.x / WARP_SIZE;\n\tconst unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n\n\t// Offset in shared memory from which the B matrix is stored.\n\t// const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\tconst size_t shmem_idx_b_off = BLOCK_SIZE_K_SPARSE * SHARED_OFFSET_A;\n\n\n\t// Each CTA slides along the 128 x 128 tiles from the top left corner of the\n\t// matrix to the right and down, and selects the next tile to compute. Once\n\t// there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n\tconst unsigned int block_tile_i =\n\t\t((block_pos * BLOCK_COL_TILES) / M_TILES) * (BLOCK_ROW_TILES);\n\tconst unsigned int block_tile_j = (block_pos * BLOCK_COL_TILES) % M_TILES;\n\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n\twmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_ROW_TILES]\n\t\t\t\t\t\t\t\t\t\t\t\t\t [WARP_COL_TILES];\n\n    // Load the C matrix tiles into fragments from shared memory.\n\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t\twmma::fill_fragment(c[i][j], 0);\n\t\t}\n\t}\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    //for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n    for(int tile_k_idx_sparse = 0, tile_k_idx = 0; tile_k_idx_sparse < K_GLOBAL_SPARSE; tile_k_idx_sparse += BLOCK_SIZE_K_SPARSE, tile_k_idx += BLOCK_SIZE_K){\n\n\t\tsize_t shmem_idx = \n\t\twarpId < (WARPS_PER_BLOCK / 2)\n\t\t\t? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A\n\t\t\t: (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B + shmem_idx_b_off;\n\n\t\tint4 *lane_ptr = NULL;\n\t\tint *lane_ptr_index = NULL;\n\t\tconst uint8_t *warp_ptr = NULL;\n\n\n\t\tif(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\t//warp_ptr = &A[block_tile_j * M] +\n\t\t\t//\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * M_GLOBAL;\n\t\t\twarp_ptr = &A[block_tile_j * M];\n\t\t\t\n\t\t\tconst int *warp_ptr_index = &B_index[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t\t\t\t\t\t((warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A);\n\n\t\t\tlane_ptr_index = (int *)(warp_ptr_index + tile_k_idx_sparse + (laneId / CHUNK_COPY_LINE_LANES_A));\n\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_A) * SHARED_OFFSET_A;\n\t\t}else{\n\t\t\twarp_ptr = &B[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * K_GLOBAL_SPARSE;\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_sparse +\n\t\t\t\t\t\t\t\t(laneId / CHUNK_COPY_LINE_LANES_B) * K_GLOBAL_SPARSE) +\n\t\t\t\t\t\t\t\t(laneId % CHUNK_COPY_LINE_LANES_B);\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_B) * SHARED_OFFSET_B;\n\t\t}\n\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      // shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n\t  int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n\t  \t? BLOCK_SIZE_K_SPARSE / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A)\n\t\t: BLOCK_SIZE_N / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\n\t  /*\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\t  */\n\n\t  /*\n      int tile_k_idx_A;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n          tile_k_idx_A = *(lane_ptr_index);\n      }\n\t  */\n\n\t  #pragma unroll\n\t  for(int i = 0; i < iter_index; i += 1){\n\t\t  if(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\tint tile_k_idx_A = *(lane_ptr_index);\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_A * M_GLOBAL) + (laneId % CHUNK_COPY_LINE_LANES_A);\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_A)) =\n\t\t\t\t*lane_ptr;\n\t\t\t//warp_ptr = (uint8_t *)((uint8_t *)warp_ptr + M_GLOBAL * (WARPS_PER_BLOCK / 2) *CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tlane_ptr_index = (int *)((int *)lane_ptr_index +  (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A;\n\t\t  }else{\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_B)) =\n\t\t\t\t*lane_ptr;\n\t\t\tlane_ptr = (int4 *)((uint8_t *)lane_ptr + K_GLOBAL_SPARSE * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B;\n\t\t  }\n\t  }\n\n      __syncthreads();\n\n\t#pragma unroll\n      for (int k_step = 0; k_step < CHUNK_K_SPARSE; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::col_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n\t#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i += 1) {\n\t\t\tsize_t shmem_idx_a = (warpId % BLOCK_COL_WARPS) * M * WARP_COL_TILES + (i * M);\n\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_a + k_step * K * SHARED_OFFSET_A;\n\n\t\t\twmma::load_matrix_sync(a[i], tile_ptr, SHARED_OFFSET_A);\n\t\t#pragma unroll\n\t\t\tfor(int j = 0; j < WARP_ROW_TILES; j += 1){\n\t\t\t\tif(i == 0){\n\t\t\t\t\tsize_t shmem_idx_b = shmem_idx_b_off +\n\t\t\t\t\t\t\t\t\t\t\t(warpId / BLOCK_COL_WARPS) * (WARP_ROW_TILES * N) * SHARED_OFFSET_B +\n\t\t\t\t\t\t\t\t\t\t\t(j * N) * SHARED_OFFSET_B;\n\t\t\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_b + k_step * K;\n\t\t\t\t\twmma::load_matrix_sync(b[j], tile_ptr, SHARED_OFFSET_B);\n\t\t\t\t}\n\t\t\t\twmma::mma_sync(c[j][i], a[i], b[j], c[j][i]);\n\t\t\t}\n\n        }\n      }\n\n      __syncthreads();\n    }\n\n    // This pointer is used to access the C and D matrix tiles this warp computes.\n\tint *shmem_warp_tile_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * N * WARP_ROW_TILES * SHMEM_STRIDE +\n\t(warpId % BLOCK_COL_WARPS) * SHMEM_OFFSET;\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t#pragma unroll\n\t\t\tfor(int t = 0; t < c[i][j].num_elements; t += 1){\n\t\t\t\tc[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n\t\t\t}\n\t\t\tint *tile_ptr = shmem_warp_tile_ptr + i * N * SHMEM_STRIDE + j * M;\n\t\t\twmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, C_LAYOUT);\n\t\t}\n\t}\n\n    __syncthreads();\n\n\tint *shmem_warp_stream_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N * SHMEM_STRIDE\n\t\t\t\t\t\t\t\t\t+ (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tconst size_t gmem_idx =\n\t\t(block_tile_i * N + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N) * GLOBAL_MEM_STRIDE +\n\t\tblock_tile_j * M + (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tuint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n\tint *shmem_lane_stream_ptr =\n\t\tshmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * SHMEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\t\n\tuint8_t *dst_gmem_lane_stream_ptr =\n\t\tdst_gmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * GLOBAL_MEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\n\tfor(int i = 0; i < WARP_ROW_TILES * N; i += SHARED_TO_GLOBAL_LINES_PER_WARP){\n\t\t*(dst_gmem_lane_stream_ptr + i * GLOBAL_MEM_STRIDE) = (uint8_t)(*(shmem_lane_stream_ptr + i * SHMEM_STRIDE));\n\t}\n\n\t__syncthreads();\n}\n", "launch_config": {"dimBlock": [256, 1], "dimGrid": [256, 1]}}, "31": {"code": "\n__global__ void compute_gemm_imma(float *input0, float *input1, float *input2, float* input3, float* input4, float*input5, float* input6,\n                                  float *output0) {\n    // keys need to be replaced 4096, 1024, K_GLOBA_VALUEL, 0.75, 64\n\n\t//extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B = reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_index = reinterpret_cast<int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n\tconst int WARP_SIZE = 32;\n    const int M = 16;\n    const int K = 16;\n    const int N = 16;\n    const int WMMA_M = 16;\n    const int WMMA_K = 16;\n    const int WMMA_N = 16;\n    const int M_GLOBAL = 4096;\n    const int K_GLOBAL = 1024;\n    const int N_GLOBAL = 1024;\n    const float SPARSITY = 0.75;\n    const int K_GLOBAL_SPARSE = (int(K_GLOBAL * (1-SPARSITY)));\n    const int M_TILES = (M_GLOBAL / M);\n    const int N_TILES = (N_GLOBAL / N);\n    const int K_TILES = (K_GLOBAL / K);\n    const int BLOCK_ROW_WARPS = 2;\n    const int BLOCK_COL_WARPS = 4;\n    const int WARP_ROW_TILES = 4;\n    const int WARP_COL_TILES = 2;\n    const int SKEW_UINT8 = 16;\n    const int BANK_VAL = 32;\n    const int NUM_BANK = (K_GLOBAL / BANK_VAL);\n    /////////// BLOCK_ROW_TILES <= N_TILES ////////////\n    // const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    // ////////// BLOCK_COL_TILES <= M_TILES ////////////\n    // const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    auto C_LAYOUT = wmma::mem_col_major;\n    const int CHUNK_K = 64;\n    const int CHUNK_K_SPARSE = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K_SPARSE = (int((CHUNK_K * K) * (1 - SPARSITY)));\n    const int WARP_COPY_BYTES = (WARP_SIZE * sizeof(int4));\n    const int CHUNK_LINE_BYTES_A (BLOCK_COL_TILES * M * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_A = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_A);\n    const int CHUNK_COPY_LINE_LANES_A = (CHUNK_LINE_BYTES_A / sizeof(int4));\n    const int SHARED_OFFSET_A = (BLOCK_COL_TILES * M + SKEW_UINT8);\n\n    const int CHUNK_LINE_BYTES_B = (BLOCK_SIZE_K_SPARSE * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_B = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_B);\n    const int CHUNK_COPY_LINE_LANES_B = (CHUNK_LINE_BYTES_B / sizeof(int4));\n    const int SHARED_OFFSET_B = (BLOCK_SIZE_K_SPARSE + SKEW_UINT8);\n    const int SHARED_TO_GLOBAL_BYTES_PER_LINE = ((WARP_COL_TILES * M) * sizeof(int));\n    const int SHARED_TO_GLOBAL_BYTES_PER_WARP = (WARP_SIZE * sizeof(int));\n    const int SHARED_TO_GLOBAL_LINES_PER_WARP = (SHARED_TO_GLOBAL_BYTES_PER_WARP / SHARED_TO_GLOBAL_BYTES_PER_LINE);\n    const int SHARED_TO_GLOBAL_LANES_PER_LINE = (WARP_SIZE / SHARED_TO_GLOBAL_LINES_PER_WARP);\n    const int SHARED_TO_GLOBAL_ITERS = ((WARP_ROW_TILES * N) / SHARED_TO_GLOBAL_LINES_PER_WARP);\n\n    const int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const int WARP_STRIDE = (WARP_COL_TILES * M);\n    const int WARPS_PER_BLOCK = (BLOCK_ROW_WARPS * BLOCK_COL_WARPS);\n    const int THREADS_PER_BLOCK = (WARP_SIZE * WARPS_PER_BLOCK);\n    const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    const int GLOBAL_MEM_STRIDE = M_GLOBAL;\n    const int SHMEM_STRIDE = (M * BLOCK_COL_TILES);\n    const int SHMEM_OFFSET = (M * WARP_COL_TILES);\n    const int BLOCK_SIZE_M = (M * BLOCK_COL_TILES);\n    const int BLOCK_SIZE_N = (N * BLOCK_ROW_TILES);\n\n   extern __shared__ uint8_t shmem[];\n\n\t// Warp and lane identification.\n\tconst unsigned int warpId = threadIdx.x / WARP_SIZE;\n\tconst unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n\n\t// Offset in shared memory from which the B matrix is stored.\n\t// const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\tconst size_t shmem_idx_b_off = BLOCK_SIZE_K_SPARSE * SHARED_OFFSET_A;\n\n\n\t// Each CTA slides along the 128 x 128 tiles from the top left corner of the\n\t// matrix to the right and down, and selects the next tile to compute. Once\n\t// there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n\tconst unsigned int block_tile_i =\n\t\t((block_pos * BLOCK_COL_TILES) / M_TILES) * (BLOCK_ROW_TILES);\n\tconst unsigned int block_tile_j = (block_pos * BLOCK_COL_TILES) % M_TILES;\n\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n\twmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_ROW_TILES]\n\t\t\t\t\t\t\t\t\t\t\t\t\t [WARP_COL_TILES];\n\n    // Load the C matrix tiles into fragments from shared memory.\n\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t\twmma::fill_fragment(c[i][j], 0);\n\t\t}\n\t}\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    //for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n    for(int tile_k_idx_sparse = 0, tile_k_idx = 0; tile_k_idx_sparse < K_GLOBAL_SPARSE; tile_k_idx_sparse += BLOCK_SIZE_K_SPARSE, tile_k_idx += BLOCK_SIZE_K){\n\n\t\tsize_t shmem_idx = \n\t\twarpId < (WARPS_PER_BLOCK / 2)\n\t\t\t? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A\n\t\t\t: (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B + shmem_idx_b_off;\n\n\t\tint4 *lane_ptr = NULL;\n\t\tint *lane_ptr_index = NULL;\n\t\tconst uint8_t *warp_ptr = NULL;\n\n\n\t\tif(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\t//warp_ptr = &A[block_tile_j * M] +\n\t\t\t//\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * M_GLOBAL;\n\t\t\twarp_ptr = &A[block_tile_j * M];\n\t\t\t\n\t\t\tconst int *warp_ptr_index = &B_index[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t\t\t\t\t\t((warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A);\n\n\t\t\tlane_ptr_index = (int *)(warp_ptr_index + tile_k_idx_sparse + (laneId / CHUNK_COPY_LINE_LANES_A));\n\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_A) * SHARED_OFFSET_A;\n\t\t}else{\n\t\t\twarp_ptr = &B[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * K_GLOBAL_SPARSE;\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_sparse +\n\t\t\t\t\t\t\t\t(laneId / CHUNK_COPY_LINE_LANES_B) * K_GLOBAL_SPARSE) +\n\t\t\t\t\t\t\t\t(laneId % CHUNK_COPY_LINE_LANES_B);\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_B) * SHARED_OFFSET_B;\n\t\t}\n\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      // shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n\t  int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n\t  \t? BLOCK_SIZE_K_SPARSE / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A)\n\t\t: BLOCK_SIZE_N / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\n\t  /*\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\t  */\n\n\t  /*\n      int tile_k_idx_A;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n          tile_k_idx_A = *(lane_ptr_index);\n      }\n\t  */\n\n\t  #pragma unroll\n\t  for(int i = 0; i < iter_index; i += 1){\n\t\t  if(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\tint tile_k_idx_A = *(lane_ptr_index);\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_A * M_GLOBAL) + (laneId % CHUNK_COPY_LINE_LANES_A);\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_A)) =\n\t\t\t\t*lane_ptr;\n\t\t\t//warp_ptr = (uint8_t *)((uint8_t *)warp_ptr + M_GLOBAL * (WARPS_PER_BLOCK / 2) *CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tlane_ptr_index = (int *)((int *)lane_ptr_index +  (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A;\n\t\t  }else{\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_B)) =\n\t\t\t\t*lane_ptr;\n\t\t\tlane_ptr = (int4 *)((uint8_t *)lane_ptr + K_GLOBAL_SPARSE * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B;\n\t\t  }\n\t  }\n\n      __syncthreads();\n\n\t#pragma unroll\n      for (int k_step = 0; k_step < CHUNK_K_SPARSE; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::col_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n\t#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i += 1) {\n\t\t\tsize_t shmem_idx_a = (warpId % BLOCK_COL_WARPS) * M * WARP_COL_TILES + (i * M);\n\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_a + k_step * K * SHARED_OFFSET_A;\n\n\t\t\twmma::load_matrix_sync(a[i], tile_ptr, SHARED_OFFSET_A);\n\t\t#pragma unroll\n\t\t\tfor(int j = 0; j < WARP_ROW_TILES; j += 1){\n\t\t\t\tif(i == 0){\n\t\t\t\t\tsize_t shmem_idx_b = shmem_idx_b_off +\n\t\t\t\t\t\t\t\t\t\t\t(warpId / BLOCK_COL_WARPS) * (WARP_ROW_TILES * N) * SHARED_OFFSET_B +\n\t\t\t\t\t\t\t\t\t\t\t(j * N) * SHARED_OFFSET_B;\n\t\t\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_b + k_step * K;\n\t\t\t\t\twmma::load_matrix_sync(b[j], tile_ptr, SHARED_OFFSET_B);\n\t\t\t\t}\n\t\t\t\twmma::mma_sync(c[j][i], a[i], b[j], c[j][i]);\n\t\t\t}\n\n        }\n      }\n\n      __syncthreads();\n    }\n\n    // This pointer is used to access the C and D matrix tiles this warp computes.\n\tint *shmem_warp_tile_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * N * WARP_ROW_TILES * SHMEM_STRIDE +\n\t(warpId % BLOCK_COL_WARPS) * SHMEM_OFFSET;\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t#pragma unroll\n\t\t\tfor(int t = 0; t < c[i][j].num_elements; t += 1){\n\t\t\t\tc[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n\t\t\t}\n\t\t\tint *tile_ptr = shmem_warp_tile_ptr + i * N * SHMEM_STRIDE + j * M;\n\t\t\twmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, C_LAYOUT);\n\t\t}\n\t}\n\n    __syncthreads();\n\n\tint *shmem_warp_stream_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N * SHMEM_STRIDE\n\t\t\t\t\t\t\t\t\t+ (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tconst size_t gmem_idx =\n\t\t(block_tile_i * N + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N) * GLOBAL_MEM_STRIDE +\n\t\tblock_tile_j * M + (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tuint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n\tint *shmem_lane_stream_ptr =\n\t\tshmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * SHMEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\t\n\tuint8_t *dst_gmem_lane_stream_ptr =\n\t\tdst_gmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * GLOBAL_MEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\n\tfor(int i = 0; i < WARP_ROW_TILES * N; i += SHARED_TO_GLOBAL_LINES_PER_WARP){\n\t\t*(dst_gmem_lane_stream_ptr + i * GLOBAL_MEM_STRIDE) = (uint8_t)(*(shmem_lane_stream_ptr + i * SHMEM_STRIDE));\n\t}\n\n\t__syncthreads();\n}\n", "launch_config": {"dimBlock": [256, 1], "dimGrid": [256, 1]}}, "32": {"code": "\n__global__ void compute_gemm_imma(float *input0, float *input1, float *input2, float* input3, float* input4, float*input5, float* input6,\n                                  float *output0) {\n    // keys need to be replaced 4096, 1024, K_GLOBA_VALUEL, 0.75, 64\n\n\t//extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B = reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_index = reinterpret_cast<int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n\tconst int WARP_SIZE = 32;\n    const int M = 16;\n    const int K = 16;\n    const int N = 16;\n    const int WMMA_M = 16;\n    const int WMMA_K = 16;\n    const int WMMA_N = 16;\n    const int M_GLOBAL = 4096;\n    const int K_GLOBAL = 1024;\n    const int N_GLOBAL = 1024;\n    const float SPARSITY = 0.75;\n    const int K_GLOBAL_SPARSE = (int(K_GLOBAL * (1-SPARSITY)));\n    const int M_TILES = (M_GLOBAL / M);\n    const int N_TILES = (N_GLOBAL / N);\n    const int K_TILES = (K_GLOBAL / K);\n    const int BLOCK_ROW_WARPS = 2;\n    const int BLOCK_COL_WARPS = 4;\n    const int WARP_ROW_TILES = 4;\n    const int WARP_COL_TILES = 2;\n    const int SKEW_UINT8 = 16;\n    const int BANK_VAL = 32;\n    const int NUM_BANK = (K_GLOBAL / BANK_VAL);\n    /////////// BLOCK_ROW_TILES <= N_TILES ////////////\n    // const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    // ////////// BLOCK_COL_TILES <= M_TILES ////////////\n    // const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    auto C_LAYOUT = wmma::mem_col_major;\n    const int CHUNK_K = 64;\n    const int CHUNK_K_SPARSE = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K_SPARSE = (int((CHUNK_K * K) * (1 - SPARSITY)));\n    const int WARP_COPY_BYTES = (WARP_SIZE * sizeof(int4));\n    const int CHUNK_LINE_BYTES_A (BLOCK_COL_TILES * M * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_A = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_A);\n    const int CHUNK_COPY_LINE_LANES_A = (CHUNK_LINE_BYTES_A / sizeof(int4));\n    const int SHARED_OFFSET_A = (BLOCK_COL_TILES * M + SKEW_UINT8);\n\n    const int CHUNK_LINE_BYTES_B = (BLOCK_SIZE_K_SPARSE * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_B = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_B);\n    const int CHUNK_COPY_LINE_LANES_B = (CHUNK_LINE_BYTES_B / sizeof(int4));\n    const int SHARED_OFFSET_B = (BLOCK_SIZE_K_SPARSE + SKEW_UINT8);\n    const int SHARED_TO_GLOBAL_BYTES_PER_LINE = ((WARP_COL_TILES * M) * sizeof(int));\n    const int SHARED_TO_GLOBAL_BYTES_PER_WARP = (WARP_SIZE * sizeof(int));\n    const int SHARED_TO_GLOBAL_LINES_PER_WARP = (SHARED_TO_GLOBAL_BYTES_PER_WARP / SHARED_TO_GLOBAL_BYTES_PER_LINE);\n    const int SHARED_TO_GLOBAL_LANES_PER_LINE = (WARP_SIZE / SHARED_TO_GLOBAL_LINES_PER_WARP);\n    const int SHARED_TO_GLOBAL_ITERS = ((WARP_ROW_TILES * N) / SHARED_TO_GLOBAL_LINES_PER_WARP);\n\n    const int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const int WARP_STRIDE = (WARP_COL_TILES * M);\n    const int WARPS_PER_BLOCK = (BLOCK_ROW_WARPS * BLOCK_COL_WARPS);\n    const int THREADS_PER_BLOCK = (WARP_SIZE * WARPS_PER_BLOCK);\n    const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    const int GLOBAL_MEM_STRIDE = M_GLOBAL;\n    const int SHMEM_STRIDE = (M * BLOCK_COL_TILES);\n    const int SHMEM_OFFSET = (M * WARP_COL_TILES);\n    const int BLOCK_SIZE_M = (M * BLOCK_COL_TILES);\n    const int BLOCK_SIZE_N = (N * BLOCK_ROW_TILES);\n\n   extern __shared__ uint8_t shmem[];\n\n\t// Warp and lane identification.\n\tconst unsigned int warpId = threadIdx.x / WARP_SIZE;\n\tconst unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n\n\t// Offset in shared memory from which the B matrix is stored.\n\t// const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\tconst size_t shmem_idx_b_off = BLOCK_SIZE_K_SPARSE * SHARED_OFFSET_A;\n\n\n\t// Each CTA slides along the 128 x 128 tiles from the top left corner of the\n\t// matrix to the right and down, and selects the next tile to compute. Once\n\t// there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n\tconst unsigned int block_tile_i =\n\t\t((block_pos * BLOCK_COL_TILES) / M_TILES) * (BLOCK_ROW_TILES);\n\tconst unsigned int block_tile_j = (block_pos * BLOCK_COL_TILES) % M_TILES;\n\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n\twmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_ROW_TILES]\n\t\t\t\t\t\t\t\t\t\t\t\t\t [WARP_COL_TILES];\n\n    // Load the C matrix tiles into fragments from shared memory.\n\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t\twmma::fill_fragment(c[i][j], 0);\n\t\t}\n\t}\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    //for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n    for(int tile_k_idx_sparse = 0, tile_k_idx = 0; tile_k_idx_sparse < K_GLOBAL_SPARSE; tile_k_idx_sparse += BLOCK_SIZE_K_SPARSE, tile_k_idx += BLOCK_SIZE_K){\n\n\t\tsize_t shmem_idx = \n\t\twarpId < (WARPS_PER_BLOCK / 2)\n\t\t\t? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A\n\t\t\t: (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B + shmem_idx_b_off;\n\n\t\tint4 *lane_ptr = NULL;\n\t\tint *lane_ptr_index = NULL;\n\t\tconst uint8_t *warp_ptr = NULL;\n\n\n\t\tif(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\t//warp_ptr = &A[block_tile_j * M] +\n\t\t\t//\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * M_GLOBAL;\n\t\t\twarp_ptr = &A[block_tile_j * M];\n\t\t\t\n\t\t\tconst int *warp_ptr_index = &B_index[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t\t\t\t\t\t((warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A);\n\n\t\t\tlane_ptr_index = (int *)(warp_ptr_index + tile_k_idx_sparse + (laneId / CHUNK_COPY_LINE_LANES_A));\n\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_A) * SHARED_OFFSET_A;\n\t\t}else{\n\t\t\twarp_ptr = &B[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * K_GLOBAL_SPARSE;\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_sparse +\n\t\t\t\t\t\t\t\t(laneId / CHUNK_COPY_LINE_LANES_B) * K_GLOBAL_SPARSE) +\n\t\t\t\t\t\t\t\t(laneId % CHUNK_COPY_LINE_LANES_B);\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_B) * SHARED_OFFSET_B;\n\t\t}\n\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      // shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n\t  int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n\t  \t? BLOCK_SIZE_K_SPARSE / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A)\n\t\t: BLOCK_SIZE_N / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\n\t  /*\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\t  */\n\n\t  /*\n      int tile_k_idx_A;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n          tile_k_idx_A = *(lane_ptr_index);\n      }\n\t  */\n\n\t  #pragma unroll\n\t  for(int i = 0; i < iter_index; i += 1){\n\t\t  if(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\tint tile_k_idx_A = *(lane_ptr_index);\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_A * M_GLOBAL) + (laneId % CHUNK_COPY_LINE_LANES_A);\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_A)) =\n\t\t\t\t*lane_ptr;\n\t\t\t//warp_ptr = (uint8_t *)((uint8_t *)warp_ptr + M_GLOBAL * (WARPS_PER_BLOCK / 2) *CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tlane_ptr_index = (int *)((int *)lane_ptr_index +  (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A;\n\t\t  }else{\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_B)) =\n\t\t\t\t*lane_ptr;\n\t\t\tlane_ptr = (int4 *)((uint8_t *)lane_ptr + K_GLOBAL_SPARSE * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B;\n\t\t  }\n\t  }\n\n      __syncthreads();\n\n\t#pragma unroll\n      for (int k_step = 0; k_step < CHUNK_K_SPARSE; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::col_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n\t#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i += 1) {\n\t\t\tsize_t shmem_idx_a = (warpId % BLOCK_COL_WARPS) * M * WARP_COL_TILES + (i * M);\n\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_a + k_step * K * SHARED_OFFSET_A;\n\n\t\t\twmma::load_matrix_sync(a[i], tile_ptr, SHARED_OFFSET_A);\n\t\t#pragma unroll\n\t\t\tfor(int j = 0; j < WARP_ROW_TILES; j += 1){\n\t\t\t\tif(i == 0){\n\t\t\t\t\tsize_t shmem_idx_b = shmem_idx_b_off +\n\t\t\t\t\t\t\t\t\t\t\t(warpId / BLOCK_COL_WARPS) * (WARP_ROW_TILES * N) * SHARED_OFFSET_B +\n\t\t\t\t\t\t\t\t\t\t\t(j * N) * SHARED_OFFSET_B;\n\t\t\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_b + k_step * K;\n\t\t\t\t\twmma::load_matrix_sync(b[j], tile_ptr, SHARED_OFFSET_B);\n\t\t\t\t}\n\t\t\t\twmma::mma_sync(c[j][i], a[i], b[j], c[j][i]);\n\t\t\t}\n\n        }\n      }\n\n      __syncthreads();\n    }\n\n    // This pointer is used to access the C and D matrix tiles this warp computes.\n\tint *shmem_warp_tile_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * N * WARP_ROW_TILES * SHMEM_STRIDE +\n\t(warpId % BLOCK_COL_WARPS) * SHMEM_OFFSET;\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t#pragma unroll\n\t\t\tfor(int t = 0; t < c[i][j].num_elements; t += 1){\n\t\t\t\tc[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n\t\t\t}\n\t\t\tint *tile_ptr = shmem_warp_tile_ptr + i * N * SHMEM_STRIDE + j * M;\n\t\t\twmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, C_LAYOUT);\n\t\t}\n\t}\n\n    __syncthreads();\n\n\tint *shmem_warp_stream_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N * SHMEM_STRIDE\n\t\t\t\t\t\t\t\t\t+ (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tconst size_t gmem_idx =\n\t\t(block_tile_i * N + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N) * GLOBAL_MEM_STRIDE +\n\t\tblock_tile_j * M + (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tuint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n\tint *shmem_lane_stream_ptr =\n\t\tshmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * SHMEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\t\n\tuint8_t *dst_gmem_lane_stream_ptr =\n\t\tdst_gmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * GLOBAL_MEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\n\tfor(int i = 0; i < WARP_ROW_TILES * N; i += SHARED_TO_GLOBAL_LINES_PER_WARP){\n\t\t*(dst_gmem_lane_stream_ptr + i * GLOBAL_MEM_STRIDE) = (uint8_t)(*(shmem_lane_stream_ptr + i * SHMEM_STRIDE));\n\t}\n\n\t__syncthreads();\n}\n", "launch_config": {"dimBlock": [256, 1], "dimGrid": [256, 1]}}, "33": {"code": "\n__global__ void compute_gemm_imma(float *input0, float *input1, float *input2, float* input3, float* input4, float*input5, float* input6,\n                                  float *output0) {\n    // keys need to be replaced 4096, 1024, K_GLOBA_VALUEL, 0.75, 64\n\n\t//extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B = reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_index = reinterpret_cast<int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n\tconst int WARP_SIZE = 32;\n    const int M = 16;\n    const int K = 16;\n    const int N = 16;\n    const int WMMA_M = 16;\n    const int WMMA_K = 16;\n    const int WMMA_N = 16;\n    const int M_GLOBAL = 4096;\n    const int K_GLOBAL = 1024;\n    const int N_GLOBAL = 1024;\n    const float SPARSITY = 0.75;\n    const int K_GLOBAL_SPARSE = (int(K_GLOBAL * (1-SPARSITY)));\n    const int M_TILES = (M_GLOBAL / M);\n    const int N_TILES = (N_GLOBAL / N);\n    const int K_TILES = (K_GLOBAL / K);\n    const int BLOCK_ROW_WARPS = 2;\n    const int BLOCK_COL_WARPS = 4;\n    const int WARP_ROW_TILES = 4;\n    const int WARP_COL_TILES = 2;\n    const int SKEW_UINT8 = 16;\n    const int BANK_VAL = 32;\n    const int NUM_BANK = (K_GLOBAL / BANK_VAL);\n    /////////// BLOCK_ROW_TILES <= N_TILES ////////////\n    // const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    // ////////// BLOCK_COL_TILES <= M_TILES ////////////\n    // const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    auto C_LAYOUT = wmma::mem_col_major;\n    const int CHUNK_K = 64;\n    const int CHUNK_K_SPARSE = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K_SPARSE = (int((CHUNK_K * K) * (1 - SPARSITY)));\n    const int WARP_COPY_BYTES = (WARP_SIZE * sizeof(int4));\n    const int CHUNK_LINE_BYTES_A (BLOCK_COL_TILES * M * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_A = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_A);\n    const int CHUNK_COPY_LINE_LANES_A = (CHUNK_LINE_BYTES_A / sizeof(int4));\n    const int SHARED_OFFSET_A = (BLOCK_COL_TILES * M + SKEW_UINT8);\n\n    const int CHUNK_LINE_BYTES_B = (BLOCK_SIZE_K_SPARSE * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_B = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_B);\n    const int CHUNK_COPY_LINE_LANES_B = (CHUNK_LINE_BYTES_B / sizeof(int4));\n    const int SHARED_OFFSET_B = (BLOCK_SIZE_K_SPARSE + SKEW_UINT8);\n    const int SHARED_TO_GLOBAL_BYTES_PER_LINE = ((WARP_COL_TILES * M) * sizeof(int));\n    const int SHARED_TO_GLOBAL_BYTES_PER_WARP = (WARP_SIZE * sizeof(int));\n    const int SHARED_TO_GLOBAL_LINES_PER_WARP = (SHARED_TO_GLOBAL_BYTES_PER_WARP / SHARED_TO_GLOBAL_BYTES_PER_LINE);\n    const int SHARED_TO_GLOBAL_LANES_PER_LINE = (WARP_SIZE / SHARED_TO_GLOBAL_LINES_PER_WARP);\n    const int SHARED_TO_GLOBAL_ITERS = ((WARP_ROW_TILES * N) / SHARED_TO_GLOBAL_LINES_PER_WARP);\n\n    const int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const int WARP_STRIDE = (WARP_COL_TILES * M);\n    const int WARPS_PER_BLOCK = (BLOCK_ROW_WARPS * BLOCK_COL_WARPS);\n    const int THREADS_PER_BLOCK = (WARP_SIZE * WARPS_PER_BLOCK);\n    const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    const int GLOBAL_MEM_STRIDE = M_GLOBAL;\n    const int SHMEM_STRIDE = (M * BLOCK_COL_TILES);\n    const int SHMEM_OFFSET = (M * WARP_COL_TILES);\n    const int BLOCK_SIZE_M = (M * BLOCK_COL_TILES);\n    const int BLOCK_SIZE_N = (N * BLOCK_ROW_TILES);\n\n   extern __shared__ uint8_t shmem[];\n\n\t// Warp and lane identification.\n\tconst unsigned int warpId = threadIdx.x / WARP_SIZE;\n\tconst unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n\n\t// Offset in shared memory from which the B matrix is stored.\n\t// const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\tconst size_t shmem_idx_b_off = BLOCK_SIZE_K_SPARSE * SHARED_OFFSET_A;\n\n\n\t// Each CTA slides along the 128 x 128 tiles from the top left corner of the\n\t// matrix to the right and down, and selects the next tile to compute. Once\n\t// there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n\tconst unsigned int block_tile_i =\n\t\t((block_pos * BLOCK_COL_TILES) / M_TILES) * (BLOCK_ROW_TILES);\n\tconst unsigned int block_tile_j = (block_pos * BLOCK_COL_TILES) % M_TILES;\n\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n\twmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_ROW_TILES]\n\t\t\t\t\t\t\t\t\t\t\t\t\t [WARP_COL_TILES];\n\n    // Load the C matrix tiles into fragments from shared memory.\n\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t\twmma::fill_fragment(c[i][j], 0);\n\t\t}\n\t}\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    //for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n    for(int tile_k_idx_sparse = 0, tile_k_idx = 0; tile_k_idx_sparse < K_GLOBAL_SPARSE; tile_k_idx_sparse += BLOCK_SIZE_K_SPARSE, tile_k_idx += BLOCK_SIZE_K){\n\n\t\tsize_t shmem_idx = \n\t\twarpId < (WARPS_PER_BLOCK / 2)\n\t\t\t? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A\n\t\t\t: (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B + shmem_idx_b_off;\n\n\t\tint4 *lane_ptr = NULL;\n\t\tint *lane_ptr_index = NULL;\n\t\tconst uint8_t *warp_ptr = NULL;\n\n\n\t\tif(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\t//warp_ptr = &A[block_tile_j * M] +\n\t\t\t//\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * M_GLOBAL;\n\t\t\twarp_ptr = &A[block_tile_j * M];\n\t\t\t\n\t\t\tconst int *warp_ptr_index = &B_index[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t\t\t\t\t\t((warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A);\n\n\t\t\tlane_ptr_index = (int *)(warp_ptr_index + tile_k_idx_sparse + (laneId / CHUNK_COPY_LINE_LANES_A));\n\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_A) * SHARED_OFFSET_A;\n\t\t}else{\n\t\t\twarp_ptr = &B[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * K_GLOBAL_SPARSE;\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_sparse +\n\t\t\t\t\t\t\t\t(laneId / CHUNK_COPY_LINE_LANES_B) * K_GLOBAL_SPARSE) +\n\t\t\t\t\t\t\t\t(laneId % CHUNK_COPY_LINE_LANES_B);\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_B) * SHARED_OFFSET_B;\n\t\t}\n\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      // shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n\t  int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n\t  \t? BLOCK_SIZE_K_SPARSE / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A)\n\t\t: BLOCK_SIZE_N / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\n\t  /*\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\t  */\n\n\t  /*\n      int tile_k_idx_A;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n          tile_k_idx_A = *(lane_ptr_index);\n      }\n\t  */\n\n\t  #pragma unroll\n\t  for(int i = 0; i < iter_index; i += 1){\n\t\t  if(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\tint tile_k_idx_A = *(lane_ptr_index);\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_A * M_GLOBAL) + (laneId % CHUNK_COPY_LINE_LANES_A);\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_A)) =\n\t\t\t\t*lane_ptr;\n\t\t\t//warp_ptr = (uint8_t *)((uint8_t *)warp_ptr + M_GLOBAL * (WARPS_PER_BLOCK / 2) *CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tlane_ptr_index = (int *)((int *)lane_ptr_index +  (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A;\n\t\t  }else{\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_B)) =\n\t\t\t\t*lane_ptr;\n\t\t\tlane_ptr = (int4 *)((uint8_t *)lane_ptr + K_GLOBAL_SPARSE * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B;\n\t\t  }\n\t  }\n\n      __syncthreads();\n\n\t#pragma unroll\n      for (int k_step = 0; k_step < CHUNK_K_SPARSE; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::col_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n\t#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i += 1) {\n\t\t\tsize_t shmem_idx_a = (warpId % BLOCK_COL_WARPS) * M * WARP_COL_TILES + (i * M);\n\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_a + k_step * K * SHARED_OFFSET_A;\n\n\t\t\twmma::load_matrix_sync(a[i], tile_ptr, SHARED_OFFSET_A);\n\t\t#pragma unroll\n\t\t\tfor(int j = 0; j < WARP_ROW_TILES; j += 1){\n\t\t\t\tif(i == 0){\n\t\t\t\t\tsize_t shmem_idx_b = shmem_idx_b_off +\n\t\t\t\t\t\t\t\t\t\t\t(warpId / BLOCK_COL_WARPS) * (WARP_ROW_TILES * N) * SHARED_OFFSET_B +\n\t\t\t\t\t\t\t\t\t\t\t(j * N) * SHARED_OFFSET_B;\n\t\t\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_b + k_step * K;\n\t\t\t\t\twmma::load_matrix_sync(b[j], tile_ptr, SHARED_OFFSET_B);\n\t\t\t\t}\n\t\t\t\twmma::mma_sync(c[j][i], a[i], b[j], c[j][i]);\n\t\t\t}\n\n        }\n      }\n\n      __syncthreads();\n    }\n\n    // This pointer is used to access the C and D matrix tiles this warp computes.\n\tint *shmem_warp_tile_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * N * WARP_ROW_TILES * SHMEM_STRIDE +\n\t(warpId % BLOCK_COL_WARPS) * SHMEM_OFFSET;\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t#pragma unroll\n\t\t\tfor(int t = 0; t < c[i][j].num_elements; t += 1){\n\t\t\t\tc[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n\t\t\t}\n\t\t\tint *tile_ptr = shmem_warp_tile_ptr + i * N * SHMEM_STRIDE + j * M;\n\t\t\twmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, C_LAYOUT);\n\t\t}\n\t}\n\n    __syncthreads();\n\n\tint *shmem_warp_stream_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N * SHMEM_STRIDE\n\t\t\t\t\t\t\t\t\t+ (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tconst size_t gmem_idx =\n\t\t(block_tile_i * N + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N) * GLOBAL_MEM_STRIDE +\n\t\tblock_tile_j * M + (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tuint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n\tint *shmem_lane_stream_ptr =\n\t\tshmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * SHMEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\t\n\tuint8_t *dst_gmem_lane_stream_ptr =\n\t\tdst_gmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * GLOBAL_MEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\n\tfor(int i = 0; i < WARP_ROW_TILES * N; i += SHARED_TO_GLOBAL_LINES_PER_WARP){\n\t\t*(dst_gmem_lane_stream_ptr + i * GLOBAL_MEM_STRIDE) = (uint8_t)(*(shmem_lane_stream_ptr + i * SHMEM_STRIDE));\n\t}\n\n\t__syncthreads();\n}\n", "launch_config": {"dimBlock": [256, 1], "dimGrid": [256, 1]}}, "34": {"code": "\n__global__ void compute_gemm_imma(float *input0, float *input1, float *input2, float* input3, float* input4, float*input5, float* input6,\n                                  float *output0) {\n    // keys need to be replaced 4096, 1024, K_GLOBA_VALUEL, 0.75, 64\n\n\t//extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B = reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_index = reinterpret_cast<int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n\tconst int WARP_SIZE = 32;\n    const int M = 16;\n    const int K = 16;\n    const int N = 16;\n    const int WMMA_M = 16;\n    const int WMMA_K = 16;\n    const int WMMA_N = 16;\n    const int M_GLOBAL = 4096;\n    const int K_GLOBAL = 1024;\n    const int N_GLOBAL = 1024;\n    const float SPARSITY = 0.75;\n    const int K_GLOBAL_SPARSE = (int(K_GLOBAL * (1-SPARSITY)));\n    const int M_TILES = (M_GLOBAL / M);\n    const int N_TILES = (N_GLOBAL / N);\n    const int K_TILES = (K_GLOBAL / K);\n    const int BLOCK_ROW_WARPS = 2;\n    const int BLOCK_COL_WARPS = 4;\n    const int WARP_ROW_TILES = 4;\n    const int WARP_COL_TILES = 2;\n    const int SKEW_UINT8 = 16;\n    const int BANK_VAL = 32;\n    const int NUM_BANK = (K_GLOBAL / BANK_VAL);\n    /////////// BLOCK_ROW_TILES <= N_TILES ////////////\n    // const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    // ////////// BLOCK_COL_TILES <= M_TILES ////////////\n    // const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    auto C_LAYOUT = wmma::mem_col_major;\n    const int CHUNK_K = 64;\n    const int CHUNK_K_SPARSE = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K_SPARSE = (int((CHUNK_K * K) * (1 - SPARSITY)));\n    const int WARP_COPY_BYTES = (WARP_SIZE * sizeof(int4));\n    const int CHUNK_LINE_BYTES_A (BLOCK_COL_TILES * M * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_A = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_A);\n    const int CHUNK_COPY_LINE_LANES_A = (CHUNK_LINE_BYTES_A / sizeof(int4));\n    const int SHARED_OFFSET_A = (BLOCK_COL_TILES * M + SKEW_UINT8);\n\n    const int CHUNK_LINE_BYTES_B = (BLOCK_SIZE_K_SPARSE * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_B = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_B);\n    const int CHUNK_COPY_LINE_LANES_B = (CHUNK_LINE_BYTES_B / sizeof(int4));\n    const int SHARED_OFFSET_B = (BLOCK_SIZE_K_SPARSE + SKEW_UINT8);\n    const int SHARED_TO_GLOBAL_BYTES_PER_LINE = ((WARP_COL_TILES * M) * sizeof(int));\n    const int SHARED_TO_GLOBAL_BYTES_PER_WARP = (WARP_SIZE * sizeof(int));\n    const int SHARED_TO_GLOBAL_LINES_PER_WARP = (SHARED_TO_GLOBAL_BYTES_PER_WARP / SHARED_TO_GLOBAL_BYTES_PER_LINE);\n    const int SHARED_TO_GLOBAL_LANES_PER_LINE = (WARP_SIZE / SHARED_TO_GLOBAL_LINES_PER_WARP);\n    const int SHARED_TO_GLOBAL_ITERS = ((WARP_ROW_TILES * N) / SHARED_TO_GLOBAL_LINES_PER_WARP);\n\n    const int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const int WARP_STRIDE = (WARP_COL_TILES * M);\n    const int WARPS_PER_BLOCK = (BLOCK_ROW_WARPS * BLOCK_COL_WARPS);\n    const int THREADS_PER_BLOCK = (WARP_SIZE * WARPS_PER_BLOCK);\n    const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    const int GLOBAL_MEM_STRIDE = M_GLOBAL;\n    const int SHMEM_STRIDE = (M * BLOCK_COL_TILES);\n    const int SHMEM_OFFSET = (M * WARP_COL_TILES);\n    const int BLOCK_SIZE_M = (M * BLOCK_COL_TILES);\n    const int BLOCK_SIZE_N = (N * BLOCK_ROW_TILES);\n\n   extern __shared__ uint8_t shmem[];\n\n\t// Warp and lane identification.\n\tconst unsigned int warpId = threadIdx.x / WARP_SIZE;\n\tconst unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n\n\t// Offset in shared memory from which the B matrix is stored.\n\t// const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\tconst size_t shmem_idx_b_off = BLOCK_SIZE_K_SPARSE * SHARED_OFFSET_A;\n\n\n\t// Each CTA slides along the 128 x 128 tiles from the top left corner of the\n\t// matrix to the right and down, and selects the next tile to compute. Once\n\t// there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n\tconst unsigned int block_tile_i =\n\t\t((block_pos * BLOCK_COL_TILES) / M_TILES) * (BLOCK_ROW_TILES);\n\tconst unsigned int block_tile_j = (block_pos * BLOCK_COL_TILES) % M_TILES;\n\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n\twmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_ROW_TILES]\n\t\t\t\t\t\t\t\t\t\t\t\t\t [WARP_COL_TILES];\n\n    // Load the C matrix tiles into fragments from shared memory.\n\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t\twmma::fill_fragment(c[i][j], 0);\n\t\t}\n\t}\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    //for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n    for(int tile_k_idx_sparse = 0, tile_k_idx = 0; tile_k_idx_sparse < K_GLOBAL_SPARSE; tile_k_idx_sparse += BLOCK_SIZE_K_SPARSE, tile_k_idx += BLOCK_SIZE_K){\n\n\t\tsize_t shmem_idx = \n\t\twarpId < (WARPS_PER_BLOCK / 2)\n\t\t\t? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A\n\t\t\t: (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B + shmem_idx_b_off;\n\n\t\tint4 *lane_ptr = NULL;\n\t\tint *lane_ptr_index = NULL;\n\t\tconst uint8_t *warp_ptr = NULL;\n\n\n\t\tif(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\t//warp_ptr = &A[block_tile_j * M] +\n\t\t\t//\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * M_GLOBAL;\n\t\t\twarp_ptr = &A[block_tile_j * M];\n\t\t\t\n\t\t\tconst int *warp_ptr_index = &B_index[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t\t\t\t\t\t((warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A);\n\n\t\t\tlane_ptr_index = (int *)(warp_ptr_index + tile_k_idx_sparse + (laneId / CHUNK_COPY_LINE_LANES_A));\n\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_A) * SHARED_OFFSET_A;\n\t\t}else{\n\t\t\twarp_ptr = &B[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * K_GLOBAL_SPARSE;\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_sparse +\n\t\t\t\t\t\t\t\t(laneId / CHUNK_COPY_LINE_LANES_B) * K_GLOBAL_SPARSE) +\n\t\t\t\t\t\t\t\t(laneId % CHUNK_COPY_LINE_LANES_B);\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_B) * SHARED_OFFSET_B;\n\t\t}\n\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      // shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n\t  int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n\t  \t? BLOCK_SIZE_K_SPARSE / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A)\n\t\t: BLOCK_SIZE_N / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\n\t  /*\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\t  */\n\n\t  /*\n      int tile_k_idx_A;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n          tile_k_idx_A = *(lane_ptr_index);\n      }\n\t  */\n\n\t  #pragma unroll\n\t  for(int i = 0; i < iter_index; i += 1){\n\t\t  if(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\tint tile_k_idx_A = *(lane_ptr_index);\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_A * M_GLOBAL) + (laneId % CHUNK_COPY_LINE_LANES_A);\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_A)) =\n\t\t\t\t*lane_ptr;\n\t\t\t//warp_ptr = (uint8_t *)((uint8_t *)warp_ptr + M_GLOBAL * (WARPS_PER_BLOCK / 2) *CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tlane_ptr_index = (int *)((int *)lane_ptr_index +  (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A;\n\t\t  }else{\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_B)) =\n\t\t\t\t*lane_ptr;\n\t\t\tlane_ptr = (int4 *)((uint8_t *)lane_ptr + K_GLOBAL_SPARSE * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B;\n\t\t  }\n\t  }\n\n      __syncthreads();\n\n\t#pragma unroll\n      for (int k_step = 0; k_step < CHUNK_K_SPARSE; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::col_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n\t#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i += 1) {\n\t\t\tsize_t shmem_idx_a = (warpId % BLOCK_COL_WARPS) * M * WARP_COL_TILES + (i * M);\n\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_a + k_step * K * SHARED_OFFSET_A;\n\n\t\t\twmma::load_matrix_sync(a[i], tile_ptr, SHARED_OFFSET_A);\n\t\t#pragma unroll\n\t\t\tfor(int j = 0; j < WARP_ROW_TILES; j += 1){\n\t\t\t\tif(i == 0){\n\t\t\t\t\tsize_t shmem_idx_b = shmem_idx_b_off +\n\t\t\t\t\t\t\t\t\t\t\t(warpId / BLOCK_COL_WARPS) * (WARP_ROW_TILES * N) * SHARED_OFFSET_B +\n\t\t\t\t\t\t\t\t\t\t\t(j * N) * SHARED_OFFSET_B;\n\t\t\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_b + k_step * K;\n\t\t\t\t\twmma::load_matrix_sync(b[j], tile_ptr, SHARED_OFFSET_B);\n\t\t\t\t}\n\t\t\t\twmma::mma_sync(c[j][i], a[i], b[j], c[j][i]);\n\t\t\t}\n\n        }\n      }\n\n      __syncthreads();\n    }\n\n    // This pointer is used to access the C and D matrix tiles this warp computes.\n\tint *shmem_warp_tile_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * N * WARP_ROW_TILES * SHMEM_STRIDE +\n\t(warpId % BLOCK_COL_WARPS) * SHMEM_OFFSET;\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t#pragma unroll\n\t\t\tfor(int t = 0; t < c[i][j].num_elements; t += 1){\n\t\t\t\tc[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n\t\t\t}\n\t\t\tint *tile_ptr = shmem_warp_tile_ptr + i * N * SHMEM_STRIDE + j * M;\n\t\t\twmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, C_LAYOUT);\n\t\t}\n\t}\n\n    __syncthreads();\n\n\tint *shmem_warp_stream_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N * SHMEM_STRIDE\n\t\t\t\t\t\t\t\t\t+ (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tconst size_t gmem_idx =\n\t\t(block_tile_i * N + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N) * GLOBAL_MEM_STRIDE +\n\t\tblock_tile_j * M + (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tuint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n\tint *shmem_lane_stream_ptr =\n\t\tshmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * SHMEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\t\n\tuint8_t *dst_gmem_lane_stream_ptr =\n\t\tdst_gmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * GLOBAL_MEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\n\tfor(int i = 0; i < WARP_ROW_TILES * N; i += SHARED_TO_GLOBAL_LINES_PER_WARP){\n\t\t*(dst_gmem_lane_stream_ptr + i * GLOBAL_MEM_STRIDE) = (uint8_t)(*(shmem_lane_stream_ptr + i * SHMEM_STRIDE));\n\t}\n\n\t__syncthreads();\n}\n", "launch_config": {"dimBlock": [256, 1], "dimGrid": [256, 1]}}, "35": {"code": "\n__global__ void compute_gemm_imma(float *input0, float *input1, float *input2, float* input3, float* input4, float*input5, float* input6,\n                                  float *output0) {\n    // keys need to be replaced 4096, 4096, K_GLOBA_VALUEL, 0.75, 64\n\n\t//extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B = reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_index = reinterpret_cast<int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n\tconst int WARP_SIZE = 32;\n    const int M = 16;\n    const int K = 16;\n    const int N = 16;\n    const int WMMA_M = 16;\n    const int WMMA_K = 16;\n    const int WMMA_N = 16;\n    const int M_GLOBAL = 4096;\n    const int K_GLOBAL = 1024;\n    const int N_GLOBAL = 4096;\n    const float SPARSITY = 0.75;\n    const int K_GLOBAL_SPARSE = (int(K_GLOBAL * (1-SPARSITY)));\n    const int M_TILES = (M_GLOBAL / M);\n    const int N_TILES = (N_GLOBAL / N);\n    const int K_TILES = (K_GLOBAL / K);\n    const int BLOCK_ROW_WARPS = 2;\n    const int BLOCK_COL_WARPS = 4;\n    const int WARP_ROW_TILES = 4;\n    const int WARP_COL_TILES = 2;\n    const int SKEW_UINT8 = 16;\n    const int BANK_VAL = 32;\n    const int NUM_BANK = (K_GLOBAL / BANK_VAL);\n    /////////// BLOCK_ROW_TILES <= N_TILES ////////////\n    // const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    // ////////// BLOCK_COL_TILES <= M_TILES ////////////\n    // const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    auto C_LAYOUT = wmma::mem_col_major;\n    const int CHUNK_K = 64;\n    const int CHUNK_K_SPARSE = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K_SPARSE = (int((CHUNK_K * K) * (1 - SPARSITY)));\n    const int WARP_COPY_BYTES = (WARP_SIZE * sizeof(int4));\n    const int CHUNK_LINE_BYTES_A (BLOCK_COL_TILES * M * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_A = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_A);\n    const int CHUNK_COPY_LINE_LANES_A = (CHUNK_LINE_BYTES_A / sizeof(int4));\n    const int SHARED_OFFSET_A = (BLOCK_COL_TILES * M + SKEW_UINT8);\n\n    const int CHUNK_LINE_BYTES_B = (BLOCK_SIZE_K_SPARSE * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_B = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_B);\n    const int CHUNK_COPY_LINE_LANES_B = (CHUNK_LINE_BYTES_B / sizeof(int4));\n    const int SHARED_OFFSET_B = (BLOCK_SIZE_K_SPARSE + SKEW_UINT8);\n    const int SHARED_TO_GLOBAL_BYTES_PER_LINE = ((WARP_COL_TILES * M) * sizeof(int));\n    const int SHARED_TO_GLOBAL_BYTES_PER_WARP = (WARP_SIZE * sizeof(int));\n    const int SHARED_TO_GLOBAL_LINES_PER_WARP = (SHARED_TO_GLOBAL_BYTES_PER_WARP / SHARED_TO_GLOBAL_BYTES_PER_LINE);\n    const int SHARED_TO_GLOBAL_LANES_PER_LINE = (WARP_SIZE / SHARED_TO_GLOBAL_LINES_PER_WARP);\n    const int SHARED_TO_GLOBAL_ITERS = ((WARP_ROW_TILES * N) / SHARED_TO_GLOBAL_LINES_PER_WARP);\n\n    const int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const int WARP_STRIDE = (WARP_COL_TILES * M);\n    const int WARPS_PER_BLOCK = (BLOCK_ROW_WARPS * BLOCK_COL_WARPS);\n    const int THREADS_PER_BLOCK = (WARP_SIZE * WARPS_PER_BLOCK);\n    const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    const int GLOBAL_MEM_STRIDE = M_GLOBAL;\n    const int SHMEM_STRIDE = (M * BLOCK_COL_TILES);\n    const int SHMEM_OFFSET = (M * WARP_COL_TILES);\n    const int BLOCK_SIZE_M = (M * BLOCK_COL_TILES);\n    const int BLOCK_SIZE_N = (N * BLOCK_ROW_TILES);\n\n   extern __shared__ uint8_t shmem[];\n\n\t// Warp and lane identification.\n\tconst unsigned int warpId = threadIdx.x / WARP_SIZE;\n\tconst unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n\n\t// Offset in shared memory from which the B matrix is stored.\n\t// const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\tconst size_t shmem_idx_b_off = BLOCK_SIZE_K_SPARSE * SHARED_OFFSET_A;\n\n\n\t// Each CTA slides along the 128 x 128 tiles from the top left corner of the\n\t// matrix to the right and down, and selects the next tile to compute. Once\n\t// there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n\tconst unsigned int block_tile_i =\n\t\t((block_pos * BLOCK_COL_TILES) / M_TILES) * (BLOCK_ROW_TILES);\n\tconst unsigned int block_tile_j = (block_pos * BLOCK_COL_TILES) % M_TILES;\n\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n\twmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_ROW_TILES]\n\t\t\t\t\t\t\t\t\t\t\t\t\t [WARP_COL_TILES];\n\n    // Load the C matrix tiles into fragments from shared memory.\n\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t\twmma::fill_fragment(c[i][j], 0);\n\t\t}\n\t}\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    //for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n    for(int tile_k_idx_sparse = 0, tile_k_idx = 0; tile_k_idx_sparse < K_GLOBAL_SPARSE; tile_k_idx_sparse += BLOCK_SIZE_K_SPARSE, tile_k_idx += BLOCK_SIZE_K){\n\n\t\tsize_t shmem_idx = \n\t\twarpId < (WARPS_PER_BLOCK / 2)\n\t\t\t? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A\n\t\t\t: (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B + shmem_idx_b_off;\n\n\t\tint4 *lane_ptr = NULL;\n\t\tint *lane_ptr_index = NULL;\n\t\tconst uint8_t *warp_ptr = NULL;\n\n\n\t\tif(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\t//warp_ptr = &A[block_tile_j * M] +\n\t\t\t//\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * M_GLOBAL;\n\t\t\twarp_ptr = &A[block_tile_j * M];\n\t\t\t\n\t\t\tconst int *warp_ptr_index = &B_index[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t\t\t\t\t\t((warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A);\n\n\t\t\tlane_ptr_index = (int *)(warp_ptr_index + tile_k_idx_sparse + (laneId / CHUNK_COPY_LINE_LANES_A));\n\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_A) * SHARED_OFFSET_A;\n\t\t}else{\n\t\t\twarp_ptr = &B[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * K_GLOBAL_SPARSE;\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_sparse +\n\t\t\t\t\t\t\t\t(laneId / CHUNK_COPY_LINE_LANES_B) * K_GLOBAL_SPARSE) +\n\t\t\t\t\t\t\t\t(laneId % CHUNK_COPY_LINE_LANES_B);\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_B) * SHARED_OFFSET_B;\n\t\t}\n\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      // shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n\t  int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n\t  \t? BLOCK_SIZE_K_SPARSE / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A)\n\t\t: BLOCK_SIZE_N / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\n\t  /*\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\t  */\n\n\t  /*\n      int tile_k_idx_A;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n          tile_k_idx_A = *(lane_ptr_index);\n      }\n\t  */\n\n\t  #pragma unroll\n\t  for(int i = 0; i < iter_index; i += 1){\n\t\t  if(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\tint tile_k_idx_A = *(lane_ptr_index);\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_A * M_GLOBAL) + (laneId % CHUNK_COPY_LINE_LANES_A);\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_A)) =\n\t\t\t\t*lane_ptr;\n\t\t\t//warp_ptr = (uint8_t *)((uint8_t *)warp_ptr + M_GLOBAL * (WARPS_PER_BLOCK / 2) *CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tlane_ptr_index = (int *)((int *)lane_ptr_index +  (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A;\n\t\t  }else{\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_B)) =\n\t\t\t\t*lane_ptr;\n\t\t\tlane_ptr = (int4 *)((uint8_t *)lane_ptr + K_GLOBAL_SPARSE * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B;\n\t\t  }\n\t  }\n\n      __syncthreads();\n\n\t#pragma unroll\n      for (int k_step = 0; k_step < CHUNK_K_SPARSE; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::col_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n\t#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i += 1) {\n\t\t\tsize_t shmem_idx_a = (warpId % BLOCK_COL_WARPS) * M * WARP_COL_TILES + (i * M);\n\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_a + k_step * K * SHARED_OFFSET_A;\n\n\t\t\twmma::load_matrix_sync(a[i], tile_ptr, SHARED_OFFSET_A);\n\t\t#pragma unroll\n\t\t\tfor(int j = 0; j < WARP_ROW_TILES; j += 1){\n\t\t\t\tif(i == 0){\n\t\t\t\t\tsize_t shmem_idx_b = shmem_idx_b_off +\n\t\t\t\t\t\t\t\t\t\t\t(warpId / BLOCK_COL_WARPS) * (WARP_ROW_TILES * N) * SHARED_OFFSET_B +\n\t\t\t\t\t\t\t\t\t\t\t(j * N) * SHARED_OFFSET_B;\n\t\t\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_b + k_step * K;\n\t\t\t\t\twmma::load_matrix_sync(b[j], tile_ptr, SHARED_OFFSET_B);\n\t\t\t\t}\n\t\t\t\twmma::mma_sync(c[j][i], a[i], b[j], c[j][i]);\n\t\t\t}\n\n        }\n      }\n\n      __syncthreads();\n    }\n\n    // This pointer is used to access the C and D matrix tiles this warp computes.\n\tint *shmem_warp_tile_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * N * WARP_ROW_TILES * SHMEM_STRIDE +\n\t(warpId % BLOCK_COL_WARPS) * SHMEM_OFFSET;\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t#pragma unroll\n\t\t\tfor(int t = 0; t < c[i][j].num_elements; t += 1){\n\t\t\t\tc[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n\t\t\t}\n\t\t\tint *tile_ptr = shmem_warp_tile_ptr + i * N * SHMEM_STRIDE + j * M;\n\t\t\twmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, C_LAYOUT);\n\t\t}\n\t}\n\n    __syncthreads();\n\n\tint *shmem_warp_stream_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N * SHMEM_STRIDE\n\t\t\t\t\t\t\t\t\t+ (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tconst size_t gmem_idx =\n\t\t(block_tile_i * N + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N) * GLOBAL_MEM_STRIDE +\n\t\tblock_tile_j * M + (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tuint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n\tint *shmem_lane_stream_ptr =\n\t\tshmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * SHMEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\t\n\tuint8_t *dst_gmem_lane_stream_ptr =\n\t\tdst_gmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * GLOBAL_MEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\n\tfor(int i = 0; i < WARP_ROW_TILES * N; i += SHARED_TO_GLOBAL_LINES_PER_WARP){\n\t\t*(dst_gmem_lane_stream_ptr + i * GLOBAL_MEM_STRIDE) = (uint8_t)(*(shmem_lane_stream_ptr + i * SHMEM_STRIDE));\n\t}\n\n\t__syncthreads();\n}\n", "launch_config": {"dimBlock": [1024, 1], "dimGrid": [256, 1]}}, "36": {"code": "\n__global__ void compute_gemm_imma(float *input0, float *input1, float *input2, float* input3, float* input4, float*input5, float* input6,\n                                  float *output0) {\n    // keys need to be replaced 4096, 1024, K_GLOBA_VALUEL, 0.75, 64\n\n\t//extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B = reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_index = reinterpret_cast<int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n\tconst int WARP_SIZE = 32;\n    const int M = 16;\n    const int K = 16;\n    const int N = 16;\n    const int WMMA_M = 16;\n    const int WMMA_K = 16;\n    const int WMMA_N = 16;\n    const int M_GLOBAL = 4096;\n    const int K_GLOBAL = 4096;\n    const int N_GLOBAL = 1024;\n    const float SPARSITY = 0.75;\n    const int K_GLOBAL_SPARSE = (int(K_GLOBAL * (1-SPARSITY)));\n    const int M_TILES = (M_GLOBAL / M);\n    const int N_TILES = (N_GLOBAL / N);\n    const int K_TILES = (K_GLOBAL / K);\n    const int BLOCK_ROW_WARPS = 2;\n    const int BLOCK_COL_WARPS = 4;\n    const int WARP_ROW_TILES = 4;\n    const int WARP_COL_TILES = 2;\n    const int SKEW_UINT8 = 16;\n    const int BANK_VAL = 32;\n    const int NUM_BANK = (K_GLOBAL / BANK_VAL);\n    /////////// BLOCK_ROW_TILES <= N_TILES ////////////\n    // const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    // ////////// BLOCK_COL_TILES <= M_TILES ////////////\n    // const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    auto C_LAYOUT = wmma::mem_col_major;\n    const int CHUNK_K = 64;\n    const int CHUNK_K_SPARSE = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K_SPARSE = (int((CHUNK_K * K) * (1 - SPARSITY)));\n    const int WARP_COPY_BYTES = (WARP_SIZE * sizeof(int4));\n    const int CHUNK_LINE_BYTES_A (BLOCK_COL_TILES * M * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_A = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_A);\n    const int CHUNK_COPY_LINE_LANES_A = (CHUNK_LINE_BYTES_A / sizeof(int4));\n    const int SHARED_OFFSET_A = (BLOCK_COL_TILES * M + SKEW_UINT8);\n\n    const int CHUNK_LINE_BYTES_B = (BLOCK_SIZE_K_SPARSE * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_B = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_B);\n    const int CHUNK_COPY_LINE_LANES_B = (CHUNK_LINE_BYTES_B / sizeof(int4));\n    const int SHARED_OFFSET_B = (BLOCK_SIZE_K_SPARSE + SKEW_UINT8);\n    const int SHARED_TO_GLOBAL_BYTES_PER_LINE = ((WARP_COL_TILES * M) * sizeof(int));\n    const int SHARED_TO_GLOBAL_BYTES_PER_WARP = (WARP_SIZE * sizeof(int));\n    const int SHARED_TO_GLOBAL_LINES_PER_WARP = (SHARED_TO_GLOBAL_BYTES_PER_WARP / SHARED_TO_GLOBAL_BYTES_PER_LINE);\n    const int SHARED_TO_GLOBAL_LANES_PER_LINE = (WARP_SIZE / SHARED_TO_GLOBAL_LINES_PER_WARP);\n    const int SHARED_TO_GLOBAL_ITERS = ((WARP_ROW_TILES * N) / SHARED_TO_GLOBAL_LINES_PER_WARP);\n\n    const int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const int WARP_STRIDE = (WARP_COL_TILES * M);\n    const int WARPS_PER_BLOCK = (BLOCK_ROW_WARPS * BLOCK_COL_WARPS);\n    const int THREADS_PER_BLOCK = (WARP_SIZE * WARPS_PER_BLOCK);\n    const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    const int GLOBAL_MEM_STRIDE = M_GLOBAL;\n    const int SHMEM_STRIDE = (M * BLOCK_COL_TILES);\n    const int SHMEM_OFFSET = (M * WARP_COL_TILES);\n    const int BLOCK_SIZE_M = (M * BLOCK_COL_TILES);\n    const int BLOCK_SIZE_N = (N * BLOCK_ROW_TILES);\n\n   extern __shared__ uint8_t shmem[];\n\n\t// Warp and lane identification.\n\tconst unsigned int warpId = threadIdx.x / WARP_SIZE;\n\tconst unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n\n\t// Offset in shared memory from which the B matrix is stored.\n\t// const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\tconst size_t shmem_idx_b_off = BLOCK_SIZE_K_SPARSE * SHARED_OFFSET_A;\n\n\n\t// Each CTA slides along the 128 x 128 tiles from the top left corner of the\n\t// matrix to the right and down, and selects the next tile to compute. Once\n\t// there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n\tconst unsigned int block_tile_i =\n\t\t((block_pos * BLOCK_COL_TILES) / M_TILES) * (BLOCK_ROW_TILES);\n\tconst unsigned int block_tile_j = (block_pos * BLOCK_COL_TILES) % M_TILES;\n\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n\twmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_ROW_TILES]\n\t\t\t\t\t\t\t\t\t\t\t\t\t [WARP_COL_TILES];\n\n    // Load the C matrix tiles into fragments from shared memory.\n\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t\twmma::fill_fragment(c[i][j], 0);\n\t\t}\n\t}\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    //for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n    for(int tile_k_idx_sparse = 0, tile_k_idx = 0; tile_k_idx_sparse < K_GLOBAL_SPARSE; tile_k_idx_sparse += BLOCK_SIZE_K_SPARSE, tile_k_idx += BLOCK_SIZE_K){\n\n\t\tsize_t shmem_idx = \n\t\twarpId < (WARPS_PER_BLOCK / 2)\n\t\t\t? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A\n\t\t\t: (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B + shmem_idx_b_off;\n\n\t\tint4 *lane_ptr = NULL;\n\t\tint *lane_ptr_index = NULL;\n\t\tconst uint8_t *warp_ptr = NULL;\n\n\n\t\tif(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\t//warp_ptr = &A[block_tile_j * M] +\n\t\t\t//\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * M_GLOBAL;\n\t\t\twarp_ptr = &A[block_tile_j * M];\n\t\t\t\n\t\t\tconst int *warp_ptr_index = &B_index[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t\t\t\t\t\t((warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A);\n\n\t\t\tlane_ptr_index = (int *)(warp_ptr_index + tile_k_idx_sparse + (laneId / CHUNK_COPY_LINE_LANES_A));\n\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_A) * SHARED_OFFSET_A;\n\t\t}else{\n\t\t\twarp_ptr = &B[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * K_GLOBAL_SPARSE;\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_sparse +\n\t\t\t\t\t\t\t\t(laneId / CHUNK_COPY_LINE_LANES_B) * K_GLOBAL_SPARSE) +\n\t\t\t\t\t\t\t\t(laneId % CHUNK_COPY_LINE_LANES_B);\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_B) * SHARED_OFFSET_B;\n\t\t}\n\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      // shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n\t  int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n\t  \t? BLOCK_SIZE_K_SPARSE / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A)\n\t\t: BLOCK_SIZE_N / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\n\t  /*\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\t  */\n\n\t  /*\n      int tile_k_idx_A;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n          tile_k_idx_A = *(lane_ptr_index);\n      }\n\t  */\n\n\t  #pragma unroll\n\t  for(int i = 0; i < iter_index; i += 1){\n\t\t  if(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\tint tile_k_idx_A = *(lane_ptr_index);\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_A * M_GLOBAL) + (laneId % CHUNK_COPY_LINE_LANES_A);\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_A)) =\n\t\t\t\t*lane_ptr;\n\t\t\t//warp_ptr = (uint8_t *)((uint8_t *)warp_ptr + M_GLOBAL * (WARPS_PER_BLOCK / 2) *CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tlane_ptr_index = (int *)((int *)lane_ptr_index +  (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A;\n\t\t  }else{\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_B)) =\n\t\t\t\t*lane_ptr;\n\t\t\tlane_ptr = (int4 *)((uint8_t *)lane_ptr + K_GLOBAL_SPARSE * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B;\n\t\t  }\n\t  }\n\n      __syncthreads();\n\n\t#pragma unroll\n      for (int k_step = 0; k_step < CHUNK_K_SPARSE; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::col_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n\t#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i += 1) {\n\t\t\tsize_t shmem_idx_a = (warpId % BLOCK_COL_WARPS) * M * WARP_COL_TILES + (i * M);\n\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_a + k_step * K * SHARED_OFFSET_A;\n\n\t\t\twmma::load_matrix_sync(a[i], tile_ptr, SHARED_OFFSET_A);\n\t\t#pragma unroll\n\t\t\tfor(int j = 0; j < WARP_ROW_TILES; j += 1){\n\t\t\t\tif(i == 0){\n\t\t\t\t\tsize_t shmem_idx_b = shmem_idx_b_off +\n\t\t\t\t\t\t\t\t\t\t\t(warpId / BLOCK_COL_WARPS) * (WARP_ROW_TILES * N) * SHARED_OFFSET_B +\n\t\t\t\t\t\t\t\t\t\t\t(j * N) * SHARED_OFFSET_B;\n\t\t\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_b + k_step * K;\n\t\t\t\t\twmma::load_matrix_sync(b[j], tile_ptr, SHARED_OFFSET_B);\n\t\t\t\t}\n\t\t\t\twmma::mma_sync(c[j][i], a[i], b[j], c[j][i]);\n\t\t\t}\n\n        }\n      }\n\n      __syncthreads();\n    }\n\n    // This pointer is used to access the C and D matrix tiles this warp computes.\n\tint *shmem_warp_tile_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * N * WARP_ROW_TILES * SHMEM_STRIDE +\n\t(warpId % BLOCK_COL_WARPS) * SHMEM_OFFSET;\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t#pragma unroll\n\t\t\tfor(int t = 0; t < c[i][j].num_elements; t += 1){\n\t\t\t\tc[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n\t\t\t}\n\t\t\tint *tile_ptr = shmem_warp_tile_ptr + i * N * SHMEM_STRIDE + j * M;\n\t\t\twmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, C_LAYOUT);\n\t\t}\n\t}\n\n    __syncthreads();\n\n\tint *shmem_warp_stream_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N * SHMEM_STRIDE\n\t\t\t\t\t\t\t\t\t+ (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tconst size_t gmem_idx =\n\t\t(block_tile_i * N + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N) * GLOBAL_MEM_STRIDE +\n\t\tblock_tile_j * M + (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tuint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n\tint *shmem_lane_stream_ptr =\n\t\tshmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * SHMEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\t\n\tuint8_t *dst_gmem_lane_stream_ptr =\n\t\tdst_gmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * GLOBAL_MEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\n\tfor(int i = 0; i < WARP_ROW_TILES * N; i += SHARED_TO_GLOBAL_LINES_PER_WARP){\n\t\t*(dst_gmem_lane_stream_ptr + i * GLOBAL_MEM_STRIDE) = (uint8_t)(*(shmem_lane_stream_ptr + i * SHMEM_STRIDE));\n\t}\n\n\t__syncthreads();\n}\n", "launch_config": {"dimBlock": [256, 1], "dimGrid": [256, 1]}}, "37": {"code": "\n__global__ void compute_gemm_imma(float *input0, float *input1, float *input2, float* input3, float* input4, float*input5, float* input6,\n                                  float *output0) {\n    // keys need to be replaced 4096, 1024, K_GLOBA_VALUEL, 0.75, 64\n\n\t//extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B = reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_index = reinterpret_cast<int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n\tconst int WARP_SIZE = 32;\n    const int M = 16;\n    const int K = 16;\n    const int N = 16;\n    const int WMMA_M = 16;\n    const int WMMA_K = 16;\n    const int WMMA_N = 16;\n    const int M_GLOBAL = 4096;\n    const int K_GLOBAL = 1024;\n    const int N_GLOBAL = 1024;\n    const float SPARSITY = 0.75;\n    const int K_GLOBAL_SPARSE = (int(K_GLOBAL * (1-SPARSITY)));\n    const int M_TILES = (M_GLOBAL / M);\n    const int N_TILES = (N_GLOBAL / N);\n    const int K_TILES = (K_GLOBAL / K);\n    const int BLOCK_ROW_WARPS = 2;\n    const int BLOCK_COL_WARPS = 4;\n    const int WARP_ROW_TILES = 4;\n    const int WARP_COL_TILES = 2;\n    const int SKEW_UINT8 = 16;\n    const int BANK_VAL = 32;\n    const int NUM_BANK = (K_GLOBAL / BANK_VAL);\n    /////////// BLOCK_ROW_TILES <= N_TILES ////////////\n    // const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    // ////////// BLOCK_COL_TILES <= M_TILES ////////////\n    // const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    auto C_LAYOUT = wmma::mem_col_major;\n    const int CHUNK_K = 64;\n    const int CHUNK_K_SPARSE = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K_SPARSE = (int((CHUNK_K * K) * (1 - SPARSITY)));\n    const int WARP_COPY_BYTES = (WARP_SIZE * sizeof(int4));\n    const int CHUNK_LINE_BYTES_A (BLOCK_COL_TILES * M * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_A = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_A);\n    const int CHUNK_COPY_LINE_LANES_A = (CHUNK_LINE_BYTES_A / sizeof(int4));\n    const int SHARED_OFFSET_A = (BLOCK_COL_TILES * M + SKEW_UINT8);\n\n    const int CHUNK_LINE_BYTES_B = (BLOCK_SIZE_K_SPARSE * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_B = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_B);\n    const int CHUNK_COPY_LINE_LANES_B = (CHUNK_LINE_BYTES_B / sizeof(int4));\n    const int SHARED_OFFSET_B = (BLOCK_SIZE_K_SPARSE + SKEW_UINT8);\n    const int SHARED_TO_GLOBAL_BYTES_PER_LINE = ((WARP_COL_TILES * M) * sizeof(int));\n    const int SHARED_TO_GLOBAL_BYTES_PER_WARP = (WARP_SIZE * sizeof(int));\n    const int SHARED_TO_GLOBAL_LINES_PER_WARP = (SHARED_TO_GLOBAL_BYTES_PER_WARP / SHARED_TO_GLOBAL_BYTES_PER_LINE);\n    const int SHARED_TO_GLOBAL_LANES_PER_LINE = (WARP_SIZE / SHARED_TO_GLOBAL_LINES_PER_WARP);\n    const int SHARED_TO_GLOBAL_ITERS = ((WARP_ROW_TILES * N) / SHARED_TO_GLOBAL_LINES_PER_WARP);\n\n    const int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const int WARP_STRIDE = (WARP_COL_TILES * M);\n    const int WARPS_PER_BLOCK = (BLOCK_ROW_WARPS * BLOCK_COL_WARPS);\n    const int THREADS_PER_BLOCK = (WARP_SIZE * WARPS_PER_BLOCK);\n    const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    const int GLOBAL_MEM_STRIDE = M_GLOBAL;\n    const int SHMEM_STRIDE = (M * BLOCK_COL_TILES);\n    const int SHMEM_OFFSET = (M * WARP_COL_TILES);\n    const int BLOCK_SIZE_M = (M * BLOCK_COL_TILES);\n    const int BLOCK_SIZE_N = (N * BLOCK_ROW_TILES);\n\n   extern __shared__ uint8_t shmem[];\n\n\t// Warp and lane identification.\n\tconst unsigned int warpId = threadIdx.x / WARP_SIZE;\n\tconst unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n\n\t// Offset in shared memory from which the B matrix is stored.\n\t// const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\tconst size_t shmem_idx_b_off = BLOCK_SIZE_K_SPARSE * SHARED_OFFSET_A;\n\n\n\t// Each CTA slides along the 128 x 128 tiles from the top left corner of the\n\t// matrix to the right and down, and selects the next tile to compute. Once\n\t// there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n\tconst unsigned int block_tile_i =\n\t\t((block_pos * BLOCK_COL_TILES) / M_TILES) * (BLOCK_ROW_TILES);\n\tconst unsigned int block_tile_j = (block_pos * BLOCK_COL_TILES) % M_TILES;\n\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n\twmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_ROW_TILES]\n\t\t\t\t\t\t\t\t\t\t\t\t\t [WARP_COL_TILES];\n\n    // Load the C matrix tiles into fragments from shared memory.\n\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t\twmma::fill_fragment(c[i][j], 0);\n\t\t}\n\t}\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    //for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n    for(int tile_k_idx_sparse = 0, tile_k_idx = 0; tile_k_idx_sparse < K_GLOBAL_SPARSE; tile_k_idx_sparse += BLOCK_SIZE_K_SPARSE, tile_k_idx += BLOCK_SIZE_K){\n\n\t\tsize_t shmem_idx = \n\t\twarpId < (WARPS_PER_BLOCK / 2)\n\t\t\t? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A\n\t\t\t: (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B + shmem_idx_b_off;\n\n\t\tint4 *lane_ptr = NULL;\n\t\tint *lane_ptr_index = NULL;\n\t\tconst uint8_t *warp_ptr = NULL;\n\n\n\t\tif(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\t//warp_ptr = &A[block_tile_j * M] +\n\t\t\t//\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * M_GLOBAL;\n\t\t\twarp_ptr = &A[block_tile_j * M];\n\t\t\t\n\t\t\tconst int *warp_ptr_index = &B_index[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t\t\t\t\t\t((warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A);\n\n\t\t\tlane_ptr_index = (int *)(warp_ptr_index + tile_k_idx_sparse + (laneId / CHUNK_COPY_LINE_LANES_A));\n\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_A) * SHARED_OFFSET_A;\n\t\t}else{\n\t\t\twarp_ptr = &B[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * K_GLOBAL_SPARSE;\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_sparse +\n\t\t\t\t\t\t\t\t(laneId / CHUNK_COPY_LINE_LANES_B) * K_GLOBAL_SPARSE) +\n\t\t\t\t\t\t\t\t(laneId % CHUNK_COPY_LINE_LANES_B);\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_B) * SHARED_OFFSET_B;\n\t\t}\n\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      // shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n\t  int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n\t  \t? BLOCK_SIZE_K_SPARSE / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A)\n\t\t: BLOCK_SIZE_N / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\n\t  /*\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\t  */\n\n\t  /*\n      int tile_k_idx_A;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n          tile_k_idx_A = *(lane_ptr_index);\n      }\n\t  */\n\n\t  #pragma unroll\n\t  for(int i = 0; i < iter_index; i += 1){\n\t\t  if(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\tint tile_k_idx_A = *(lane_ptr_index);\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_A * M_GLOBAL) + (laneId % CHUNK_COPY_LINE_LANES_A);\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_A)) =\n\t\t\t\t*lane_ptr;\n\t\t\t//warp_ptr = (uint8_t *)((uint8_t *)warp_ptr + M_GLOBAL * (WARPS_PER_BLOCK / 2) *CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tlane_ptr_index = (int *)((int *)lane_ptr_index +  (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A;\n\t\t  }else{\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_B)) =\n\t\t\t\t*lane_ptr;\n\t\t\tlane_ptr = (int4 *)((uint8_t *)lane_ptr + K_GLOBAL_SPARSE * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B;\n\t\t  }\n\t  }\n\n      __syncthreads();\n\n\t#pragma unroll\n      for (int k_step = 0; k_step < CHUNK_K_SPARSE; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::col_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n\t#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i += 1) {\n\t\t\tsize_t shmem_idx_a = (warpId % BLOCK_COL_WARPS) * M * WARP_COL_TILES + (i * M);\n\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_a + k_step * K * SHARED_OFFSET_A;\n\n\t\t\twmma::load_matrix_sync(a[i], tile_ptr, SHARED_OFFSET_A);\n\t\t#pragma unroll\n\t\t\tfor(int j = 0; j < WARP_ROW_TILES; j += 1){\n\t\t\t\tif(i == 0){\n\t\t\t\t\tsize_t shmem_idx_b = shmem_idx_b_off +\n\t\t\t\t\t\t\t\t\t\t\t(warpId / BLOCK_COL_WARPS) * (WARP_ROW_TILES * N) * SHARED_OFFSET_B +\n\t\t\t\t\t\t\t\t\t\t\t(j * N) * SHARED_OFFSET_B;\n\t\t\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_b + k_step * K;\n\t\t\t\t\twmma::load_matrix_sync(b[j], tile_ptr, SHARED_OFFSET_B);\n\t\t\t\t}\n\t\t\t\twmma::mma_sync(c[j][i], a[i], b[j], c[j][i]);\n\t\t\t}\n\n        }\n      }\n\n      __syncthreads();\n    }\n\n    // This pointer is used to access the C and D matrix tiles this warp computes.\n\tint *shmem_warp_tile_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * N * WARP_ROW_TILES * SHMEM_STRIDE +\n\t(warpId % BLOCK_COL_WARPS) * SHMEM_OFFSET;\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t#pragma unroll\n\t\t\tfor(int t = 0; t < c[i][j].num_elements; t += 1){\n\t\t\t\tc[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n\t\t\t}\n\t\t\tint *tile_ptr = shmem_warp_tile_ptr + i * N * SHMEM_STRIDE + j * M;\n\t\t\twmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, C_LAYOUT);\n\t\t}\n\t}\n\n    __syncthreads();\n\n\tint *shmem_warp_stream_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N * SHMEM_STRIDE\n\t\t\t\t\t\t\t\t\t+ (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tconst size_t gmem_idx =\n\t\t(block_tile_i * N + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N) * GLOBAL_MEM_STRIDE +\n\t\tblock_tile_j * M + (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tuint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n\tint *shmem_lane_stream_ptr =\n\t\tshmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * SHMEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\t\n\tuint8_t *dst_gmem_lane_stream_ptr =\n\t\tdst_gmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * GLOBAL_MEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\n\tfor(int i = 0; i < WARP_ROW_TILES * N; i += SHARED_TO_GLOBAL_LINES_PER_WARP){\n\t\t*(dst_gmem_lane_stream_ptr + i * GLOBAL_MEM_STRIDE) = (uint8_t)(*(shmem_lane_stream_ptr + i * SHMEM_STRIDE));\n\t}\n\n\t__syncthreads();\n}\n", "launch_config": {"dimBlock": [256, 1], "dimGrid": [256, 1]}}, "38": {"code": "\n__global__ void compute_gemm_imma(float *input0, float *input1, float *input2, float* input3, float* input4, float*input5, float* input6,\n                                  float *output0) {\n    // keys need to be replaced 4096, 1024, K_GLOBA_VALUEL, 0.75, 64\n\n\t//extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B = reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_index = reinterpret_cast<int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n\tconst int WARP_SIZE = 32;\n    const int M = 16;\n    const int K = 16;\n    const int N = 16;\n    const int WMMA_M = 16;\n    const int WMMA_K = 16;\n    const int WMMA_N = 16;\n    const int M_GLOBAL = 4096;\n    const int K_GLOBAL = 1024;\n    const int N_GLOBAL = 1024;\n    const float SPARSITY = 0.75;\n    const int K_GLOBAL_SPARSE = (int(K_GLOBAL * (1-SPARSITY)));\n    const int M_TILES = (M_GLOBAL / M);\n    const int N_TILES = (N_GLOBAL / N);\n    const int K_TILES = (K_GLOBAL / K);\n    const int BLOCK_ROW_WARPS = 2;\n    const int BLOCK_COL_WARPS = 4;\n    const int WARP_ROW_TILES = 4;\n    const int WARP_COL_TILES = 2;\n    const int SKEW_UINT8 = 16;\n    const int BANK_VAL = 32;\n    const int NUM_BANK = (K_GLOBAL / BANK_VAL);\n    /////////// BLOCK_ROW_TILES <= N_TILES ////////////\n    // const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    // ////////// BLOCK_COL_TILES <= M_TILES ////////////\n    // const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    auto C_LAYOUT = wmma::mem_col_major;\n    const int CHUNK_K = 64;\n    const int CHUNK_K_SPARSE = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K_SPARSE = (int((CHUNK_K * K) * (1 - SPARSITY)));\n    const int WARP_COPY_BYTES = (WARP_SIZE * sizeof(int4));\n    const int CHUNK_LINE_BYTES_A (BLOCK_COL_TILES * M * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_A = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_A);\n    const int CHUNK_COPY_LINE_LANES_A = (CHUNK_LINE_BYTES_A / sizeof(int4));\n    const int SHARED_OFFSET_A = (BLOCK_COL_TILES * M + SKEW_UINT8);\n\n    const int CHUNK_LINE_BYTES_B = (BLOCK_SIZE_K_SPARSE * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_B = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_B);\n    const int CHUNK_COPY_LINE_LANES_B = (CHUNK_LINE_BYTES_B / sizeof(int4));\n    const int SHARED_OFFSET_B = (BLOCK_SIZE_K_SPARSE + SKEW_UINT8);\n    const int SHARED_TO_GLOBAL_BYTES_PER_LINE = ((WARP_COL_TILES * M) * sizeof(int));\n    const int SHARED_TO_GLOBAL_BYTES_PER_WARP = (WARP_SIZE * sizeof(int));\n    const int SHARED_TO_GLOBAL_LINES_PER_WARP = (SHARED_TO_GLOBAL_BYTES_PER_WARP / SHARED_TO_GLOBAL_BYTES_PER_LINE);\n    const int SHARED_TO_GLOBAL_LANES_PER_LINE = (WARP_SIZE / SHARED_TO_GLOBAL_LINES_PER_WARP);\n    const int SHARED_TO_GLOBAL_ITERS = ((WARP_ROW_TILES * N) / SHARED_TO_GLOBAL_LINES_PER_WARP);\n\n    const int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const int WARP_STRIDE = (WARP_COL_TILES * M);\n    const int WARPS_PER_BLOCK = (BLOCK_ROW_WARPS * BLOCK_COL_WARPS);\n    const int THREADS_PER_BLOCK = (WARP_SIZE * WARPS_PER_BLOCK);\n    const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    const int GLOBAL_MEM_STRIDE = M_GLOBAL;\n    const int SHMEM_STRIDE = (M * BLOCK_COL_TILES);\n    const int SHMEM_OFFSET = (M * WARP_COL_TILES);\n    const int BLOCK_SIZE_M = (M * BLOCK_COL_TILES);\n    const int BLOCK_SIZE_N = (N * BLOCK_ROW_TILES);\n\n   extern __shared__ uint8_t shmem[];\n\n\t// Warp and lane identification.\n\tconst unsigned int warpId = threadIdx.x / WARP_SIZE;\n\tconst unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n\n\t// Offset in shared memory from which the B matrix is stored.\n\t// const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\tconst size_t shmem_idx_b_off = BLOCK_SIZE_K_SPARSE * SHARED_OFFSET_A;\n\n\n\t// Each CTA slides along the 128 x 128 tiles from the top left corner of the\n\t// matrix to the right and down, and selects the next tile to compute. Once\n\t// there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n\tconst unsigned int block_tile_i =\n\t\t((block_pos * BLOCK_COL_TILES) / M_TILES) * (BLOCK_ROW_TILES);\n\tconst unsigned int block_tile_j = (block_pos * BLOCK_COL_TILES) % M_TILES;\n\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n\twmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_ROW_TILES]\n\t\t\t\t\t\t\t\t\t\t\t\t\t [WARP_COL_TILES];\n\n    // Load the C matrix tiles into fragments from shared memory.\n\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t\twmma::fill_fragment(c[i][j], 0);\n\t\t}\n\t}\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    //for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n    for(int tile_k_idx_sparse = 0, tile_k_idx = 0; tile_k_idx_sparse < K_GLOBAL_SPARSE; tile_k_idx_sparse += BLOCK_SIZE_K_SPARSE, tile_k_idx += BLOCK_SIZE_K){\n\n\t\tsize_t shmem_idx = \n\t\twarpId < (WARPS_PER_BLOCK / 2)\n\t\t\t? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A\n\t\t\t: (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B + shmem_idx_b_off;\n\n\t\tint4 *lane_ptr = NULL;\n\t\tint *lane_ptr_index = NULL;\n\t\tconst uint8_t *warp_ptr = NULL;\n\n\n\t\tif(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\t//warp_ptr = &A[block_tile_j * M] +\n\t\t\t//\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * M_GLOBAL;\n\t\t\twarp_ptr = &A[block_tile_j * M];\n\t\t\t\n\t\t\tconst int *warp_ptr_index = &B_index[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t\t\t\t\t\t((warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A);\n\n\t\t\tlane_ptr_index = (int *)(warp_ptr_index + tile_k_idx_sparse + (laneId / CHUNK_COPY_LINE_LANES_A));\n\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_A) * SHARED_OFFSET_A;\n\t\t}else{\n\t\t\twarp_ptr = &B[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * K_GLOBAL_SPARSE;\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_sparse +\n\t\t\t\t\t\t\t\t(laneId / CHUNK_COPY_LINE_LANES_B) * K_GLOBAL_SPARSE) +\n\t\t\t\t\t\t\t\t(laneId % CHUNK_COPY_LINE_LANES_B);\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_B) * SHARED_OFFSET_B;\n\t\t}\n\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      // shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n\t  int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n\t  \t? BLOCK_SIZE_K_SPARSE / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A)\n\t\t: BLOCK_SIZE_N / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\n\t  /*\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\t  */\n\n\t  /*\n      int tile_k_idx_A;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n          tile_k_idx_A = *(lane_ptr_index);\n      }\n\t  */\n\n\t  #pragma unroll\n\t  for(int i = 0; i < iter_index; i += 1){\n\t\t  if(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\tint tile_k_idx_A = *(lane_ptr_index);\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_A * M_GLOBAL) + (laneId % CHUNK_COPY_LINE_LANES_A);\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_A)) =\n\t\t\t\t*lane_ptr;\n\t\t\t//warp_ptr = (uint8_t *)((uint8_t *)warp_ptr + M_GLOBAL * (WARPS_PER_BLOCK / 2) *CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tlane_ptr_index = (int *)((int *)lane_ptr_index +  (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A;\n\t\t  }else{\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_B)) =\n\t\t\t\t*lane_ptr;\n\t\t\tlane_ptr = (int4 *)((uint8_t *)lane_ptr + K_GLOBAL_SPARSE * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B;\n\t\t  }\n\t  }\n\n      __syncthreads();\n\n\t#pragma unroll\n      for (int k_step = 0; k_step < CHUNK_K_SPARSE; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::col_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n\t#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i += 1) {\n\t\t\tsize_t shmem_idx_a = (warpId % BLOCK_COL_WARPS) * M * WARP_COL_TILES + (i * M);\n\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_a + k_step * K * SHARED_OFFSET_A;\n\n\t\t\twmma::load_matrix_sync(a[i], tile_ptr, SHARED_OFFSET_A);\n\t\t#pragma unroll\n\t\t\tfor(int j = 0; j < WARP_ROW_TILES; j += 1){\n\t\t\t\tif(i == 0){\n\t\t\t\t\tsize_t shmem_idx_b = shmem_idx_b_off +\n\t\t\t\t\t\t\t\t\t\t\t(warpId / BLOCK_COL_WARPS) * (WARP_ROW_TILES * N) * SHARED_OFFSET_B +\n\t\t\t\t\t\t\t\t\t\t\t(j * N) * SHARED_OFFSET_B;\n\t\t\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_b + k_step * K;\n\t\t\t\t\twmma::load_matrix_sync(b[j], tile_ptr, SHARED_OFFSET_B);\n\t\t\t\t}\n\t\t\t\twmma::mma_sync(c[j][i], a[i], b[j], c[j][i]);\n\t\t\t}\n\n        }\n      }\n\n      __syncthreads();\n    }\n\n    // This pointer is used to access the C and D matrix tiles this warp computes.\n\tint *shmem_warp_tile_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * N * WARP_ROW_TILES * SHMEM_STRIDE +\n\t(warpId % BLOCK_COL_WARPS) * SHMEM_OFFSET;\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t#pragma unroll\n\t\t\tfor(int t = 0; t < c[i][j].num_elements; t += 1){\n\t\t\t\tc[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n\t\t\t}\n\t\t\tint *tile_ptr = shmem_warp_tile_ptr + i * N * SHMEM_STRIDE + j * M;\n\t\t\twmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, C_LAYOUT);\n\t\t}\n\t}\n\n    __syncthreads();\n\n\tint *shmem_warp_stream_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N * SHMEM_STRIDE\n\t\t\t\t\t\t\t\t\t+ (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tconst size_t gmem_idx =\n\t\t(block_tile_i * N + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N) * GLOBAL_MEM_STRIDE +\n\t\tblock_tile_j * M + (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tuint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n\tint *shmem_lane_stream_ptr =\n\t\tshmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * SHMEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\t\n\tuint8_t *dst_gmem_lane_stream_ptr =\n\t\tdst_gmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * GLOBAL_MEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\n\tfor(int i = 0; i < WARP_ROW_TILES * N; i += SHARED_TO_GLOBAL_LINES_PER_WARP){\n\t\t*(dst_gmem_lane_stream_ptr + i * GLOBAL_MEM_STRIDE) = (uint8_t)(*(shmem_lane_stream_ptr + i * SHMEM_STRIDE));\n\t}\n\n\t__syncthreads();\n}\n", "launch_config": {"dimBlock": [256, 1], "dimGrid": [256, 1]}}, "39": {"code": "\n__global__ void compute_gemm_imma(float *input0, float *input1, float *input2, float* input3, float* input4, float*input5, float* input6,\n                                  float *output0) {\n    // keys need to be replaced 4096, 1024, K_GLOBA_VALUEL, 0.75, 64\n\n\t//extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B = reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_index = reinterpret_cast<int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n\tconst int WARP_SIZE = 32;\n    const int M = 16;\n    const int K = 16;\n    const int N = 16;\n    const int WMMA_M = 16;\n    const int WMMA_K = 16;\n    const int WMMA_N = 16;\n    const int M_GLOBAL = 4096;\n    const int K_GLOBAL = 1024;\n    const int N_GLOBAL = 1024;\n    const float SPARSITY = 0.75;\n    const int K_GLOBAL_SPARSE = (int(K_GLOBAL * (1-SPARSITY)));\n    const int M_TILES = (M_GLOBAL / M);\n    const int N_TILES = (N_GLOBAL / N);\n    const int K_TILES = (K_GLOBAL / K);\n    const int BLOCK_ROW_WARPS = 2;\n    const int BLOCK_COL_WARPS = 4;\n    const int WARP_ROW_TILES = 4;\n    const int WARP_COL_TILES = 2;\n    const int SKEW_UINT8 = 16;\n    const int BANK_VAL = 32;\n    const int NUM_BANK = (K_GLOBAL / BANK_VAL);\n    /////////// BLOCK_ROW_TILES <= N_TILES ////////////\n    // const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    // ////////// BLOCK_COL_TILES <= M_TILES ////////////\n    // const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    auto C_LAYOUT = wmma::mem_col_major;\n    const int CHUNK_K = 64;\n    const int CHUNK_K_SPARSE = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K_SPARSE = (int((CHUNK_K * K) * (1 - SPARSITY)));\n    const int WARP_COPY_BYTES = (WARP_SIZE * sizeof(int4));\n    const int CHUNK_LINE_BYTES_A (BLOCK_COL_TILES * M * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_A = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_A);\n    const int CHUNK_COPY_LINE_LANES_A = (CHUNK_LINE_BYTES_A / sizeof(int4));\n    const int SHARED_OFFSET_A = (BLOCK_COL_TILES * M + SKEW_UINT8);\n\n    const int CHUNK_LINE_BYTES_B = (BLOCK_SIZE_K_SPARSE * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_B = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_B);\n    const int CHUNK_COPY_LINE_LANES_B = (CHUNK_LINE_BYTES_B / sizeof(int4));\n    const int SHARED_OFFSET_B = (BLOCK_SIZE_K_SPARSE + SKEW_UINT8);\n    const int SHARED_TO_GLOBAL_BYTES_PER_LINE = ((WARP_COL_TILES * M) * sizeof(int));\n    const int SHARED_TO_GLOBAL_BYTES_PER_WARP = (WARP_SIZE * sizeof(int));\n    const int SHARED_TO_GLOBAL_LINES_PER_WARP = (SHARED_TO_GLOBAL_BYTES_PER_WARP / SHARED_TO_GLOBAL_BYTES_PER_LINE);\n    const int SHARED_TO_GLOBAL_LANES_PER_LINE = (WARP_SIZE / SHARED_TO_GLOBAL_LINES_PER_WARP);\n    const int SHARED_TO_GLOBAL_ITERS = ((WARP_ROW_TILES * N) / SHARED_TO_GLOBAL_LINES_PER_WARP);\n\n    const int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const int WARP_STRIDE = (WARP_COL_TILES * M);\n    const int WARPS_PER_BLOCK = (BLOCK_ROW_WARPS * BLOCK_COL_WARPS);\n    const int THREADS_PER_BLOCK = (WARP_SIZE * WARPS_PER_BLOCK);\n    const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    const int GLOBAL_MEM_STRIDE = M_GLOBAL;\n    const int SHMEM_STRIDE = (M * BLOCK_COL_TILES);\n    const int SHMEM_OFFSET = (M * WARP_COL_TILES);\n    const int BLOCK_SIZE_M = (M * BLOCK_COL_TILES);\n    const int BLOCK_SIZE_N = (N * BLOCK_ROW_TILES);\n\n   extern __shared__ uint8_t shmem[];\n\n\t// Warp and lane identification.\n\tconst unsigned int warpId = threadIdx.x / WARP_SIZE;\n\tconst unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n\n\t// Offset in shared memory from which the B matrix is stored.\n\t// const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\tconst size_t shmem_idx_b_off = BLOCK_SIZE_K_SPARSE * SHARED_OFFSET_A;\n\n\n\t// Each CTA slides along the 128 x 128 tiles from the top left corner of the\n\t// matrix to the right and down, and selects the next tile to compute. Once\n\t// there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n\tconst unsigned int block_tile_i =\n\t\t((block_pos * BLOCK_COL_TILES) / M_TILES) * (BLOCK_ROW_TILES);\n\tconst unsigned int block_tile_j = (block_pos * BLOCK_COL_TILES) % M_TILES;\n\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n\twmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_ROW_TILES]\n\t\t\t\t\t\t\t\t\t\t\t\t\t [WARP_COL_TILES];\n\n    // Load the C matrix tiles into fragments from shared memory.\n\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t\twmma::fill_fragment(c[i][j], 0);\n\t\t}\n\t}\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    //for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n    for(int tile_k_idx_sparse = 0, tile_k_idx = 0; tile_k_idx_sparse < K_GLOBAL_SPARSE; tile_k_idx_sparse += BLOCK_SIZE_K_SPARSE, tile_k_idx += BLOCK_SIZE_K){\n\n\t\tsize_t shmem_idx = \n\t\twarpId < (WARPS_PER_BLOCK / 2)\n\t\t\t? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A\n\t\t\t: (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B + shmem_idx_b_off;\n\n\t\tint4 *lane_ptr = NULL;\n\t\tint *lane_ptr_index = NULL;\n\t\tconst uint8_t *warp_ptr = NULL;\n\n\n\t\tif(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\t//warp_ptr = &A[block_tile_j * M] +\n\t\t\t//\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * M_GLOBAL;\n\t\t\twarp_ptr = &A[block_tile_j * M];\n\t\t\t\n\t\t\tconst int *warp_ptr_index = &B_index[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t\t\t\t\t\t((warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A);\n\n\t\t\tlane_ptr_index = (int *)(warp_ptr_index + tile_k_idx_sparse + (laneId / CHUNK_COPY_LINE_LANES_A));\n\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_A) * SHARED_OFFSET_A;\n\t\t}else{\n\t\t\twarp_ptr = &B[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * K_GLOBAL_SPARSE;\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_sparse +\n\t\t\t\t\t\t\t\t(laneId / CHUNK_COPY_LINE_LANES_B) * K_GLOBAL_SPARSE) +\n\t\t\t\t\t\t\t\t(laneId % CHUNK_COPY_LINE_LANES_B);\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_B) * SHARED_OFFSET_B;\n\t\t}\n\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      // shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n\t  int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n\t  \t? BLOCK_SIZE_K_SPARSE / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A)\n\t\t: BLOCK_SIZE_N / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\n\t  /*\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\t  */\n\n\t  /*\n      int tile_k_idx_A;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n          tile_k_idx_A = *(lane_ptr_index);\n      }\n\t  */\n\n\t  #pragma unroll\n\t  for(int i = 0; i < iter_index; i += 1){\n\t\t  if(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\tint tile_k_idx_A = *(lane_ptr_index);\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_A * M_GLOBAL) + (laneId % CHUNK_COPY_LINE_LANES_A);\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_A)) =\n\t\t\t\t*lane_ptr;\n\t\t\t//warp_ptr = (uint8_t *)((uint8_t *)warp_ptr + M_GLOBAL * (WARPS_PER_BLOCK / 2) *CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tlane_ptr_index = (int *)((int *)lane_ptr_index +  (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A;\n\t\t  }else{\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_B)) =\n\t\t\t\t*lane_ptr;\n\t\t\tlane_ptr = (int4 *)((uint8_t *)lane_ptr + K_GLOBAL_SPARSE * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B;\n\t\t  }\n\t  }\n\n      __syncthreads();\n\n\t#pragma unroll\n      for (int k_step = 0; k_step < CHUNK_K_SPARSE; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::col_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n\t#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i += 1) {\n\t\t\tsize_t shmem_idx_a = (warpId % BLOCK_COL_WARPS) * M * WARP_COL_TILES + (i * M);\n\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_a + k_step * K * SHARED_OFFSET_A;\n\n\t\t\twmma::load_matrix_sync(a[i], tile_ptr, SHARED_OFFSET_A);\n\t\t#pragma unroll\n\t\t\tfor(int j = 0; j < WARP_ROW_TILES; j += 1){\n\t\t\t\tif(i == 0){\n\t\t\t\t\tsize_t shmem_idx_b = shmem_idx_b_off +\n\t\t\t\t\t\t\t\t\t\t\t(warpId / BLOCK_COL_WARPS) * (WARP_ROW_TILES * N) * SHARED_OFFSET_B +\n\t\t\t\t\t\t\t\t\t\t\t(j * N) * SHARED_OFFSET_B;\n\t\t\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_b + k_step * K;\n\t\t\t\t\twmma::load_matrix_sync(b[j], tile_ptr, SHARED_OFFSET_B);\n\t\t\t\t}\n\t\t\t\twmma::mma_sync(c[j][i], a[i], b[j], c[j][i]);\n\t\t\t}\n\n        }\n      }\n\n      __syncthreads();\n    }\n\n    // This pointer is used to access the C and D matrix tiles this warp computes.\n\tint *shmem_warp_tile_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * N * WARP_ROW_TILES * SHMEM_STRIDE +\n\t(warpId % BLOCK_COL_WARPS) * SHMEM_OFFSET;\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t#pragma unroll\n\t\t\tfor(int t = 0; t < c[i][j].num_elements; t += 1){\n\t\t\t\tc[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n\t\t\t}\n\t\t\tint *tile_ptr = shmem_warp_tile_ptr + i * N * SHMEM_STRIDE + j * M;\n\t\t\twmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, C_LAYOUT);\n\t\t}\n\t}\n\n    __syncthreads();\n\n\tint *shmem_warp_stream_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N * SHMEM_STRIDE\n\t\t\t\t\t\t\t\t\t+ (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tconst size_t gmem_idx =\n\t\t(block_tile_i * N + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N) * GLOBAL_MEM_STRIDE +\n\t\tblock_tile_j * M + (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tuint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n\tint *shmem_lane_stream_ptr =\n\t\tshmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * SHMEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\t\n\tuint8_t *dst_gmem_lane_stream_ptr =\n\t\tdst_gmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * GLOBAL_MEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\n\tfor(int i = 0; i < WARP_ROW_TILES * N; i += SHARED_TO_GLOBAL_LINES_PER_WARP){\n\t\t*(dst_gmem_lane_stream_ptr + i * GLOBAL_MEM_STRIDE) = (uint8_t)(*(shmem_lane_stream_ptr + i * SHMEM_STRIDE));\n\t}\n\n\t__syncthreads();\n}\n", "launch_config": {"dimBlock": [256, 1], "dimGrid": [256, 1]}}, "40": {"code": "\n__global__ void compute_gemm_imma(float *input0, float *input1, float *input2, float* input3, float* input4, float*input5, float* input6,\n                                  float *output0) {\n    // keys need to be replaced 4096, 1024, K_GLOBA_VALUEL, 0.75, 64\n\n\t//extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B = reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_index = reinterpret_cast<int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n\tconst int WARP_SIZE = 32;\n    const int M = 16;\n    const int K = 16;\n    const int N = 16;\n    const int WMMA_M = 16;\n    const int WMMA_K = 16;\n    const int WMMA_N = 16;\n    const int M_GLOBAL = 4096;\n    const int K_GLOBAL = 1024;\n    const int N_GLOBAL = 1024;\n    const float SPARSITY = 0.75;\n    const int K_GLOBAL_SPARSE = (int(K_GLOBAL * (1-SPARSITY)));\n    const int M_TILES = (M_GLOBAL / M);\n    const int N_TILES = (N_GLOBAL / N);\n    const int K_TILES = (K_GLOBAL / K);\n    const int BLOCK_ROW_WARPS = 2;\n    const int BLOCK_COL_WARPS = 4;\n    const int WARP_ROW_TILES = 4;\n    const int WARP_COL_TILES = 2;\n    const int SKEW_UINT8 = 16;\n    const int BANK_VAL = 32;\n    const int NUM_BANK = (K_GLOBAL / BANK_VAL);\n    /////////// BLOCK_ROW_TILES <= N_TILES ////////////\n    // const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    // ////////// BLOCK_COL_TILES <= M_TILES ////////////\n    // const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    auto C_LAYOUT = wmma::mem_col_major;\n    const int CHUNK_K = 64;\n    const int CHUNK_K_SPARSE = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K_SPARSE = (int((CHUNK_K * K) * (1 - SPARSITY)));\n    const int WARP_COPY_BYTES = (WARP_SIZE * sizeof(int4));\n    const int CHUNK_LINE_BYTES_A (BLOCK_COL_TILES * M * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_A = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_A);\n    const int CHUNK_COPY_LINE_LANES_A = (CHUNK_LINE_BYTES_A / sizeof(int4));\n    const int SHARED_OFFSET_A = (BLOCK_COL_TILES * M + SKEW_UINT8);\n\n    const int CHUNK_LINE_BYTES_B = (BLOCK_SIZE_K_SPARSE * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_B = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_B);\n    const int CHUNK_COPY_LINE_LANES_B = (CHUNK_LINE_BYTES_B / sizeof(int4));\n    const int SHARED_OFFSET_B = (BLOCK_SIZE_K_SPARSE + SKEW_UINT8);\n    const int SHARED_TO_GLOBAL_BYTES_PER_LINE = ((WARP_COL_TILES * M) * sizeof(int));\n    const int SHARED_TO_GLOBAL_BYTES_PER_WARP = (WARP_SIZE * sizeof(int));\n    const int SHARED_TO_GLOBAL_LINES_PER_WARP = (SHARED_TO_GLOBAL_BYTES_PER_WARP / SHARED_TO_GLOBAL_BYTES_PER_LINE);\n    const int SHARED_TO_GLOBAL_LANES_PER_LINE = (WARP_SIZE / SHARED_TO_GLOBAL_LINES_PER_WARP);\n    const int SHARED_TO_GLOBAL_ITERS = ((WARP_ROW_TILES * N) / SHARED_TO_GLOBAL_LINES_PER_WARP);\n\n    const int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const int WARP_STRIDE = (WARP_COL_TILES * M);\n    const int WARPS_PER_BLOCK = (BLOCK_ROW_WARPS * BLOCK_COL_WARPS);\n    const int THREADS_PER_BLOCK = (WARP_SIZE * WARPS_PER_BLOCK);\n    const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    const int GLOBAL_MEM_STRIDE = M_GLOBAL;\n    const int SHMEM_STRIDE = (M * BLOCK_COL_TILES);\n    const int SHMEM_OFFSET = (M * WARP_COL_TILES);\n    const int BLOCK_SIZE_M = (M * BLOCK_COL_TILES);\n    const int BLOCK_SIZE_N = (N * BLOCK_ROW_TILES);\n\n   extern __shared__ uint8_t shmem[];\n\n\t// Warp and lane identification.\n\tconst unsigned int warpId = threadIdx.x / WARP_SIZE;\n\tconst unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n\n\t// Offset in shared memory from which the B matrix is stored.\n\t// const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\tconst size_t shmem_idx_b_off = BLOCK_SIZE_K_SPARSE * SHARED_OFFSET_A;\n\n\n\t// Each CTA slides along the 128 x 128 tiles from the top left corner of the\n\t// matrix to the right and down, and selects the next tile to compute. Once\n\t// there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n\tconst unsigned int block_tile_i =\n\t\t((block_pos * BLOCK_COL_TILES) / M_TILES) * (BLOCK_ROW_TILES);\n\tconst unsigned int block_tile_j = (block_pos * BLOCK_COL_TILES) % M_TILES;\n\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n\twmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_ROW_TILES]\n\t\t\t\t\t\t\t\t\t\t\t\t\t [WARP_COL_TILES];\n\n    // Load the C matrix tiles into fragments from shared memory.\n\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t\twmma::fill_fragment(c[i][j], 0);\n\t\t}\n\t}\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    //for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n    for(int tile_k_idx_sparse = 0, tile_k_idx = 0; tile_k_idx_sparse < K_GLOBAL_SPARSE; tile_k_idx_sparse += BLOCK_SIZE_K_SPARSE, tile_k_idx += BLOCK_SIZE_K){\n\n\t\tsize_t shmem_idx = \n\t\twarpId < (WARPS_PER_BLOCK / 2)\n\t\t\t? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A\n\t\t\t: (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B + shmem_idx_b_off;\n\n\t\tint4 *lane_ptr = NULL;\n\t\tint *lane_ptr_index = NULL;\n\t\tconst uint8_t *warp_ptr = NULL;\n\n\n\t\tif(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\t//warp_ptr = &A[block_tile_j * M] +\n\t\t\t//\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * M_GLOBAL;\n\t\t\twarp_ptr = &A[block_tile_j * M];\n\t\t\t\n\t\t\tconst int *warp_ptr_index = &B_index[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t\t\t\t\t\t((warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A);\n\n\t\t\tlane_ptr_index = (int *)(warp_ptr_index + tile_k_idx_sparse + (laneId / CHUNK_COPY_LINE_LANES_A));\n\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_A) * SHARED_OFFSET_A;\n\t\t}else{\n\t\t\twarp_ptr = &B[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * K_GLOBAL_SPARSE;\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_sparse +\n\t\t\t\t\t\t\t\t(laneId / CHUNK_COPY_LINE_LANES_B) * K_GLOBAL_SPARSE) +\n\t\t\t\t\t\t\t\t(laneId % CHUNK_COPY_LINE_LANES_B);\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_B) * SHARED_OFFSET_B;\n\t\t}\n\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      // shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n\t  int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n\t  \t? BLOCK_SIZE_K_SPARSE / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A)\n\t\t: BLOCK_SIZE_N / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\n\t  /*\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\t  */\n\n\t  /*\n      int tile_k_idx_A;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n          tile_k_idx_A = *(lane_ptr_index);\n      }\n\t  */\n\n\t  #pragma unroll\n\t  for(int i = 0; i < iter_index; i += 1){\n\t\t  if(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\tint tile_k_idx_A = *(lane_ptr_index);\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_A * M_GLOBAL) + (laneId % CHUNK_COPY_LINE_LANES_A);\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_A)) =\n\t\t\t\t*lane_ptr;\n\t\t\t//warp_ptr = (uint8_t *)((uint8_t *)warp_ptr + M_GLOBAL * (WARPS_PER_BLOCK / 2) *CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tlane_ptr_index = (int *)((int *)lane_ptr_index +  (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A;\n\t\t  }else{\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_B)) =\n\t\t\t\t*lane_ptr;\n\t\t\tlane_ptr = (int4 *)((uint8_t *)lane_ptr + K_GLOBAL_SPARSE * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B;\n\t\t  }\n\t  }\n\n      __syncthreads();\n\n\t#pragma unroll\n      for (int k_step = 0; k_step < CHUNK_K_SPARSE; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::col_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n\t#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i += 1) {\n\t\t\tsize_t shmem_idx_a = (warpId % BLOCK_COL_WARPS) * M * WARP_COL_TILES + (i * M);\n\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_a + k_step * K * SHARED_OFFSET_A;\n\n\t\t\twmma::load_matrix_sync(a[i], tile_ptr, SHARED_OFFSET_A);\n\t\t#pragma unroll\n\t\t\tfor(int j = 0; j < WARP_ROW_TILES; j += 1){\n\t\t\t\tif(i == 0){\n\t\t\t\t\tsize_t shmem_idx_b = shmem_idx_b_off +\n\t\t\t\t\t\t\t\t\t\t\t(warpId / BLOCK_COL_WARPS) * (WARP_ROW_TILES * N) * SHARED_OFFSET_B +\n\t\t\t\t\t\t\t\t\t\t\t(j * N) * SHARED_OFFSET_B;\n\t\t\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_b + k_step * K;\n\t\t\t\t\twmma::load_matrix_sync(b[j], tile_ptr, SHARED_OFFSET_B);\n\t\t\t\t}\n\t\t\t\twmma::mma_sync(c[j][i], a[i], b[j], c[j][i]);\n\t\t\t}\n\n        }\n      }\n\n      __syncthreads();\n    }\n\n    // This pointer is used to access the C and D matrix tiles this warp computes.\n\tint *shmem_warp_tile_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * N * WARP_ROW_TILES * SHMEM_STRIDE +\n\t(warpId % BLOCK_COL_WARPS) * SHMEM_OFFSET;\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t#pragma unroll\n\t\t\tfor(int t = 0; t < c[i][j].num_elements; t += 1){\n\t\t\t\tc[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n\t\t\t}\n\t\t\tint *tile_ptr = shmem_warp_tile_ptr + i * N * SHMEM_STRIDE + j * M;\n\t\t\twmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, C_LAYOUT);\n\t\t}\n\t}\n\n    __syncthreads();\n\n\tint *shmem_warp_stream_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N * SHMEM_STRIDE\n\t\t\t\t\t\t\t\t\t+ (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tconst size_t gmem_idx =\n\t\t(block_tile_i * N + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N) * GLOBAL_MEM_STRIDE +\n\t\tblock_tile_j * M + (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tuint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n\tint *shmem_lane_stream_ptr =\n\t\tshmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * SHMEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\t\n\tuint8_t *dst_gmem_lane_stream_ptr =\n\t\tdst_gmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * GLOBAL_MEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\n\tfor(int i = 0; i < WARP_ROW_TILES * N; i += SHARED_TO_GLOBAL_LINES_PER_WARP){\n\t\t*(dst_gmem_lane_stream_ptr + i * GLOBAL_MEM_STRIDE) = (uint8_t)(*(shmem_lane_stream_ptr + i * SHMEM_STRIDE));\n\t}\n\n\t__syncthreads();\n}\n", "launch_config": {"dimBlock": [256, 1], "dimGrid": [256, 1]}}, "41": {"code": "\n__global__ void compute_gemm_imma(float *input0, float *input1, float *input2, float* input3, float* input4, float*input5, float* input6,\n                                  float *output0) {\n    // keys need to be replaced 4096, 4096, K_GLOBA_VALUEL, 0.75, 64\n\n\t//extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B = reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_index = reinterpret_cast<int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n\tconst int WARP_SIZE = 32;\n    const int M = 16;\n    const int K = 16;\n    const int N = 16;\n    const int WMMA_M = 16;\n    const int WMMA_K = 16;\n    const int WMMA_N = 16;\n    const int M_GLOBAL = 4096;\n    const int K_GLOBAL = 1024;\n    const int N_GLOBAL = 4096;\n    const float SPARSITY = 0.75;\n    const int K_GLOBAL_SPARSE = (int(K_GLOBAL * (1-SPARSITY)));\n    const int M_TILES = (M_GLOBAL / M);\n    const int N_TILES = (N_GLOBAL / N);\n    const int K_TILES = (K_GLOBAL / K);\n    const int BLOCK_ROW_WARPS = 2;\n    const int BLOCK_COL_WARPS = 4;\n    const int WARP_ROW_TILES = 4;\n    const int WARP_COL_TILES = 2;\n    const int SKEW_UINT8 = 16;\n    const int BANK_VAL = 32;\n    const int NUM_BANK = (K_GLOBAL / BANK_VAL);\n    /////////// BLOCK_ROW_TILES <= N_TILES ////////////\n    // const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    // ////////// BLOCK_COL_TILES <= M_TILES ////////////\n    // const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    auto C_LAYOUT = wmma::mem_col_major;\n    const int CHUNK_K = 64;\n    const int CHUNK_K_SPARSE = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K_SPARSE = (int((CHUNK_K * K) * (1 - SPARSITY)));\n    const int WARP_COPY_BYTES = (WARP_SIZE * sizeof(int4));\n    const int CHUNK_LINE_BYTES_A (BLOCK_COL_TILES * M * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_A = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_A);\n    const int CHUNK_COPY_LINE_LANES_A = (CHUNK_LINE_BYTES_A / sizeof(int4));\n    const int SHARED_OFFSET_A = (BLOCK_COL_TILES * M + SKEW_UINT8);\n\n    const int CHUNK_LINE_BYTES_B = (BLOCK_SIZE_K_SPARSE * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_B = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_B);\n    const int CHUNK_COPY_LINE_LANES_B = (CHUNK_LINE_BYTES_B / sizeof(int4));\n    const int SHARED_OFFSET_B = (BLOCK_SIZE_K_SPARSE + SKEW_UINT8);\n    const int SHARED_TO_GLOBAL_BYTES_PER_LINE = ((WARP_COL_TILES * M) * sizeof(int));\n    const int SHARED_TO_GLOBAL_BYTES_PER_WARP = (WARP_SIZE * sizeof(int));\n    const int SHARED_TO_GLOBAL_LINES_PER_WARP = (SHARED_TO_GLOBAL_BYTES_PER_WARP / SHARED_TO_GLOBAL_BYTES_PER_LINE);\n    const int SHARED_TO_GLOBAL_LANES_PER_LINE = (WARP_SIZE / SHARED_TO_GLOBAL_LINES_PER_WARP);\n    const int SHARED_TO_GLOBAL_ITERS = ((WARP_ROW_TILES * N) / SHARED_TO_GLOBAL_LINES_PER_WARP);\n\n    const int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const int WARP_STRIDE = (WARP_COL_TILES * M);\n    const int WARPS_PER_BLOCK = (BLOCK_ROW_WARPS * BLOCK_COL_WARPS);\n    const int THREADS_PER_BLOCK = (WARP_SIZE * WARPS_PER_BLOCK);\n    const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    const int GLOBAL_MEM_STRIDE = M_GLOBAL;\n    const int SHMEM_STRIDE = (M * BLOCK_COL_TILES);\n    const int SHMEM_OFFSET = (M * WARP_COL_TILES);\n    const int BLOCK_SIZE_M = (M * BLOCK_COL_TILES);\n    const int BLOCK_SIZE_N = (N * BLOCK_ROW_TILES);\n\n   extern __shared__ uint8_t shmem[];\n\n\t// Warp and lane identification.\n\tconst unsigned int warpId = threadIdx.x / WARP_SIZE;\n\tconst unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n\n\t// Offset in shared memory from which the B matrix is stored.\n\t// const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\tconst size_t shmem_idx_b_off = BLOCK_SIZE_K_SPARSE * SHARED_OFFSET_A;\n\n\n\t// Each CTA slides along the 128 x 128 tiles from the top left corner of the\n\t// matrix to the right and down, and selects the next tile to compute. Once\n\t// there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n\tconst unsigned int block_tile_i =\n\t\t((block_pos * BLOCK_COL_TILES) / M_TILES) * (BLOCK_ROW_TILES);\n\tconst unsigned int block_tile_j = (block_pos * BLOCK_COL_TILES) % M_TILES;\n\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n\twmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_ROW_TILES]\n\t\t\t\t\t\t\t\t\t\t\t\t\t [WARP_COL_TILES];\n\n    // Load the C matrix tiles into fragments from shared memory.\n\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t\twmma::fill_fragment(c[i][j], 0);\n\t\t}\n\t}\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    //for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n    for(int tile_k_idx_sparse = 0, tile_k_idx = 0; tile_k_idx_sparse < K_GLOBAL_SPARSE; tile_k_idx_sparse += BLOCK_SIZE_K_SPARSE, tile_k_idx += BLOCK_SIZE_K){\n\n\t\tsize_t shmem_idx = \n\t\twarpId < (WARPS_PER_BLOCK / 2)\n\t\t\t? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A\n\t\t\t: (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B + shmem_idx_b_off;\n\n\t\tint4 *lane_ptr = NULL;\n\t\tint *lane_ptr_index = NULL;\n\t\tconst uint8_t *warp_ptr = NULL;\n\n\n\t\tif(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\t//warp_ptr = &A[block_tile_j * M] +\n\t\t\t//\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * M_GLOBAL;\n\t\t\twarp_ptr = &A[block_tile_j * M];\n\t\t\t\n\t\t\tconst int *warp_ptr_index = &B_index[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t\t\t\t\t\t((warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A);\n\n\t\t\tlane_ptr_index = (int *)(warp_ptr_index + tile_k_idx_sparse + (laneId / CHUNK_COPY_LINE_LANES_A));\n\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_A) * SHARED_OFFSET_A;\n\t\t}else{\n\t\t\twarp_ptr = &B[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * K_GLOBAL_SPARSE;\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_sparse +\n\t\t\t\t\t\t\t\t(laneId / CHUNK_COPY_LINE_LANES_B) * K_GLOBAL_SPARSE) +\n\t\t\t\t\t\t\t\t(laneId % CHUNK_COPY_LINE_LANES_B);\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_B) * SHARED_OFFSET_B;\n\t\t}\n\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      // shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n\t  int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n\t  \t? BLOCK_SIZE_K_SPARSE / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A)\n\t\t: BLOCK_SIZE_N / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\n\t  /*\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\t  */\n\n\t  /*\n      int tile_k_idx_A;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n          tile_k_idx_A = *(lane_ptr_index);\n      }\n\t  */\n\n\t  #pragma unroll\n\t  for(int i = 0; i < iter_index; i += 1){\n\t\t  if(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\tint tile_k_idx_A = *(lane_ptr_index);\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_A * M_GLOBAL) + (laneId % CHUNK_COPY_LINE_LANES_A);\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_A)) =\n\t\t\t\t*lane_ptr;\n\t\t\t//warp_ptr = (uint8_t *)((uint8_t *)warp_ptr + M_GLOBAL * (WARPS_PER_BLOCK / 2) *CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tlane_ptr_index = (int *)((int *)lane_ptr_index +  (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A;\n\t\t  }else{\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_B)) =\n\t\t\t\t*lane_ptr;\n\t\t\tlane_ptr = (int4 *)((uint8_t *)lane_ptr + K_GLOBAL_SPARSE * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B;\n\t\t  }\n\t  }\n\n      __syncthreads();\n\n\t#pragma unroll\n      for (int k_step = 0; k_step < CHUNK_K_SPARSE; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::col_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n\t#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i += 1) {\n\t\t\tsize_t shmem_idx_a = (warpId % BLOCK_COL_WARPS) * M * WARP_COL_TILES + (i * M);\n\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_a + k_step * K * SHARED_OFFSET_A;\n\n\t\t\twmma::load_matrix_sync(a[i], tile_ptr, SHARED_OFFSET_A);\n\t\t#pragma unroll\n\t\t\tfor(int j = 0; j < WARP_ROW_TILES; j += 1){\n\t\t\t\tif(i == 0){\n\t\t\t\t\tsize_t shmem_idx_b = shmem_idx_b_off +\n\t\t\t\t\t\t\t\t\t\t\t(warpId / BLOCK_COL_WARPS) * (WARP_ROW_TILES * N) * SHARED_OFFSET_B +\n\t\t\t\t\t\t\t\t\t\t\t(j * N) * SHARED_OFFSET_B;\n\t\t\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_b + k_step * K;\n\t\t\t\t\twmma::load_matrix_sync(b[j], tile_ptr, SHARED_OFFSET_B);\n\t\t\t\t}\n\t\t\t\twmma::mma_sync(c[j][i], a[i], b[j], c[j][i]);\n\t\t\t}\n\n        }\n      }\n\n      __syncthreads();\n    }\n\n    // This pointer is used to access the C and D matrix tiles this warp computes.\n\tint *shmem_warp_tile_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * N * WARP_ROW_TILES * SHMEM_STRIDE +\n\t(warpId % BLOCK_COL_WARPS) * SHMEM_OFFSET;\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t#pragma unroll\n\t\t\tfor(int t = 0; t < c[i][j].num_elements; t += 1){\n\t\t\t\tc[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n\t\t\t}\n\t\t\tint *tile_ptr = shmem_warp_tile_ptr + i * N * SHMEM_STRIDE + j * M;\n\t\t\twmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, C_LAYOUT);\n\t\t}\n\t}\n\n    __syncthreads();\n\n\tint *shmem_warp_stream_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N * SHMEM_STRIDE\n\t\t\t\t\t\t\t\t\t+ (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tconst size_t gmem_idx =\n\t\t(block_tile_i * N + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N) * GLOBAL_MEM_STRIDE +\n\t\tblock_tile_j * M + (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tuint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n\tint *shmem_lane_stream_ptr =\n\t\tshmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * SHMEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\t\n\tuint8_t *dst_gmem_lane_stream_ptr =\n\t\tdst_gmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * GLOBAL_MEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\n\tfor(int i = 0; i < WARP_ROW_TILES * N; i += SHARED_TO_GLOBAL_LINES_PER_WARP){\n\t\t*(dst_gmem_lane_stream_ptr + i * GLOBAL_MEM_STRIDE) = (uint8_t)(*(shmem_lane_stream_ptr + i * SHMEM_STRIDE));\n\t}\n\n\t__syncthreads();\n}\n", "launch_config": {"dimBlock": [1024, 1], "dimGrid": [256, 1]}}, "42": {"code": "\n__global__ void compute_gemm_imma(float *input0, float *input1, float *input2, float* input3, float* input4, float*input5, float* input6,\n                                  float *output0) {\n    // keys need to be replaced 4096, 1024, K_GLOBA_VALUEL, 0.75, 64\n\n\t//extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B = reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_index = reinterpret_cast<int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n\tconst int WARP_SIZE = 32;\n    const int M = 16;\n    const int K = 16;\n    const int N = 16;\n    const int WMMA_M = 16;\n    const int WMMA_K = 16;\n    const int WMMA_N = 16;\n    const int M_GLOBAL = 4096;\n    const int K_GLOBAL = 4096;\n    const int N_GLOBAL = 1024;\n    const float SPARSITY = 0.75;\n    const int K_GLOBAL_SPARSE = (int(K_GLOBAL * (1-SPARSITY)));\n    const int M_TILES = (M_GLOBAL / M);\n    const int N_TILES = (N_GLOBAL / N);\n    const int K_TILES = (K_GLOBAL / K);\n    const int BLOCK_ROW_WARPS = 2;\n    const int BLOCK_COL_WARPS = 4;\n    const int WARP_ROW_TILES = 4;\n    const int WARP_COL_TILES = 2;\n    const int SKEW_UINT8 = 16;\n    const int BANK_VAL = 32;\n    const int NUM_BANK = (K_GLOBAL / BANK_VAL);\n    /////////// BLOCK_ROW_TILES <= N_TILES ////////////\n    // const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    // ////////// BLOCK_COL_TILES <= M_TILES ////////////\n    // const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    auto C_LAYOUT = wmma::mem_col_major;\n    const int CHUNK_K = 64;\n    const int CHUNK_K_SPARSE = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K_SPARSE = (int((CHUNK_K * K) * (1 - SPARSITY)));\n    const int WARP_COPY_BYTES = (WARP_SIZE * sizeof(int4));\n    const int CHUNK_LINE_BYTES_A (BLOCK_COL_TILES * M * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_A = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_A);\n    const int CHUNK_COPY_LINE_LANES_A = (CHUNK_LINE_BYTES_A / sizeof(int4));\n    const int SHARED_OFFSET_A = (BLOCK_COL_TILES * M + SKEW_UINT8);\n\n    const int CHUNK_LINE_BYTES_B = (BLOCK_SIZE_K_SPARSE * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_B = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_B);\n    const int CHUNK_COPY_LINE_LANES_B = (CHUNK_LINE_BYTES_B / sizeof(int4));\n    const int SHARED_OFFSET_B = (BLOCK_SIZE_K_SPARSE + SKEW_UINT8);\n    const int SHARED_TO_GLOBAL_BYTES_PER_LINE = ((WARP_COL_TILES * M) * sizeof(int));\n    const int SHARED_TO_GLOBAL_BYTES_PER_WARP = (WARP_SIZE * sizeof(int));\n    const int SHARED_TO_GLOBAL_LINES_PER_WARP = (SHARED_TO_GLOBAL_BYTES_PER_WARP / SHARED_TO_GLOBAL_BYTES_PER_LINE);\n    const int SHARED_TO_GLOBAL_LANES_PER_LINE = (WARP_SIZE / SHARED_TO_GLOBAL_LINES_PER_WARP);\n    const int SHARED_TO_GLOBAL_ITERS = ((WARP_ROW_TILES * N) / SHARED_TO_GLOBAL_LINES_PER_WARP);\n\n    const int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const int WARP_STRIDE = (WARP_COL_TILES * M);\n    const int WARPS_PER_BLOCK = (BLOCK_ROW_WARPS * BLOCK_COL_WARPS);\n    const int THREADS_PER_BLOCK = (WARP_SIZE * WARPS_PER_BLOCK);\n    const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    const int GLOBAL_MEM_STRIDE = M_GLOBAL;\n    const int SHMEM_STRIDE = (M * BLOCK_COL_TILES);\n    const int SHMEM_OFFSET = (M * WARP_COL_TILES);\n    const int BLOCK_SIZE_M = (M * BLOCK_COL_TILES);\n    const int BLOCK_SIZE_N = (N * BLOCK_ROW_TILES);\n\n   extern __shared__ uint8_t shmem[];\n\n\t// Warp and lane identification.\n\tconst unsigned int warpId = threadIdx.x / WARP_SIZE;\n\tconst unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n\n\t// Offset in shared memory from which the B matrix is stored.\n\t// const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\tconst size_t shmem_idx_b_off = BLOCK_SIZE_K_SPARSE * SHARED_OFFSET_A;\n\n\n\t// Each CTA slides along the 128 x 128 tiles from the top left corner of the\n\t// matrix to the right and down, and selects the next tile to compute. Once\n\t// there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n\tconst unsigned int block_tile_i =\n\t\t((block_pos * BLOCK_COL_TILES) / M_TILES) * (BLOCK_ROW_TILES);\n\tconst unsigned int block_tile_j = (block_pos * BLOCK_COL_TILES) % M_TILES;\n\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n\twmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_ROW_TILES]\n\t\t\t\t\t\t\t\t\t\t\t\t\t [WARP_COL_TILES];\n\n    // Load the C matrix tiles into fragments from shared memory.\n\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t\twmma::fill_fragment(c[i][j], 0);\n\t\t}\n\t}\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    //for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n    for(int tile_k_idx_sparse = 0, tile_k_idx = 0; tile_k_idx_sparse < K_GLOBAL_SPARSE; tile_k_idx_sparse += BLOCK_SIZE_K_SPARSE, tile_k_idx += BLOCK_SIZE_K){\n\n\t\tsize_t shmem_idx = \n\t\twarpId < (WARPS_PER_BLOCK / 2)\n\t\t\t? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A\n\t\t\t: (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B + shmem_idx_b_off;\n\n\t\tint4 *lane_ptr = NULL;\n\t\tint *lane_ptr_index = NULL;\n\t\tconst uint8_t *warp_ptr = NULL;\n\n\n\t\tif(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\t//warp_ptr = &A[block_tile_j * M] +\n\t\t\t//\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * M_GLOBAL;\n\t\t\twarp_ptr = &A[block_tile_j * M];\n\t\t\t\n\t\t\tconst int *warp_ptr_index = &B_index[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t\t\t\t\t\t((warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A);\n\n\t\t\tlane_ptr_index = (int *)(warp_ptr_index + tile_k_idx_sparse + (laneId / CHUNK_COPY_LINE_LANES_A));\n\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_A) * SHARED_OFFSET_A;\n\t\t}else{\n\t\t\twarp_ptr = &B[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * K_GLOBAL_SPARSE;\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_sparse +\n\t\t\t\t\t\t\t\t(laneId / CHUNK_COPY_LINE_LANES_B) * K_GLOBAL_SPARSE) +\n\t\t\t\t\t\t\t\t(laneId % CHUNK_COPY_LINE_LANES_B);\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_B) * SHARED_OFFSET_B;\n\t\t}\n\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      // shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n\t  int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n\t  \t? BLOCK_SIZE_K_SPARSE / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A)\n\t\t: BLOCK_SIZE_N / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\n\t  /*\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\t  */\n\n\t  /*\n      int tile_k_idx_A;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n          tile_k_idx_A = *(lane_ptr_index);\n      }\n\t  */\n\n\t  #pragma unroll\n\t  for(int i = 0; i < iter_index; i += 1){\n\t\t  if(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\tint tile_k_idx_A = *(lane_ptr_index);\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_A * M_GLOBAL) + (laneId % CHUNK_COPY_LINE_LANES_A);\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_A)) =\n\t\t\t\t*lane_ptr;\n\t\t\t//warp_ptr = (uint8_t *)((uint8_t *)warp_ptr + M_GLOBAL * (WARPS_PER_BLOCK / 2) *CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tlane_ptr_index = (int *)((int *)lane_ptr_index +  (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A;\n\t\t  }else{\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_B)) =\n\t\t\t\t*lane_ptr;\n\t\t\tlane_ptr = (int4 *)((uint8_t *)lane_ptr + K_GLOBAL_SPARSE * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B;\n\t\t  }\n\t  }\n\n      __syncthreads();\n\n\t#pragma unroll\n      for (int k_step = 0; k_step < CHUNK_K_SPARSE; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::col_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n\t#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i += 1) {\n\t\t\tsize_t shmem_idx_a = (warpId % BLOCK_COL_WARPS) * M * WARP_COL_TILES + (i * M);\n\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_a + k_step * K * SHARED_OFFSET_A;\n\n\t\t\twmma::load_matrix_sync(a[i], tile_ptr, SHARED_OFFSET_A);\n\t\t#pragma unroll\n\t\t\tfor(int j = 0; j < WARP_ROW_TILES; j += 1){\n\t\t\t\tif(i == 0){\n\t\t\t\t\tsize_t shmem_idx_b = shmem_idx_b_off +\n\t\t\t\t\t\t\t\t\t\t\t(warpId / BLOCK_COL_WARPS) * (WARP_ROW_TILES * N) * SHARED_OFFSET_B +\n\t\t\t\t\t\t\t\t\t\t\t(j * N) * SHARED_OFFSET_B;\n\t\t\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_b + k_step * K;\n\t\t\t\t\twmma::load_matrix_sync(b[j], tile_ptr, SHARED_OFFSET_B);\n\t\t\t\t}\n\t\t\t\twmma::mma_sync(c[j][i], a[i], b[j], c[j][i]);\n\t\t\t}\n\n        }\n      }\n\n      __syncthreads();\n    }\n\n    // This pointer is used to access the C and D matrix tiles this warp computes.\n\tint *shmem_warp_tile_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * N * WARP_ROW_TILES * SHMEM_STRIDE +\n\t(warpId % BLOCK_COL_WARPS) * SHMEM_OFFSET;\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t#pragma unroll\n\t\t\tfor(int t = 0; t < c[i][j].num_elements; t += 1){\n\t\t\t\tc[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n\t\t\t}\n\t\t\tint *tile_ptr = shmem_warp_tile_ptr + i * N * SHMEM_STRIDE + j * M;\n\t\t\twmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, C_LAYOUT);\n\t\t}\n\t}\n\n    __syncthreads();\n\n\tint *shmem_warp_stream_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N * SHMEM_STRIDE\n\t\t\t\t\t\t\t\t\t+ (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tconst size_t gmem_idx =\n\t\t(block_tile_i * N + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N) * GLOBAL_MEM_STRIDE +\n\t\tblock_tile_j * M + (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tuint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n\tint *shmem_lane_stream_ptr =\n\t\tshmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * SHMEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\t\n\tuint8_t *dst_gmem_lane_stream_ptr =\n\t\tdst_gmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * GLOBAL_MEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\n\tfor(int i = 0; i < WARP_ROW_TILES * N; i += SHARED_TO_GLOBAL_LINES_PER_WARP){\n\t\t*(dst_gmem_lane_stream_ptr + i * GLOBAL_MEM_STRIDE) = (uint8_t)(*(shmem_lane_stream_ptr + i * SHMEM_STRIDE));\n\t}\n\n\t__syncthreads();\n}\n", "launch_config": {"dimBlock": [256, 1], "dimGrid": [256, 1]}}, "43": {"code": "\n__global__ void compute_gemm_imma(float *input0, float *input1, float *input2, float* input3, float* input4, float*input5, float* input6,\n                                  float *output0) {\n    // keys need to be replaced 4096, 1024, K_GLOBA_VALUEL, 0.75, 64\n\n\t//extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B = reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_index = reinterpret_cast<int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n\tconst int WARP_SIZE = 32;\n    const int M = 16;\n    const int K = 16;\n    const int N = 16;\n    const int WMMA_M = 16;\n    const int WMMA_K = 16;\n    const int WMMA_N = 16;\n    const int M_GLOBAL = 4096;\n    const int K_GLOBAL = 1024;\n    const int N_GLOBAL = 1024;\n    const float SPARSITY = 0.75;\n    const int K_GLOBAL_SPARSE = (int(K_GLOBAL * (1-SPARSITY)));\n    const int M_TILES = (M_GLOBAL / M);\n    const int N_TILES = (N_GLOBAL / N);\n    const int K_TILES = (K_GLOBAL / K);\n    const int BLOCK_ROW_WARPS = 2;\n    const int BLOCK_COL_WARPS = 4;\n    const int WARP_ROW_TILES = 4;\n    const int WARP_COL_TILES = 2;\n    const int SKEW_UINT8 = 16;\n    const int BANK_VAL = 32;\n    const int NUM_BANK = (K_GLOBAL / BANK_VAL);\n    /////////// BLOCK_ROW_TILES <= N_TILES ////////////\n    // const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    // ////////// BLOCK_COL_TILES <= M_TILES ////////////\n    // const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    auto C_LAYOUT = wmma::mem_col_major;\n    const int CHUNK_K = 64;\n    const int CHUNK_K_SPARSE = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K_SPARSE = (int((CHUNK_K * K) * (1 - SPARSITY)));\n    const int WARP_COPY_BYTES = (WARP_SIZE * sizeof(int4));\n    const int CHUNK_LINE_BYTES_A (BLOCK_COL_TILES * M * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_A = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_A);\n    const int CHUNK_COPY_LINE_LANES_A = (CHUNK_LINE_BYTES_A / sizeof(int4));\n    const int SHARED_OFFSET_A = (BLOCK_COL_TILES * M + SKEW_UINT8);\n\n    const int CHUNK_LINE_BYTES_B = (BLOCK_SIZE_K_SPARSE * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_B = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_B);\n    const int CHUNK_COPY_LINE_LANES_B = (CHUNK_LINE_BYTES_B / sizeof(int4));\n    const int SHARED_OFFSET_B = (BLOCK_SIZE_K_SPARSE + SKEW_UINT8);\n    const int SHARED_TO_GLOBAL_BYTES_PER_LINE = ((WARP_COL_TILES * M) * sizeof(int));\n    const int SHARED_TO_GLOBAL_BYTES_PER_WARP = (WARP_SIZE * sizeof(int));\n    const int SHARED_TO_GLOBAL_LINES_PER_WARP = (SHARED_TO_GLOBAL_BYTES_PER_WARP / SHARED_TO_GLOBAL_BYTES_PER_LINE);\n    const int SHARED_TO_GLOBAL_LANES_PER_LINE = (WARP_SIZE / SHARED_TO_GLOBAL_LINES_PER_WARP);\n    const int SHARED_TO_GLOBAL_ITERS = ((WARP_ROW_TILES * N) / SHARED_TO_GLOBAL_LINES_PER_WARP);\n\n    const int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const int WARP_STRIDE = (WARP_COL_TILES * M);\n    const int WARPS_PER_BLOCK = (BLOCK_ROW_WARPS * BLOCK_COL_WARPS);\n    const int THREADS_PER_BLOCK = (WARP_SIZE * WARPS_PER_BLOCK);\n    const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    const int GLOBAL_MEM_STRIDE = M_GLOBAL;\n    const int SHMEM_STRIDE = (M * BLOCK_COL_TILES);\n    const int SHMEM_OFFSET = (M * WARP_COL_TILES);\n    const int BLOCK_SIZE_M = (M * BLOCK_COL_TILES);\n    const int BLOCK_SIZE_N = (N * BLOCK_ROW_TILES);\n\n   extern __shared__ uint8_t shmem[];\n\n\t// Warp and lane identification.\n\tconst unsigned int warpId = threadIdx.x / WARP_SIZE;\n\tconst unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n\n\t// Offset in shared memory from which the B matrix is stored.\n\t// const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\tconst size_t shmem_idx_b_off = BLOCK_SIZE_K_SPARSE * SHARED_OFFSET_A;\n\n\n\t// Each CTA slides along the 128 x 128 tiles from the top left corner of the\n\t// matrix to the right and down, and selects the next tile to compute. Once\n\t// there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n\tconst unsigned int block_tile_i =\n\t\t((block_pos * BLOCK_COL_TILES) / M_TILES) * (BLOCK_ROW_TILES);\n\tconst unsigned int block_tile_j = (block_pos * BLOCK_COL_TILES) % M_TILES;\n\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n\twmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_ROW_TILES]\n\t\t\t\t\t\t\t\t\t\t\t\t\t [WARP_COL_TILES];\n\n    // Load the C matrix tiles into fragments from shared memory.\n\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t\twmma::fill_fragment(c[i][j], 0);\n\t\t}\n\t}\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    //for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n    for(int tile_k_idx_sparse = 0, tile_k_idx = 0; tile_k_idx_sparse < K_GLOBAL_SPARSE; tile_k_idx_sparse += BLOCK_SIZE_K_SPARSE, tile_k_idx += BLOCK_SIZE_K){\n\n\t\tsize_t shmem_idx = \n\t\twarpId < (WARPS_PER_BLOCK / 2)\n\t\t\t? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A\n\t\t\t: (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B + shmem_idx_b_off;\n\n\t\tint4 *lane_ptr = NULL;\n\t\tint *lane_ptr_index = NULL;\n\t\tconst uint8_t *warp_ptr = NULL;\n\n\n\t\tif(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\t//warp_ptr = &A[block_tile_j * M] +\n\t\t\t//\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * M_GLOBAL;\n\t\t\twarp_ptr = &A[block_tile_j * M];\n\t\t\t\n\t\t\tconst int *warp_ptr_index = &B_index[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t\t\t\t\t\t((warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A);\n\n\t\t\tlane_ptr_index = (int *)(warp_ptr_index + tile_k_idx_sparse + (laneId / CHUNK_COPY_LINE_LANES_A));\n\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_A) * SHARED_OFFSET_A;\n\t\t}else{\n\t\t\twarp_ptr = &B[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * K_GLOBAL_SPARSE;\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_sparse +\n\t\t\t\t\t\t\t\t(laneId / CHUNK_COPY_LINE_LANES_B) * K_GLOBAL_SPARSE) +\n\t\t\t\t\t\t\t\t(laneId % CHUNK_COPY_LINE_LANES_B);\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_B) * SHARED_OFFSET_B;\n\t\t}\n\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      // shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n\t  int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n\t  \t? BLOCK_SIZE_K_SPARSE / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A)\n\t\t: BLOCK_SIZE_N / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\n\t  /*\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\t  */\n\n\t  /*\n      int tile_k_idx_A;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n          tile_k_idx_A = *(lane_ptr_index);\n      }\n\t  */\n\n\t  #pragma unroll\n\t  for(int i = 0; i < iter_index; i += 1){\n\t\t  if(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\tint tile_k_idx_A = *(lane_ptr_index);\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_A * M_GLOBAL) + (laneId % CHUNK_COPY_LINE_LANES_A);\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_A)) =\n\t\t\t\t*lane_ptr;\n\t\t\t//warp_ptr = (uint8_t *)((uint8_t *)warp_ptr + M_GLOBAL * (WARPS_PER_BLOCK / 2) *CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tlane_ptr_index = (int *)((int *)lane_ptr_index +  (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A;\n\t\t  }else{\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_B)) =\n\t\t\t\t*lane_ptr;\n\t\t\tlane_ptr = (int4 *)((uint8_t *)lane_ptr + K_GLOBAL_SPARSE * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B;\n\t\t  }\n\t  }\n\n      __syncthreads();\n\n\t#pragma unroll\n      for (int k_step = 0; k_step < CHUNK_K_SPARSE; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::col_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n\t#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i += 1) {\n\t\t\tsize_t shmem_idx_a = (warpId % BLOCK_COL_WARPS) * M * WARP_COL_TILES + (i * M);\n\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_a + k_step * K * SHARED_OFFSET_A;\n\n\t\t\twmma::load_matrix_sync(a[i], tile_ptr, SHARED_OFFSET_A);\n\t\t#pragma unroll\n\t\t\tfor(int j = 0; j < WARP_ROW_TILES; j += 1){\n\t\t\t\tif(i == 0){\n\t\t\t\t\tsize_t shmem_idx_b = shmem_idx_b_off +\n\t\t\t\t\t\t\t\t\t\t\t(warpId / BLOCK_COL_WARPS) * (WARP_ROW_TILES * N) * SHARED_OFFSET_B +\n\t\t\t\t\t\t\t\t\t\t\t(j * N) * SHARED_OFFSET_B;\n\t\t\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_b + k_step * K;\n\t\t\t\t\twmma::load_matrix_sync(b[j], tile_ptr, SHARED_OFFSET_B);\n\t\t\t\t}\n\t\t\t\twmma::mma_sync(c[j][i], a[i], b[j], c[j][i]);\n\t\t\t}\n\n        }\n      }\n\n      __syncthreads();\n    }\n\n    // This pointer is used to access the C and D matrix tiles this warp computes.\n\tint *shmem_warp_tile_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * N * WARP_ROW_TILES * SHMEM_STRIDE +\n\t(warpId % BLOCK_COL_WARPS) * SHMEM_OFFSET;\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t#pragma unroll\n\t\t\tfor(int t = 0; t < c[i][j].num_elements; t += 1){\n\t\t\t\tc[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n\t\t\t}\n\t\t\tint *tile_ptr = shmem_warp_tile_ptr + i * N * SHMEM_STRIDE + j * M;\n\t\t\twmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, C_LAYOUT);\n\t\t}\n\t}\n\n    __syncthreads();\n\n\tint *shmem_warp_stream_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N * SHMEM_STRIDE\n\t\t\t\t\t\t\t\t\t+ (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tconst size_t gmem_idx =\n\t\t(block_tile_i * N + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N) * GLOBAL_MEM_STRIDE +\n\t\tblock_tile_j * M + (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tuint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n\tint *shmem_lane_stream_ptr =\n\t\tshmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * SHMEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\t\n\tuint8_t *dst_gmem_lane_stream_ptr =\n\t\tdst_gmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * GLOBAL_MEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\n\tfor(int i = 0; i < WARP_ROW_TILES * N; i += SHARED_TO_GLOBAL_LINES_PER_WARP){\n\t\t*(dst_gmem_lane_stream_ptr + i * GLOBAL_MEM_STRIDE) = (uint8_t)(*(shmem_lane_stream_ptr + i * SHMEM_STRIDE));\n\t}\n\n\t__syncthreads();\n}\n", "launch_config": {"dimBlock": [256, 1], "dimGrid": [256, 1]}}, "44": {"code": "\n__global__ void compute_gemm_imma(float *input0, float *input1, float *input2, float* input3, float* input4, float*input5, float* input6,\n                                  float *output0) {\n    // keys need to be replaced 4096, 1024, K_GLOBA_VALUEL, 0.75, 64\n\n\t//extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B = reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_index = reinterpret_cast<int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n\tconst int WARP_SIZE = 32;\n    const int M = 16;\n    const int K = 16;\n    const int N = 16;\n    const int WMMA_M = 16;\n    const int WMMA_K = 16;\n    const int WMMA_N = 16;\n    const int M_GLOBAL = 4096;\n    const int K_GLOBAL = 1024;\n    const int N_GLOBAL = 1024;\n    const float SPARSITY = 0.75;\n    const int K_GLOBAL_SPARSE = (int(K_GLOBAL * (1-SPARSITY)));\n    const int M_TILES = (M_GLOBAL / M);\n    const int N_TILES = (N_GLOBAL / N);\n    const int K_TILES = (K_GLOBAL / K);\n    const int BLOCK_ROW_WARPS = 2;\n    const int BLOCK_COL_WARPS = 4;\n    const int WARP_ROW_TILES = 4;\n    const int WARP_COL_TILES = 2;\n    const int SKEW_UINT8 = 16;\n    const int BANK_VAL = 32;\n    const int NUM_BANK = (K_GLOBAL / BANK_VAL);\n    /////////// BLOCK_ROW_TILES <= N_TILES ////////////\n    // const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    // ////////// BLOCK_COL_TILES <= M_TILES ////////////\n    // const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    auto C_LAYOUT = wmma::mem_col_major;\n    const int CHUNK_K = 64;\n    const int CHUNK_K_SPARSE = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K_SPARSE = (int((CHUNK_K * K) * (1 - SPARSITY)));\n    const int WARP_COPY_BYTES = (WARP_SIZE * sizeof(int4));\n    const int CHUNK_LINE_BYTES_A (BLOCK_COL_TILES * M * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_A = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_A);\n    const int CHUNK_COPY_LINE_LANES_A = (CHUNK_LINE_BYTES_A / sizeof(int4));\n    const int SHARED_OFFSET_A = (BLOCK_COL_TILES * M + SKEW_UINT8);\n\n    const int CHUNK_LINE_BYTES_B = (BLOCK_SIZE_K_SPARSE * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_B = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_B);\n    const int CHUNK_COPY_LINE_LANES_B = (CHUNK_LINE_BYTES_B / sizeof(int4));\n    const int SHARED_OFFSET_B = (BLOCK_SIZE_K_SPARSE + SKEW_UINT8);\n    const int SHARED_TO_GLOBAL_BYTES_PER_LINE = ((WARP_COL_TILES * M) * sizeof(int));\n    const int SHARED_TO_GLOBAL_BYTES_PER_WARP = (WARP_SIZE * sizeof(int));\n    const int SHARED_TO_GLOBAL_LINES_PER_WARP = (SHARED_TO_GLOBAL_BYTES_PER_WARP / SHARED_TO_GLOBAL_BYTES_PER_LINE);\n    const int SHARED_TO_GLOBAL_LANES_PER_LINE = (WARP_SIZE / SHARED_TO_GLOBAL_LINES_PER_WARP);\n    const int SHARED_TO_GLOBAL_ITERS = ((WARP_ROW_TILES * N) / SHARED_TO_GLOBAL_LINES_PER_WARP);\n\n    const int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const int WARP_STRIDE = (WARP_COL_TILES * M);\n    const int WARPS_PER_BLOCK = (BLOCK_ROW_WARPS * BLOCK_COL_WARPS);\n    const int THREADS_PER_BLOCK = (WARP_SIZE * WARPS_PER_BLOCK);\n    const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    const int GLOBAL_MEM_STRIDE = M_GLOBAL;\n    const int SHMEM_STRIDE = (M * BLOCK_COL_TILES);\n    const int SHMEM_OFFSET = (M * WARP_COL_TILES);\n    const int BLOCK_SIZE_M = (M * BLOCK_COL_TILES);\n    const int BLOCK_SIZE_N = (N * BLOCK_ROW_TILES);\n\n   extern __shared__ uint8_t shmem[];\n\n\t// Warp and lane identification.\n\tconst unsigned int warpId = threadIdx.x / WARP_SIZE;\n\tconst unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n\n\t// Offset in shared memory from which the B matrix is stored.\n\t// const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\tconst size_t shmem_idx_b_off = BLOCK_SIZE_K_SPARSE * SHARED_OFFSET_A;\n\n\n\t// Each CTA slides along the 128 x 128 tiles from the top left corner of the\n\t// matrix to the right and down, and selects the next tile to compute. Once\n\t// there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n\tconst unsigned int block_tile_i =\n\t\t((block_pos * BLOCK_COL_TILES) / M_TILES) * (BLOCK_ROW_TILES);\n\tconst unsigned int block_tile_j = (block_pos * BLOCK_COL_TILES) % M_TILES;\n\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n\twmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_ROW_TILES]\n\t\t\t\t\t\t\t\t\t\t\t\t\t [WARP_COL_TILES];\n\n    // Load the C matrix tiles into fragments from shared memory.\n\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t\twmma::fill_fragment(c[i][j], 0);\n\t\t}\n\t}\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    //for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n    for(int tile_k_idx_sparse = 0, tile_k_idx = 0; tile_k_idx_sparse < K_GLOBAL_SPARSE; tile_k_idx_sparse += BLOCK_SIZE_K_SPARSE, tile_k_idx += BLOCK_SIZE_K){\n\n\t\tsize_t shmem_idx = \n\t\twarpId < (WARPS_PER_BLOCK / 2)\n\t\t\t? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A\n\t\t\t: (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B + shmem_idx_b_off;\n\n\t\tint4 *lane_ptr = NULL;\n\t\tint *lane_ptr_index = NULL;\n\t\tconst uint8_t *warp_ptr = NULL;\n\n\n\t\tif(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\t//warp_ptr = &A[block_tile_j * M] +\n\t\t\t//\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * M_GLOBAL;\n\t\t\twarp_ptr = &A[block_tile_j * M];\n\t\t\t\n\t\t\tconst int *warp_ptr_index = &B_index[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t\t\t\t\t\t((warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A);\n\n\t\t\tlane_ptr_index = (int *)(warp_ptr_index + tile_k_idx_sparse + (laneId / CHUNK_COPY_LINE_LANES_A));\n\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_A) * SHARED_OFFSET_A;\n\t\t}else{\n\t\t\twarp_ptr = &B[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * K_GLOBAL_SPARSE;\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_sparse +\n\t\t\t\t\t\t\t\t(laneId / CHUNK_COPY_LINE_LANES_B) * K_GLOBAL_SPARSE) +\n\t\t\t\t\t\t\t\t(laneId % CHUNK_COPY_LINE_LANES_B);\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_B) * SHARED_OFFSET_B;\n\t\t}\n\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      // shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n\t  int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n\t  \t? BLOCK_SIZE_K_SPARSE / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A)\n\t\t: BLOCK_SIZE_N / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\n\t  /*\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\t  */\n\n\t  /*\n      int tile_k_idx_A;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n          tile_k_idx_A = *(lane_ptr_index);\n      }\n\t  */\n\n\t  #pragma unroll\n\t  for(int i = 0; i < iter_index; i += 1){\n\t\t  if(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\tint tile_k_idx_A = *(lane_ptr_index);\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_A * M_GLOBAL) + (laneId % CHUNK_COPY_LINE_LANES_A);\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_A)) =\n\t\t\t\t*lane_ptr;\n\t\t\t//warp_ptr = (uint8_t *)((uint8_t *)warp_ptr + M_GLOBAL * (WARPS_PER_BLOCK / 2) *CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tlane_ptr_index = (int *)((int *)lane_ptr_index +  (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A;\n\t\t  }else{\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_B)) =\n\t\t\t\t*lane_ptr;\n\t\t\tlane_ptr = (int4 *)((uint8_t *)lane_ptr + K_GLOBAL_SPARSE * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B;\n\t\t  }\n\t  }\n\n      __syncthreads();\n\n\t#pragma unroll\n      for (int k_step = 0; k_step < CHUNK_K_SPARSE; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::col_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n\t#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i += 1) {\n\t\t\tsize_t shmem_idx_a = (warpId % BLOCK_COL_WARPS) * M * WARP_COL_TILES + (i * M);\n\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_a + k_step * K * SHARED_OFFSET_A;\n\n\t\t\twmma::load_matrix_sync(a[i], tile_ptr, SHARED_OFFSET_A);\n\t\t#pragma unroll\n\t\t\tfor(int j = 0; j < WARP_ROW_TILES; j += 1){\n\t\t\t\tif(i == 0){\n\t\t\t\t\tsize_t shmem_idx_b = shmem_idx_b_off +\n\t\t\t\t\t\t\t\t\t\t\t(warpId / BLOCK_COL_WARPS) * (WARP_ROW_TILES * N) * SHARED_OFFSET_B +\n\t\t\t\t\t\t\t\t\t\t\t(j * N) * SHARED_OFFSET_B;\n\t\t\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_b + k_step * K;\n\t\t\t\t\twmma::load_matrix_sync(b[j], tile_ptr, SHARED_OFFSET_B);\n\t\t\t\t}\n\t\t\t\twmma::mma_sync(c[j][i], a[i], b[j], c[j][i]);\n\t\t\t}\n\n        }\n      }\n\n      __syncthreads();\n    }\n\n    // This pointer is used to access the C and D matrix tiles this warp computes.\n\tint *shmem_warp_tile_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * N * WARP_ROW_TILES * SHMEM_STRIDE +\n\t(warpId % BLOCK_COL_WARPS) * SHMEM_OFFSET;\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t#pragma unroll\n\t\t\tfor(int t = 0; t < c[i][j].num_elements; t += 1){\n\t\t\t\tc[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n\t\t\t}\n\t\t\tint *tile_ptr = shmem_warp_tile_ptr + i * N * SHMEM_STRIDE + j * M;\n\t\t\twmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, C_LAYOUT);\n\t\t}\n\t}\n\n    __syncthreads();\n\n\tint *shmem_warp_stream_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N * SHMEM_STRIDE\n\t\t\t\t\t\t\t\t\t+ (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tconst size_t gmem_idx =\n\t\t(block_tile_i * N + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N) * GLOBAL_MEM_STRIDE +\n\t\tblock_tile_j * M + (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tuint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n\tint *shmem_lane_stream_ptr =\n\t\tshmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * SHMEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\t\n\tuint8_t *dst_gmem_lane_stream_ptr =\n\t\tdst_gmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * GLOBAL_MEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\n\tfor(int i = 0; i < WARP_ROW_TILES * N; i += SHARED_TO_GLOBAL_LINES_PER_WARP){\n\t\t*(dst_gmem_lane_stream_ptr + i * GLOBAL_MEM_STRIDE) = (uint8_t)(*(shmem_lane_stream_ptr + i * SHMEM_STRIDE));\n\t}\n\n\t__syncthreads();\n}\n", "launch_config": {"dimBlock": [256, 1], "dimGrid": [256, 1]}}, "45": {"code": "\n__global__ void compute_gemm_imma(float *input0, float *input1, float *input2, float* input3, float* input4, float*input5, float* input6,\n                                  float *output0) {\n    // keys need to be replaced 4096, 1024, K_GLOBA_VALUEL, 0.75, 64\n\n\t//extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B = reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_index = reinterpret_cast<int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n\tconst int WARP_SIZE = 32;\n    const int M = 16;\n    const int K = 16;\n    const int N = 16;\n    const int WMMA_M = 16;\n    const int WMMA_K = 16;\n    const int WMMA_N = 16;\n    const int M_GLOBAL = 4096;\n    const int K_GLOBAL = 1024;\n    const int N_GLOBAL = 1024;\n    const float SPARSITY = 0.75;\n    const int K_GLOBAL_SPARSE = (int(K_GLOBAL * (1-SPARSITY)));\n    const int M_TILES = (M_GLOBAL / M);\n    const int N_TILES = (N_GLOBAL / N);\n    const int K_TILES = (K_GLOBAL / K);\n    const int BLOCK_ROW_WARPS = 2;\n    const int BLOCK_COL_WARPS = 4;\n    const int WARP_ROW_TILES = 4;\n    const int WARP_COL_TILES = 2;\n    const int SKEW_UINT8 = 16;\n    const int BANK_VAL = 32;\n    const int NUM_BANK = (K_GLOBAL / BANK_VAL);\n    /////////// BLOCK_ROW_TILES <= N_TILES ////////////\n    // const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    // ////////// BLOCK_COL_TILES <= M_TILES ////////////\n    // const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    auto C_LAYOUT = wmma::mem_col_major;\n    const int CHUNK_K = 64;\n    const int CHUNK_K_SPARSE = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K_SPARSE = (int((CHUNK_K * K) * (1 - SPARSITY)));\n    const int WARP_COPY_BYTES = (WARP_SIZE * sizeof(int4));\n    const int CHUNK_LINE_BYTES_A (BLOCK_COL_TILES * M * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_A = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_A);\n    const int CHUNK_COPY_LINE_LANES_A = (CHUNK_LINE_BYTES_A / sizeof(int4));\n    const int SHARED_OFFSET_A = (BLOCK_COL_TILES * M + SKEW_UINT8);\n\n    const int CHUNK_LINE_BYTES_B = (BLOCK_SIZE_K_SPARSE * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_B = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_B);\n    const int CHUNK_COPY_LINE_LANES_B = (CHUNK_LINE_BYTES_B / sizeof(int4));\n    const int SHARED_OFFSET_B = (BLOCK_SIZE_K_SPARSE + SKEW_UINT8);\n    const int SHARED_TO_GLOBAL_BYTES_PER_LINE = ((WARP_COL_TILES * M) * sizeof(int));\n    const int SHARED_TO_GLOBAL_BYTES_PER_WARP = (WARP_SIZE * sizeof(int));\n    const int SHARED_TO_GLOBAL_LINES_PER_WARP = (SHARED_TO_GLOBAL_BYTES_PER_WARP / SHARED_TO_GLOBAL_BYTES_PER_LINE);\n    const int SHARED_TO_GLOBAL_LANES_PER_LINE = (WARP_SIZE / SHARED_TO_GLOBAL_LINES_PER_WARP);\n    const int SHARED_TO_GLOBAL_ITERS = ((WARP_ROW_TILES * N) / SHARED_TO_GLOBAL_LINES_PER_WARP);\n\n    const int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const int WARP_STRIDE = (WARP_COL_TILES * M);\n    const int WARPS_PER_BLOCK = (BLOCK_ROW_WARPS * BLOCK_COL_WARPS);\n    const int THREADS_PER_BLOCK = (WARP_SIZE * WARPS_PER_BLOCK);\n    const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    const int GLOBAL_MEM_STRIDE = M_GLOBAL;\n    const int SHMEM_STRIDE = (M * BLOCK_COL_TILES);\n    const int SHMEM_OFFSET = (M * WARP_COL_TILES);\n    const int BLOCK_SIZE_M = (M * BLOCK_COL_TILES);\n    const int BLOCK_SIZE_N = (N * BLOCK_ROW_TILES);\n\n   extern __shared__ uint8_t shmem[];\n\n\t// Warp and lane identification.\n\tconst unsigned int warpId = threadIdx.x / WARP_SIZE;\n\tconst unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n\n\t// Offset in shared memory from which the B matrix is stored.\n\t// const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\tconst size_t shmem_idx_b_off = BLOCK_SIZE_K_SPARSE * SHARED_OFFSET_A;\n\n\n\t// Each CTA slides along the 128 x 128 tiles from the top left corner of the\n\t// matrix to the right and down, and selects the next tile to compute. Once\n\t// there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n\tconst unsigned int block_tile_i =\n\t\t((block_pos * BLOCK_COL_TILES) / M_TILES) * (BLOCK_ROW_TILES);\n\tconst unsigned int block_tile_j = (block_pos * BLOCK_COL_TILES) % M_TILES;\n\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n\twmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_ROW_TILES]\n\t\t\t\t\t\t\t\t\t\t\t\t\t [WARP_COL_TILES];\n\n    // Load the C matrix tiles into fragments from shared memory.\n\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t\twmma::fill_fragment(c[i][j], 0);\n\t\t}\n\t}\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    //for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n    for(int tile_k_idx_sparse = 0, tile_k_idx = 0; tile_k_idx_sparse < K_GLOBAL_SPARSE; tile_k_idx_sparse += BLOCK_SIZE_K_SPARSE, tile_k_idx += BLOCK_SIZE_K){\n\n\t\tsize_t shmem_idx = \n\t\twarpId < (WARPS_PER_BLOCK / 2)\n\t\t\t? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A\n\t\t\t: (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B + shmem_idx_b_off;\n\n\t\tint4 *lane_ptr = NULL;\n\t\tint *lane_ptr_index = NULL;\n\t\tconst uint8_t *warp_ptr = NULL;\n\n\n\t\tif(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\t//warp_ptr = &A[block_tile_j * M] +\n\t\t\t//\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * M_GLOBAL;\n\t\t\twarp_ptr = &A[block_tile_j * M];\n\t\t\t\n\t\t\tconst int *warp_ptr_index = &B_index[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t\t\t\t\t\t((warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A);\n\n\t\t\tlane_ptr_index = (int *)(warp_ptr_index + tile_k_idx_sparse + (laneId / CHUNK_COPY_LINE_LANES_A));\n\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_A) * SHARED_OFFSET_A;\n\t\t}else{\n\t\t\twarp_ptr = &B[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * K_GLOBAL_SPARSE;\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_sparse +\n\t\t\t\t\t\t\t\t(laneId / CHUNK_COPY_LINE_LANES_B) * K_GLOBAL_SPARSE) +\n\t\t\t\t\t\t\t\t(laneId % CHUNK_COPY_LINE_LANES_B);\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_B) * SHARED_OFFSET_B;\n\t\t}\n\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      // shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n\t  int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n\t  \t? BLOCK_SIZE_K_SPARSE / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A)\n\t\t: BLOCK_SIZE_N / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\n\t  /*\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\t  */\n\n\t  /*\n      int tile_k_idx_A;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n          tile_k_idx_A = *(lane_ptr_index);\n      }\n\t  */\n\n\t  #pragma unroll\n\t  for(int i = 0; i < iter_index; i += 1){\n\t\t  if(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\tint tile_k_idx_A = *(lane_ptr_index);\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_A * M_GLOBAL) + (laneId % CHUNK_COPY_LINE_LANES_A);\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_A)) =\n\t\t\t\t*lane_ptr;\n\t\t\t//warp_ptr = (uint8_t *)((uint8_t *)warp_ptr + M_GLOBAL * (WARPS_PER_BLOCK / 2) *CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tlane_ptr_index = (int *)((int *)lane_ptr_index +  (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A;\n\t\t  }else{\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_B)) =\n\t\t\t\t*lane_ptr;\n\t\t\tlane_ptr = (int4 *)((uint8_t *)lane_ptr + K_GLOBAL_SPARSE * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B;\n\t\t  }\n\t  }\n\n      __syncthreads();\n\n\t#pragma unroll\n      for (int k_step = 0; k_step < CHUNK_K_SPARSE; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::col_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n\t#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i += 1) {\n\t\t\tsize_t shmem_idx_a = (warpId % BLOCK_COL_WARPS) * M * WARP_COL_TILES + (i * M);\n\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_a + k_step * K * SHARED_OFFSET_A;\n\n\t\t\twmma::load_matrix_sync(a[i], tile_ptr, SHARED_OFFSET_A);\n\t\t#pragma unroll\n\t\t\tfor(int j = 0; j < WARP_ROW_TILES; j += 1){\n\t\t\t\tif(i == 0){\n\t\t\t\t\tsize_t shmem_idx_b = shmem_idx_b_off +\n\t\t\t\t\t\t\t\t\t\t\t(warpId / BLOCK_COL_WARPS) * (WARP_ROW_TILES * N) * SHARED_OFFSET_B +\n\t\t\t\t\t\t\t\t\t\t\t(j * N) * SHARED_OFFSET_B;\n\t\t\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_b + k_step * K;\n\t\t\t\t\twmma::load_matrix_sync(b[j], tile_ptr, SHARED_OFFSET_B);\n\t\t\t\t}\n\t\t\t\twmma::mma_sync(c[j][i], a[i], b[j], c[j][i]);\n\t\t\t}\n\n        }\n      }\n\n      __syncthreads();\n    }\n\n    // This pointer is used to access the C and D matrix tiles this warp computes.\n\tint *shmem_warp_tile_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * N * WARP_ROW_TILES * SHMEM_STRIDE +\n\t(warpId % BLOCK_COL_WARPS) * SHMEM_OFFSET;\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t#pragma unroll\n\t\t\tfor(int t = 0; t < c[i][j].num_elements; t += 1){\n\t\t\t\tc[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n\t\t\t}\n\t\t\tint *tile_ptr = shmem_warp_tile_ptr + i * N * SHMEM_STRIDE + j * M;\n\t\t\twmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, C_LAYOUT);\n\t\t}\n\t}\n\n    __syncthreads();\n\n\tint *shmem_warp_stream_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N * SHMEM_STRIDE\n\t\t\t\t\t\t\t\t\t+ (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tconst size_t gmem_idx =\n\t\t(block_tile_i * N + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N) * GLOBAL_MEM_STRIDE +\n\t\tblock_tile_j * M + (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tuint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n\tint *shmem_lane_stream_ptr =\n\t\tshmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * SHMEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\t\n\tuint8_t *dst_gmem_lane_stream_ptr =\n\t\tdst_gmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * GLOBAL_MEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\n\tfor(int i = 0; i < WARP_ROW_TILES * N; i += SHARED_TO_GLOBAL_LINES_PER_WARP){\n\t\t*(dst_gmem_lane_stream_ptr + i * GLOBAL_MEM_STRIDE) = (uint8_t)(*(shmem_lane_stream_ptr + i * SHMEM_STRIDE));\n\t}\n\n\t__syncthreads();\n}\n", "launch_config": {"dimBlock": [256, 1], "dimGrid": [256, 1]}}, "46": {"code": "\n__global__ void compute_gemm_imma(float *input0, float *input1, float *input2, float* input3, float* input4, float*input5, float* input6,\n                                  float *output0) {\n    // keys need to be replaced 4096, 1024, K_GLOBA_VALUEL, 0.75, 64\n\n\t//extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B = reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_index = reinterpret_cast<int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n\tconst int WARP_SIZE = 32;\n    const int M = 16;\n    const int K = 16;\n    const int N = 16;\n    const int WMMA_M = 16;\n    const int WMMA_K = 16;\n    const int WMMA_N = 16;\n    const int M_GLOBAL = 4096;\n    const int K_GLOBAL = 1024;\n    const int N_GLOBAL = 1024;\n    const float SPARSITY = 0.75;\n    const int K_GLOBAL_SPARSE = (int(K_GLOBAL * (1-SPARSITY)));\n    const int M_TILES = (M_GLOBAL / M);\n    const int N_TILES = (N_GLOBAL / N);\n    const int K_TILES = (K_GLOBAL / K);\n    const int BLOCK_ROW_WARPS = 2;\n    const int BLOCK_COL_WARPS = 4;\n    const int WARP_ROW_TILES = 4;\n    const int WARP_COL_TILES = 2;\n    const int SKEW_UINT8 = 16;\n    const int BANK_VAL = 32;\n    const int NUM_BANK = (K_GLOBAL / BANK_VAL);\n    /////////// BLOCK_ROW_TILES <= N_TILES ////////////\n    // const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    // ////////// BLOCK_COL_TILES <= M_TILES ////////////\n    // const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    auto C_LAYOUT = wmma::mem_col_major;\n    const int CHUNK_K = 64;\n    const int CHUNK_K_SPARSE = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K_SPARSE = (int((CHUNK_K * K) * (1 - SPARSITY)));\n    const int WARP_COPY_BYTES = (WARP_SIZE * sizeof(int4));\n    const int CHUNK_LINE_BYTES_A (BLOCK_COL_TILES * M * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_A = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_A);\n    const int CHUNK_COPY_LINE_LANES_A = (CHUNK_LINE_BYTES_A / sizeof(int4));\n    const int SHARED_OFFSET_A = (BLOCK_COL_TILES * M + SKEW_UINT8);\n\n    const int CHUNK_LINE_BYTES_B = (BLOCK_SIZE_K_SPARSE * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_B = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_B);\n    const int CHUNK_COPY_LINE_LANES_B = (CHUNK_LINE_BYTES_B / sizeof(int4));\n    const int SHARED_OFFSET_B = (BLOCK_SIZE_K_SPARSE + SKEW_UINT8);\n    const int SHARED_TO_GLOBAL_BYTES_PER_LINE = ((WARP_COL_TILES * M) * sizeof(int));\n    const int SHARED_TO_GLOBAL_BYTES_PER_WARP = (WARP_SIZE * sizeof(int));\n    const int SHARED_TO_GLOBAL_LINES_PER_WARP = (SHARED_TO_GLOBAL_BYTES_PER_WARP / SHARED_TO_GLOBAL_BYTES_PER_LINE);\n    const int SHARED_TO_GLOBAL_LANES_PER_LINE = (WARP_SIZE / SHARED_TO_GLOBAL_LINES_PER_WARP);\n    const int SHARED_TO_GLOBAL_ITERS = ((WARP_ROW_TILES * N) / SHARED_TO_GLOBAL_LINES_PER_WARP);\n\n    const int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const int WARP_STRIDE = (WARP_COL_TILES * M);\n    const int WARPS_PER_BLOCK = (BLOCK_ROW_WARPS * BLOCK_COL_WARPS);\n    const int THREADS_PER_BLOCK = (WARP_SIZE * WARPS_PER_BLOCK);\n    const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    const int GLOBAL_MEM_STRIDE = M_GLOBAL;\n    const int SHMEM_STRIDE = (M * BLOCK_COL_TILES);\n    const int SHMEM_OFFSET = (M * WARP_COL_TILES);\n    const int BLOCK_SIZE_M = (M * BLOCK_COL_TILES);\n    const int BLOCK_SIZE_N = (N * BLOCK_ROW_TILES);\n\n   extern __shared__ uint8_t shmem[];\n\n\t// Warp and lane identification.\n\tconst unsigned int warpId = threadIdx.x / WARP_SIZE;\n\tconst unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n\n\t// Offset in shared memory from which the B matrix is stored.\n\t// const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\tconst size_t shmem_idx_b_off = BLOCK_SIZE_K_SPARSE * SHARED_OFFSET_A;\n\n\n\t// Each CTA slides along the 128 x 128 tiles from the top left corner of the\n\t// matrix to the right and down, and selects the next tile to compute. Once\n\t// there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n\tconst unsigned int block_tile_i =\n\t\t((block_pos * BLOCK_COL_TILES) / M_TILES) * (BLOCK_ROW_TILES);\n\tconst unsigned int block_tile_j = (block_pos * BLOCK_COL_TILES) % M_TILES;\n\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n\twmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_ROW_TILES]\n\t\t\t\t\t\t\t\t\t\t\t\t\t [WARP_COL_TILES];\n\n    // Load the C matrix tiles into fragments from shared memory.\n\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t\twmma::fill_fragment(c[i][j], 0);\n\t\t}\n\t}\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    //for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n    for(int tile_k_idx_sparse = 0, tile_k_idx = 0; tile_k_idx_sparse < K_GLOBAL_SPARSE; tile_k_idx_sparse += BLOCK_SIZE_K_SPARSE, tile_k_idx += BLOCK_SIZE_K){\n\n\t\tsize_t shmem_idx = \n\t\twarpId < (WARPS_PER_BLOCK / 2)\n\t\t\t? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A\n\t\t\t: (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B + shmem_idx_b_off;\n\n\t\tint4 *lane_ptr = NULL;\n\t\tint *lane_ptr_index = NULL;\n\t\tconst uint8_t *warp_ptr = NULL;\n\n\n\t\tif(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\t//warp_ptr = &A[block_tile_j * M] +\n\t\t\t//\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * M_GLOBAL;\n\t\t\twarp_ptr = &A[block_tile_j * M];\n\t\t\t\n\t\t\tconst int *warp_ptr_index = &B_index[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t\t\t\t\t\t((warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A);\n\n\t\t\tlane_ptr_index = (int *)(warp_ptr_index + tile_k_idx_sparse + (laneId / CHUNK_COPY_LINE_LANES_A));\n\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_A) * SHARED_OFFSET_A;\n\t\t}else{\n\t\t\twarp_ptr = &B[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * K_GLOBAL_SPARSE;\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_sparse +\n\t\t\t\t\t\t\t\t(laneId / CHUNK_COPY_LINE_LANES_B) * K_GLOBAL_SPARSE) +\n\t\t\t\t\t\t\t\t(laneId % CHUNK_COPY_LINE_LANES_B);\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_B) * SHARED_OFFSET_B;\n\t\t}\n\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      // shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n\t  int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n\t  \t? BLOCK_SIZE_K_SPARSE / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A)\n\t\t: BLOCK_SIZE_N / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\n\t  /*\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\t  */\n\n\t  /*\n      int tile_k_idx_A;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n          tile_k_idx_A = *(lane_ptr_index);\n      }\n\t  */\n\n\t  #pragma unroll\n\t  for(int i = 0; i < iter_index; i += 1){\n\t\t  if(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\tint tile_k_idx_A = *(lane_ptr_index);\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_A * M_GLOBAL) + (laneId % CHUNK_COPY_LINE_LANES_A);\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_A)) =\n\t\t\t\t*lane_ptr;\n\t\t\t//warp_ptr = (uint8_t *)((uint8_t *)warp_ptr + M_GLOBAL * (WARPS_PER_BLOCK / 2) *CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tlane_ptr_index = (int *)((int *)lane_ptr_index +  (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A;\n\t\t  }else{\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_B)) =\n\t\t\t\t*lane_ptr;\n\t\t\tlane_ptr = (int4 *)((uint8_t *)lane_ptr + K_GLOBAL_SPARSE * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B;\n\t\t  }\n\t  }\n\n      __syncthreads();\n\n\t#pragma unroll\n      for (int k_step = 0; k_step < CHUNK_K_SPARSE; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::col_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n\t#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i += 1) {\n\t\t\tsize_t shmem_idx_a = (warpId % BLOCK_COL_WARPS) * M * WARP_COL_TILES + (i * M);\n\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_a + k_step * K * SHARED_OFFSET_A;\n\n\t\t\twmma::load_matrix_sync(a[i], tile_ptr, SHARED_OFFSET_A);\n\t\t#pragma unroll\n\t\t\tfor(int j = 0; j < WARP_ROW_TILES; j += 1){\n\t\t\t\tif(i == 0){\n\t\t\t\t\tsize_t shmem_idx_b = shmem_idx_b_off +\n\t\t\t\t\t\t\t\t\t\t\t(warpId / BLOCK_COL_WARPS) * (WARP_ROW_TILES * N) * SHARED_OFFSET_B +\n\t\t\t\t\t\t\t\t\t\t\t(j * N) * SHARED_OFFSET_B;\n\t\t\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_b + k_step * K;\n\t\t\t\t\twmma::load_matrix_sync(b[j], tile_ptr, SHARED_OFFSET_B);\n\t\t\t\t}\n\t\t\t\twmma::mma_sync(c[j][i], a[i], b[j], c[j][i]);\n\t\t\t}\n\n        }\n      }\n\n      __syncthreads();\n    }\n\n    // This pointer is used to access the C and D matrix tiles this warp computes.\n\tint *shmem_warp_tile_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * N * WARP_ROW_TILES * SHMEM_STRIDE +\n\t(warpId % BLOCK_COL_WARPS) * SHMEM_OFFSET;\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t#pragma unroll\n\t\t\tfor(int t = 0; t < c[i][j].num_elements; t += 1){\n\t\t\t\tc[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n\t\t\t}\n\t\t\tint *tile_ptr = shmem_warp_tile_ptr + i * N * SHMEM_STRIDE + j * M;\n\t\t\twmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, C_LAYOUT);\n\t\t}\n\t}\n\n    __syncthreads();\n\n\tint *shmem_warp_stream_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N * SHMEM_STRIDE\n\t\t\t\t\t\t\t\t\t+ (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tconst size_t gmem_idx =\n\t\t(block_tile_i * N + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N) * GLOBAL_MEM_STRIDE +\n\t\tblock_tile_j * M + (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tuint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n\tint *shmem_lane_stream_ptr =\n\t\tshmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * SHMEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\t\n\tuint8_t *dst_gmem_lane_stream_ptr =\n\t\tdst_gmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * GLOBAL_MEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\n\tfor(int i = 0; i < WARP_ROW_TILES * N; i += SHARED_TO_GLOBAL_LINES_PER_WARP){\n\t\t*(dst_gmem_lane_stream_ptr + i * GLOBAL_MEM_STRIDE) = (uint8_t)(*(shmem_lane_stream_ptr + i * SHMEM_STRIDE));\n\t}\n\n\t__syncthreads();\n}\n", "launch_config": {"dimBlock": [256, 1], "dimGrid": [256, 1]}}, "47": {"code": "\n__global__ void compute_gemm_imma(float *input0, float *input1, float *input2, float* input3, float* input4, float*input5, float* input6,\n                                  float *output0) {\n    // keys need to be replaced 4096, 4096, K_GLOBA_VALUEL, 0.75, 64\n\n\t//extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B = reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_index = reinterpret_cast<int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n\tconst int WARP_SIZE = 32;\n    const int M = 16;\n    const int K = 16;\n    const int N = 16;\n    const int WMMA_M = 16;\n    const int WMMA_K = 16;\n    const int WMMA_N = 16;\n    const int M_GLOBAL = 4096;\n    const int K_GLOBAL = 1024;\n    const int N_GLOBAL = 4096;\n    const float SPARSITY = 0.75;\n    const int K_GLOBAL_SPARSE = (int(K_GLOBAL * (1-SPARSITY)));\n    const int M_TILES = (M_GLOBAL / M);\n    const int N_TILES = (N_GLOBAL / N);\n    const int K_TILES = (K_GLOBAL / K);\n    const int BLOCK_ROW_WARPS = 2;\n    const int BLOCK_COL_WARPS = 4;\n    const int WARP_ROW_TILES = 4;\n    const int WARP_COL_TILES = 2;\n    const int SKEW_UINT8 = 16;\n    const int BANK_VAL = 32;\n    const int NUM_BANK = (K_GLOBAL / BANK_VAL);\n    /////////// BLOCK_ROW_TILES <= N_TILES ////////////\n    // const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    // ////////// BLOCK_COL_TILES <= M_TILES ////////////\n    // const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    auto C_LAYOUT = wmma::mem_col_major;\n    const int CHUNK_K = 64;\n    const int CHUNK_K_SPARSE = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K_SPARSE = (int((CHUNK_K * K) * (1 - SPARSITY)));\n    const int WARP_COPY_BYTES = (WARP_SIZE * sizeof(int4));\n    const int CHUNK_LINE_BYTES_A (BLOCK_COL_TILES * M * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_A = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_A);\n    const int CHUNK_COPY_LINE_LANES_A = (CHUNK_LINE_BYTES_A / sizeof(int4));\n    const int SHARED_OFFSET_A = (BLOCK_COL_TILES * M + SKEW_UINT8);\n\n    const int CHUNK_LINE_BYTES_B = (BLOCK_SIZE_K_SPARSE * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_B = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_B);\n    const int CHUNK_COPY_LINE_LANES_B = (CHUNK_LINE_BYTES_B / sizeof(int4));\n    const int SHARED_OFFSET_B = (BLOCK_SIZE_K_SPARSE + SKEW_UINT8);\n    const int SHARED_TO_GLOBAL_BYTES_PER_LINE = ((WARP_COL_TILES * M) * sizeof(int));\n    const int SHARED_TO_GLOBAL_BYTES_PER_WARP = (WARP_SIZE * sizeof(int));\n    const int SHARED_TO_GLOBAL_LINES_PER_WARP = (SHARED_TO_GLOBAL_BYTES_PER_WARP / SHARED_TO_GLOBAL_BYTES_PER_LINE);\n    const int SHARED_TO_GLOBAL_LANES_PER_LINE = (WARP_SIZE / SHARED_TO_GLOBAL_LINES_PER_WARP);\n    const int SHARED_TO_GLOBAL_ITERS = ((WARP_ROW_TILES * N) / SHARED_TO_GLOBAL_LINES_PER_WARP);\n\n    const int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const int WARP_STRIDE = (WARP_COL_TILES * M);\n    const int WARPS_PER_BLOCK = (BLOCK_ROW_WARPS * BLOCK_COL_WARPS);\n    const int THREADS_PER_BLOCK = (WARP_SIZE * WARPS_PER_BLOCK);\n    const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    const int GLOBAL_MEM_STRIDE = M_GLOBAL;\n    const int SHMEM_STRIDE = (M * BLOCK_COL_TILES);\n    const int SHMEM_OFFSET = (M * WARP_COL_TILES);\n    const int BLOCK_SIZE_M = (M * BLOCK_COL_TILES);\n    const int BLOCK_SIZE_N = (N * BLOCK_ROW_TILES);\n\n   extern __shared__ uint8_t shmem[];\n\n\t// Warp and lane identification.\n\tconst unsigned int warpId = threadIdx.x / WARP_SIZE;\n\tconst unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n\n\t// Offset in shared memory from which the B matrix is stored.\n\t// const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\tconst size_t shmem_idx_b_off = BLOCK_SIZE_K_SPARSE * SHARED_OFFSET_A;\n\n\n\t// Each CTA slides along the 128 x 128 tiles from the top left corner of the\n\t// matrix to the right and down, and selects the next tile to compute. Once\n\t// there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n\tconst unsigned int block_tile_i =\n\t\t((block_pos * BLOCK_COL_TILES) / M_TILES) * (BLOCK_ROW_TILES);\n\tconst unsigned int block_tile_j = (block_pos * BLOCK_COL_TILES) % M_TILES;\n\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n\twmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_ROW_TILES]\n\t\t\t\t\t\t\t\t\t\t\t\t\t [WARP_COL_TILES];\n\n    // Load the C matrix tiles into fragments from shared memory.\n\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t\twmma::fill_fragment(c[i][j], 0);\n\t\t}\n\t}\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    //for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n    for(int tile_k_idx_sparse = 0, tile_k_idx = 0; tile_k_idx_sparse < K_GLOBAL_SPARSE; tile_k_idx_sparse += BLOCK_SIZE_K_SPARSE, tile_k_idx += BLOCK_SIZE_K){\n\n\t\tsize_t shmem_idx = \n\t\twarpId < (WARPS_PER_BLOCK / 2)\n\t\t\t? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A\n\t\t\t: (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B + shmem_idx_b_off;\n\n\t\tint4 *lane_ptr = NULL;\n\t\tint *lane_ptr_index = NULL;\n\t\tconst uint8_t *warp_ptr = NULL;\n\n\n\t\tif(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\t//warp_ptr = &A[block_tile_j * M] +\n\t\t\t//\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * M_GLOBAL;\n\t\t\twarp_ptr = &A[block_tile_j * M];\n\t\t\t\n\t\t\tconst int *warp_ptr_index = &B_index[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t\t\t\t\t\t((warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A);\n\n\t\t\tlane_ptr_index = (int *)(warp_ptr_index + tile_k_idx_sparse + (laneId / CHUNK_COPY_LINE_LANES_A));\n\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_A) * SHARED_OFFSET_A;\n\t\t}else{\n\t\t\twarp_ptr = &B[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * K_GLOBAL_SPARSE;\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_sparse +\n\t\t\t\t\t\t\t\t(laneId / CHUNK_COPY_LINE_LANES_B) * K_GLOBAL_SPARSE) +\n\t\t\t\t\t\t\t\t(laneId % CHUNK_COPY_LINE_LANES_B);\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_B) * SHARED_OFFSET_B;\n\t\t}\n\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      // shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n\t  int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n\t  \t? BLOCK_SIZE_K_SPARSE / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A)\n\t\t: BLOCK_SIZE_N / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\n\t  /*\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\t  */\n\n\t  /*\n      int tile_k_idx_A;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n          tile_k_idx_A = *(lane_ptr_index);\n      }\n\t  */\n\n\t  #pragma unroll\n\t  for(int i = 0; i < iter_index; i += 1){\n\t\t  if(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\tint tile_k_idx_A = *(lane_ptr_index);\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_A * M_GLOBAL) + (laneId % CHUNK_COPY_LINE_LANES_A);\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_A)) =\n\t\t\t\t*lane_ptr;\n\t\t\t//warp_ptr = (uint8_t *)((uint8_t *)warp_ptr + M_GLOBAL * (WARPS_PER_BLOCK / 2) *CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tlane_ptr_index = (int *)((int *)lane_ptr_index +  (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A;\n\t\t  }else{\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_B)) =\n\t\t\t\t*lane_ptr;\n\t\t\tlane_ptr = (int4 *)((uint8_t *)lane_ptr + K_GLOBAL_SPARSE * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B;\n\t\t  }\n\t  }\n\n      __syncthreads();\n\n\t#pragma unroll\n      for (int k_step = 0; k_step < CHUNK_K_SPARSE; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::col_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n\t#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i += 1) {\n\t\t\tsize_t shmem_idx_a = (warpId % BLOCK_COL_WARPS) * M * WARP_COL_TILES + (i * M);\n\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_a + k_step * K * SHARED_OFFSET_A;\n\n\t\t\twmma::load_matrix_sync(a[i], tile_ptr, SHARED_OFFSET_A);\n\t\t#pragma unroll\n\t\t\tfor(int j = 0; j < WARP_ROW_TILES; j += 1){\n\t\t\t\tif(i == 0){\n\t\t\t\t\tsize_t shmem_idx_b = shmem_idx_b_off +\n\t\t\t\t\t\t\t\t\t\t\t(warpId / BLOCK_COL_WARPS) * (WARP_ROW_TILES * N) * SHARED_OFFSET_B +\n\t\t\t\t\t\t\t\t\t\t\t(j * N) * SHARED_OFFSET_B;\n\t\t\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_b + k_step * K;\n\t\t\t\t\twmma::load_matrix_sync(b[j], tile_ptr, SHARED_OFFSET_B);\n\t\t\t\t}\n\t\t\t\twmma::mma_sync(c[j][i], a[i], b[j], c[j][i]);\n\t\t\t}\n\n        }\n      }\n\n      __syncthreads();\n    }\n\n    // This pointer is used to access the C and D matrix tiles this warp computes.\n\tint *shmem_warp_tile_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * N * WARP_ROW_TILES * SHMEM_STRIDE +\n\t(warpId % BLOCK_COL_WARPS) * SHMEM_OFFSET;\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t#pragma unroll\n\t\t\tfor(int t = 0; t < c[i][j].num_elements; t += 1){\n\t\t\t\tc[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n\t\t\t}\n\t\t\tint *tile_ptr = shmem_warp_tile_ptr + i * N * SHMEM_STRIDE + j * M;\n\t\t\twmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, C_LAYOUT);\n\t\t}\n\t}\n\n    __syncthreads();\n\n\tint *shmem_warp_stream_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N * SHMEM_STRIDE\n\t\t\t\t\t\t\t\t\t+ (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tconst size_t gmem_idx =\n\t\t(block_tile_i * N + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N) * GLOBAL_MEM_STRIDE +\n\t\tblock_tile_j * M + (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tuint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n\tint *shmem_lane_stream_ptr =\n\t\tshmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * SHMEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\t\n\tuint8_t *dst_gmem_lane_stream_ptr =\n\t\tdst_gmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * GLOBAL_MEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\n\tfor(int i = 0; i < WARP_ROW_TILES * N; i += SHARED_TO_GLOBAL_LINES_PER_WARP){\n\t\t*(dst_gmem_lane_stream_ptr + i * GLOBAL_MEM_STRIDE) = (uint8_t)(*(shmem_lane_stream_ptr + i * SHMEM_STRIDE));\n\t}\n\n\t__syncthreads();\n}\n", "launch_config": {"dimBlock": [1024, 1], "dimGrid": [256, 1]}}, "48": {"code": "\n__global__ void compute_gemm_imma(float *input0, float *input1, float *input2, float* input3, float* input4, float*input5, float* input6,\n                                  float *output0) {\n    // keys need to be replaced 4096, 1024, K_GLOBA_VALUEL, 0.75, 64\n\n\t//extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B = reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_index = reinterpret_cast<int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n\tconst int WARP_SIZE = 32;\n    const int M = 16;\n    const int K = 16;\n    const int N = 16;\n    const int WMMA_M = 16;\n    const int WMMA_K = 16;\n    const int WMMA_N = 16;\n    const int M_GLOBAL = 4096;\n    const int K_GLOBAL = 4096;\n    const int N_GLOBAL = 1024;\n    const float SPARSITY = 0.75;\n    const int K_GLOBAL_SPARSE = (int(K_GLOBAL * (1-SPARSITY)));\n    const int M_TILES = (M_GLOBAL / M);\n    const int N_TILES = (N_GLOBAL / N);\n    const int K_TILES = (K_GLOBAL / K);\n    const int BLOCK_ROW_WARPS = 2;\n    const int BLOCK_COL_WARPS = 4;\n    const int WARP_ROW_TILES = 4;\n    const int WARP_COL_TILES = 2;\n    const int SKEW_UINT8 = 16;\n    const int BANK_VAL = 32;\n    const int NUM_BANK = (K_GLOBAL / BANK_VAL);\n    /////////// BLOCK_ROW_TILES <= N_TILES ////////////\n    // const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    // ////////// BLOCK_COL_TILES <= M_TILES ////////////\n    // const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    auto C_LAYOUT = wmma::mem_col_major;\n    const int CHUNK_K = 64;\n    const int CHUNK_K_SPARSE = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K_SPARSE = (int((CHUNK_K * K) * (1 - SPARSITY)));\n    const int WARP_COPY_BYTES = (WARP_SIZE * sizeof(int4));\n    const int CHUNK_LINE_BYTES_A (BLOCK_COL_TILES * M * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_A = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_A);\n    const int CHUNK_COPY_LINE_LANES_A = (CHUNK_LINE_BYTES_A / sizeof(int4));\n    const int SHARED_OFFSET_A = (BLOCK_COL_TILES * M + SKEW_UINT8);\n\n    const int CHUNK_LINE_BYTES_B = (BLOCK_SIZE_K_SPARSE * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_B = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_B);\n    const int CHUNK_COPY_LINE_LANES_B = (CHUNK_LINE_BYTES_B / sizeof(int4));\n    const int SHARED_OFFSET_B = (BLOCK_SIZE_K_SPARSE + SKEW_UINT8);\n    const int SHARED_TO_GLOBAL_BYTES_PER_LINE = ((WARP_COL_TILES * M) * sizeof(int));\n    const int SHARED_TO_GLOBAL_BYTES_PER_WARP = (WARP_SIZE * sizeof(int));\n    const int SHARED_TO_GLOBAL_LINES_PER_WARP = (SHARED_TO_GLOBAL_BYTES_PER_WARP / SHARED_TO_GLOBAL_BYTES_PER_LINE);\n    const int SHARED_TO_GLOBAL_LANES_PER_LINE = (WARP_SIZE / SHARED_TO_GLOBAL_LINES_PER_WARP);\n    const int SHARED_TO_GLOBAL_ITERS = ((WARP_ROW_TILES * N) / SHARED_TO_GLOBAL_LINES_PER_WARP);\n\n    const int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const int WARP_STRIDE = (WARP_COL_TILES * M);\n    const int WARPS_PER_BLOCK = (BLOCK_ROW_WARPS * BLOCK_COL_WARPS);\n    const int THREADS_PER_BLOCK = (WARP_SIZE * WARPS_PER_BLOCK);\n    const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    const int GLOBAL_MEM_STRIDE = M_GLOBAL;\n    const int SHMEM_STRIDE = (M * BLOCK_COL_TILES);\n    const int SHMEM_OFFSET = (M * WARP_COL_TILES);\n    const int BLOCK_SIZE_M = (M * BLOCK_COL_TILES);\n    const int BLOCK_SIZE_N = (N * BLOCK_ROW_TILES);\n\n   extern __shared__ uint8_t shmem[];\n\n\t// Warp and lane identification.\n\tconst unsigned int warpId = threadIdx.x / WARP_SIZE;\n\tconst unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n\n\t// Offset in shared memory from which the B matrix is stored.\n\t// const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\tconst size_t shmem_idx_b_off = BLOCK_SIZE_K_SPARSE * SHARED_OFFSET_A;\n\n\n\t// Each CTA slides along the 128 x 128 tiles from the top left corner of the\n\t// matrix to the right and down, and selects the next tile to compute. Once\n\t// there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n\tconst unsigned int block_tile_i =\n\t\t((block_pos * BLOCK_COL_TILES) / M_TILES) * (BLOCK_ROW_TILES);\n\tconst unsigned int block_tile_j = (block_pos * BLOCK_COL_TILES) % M_TILES;\n\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n\twmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_ROW_TILES]\n\t\t\t\t\t\t\t\t\t\t\t\t\t [WARP_COL_TILES];\n\n    // Load the C matrix tiles into fragments from shared memory.\n\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t\twmma::fill_fragment(c[i][j], 0);\n\t\t}\n\t}\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    //for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n    for(int tile_k_idx_sparse = 0, tile_k_idx = 0; tile_k_idx_sparse < K_GLOBAL_SPARSE; tile_k_idx_sparse += BLOCK_SIZE_K_SPARSE, tile_k_idx += BLOCK_SIZE_K){\n\n\t\tsize_t shmem_idx = \n\t\twarpId < (WARPS_PER_BLOCK / 2)\n\t\t\t? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A\n\t\t\t: (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B + shmem_idx_b_off;\n\n\t\tint4 *lane_ptr = NULL;\n\t\tint *lane_ptr_index = NULL;\n\t\tconst uint8_t *warp_ptr = NULL;\n\n\n\t\tif(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\t//warp_ptr = &A[block_tile_j * M] +\n\t\t\t//\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * M_GLOBAL;\n\t\t\twarp_ptr = &A[block_tile_j * M];\n\t\t\t\n\t\t\tconst int *warp_ptr_index = &B_index[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t\t\t\t\t\t((warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A);\n\n\t\t\tlane_ptr_index = (int *)(warp_ptr_index + tile_k_idx_sparse + (laneId / CHUNK_COPY_LINE_LANES_A));\n\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_A) * SHARED_OFFSET_A;\n\t\t}else{\n\t\t\twarp_ptr = &B[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * K_GLOBAL_SPARSE;\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_sparse +\n\t\t\t\t\t\t\t\t(laneId / CHUNK_COPY_LINE_LANES_B) * K_GLOBAL_SPARSE) +\n\t\t\t\t\t\t\t\t(laneId % CHUNK_COPY_LINE_LANES_B);\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_B) * SHARED_OFFSET_B;\n\t\t}\n\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      // shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n\t  int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n\t  \t? BLOCK_SIZE_K_SPARSE / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A)\n\t\t: BLOCK_SIZE_N / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\n\t  /*\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\t  */\n\n\t  /*\n      int tile_k_idx_A;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n          tile_k_idx_A = *(lane_ptr_index);\n      }\n\t  */\n\n\t  #pragma unroll\n\t  for(int i = 0; i < iter_index; i += 1){\n\t\t  if(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\tint tile_k_idx_A = *(lane_ptr_index);\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_A * M_GLOBAL) + (laneId % CHUNK_COPY_LINE_LANES_A);\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_A)) =\n\t\t\t\t*lane_ptr;\n\t\t\t//warp_ptr = (uint8_t *)((uint8_t *)warp_ptr + M_GLOBAL * (WARPS_PER_BLOCK / 2) *CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tlane_ptr_index = (int *)((int *)lane_ptr_index +  (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A;\n\t\t  }else{\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_B)) =\n\t\t\t\t*lane_ptr;\n\t\t\tlane_ptr = (int4 *)((uint8_t *)lane_ptr + K_GLOBAL_SPARSE * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B;\n\t\t  }\n\t  }\n\n      __syncthreads();\n\n\t#pragma unroll\n      for (int k_step = 0; k_step < CHUNK_K_SPARSE; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::col_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n\t#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i += 1) {\n\t\t\tsize_t shmem_idx_a = (warpId % BLOCK_COL_WARPS) * M * WARP_COL_TILES + (i * M);\n\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_a + k_step * K * SHARED_OFFSET_A;\n\n\t\t\twmma::load_matrix_sync(a[i], tile_ptr, SHARED_OFFSET_A);\n\t\t#pragma unroll\n\t\t\tfor(int j = 0; j < WARP_ROW_TILES; j += 1){\n\t\t\t\tif(i == 0){\n\t\t\t\t\tsize_t shmem_idx_b = shmem_idx_b_off +\n\t\t\t\t\t\t\t\t\t\t\t(warpId / BLOCK_COL_WARPS) * (WARP_ROW_TILES * N) * SHARED_OFFSET_B +\n\t\t\t\t\t\t\t\t\t\t\t(j * N) * SHARED_OFFSET_B;\n\t\t\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_b + k_step * K;\n\t\t\t\t\twmma::load_matrix_sync(b[j], tile_ptr, SHARED_OFFSET_B);\n\t\t\t\t}\n\t\t\t\twmma::mma_sync(c[j][i], a[i], b[j], c[j][i]);\n\t\t\t}\n\n        }\n      }\n\n      __syncthreads();\n    }\n\n    // This pointer is used to access the C and D matrix tiles this warp computes.\n\tint *shmem_warp_tile_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * N * WARP_ROW_TILES * SHMEM_STRIDE +\n\t(warpId % BLOCK_COL_WARPS) * SHMEM_OFFSET;\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t#pragma unroll\n\t\t\tfor(int t = 0; t < c[i][j].num_elements; t += 1){\n\t\t\t\tc[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n\t\t\t}\n\t\t\tint *tile_ptr = shmem_warp_tile_ptr + i * N * SHMEM_STRIDE + j * M;\n\t\t\twmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, C_LAYOUT);\n\t\t}\n\t}\n\n    __syncthreads();\n\n\tint *shmem_warp_stream_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N * SHMEM_STRIDE\n\t\t\t\t\t\t\t\t\t+ (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tconst size_t gmem_idx =\n\t\t(block_tile_i * N + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N) * GLOBAL_MEM_STRIDE +\n\t\tblock_tile_j * M + (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tuint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n\tint *shmem_lane_stream_ptr =\n\t\tshmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * SHMEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\t\n\tuint8_t *dst_gmem_lane_stream_ptr =\n\t\tdst_gmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * GLOBAL_MEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\n\tfor(int i = 0; i < WARP_ROW_TILES * N; i += SHARED_TO_GLOBAL_LINES_PER_WARP){\n\t\t*(dst_gmem_lane_stream_ptr + i * GLOBAL_MEM_STRIDE) = (uint8_t)(*(shmem_lane_stream_ptr + i * SHMEM_STRIDE));\n\t}\n\n\t__syncthreads();\n}\n", "launch_config": {"dimBlock": [256, 1], "dimGrid": [256, 1]}}, "49": {"code": "\n__global__ void compute_gemm_imma(float *input0, float *input1, float *input2, float* input3, float* input4, float*input5, float* input6,\n                                  float *output0) {\n    // keys need to be replaced 4096, 1024, K_GLOBA_VALUEL, 0.75, 64\n\n\t//extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B = reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_index = reinterpret_cast<int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n\tconst int WARP_SIZE = 32;\n    const int M = 16;\n    const int K = 16;\n    const int N = 16;\n    const int WMMA_M = 16;\n    const int WMMA_K = 16;\n    const int WMMA_N = 16;\n    const int M_GLOBAL = 4096;\n    const int K_GLOBAL = 1024;\n    const int N_GLOBAL = 1024;\n    const float SPARSITY = 0.75;\n    const int K_GLOBAL_SPARSE = (int(K_GLOBAL * (1-SPARSITY)));\n    const int M_TILES = (M_GLOBAL / M);\n    const int N_TILES = (N_GLOBAL / N);\n    const int K_TILES = (K_GLOBAL / K);\n    const int BLOCK_ROW_WARPS = 2;\n    const int BLOCK_COL_WARPS = 4;\n    const int WARP_ROW_TILES = 4;\n    const int WARP_COL_TILES = 2;\n    const int SKEW_UINT8 = 16;\n    const int BANK_VAL = 32;\n    const int NUM_BANK = (K_GLOBAL / BANK_VAL);\n    /////////// BLOCK_ROW_TILES <= N_TILES ////////////\n    // const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    // ////////// BLOCK_COL_TILES <= M_TILES ////////////\n    // const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    auto C_LAYOUT = wmma::mem_col_major;\n    const int CHUNK_K = 64;\n    const int CHUNK_K_SPARSE = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K_SPARSE = (int((CHUNK_K * K) * (1 - SPARSITY)));\n    const int WARP_COPY_BYTES = (WARP_SIZE * sizeof(int4));\n    const int CHUNK_LINE_BYTES_A (BLOCK_COL_TILES * M * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_A = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_A);\n    const int CHUNK_COPY_LINE_LANES_A = (CHUNK_LINE_BYTES_A / sizeof(int4));\n    const int SHARED_OFFSET_A = (BLOCK_COL_TILES * M + SKEW_UINT8);\n\n    const int CHUNK_LINE_BYTES_B = (BLOCK_SIZE_K_SPARSE * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_B = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_B);\n    const int CHUNK_COPY_LINE_LANES_B = (CHUNK_LINE_BYTES_B / sizeof(int4));\n    const int SHARED_OFFSET_B = (BLOCK_SIZE_K_SPARSE + SKEW_UINT8);\n    const int SHARED_TO_GLOBAL_BYTES_PER_LINE = ((WARP_COL_TILES * M) * sizeof(int));\n    const int SHARED_TO_GLOBAL_BYTES_PER_WARP = (WARP_SIZE * sizeof(int));\n    const int SHARED_TO_GLOBAL_LINES_PER_WARP = (SHARED_TO_GLOBAL_BYTES_PER_WARP / SHARED_TO_GLOBAL_BYTES_PER_LINE);\n    const int SHARED_TO_GLOBAL_LANES_PER_LINE = (WARP_SIZE / SHARED_TO_GLOBAL_LINES_PER_WARP);\n    const int SHARED_TO_GLOBAL_ITERS = ((WARP_ROW_TILES * N) / SHARED_TO_GLOBAL_LINES_PER_WARP);\n\n    const int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const int WARP_STRIDE = (WARP_COL_TILES * M);\n    const int WARPS_PER_BLOCK = (BLOCK_ROW_WARPS * BLOCK_COL_WARPS);\n    const int THREADS_PER_BLOCK = (WARP_SIZE * WARPS_PER_BLOCK);\n    const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    const int GLOBAL_MEM_STRIDE = M_GLOBAL;\n    const int SHMEM_STRIDE = (M * BLOCK_COL_TILES);\n    const int SHMEM_OFFSET = (M * WARP_COL_TILES);\n    const int BLOCK_SIZE_M = (M * BLOCK_COL_TILES);\n    const int BLOCK_SIZE_N = (N * BLOCK_ROW_TILES);\n\n   extern __shared__ uint8_t shmem[];\n\n\t// Warp and lane identification.\n\tconst unsigned int warpId = threadIdx.x / WARP_SIZE;\n\tconst unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n\n\t// Offset in shared memory from which the B matrix is stored.\n\t// const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\tconst size_t shmem_idx_b_off = BLOCK_SIZE_K_SPARSE * SHARED_OFFSET_A;\n\n\n\t// Each CTA slides along the 128 x 128 tiles from the top left corner of the\n\t// matrix to the right and down, and selects the next tile to compute. Once\n\t// there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n\tconst unsigned int block_tile_i =\n\t\t((block_pos * BLOCK_COL_TILES) / M_TILES) * (BLOCK_ROW_TILES);\n\tconst unsigned int block_tile_j = (block_pos * BLOCK_COL_TILES) % M_TILES;\n\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n\twmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_ROW_TILES]\n\t\t\t\t\t\t\t\t\t\t\t\t\t [WARP_COL_TILES];\n\n    // Load the C matrix tiles into fragments from shared memory.\n\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t\twmma::fill_fragment(c[i][j], 0);\n\t\t}\n\t}\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    //for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n    for(int tile_k_idx_sparse = 0, tile_k_idx = 0; tile_k_idx_sparse < K_GLOBAL_SPARSE; tile_k_idx_sparse += BLOCK_SIZE_K_SPARSE, tile_k_idx += BLOCK_SIZE_K){\n\n\t\tsize_t shmem_idx = \n\t\twarpId < (WARPS_PER_BLOCK / 2)\n\t\t\t? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A\n\t\t\t: (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B + shmem_idx_b_off;\n\n\t\tint4 *lane_ptr = NULL;\n\t\tint *lane_ptr_index = NULL;\n\t\tconst uint8_t *warp_ptr = NULL;\n\n\n\t\tif(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\t//warp_ptr = &A[block_tile_j * M] +\n\t\t\t//\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * M_GLOBAL;\n\t\t\twarp_ptr = &A[block_tile_j * M];\n\t\t\t\n\t\t\tconst int *warp_ptr_index = &B_index[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t\t\t\t\t\t((warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A);\n\n\t\t\tlane_ptr_index = (int *)(warp_ptr_index + tile_k_idx_sparse + (laneId / CHUNK_COPY_LINE_LANES_A));\n\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_A) * SHARED_OFFSET_A;\n\t\t}else{\n\t\t\twarp_ptr = &B[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * K_GLOBAL_SPARSE;\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_sparse +\n\t\t\t\t\t\t\t\t(laneId / CHUNK_COPY_LINE_LANES_B) * K_GLOBAL_SPARSE) +\n\t\t\t\t\t\t\t\t(laneId % CHUNK_COPY_LINE_LANES_B);\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_B) * SHARED_OFFSET_B;\n\t\t}\n\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      // shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n\t  int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n\t  \t? BLOCK_SIZE_K_SPARSE / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A)\n\t\t: BLOCK_SIZE_N / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\n\t  /*\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\t  */\n\n\t  /*\n      int tile_k_idx_A;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n          tile_k_idx_A = *(lane_ptr_index);\n      }\n\t  */\n\n\t  #pragma unroll\n\t  for(int i = 0; i < iter_index; i += 1){\n\t\t  if(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\tint tile_k_idx_A = *(lane_ptr_index);\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_A * M_GLOBAL) + (laneId % CHUNK_COPY_LINE_LANES_A);\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_A)) =\n\t\t\t\t*lane_ptr;\n\t\t\t//warp_ptr = (uint8_t *)((uint8_t *)warp_ptr + M_GLOBAL * (WARPS_PER_BLOCK / 2) *CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tlane_ptr_index = (int *)((int *)lane_ptr_index +  (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A;\n\t\t  }else{\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_B)) =\n\t\t\t\t*lane_ptr;\n\t\t\tlane_ptr = (int4 *)((uint8_t *)lane_ptr + K_GLOBAL_SPARSE * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B;\n\t\t  }\n\t  }\n\n      __syncthreads();\n\n\t#pragma unroll\n      for (int k_step = 0; k_step < CHUNK_K_SPARSE; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::col_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n\t#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i += 1) {\n\t\t\tsize_t shmem_idx_a = (warpId % BLOCK_COL_WARPS) * M * WARP_COL_TILES + (i * M);\n\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_a + k_step * K * SHARED_OFFSET_A;\n\n\t\t\twmma::load_matrix_sync(a[i], tile_ptr, SHARED_OFFSET_A);\n\t\t#pragma unroll\n\t\t\tfor(int j = 0; j < WARP_ROW_TILES; j += 1){\n\t\t\t\tif(i == 0){\n\t\t\t\t\tsize_t shmem_idx_b = shmem_idx_b_off +\n\t\t\t\t\t\t\t\t\t\t\t(warpId / BLOCK_COL_WARPS) * (WARP_ROW_TILES * N) * SHARED_OFFSET_B +\n\t\t\t\t\t\t\t\t\t\t\t(j * N) * SHARED_OFFSET_B;\n\t\t\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_b + k_step * K;\n\t\t\t\t\twmma::load_matrix_sync(b[j], tile_ptr, SHARED_OFFSET_B);\n\t\t\t\t}\n\t\t\t\twmma::mma_sync(c[j][i], a[i], b[j], c[j][i]);\n\t\t\t}\n\n        }\n      }\n\n      __syncthreads();\n    }\n\n    // This pointer is used to access the C and D matrix tiles this warp computes.\n\tint *shmem_warp_tile_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * N * WARP_ROW_TILES * SHMEM_STRIDE +\n\t(warpId % BLOCK_COL_WARPS) * SHMEM_OFFSET;\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t#pragma unroll\n\t\t\tfor(int t = 0; t < c[i][j].num_elements; t += 1){\n\t\t\t\tc[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n\t\t\t}\n\t\t\tint *tile_ptr = shmem_warp_tile_ptr + i * N * SHMEM_STRIDE + j * M;\n\t\t\twmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, C_LAYOUT);\n\t\t}\n\t}\n\n    __syncthreads();\n\n\tint *shmem_warp_stream_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N * SHMEM_STRIDE\n\t\t\t\t\t\t\t\t\t+ (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tconst size_t gmem_idx =\n\t\t(block_tile_i * N + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N) * GLOBAL_MEM_STRIDE +\n\t\tblock_tile_j * M + (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tuint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n\tint *shmem_lane_stream_ptr =\n\t\tshmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * SHMEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\t\n\tuint8_t *dst_gmem_lane_stream_ptr =\n\t\tdst_gmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * GLOBAL_MEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\n\tfor(int i = 0; i < WARP_ROW_TILES * N; i += SHARED_TO_GLOBAL_LINES_PER_WARP){\n\t\t*(dst_gmem_lane_stream_ptr + i * GLOBAL_MEM_STRIDE) = (uint8_t)(*(shmem_lane_stream_ptr + i * SHMEM_STRIDE));\n\t}\n\n\t__syncthreads();\n}\n", "launch_config": {"dimBlock": [256, 1], "dimGrid": [256, 1]}}, "50": {"code": "\n__global__ void compute_gemm_imma(float *input0, float *input1, float *input2, float* input3, float* input4, float*input5, float* input6,\n                                  float *output0) {\n    // keys need to be replaced 4096, 1024, K_GLOBA_VALUEL, 0.75, 64\n\n\t//extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B = reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_index = reinterpret_cast<int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n\tconst int WARP_SIZE = 32;\n    const int M = 16;\n    const int K = 16;\n    const int N = 16;\n    const int WMMA_M = 16;\n    const int WMMA_K = 16;\n    const int WMMA_N = 16;\n    const int M_GLOBAL = 4096;\n    const int K_GLOBAL = 1024;\n    const int N_GLOBAL = 1024;\n    const float SPARSITY = 0.75;\n    const int K_GLOBAL_SPARSE = (int(K_GLOBAL * (1-SPARSITY)));\n    const int M_TILES = (M_GLOBAL / M);\n    const int N_TILES = (N_GLOBAL / N);\n    const int K_TILES = (K_GLOBAL / K);\n    const int BLOCK_ROW_WARPS = 2;\n    const int BLOCK_COL_WARPS = 4;\n    const int WARP_ROW_TILES = 4;\n    const int WARP_COL_TILES = 2;\n    const int SKEW_UINT8 = 16;\n    const int BANK_VAL = 32;\n    const int NUM_BANK = (K_GLOBAL / BANK_VAL);\n    /////////// BLOCK_ROW_TILES <= N_TILES ////////////\n    // const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    // ////////// BLOCK_COL_TILES <= M_TILES ////////////\n    // const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    auto C_LAYOUT = wmma::mem_col_major;\n    const int CHUNK_K = 64;\n    const int CHUNK_K_SPARSE = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K_SPARSE = (int((CHUNK_K * K) * (1 - SPARSITY)));\n    const int WARP_COPY_BYTES = (WARP_SIZE * sizeof(int4));\n    const int CHUNK_LINE_BYTES_A (BLOCK_COL_TILES * M * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_A = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_A);\n    const int CHUNK_COPY_LINE_LANES_A = (CHUNK_LINE_BYTES_A / sizeof(int4));\n    const int SHARED_OFFSET_A = (BLOCK_COL_TILES * M + SKEW_UINT8);\n\n    const int CHUNK_LINE_BYTES_B = (BLOCK_SIZE_K_SPARSE * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_B = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_B);\n    const int CHUNK_COPY_LINE_LANES_B = (CHUNK_LINE_BYTES_B / sizeof(int4));\n    const int SHARED_OFFSET_B = (BLOCK_SIZE_K_SPARSE + SKEW_UINT8);\n    const int SHARED_TO_GLOBAL_BYTES_PER_LINE = ((WARP_COL_TILES * M) * sizeof(int));\n    const int SHARED_TO_GLOBAL_BYTES_PER_WARP = (WARP_SIZE * sizeof(int));\n    const int SHARED_TO_GLOBAL_LINES_PER_WARP = (SHARED_TO_GLOBAL_BYTES_PER_WARP / SHARED_TO_GLOBAL_BYTES_PER_LINE);\n    const int SHARED_TO_GLOBAL_LANES_PER_LINE = (WARP_SIZE / SHARED_TO_GLOBAL_LINES_PER_WARP);\n    const int SHARED_TO_GLOBAL_ITERS = ((WARP_ROW_TILES * N) / SHARED_TO_GLOBAL_LINES_PER_WARP);\n\n    const int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const int WARP_STRIDE = (WARP_COL_TILES * M);\n    const int WARPS_PER_BLOCK = (BLOCK_ROW_WARPS * BLOCK_COL_WARPS);\n    const int THREADS_PER_BLOCK = (WARP_SIZE * WARPS_PER_BLOCK);\n    const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    const int GLOBAL_MEM_STRIDE = M_GLOBAL;\n    const int SHMEM_STRIDE = (M * BLOCK_COL_TILES);\n    const int SHMEM_OFFSET = (M * WARP_COL_TILES);\n    const int BLOCK_SIZE_M = (M * BLOCK_COL_TILES);\n    const int BLOCK_SIZE_N = (N * BLOCK_ROW_TILES);\n\n   extern __shared__ uint8_t shmem[];\n\n\t// Warp and lane identification.\n\tconst unsigned int warpId = threadIdx.x / WARP_SIZE;\n\tconst unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n\n\t// Offset in shared memory from which the B matrix is stored.\n\t// const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\tconst size_t shmem_idx_b_off = BLOCK_SIZE_K_SPARSE * SHARED_OFFSET_A;\n\n\n\t// Each CTA slides along the 128 x 128 tiles from the top left corner of the\n\t// matrix to the right and down, and selects the next tile to compute. Once\n\t// there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n\tconst unsigned int block_tile_i =\n\t\t((block_pos * BLOCK_COL_TILES) / M_TILES) * (BLOCK_ROW_TILES);\n\tconst unsigned int block_tile_j = (block_pos * BLOCK_COL_TILES) % M_TILES;\n\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n\twmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_ROW_TILES]\n\t\t\t\t\t\t\t\t\t\t\t\t\t [WARP_COL_TILES];\n\n    // Load the C matrix tiles into fragments from shared memory.\n\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t\twmma::fill_fragment(c[i][j], 0);\n\t\t}\n\t}\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    //for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n    for(int tile_k_idx_sparse = 0, tile_k_idx = 0; tile_k_idx_sparse < K_GLOBAL_SPARSE; tile_k_idx_sparse += BLOCK_SIZE_K_SPARSE, tile_k_idx += BLOCK_SIZE_K){\n\n\t\tsize_t shmem_idx = \n\t\twarpId < (WARPS_PER_BLOCK / 2)\n\t\t\t? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A\n\t\t\t: (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B + shmem_idx_b_off;\n\n\t\tint4 *lane_ptr = NULL;\n\t\tint *lane_ptr_index = NULL;\n\t\tconst uint8_t *warp_ptr = NULL;\n\n\n\t\tif(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\t//warp_ptr = &A[block_tile_j * M] +\n\t\t\t//\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * M_GLOBAL;\n\t\t\twarp_ptr = &A[block_tile_j * M];\n\t\t\t\n\t\t\tconst int *warp_ptr_index = &B_index[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t\t\t\t\t\t((warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A);\n\n\t\t\tlane_ptr_index = (int *)(warp_ptr_index + tile_k_idx_sparse + (laneId / CHUNK_COPY_LINE_LANES_A));\n\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_A) * SHARED_OFFSET_A;\n\t\t}else{\n\t\t\twarp_ptr = &B[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * K_GLOBAL_SPARSE;\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_sparse +\n\t\t\t\t\t\t\t\t(laneId / CHUNK_COPY_LINE_LANES_B) * K_GLOBAL_SPARSE) +\n\t\t\t\t\t\t\t\t(laneId % CHUNK_COPY_LINE_LANES_B);\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_B) * SHARED_OFFSET_B;\n\t\t}\n\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      // shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n\t  int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n\t  \t? BLOCK_SIZE_K_SPARSE / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A)\n\t\t: BLOCK_SIZE_N / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\n\t  /*\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\t  */\n\n\t  /*\n      int tile_k_idx_A;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n          tile_k_idx_A = *(lane_ptr_index);\n      }\n\t  */\n\n\t  #pragma unroll\n\t  for(int i = 0; i < iter_index; i += 1){\n\t\t  if(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\tint tile_k_idx_A = *(lane_ptr_index);\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_A * M_GLOBAL) + (laneId % CHUNK_COPY_LINE_LANES_A);\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_A)) =\n\t\t\t\t*lane_ptr;\n\t\t\t//warp_ptr = (uint8_t *)((uint8_t *)warp_ptr + M_GLOBAL * (WARPS_PER_BLOCK / 2) *CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tlane_ptr_index = (int *)((int *)lane_ptr_index +  (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A;\n\t\t  }else{\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_B)) =\n\t\t\t\t*lane_ptr;\n\t\t\tlane_ptr = (int4 *)((uint8_t *)lane_ptr + K_GLOBAL_SPARSE * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B;\n\t\t  }\n\t  }\n\n      __syncthreads();\n\n\t#pragma unroll\n      for (int k_step = 0; k_step < CHUNK_K_SPARSE; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::col_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n\t#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i += 1) {\n\t\t\tsize_t shmem_idx_a = (warpId % BLOCK_COL_WARPS) * M * WARP_COL_TILES + (i * M);\n\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_a + k_step * K * SHARED_OFFSET_A;\n\n\t\t\twmma::load_matrix_sync(a[i], tile_ptr, SHARED_OFFSET_A);\n\t\t#pragma unroll\n\t\t\tfor(int j = 0; j < WARP_ROW_TILES; j += 1){\n\t\t\t\tif(i == 0){\n\t\t\t\t\tsize_t shmem_idx_b = shmem_idx_b_off +\n\t\t\t\t\t\t\t\t\t\t\t(warpId / BLOCK_COL_WARPS) * (WARP_ROW_TILES * N) * SHARED_OFFSET_B +\n\t\t\t\t\t\t\t\t\t\t\t(j * N) * SHARED_OFFSET_B;\n\t\t\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_b + k_step * K;\n\t\t\t\t\twmma::load_matrix_sync(b[j], tile_ptr, SHARED_OFFSET_B);\n\t\t\t\t}\n\t\t\t\twmma::mma_sync(c[j][i], a[i], b[j], c[j][i]);\n\t\t\t}\n\n        }\n      }\n\n      __syncthreads();\n    }\n\n    // This pointer is used to access the C and D matrix tiles this warp computes.\n\tint *shmem_warp_tile_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * N * WARP_ROW_TILES * SHMEM_STRIDE +\n\t(warpId % BLOCK_COL_WARPS) * SHMEM_OFFSET;\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t#pragma unroll\n\t\t\tfor(int t = 0; t < c[i][j].num_elements; t += 1){\n\t\t\t\tc[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n\t\t\t}\n\t\t\tint *tile_ptr = shmem_warp_tile_ptr + i * N * SHMEM_STRIDE + j * M;\n\t\t\twmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, C_LAYOUT);\n\t\t}\n\t}\n\n    __syncthreads();\n\n\tint *shmem_warp_stream_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N * SHMEM_STRIDE\n\t\t\t\t\t\t\t\t\t+ (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tconst size_t gmem_idx =\n\t\t(block_tile_i * N + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N) * GLOBAL_MEM_STRIDE +\n\t\tblock_tile_j * M + (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tuint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n\tint *shmem_lane_stream_ptr =\n\t\tshmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * SHMEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\t\n\tuint8_t *dst_gmem_lane_stream_ptr =\n\t\tdst_gmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * GLOBAL_MEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\n\tfor(int i = 0; i < WARP_ROW_TILES * N; i += SHARED_TO_GLOBAL_LINES_PER_WARP){\n\t\t*(dst_gmem_lane_stream_ptr + i * GLOBAL_MEM_STRIDE) = (uint8_t)(*(shmem_lane_stream_ptr + i * SHMEM_STRIDE));\n\t}\n\n\t__syncthreads();\n}\n", "launch_config": {"dimBlock": [256, 1], "dimGrid": [256, 1]}}, "51": {"code": "\n__global__ void compute_gemm_imma(float *input0, float *input1, float *input2, float* input3, float* input4, float*input5, float* input6,\n                                  float *output0) {\n    // keys need to be replaced 4096, 1024, K_GLOBA_VALUEL, 0.75, 64\n\n\t//extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B = reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_index = reinterpret_cast<int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n\tconst int WARP_SIZE = 32;\n    const int M = 16;\n    const int K = 16;\n    const int N = 16;\n    const int WMMA_M = 16;\n    const int WMMA_K = 16;\n    const int WMMA_N = 16;\n    const int M_GLOBAL = 4096;\n    const int K_GLOBAL = 1024;\n    const int N_GLOBAL = 1024;\n    const float SPARSITY = 0.75;\n    const int K_GLOBAL_SPARSE = (int(K_GLOBAL * (1-SPARSITY)));\n    const int M_TILES = (M_GLOBAL / M);\n    const int N_TILES = (N_GLOBAL / N);\n    const int K_TILES = (K_GLOBAL / K);\n    const int BLOCK_ROW_WARPS = 2;\n    const int BLOCK_COL_WARPS = 4;\n    const int WARP_ROW_TILES = 4;\n    const int WARP_COL_TILES = 2;\n    const int SKEW_UINT8 = 16;\n    const int BANK_VAL = 32;\n    const int NUM_BANK = (K_GLOBAL / BANK_VAL);\n    /////////// BLOCK_ROW_TILES <= N_TILES ////////////\n    // const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    // ////////// BLOCK_COL_TILES <= M_TILES ////////////\n    // const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    auto C_LAYOUT = wmma::mem_col_major;\n    const int CHUNK_K = 64;\n    const int CHUNK_K_SPARSE = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K_SPARSE = (int((CHUNK_K * K) * (1 - SPARSITY)));\n    const int WARP_COPY_BYTES = (WARP_SIZE * sizeof(int4));\n    const int CHUNK_LINE_BYTES_A (BLOCK_COL_TILES * M * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_A = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_A);\n    const int CHUNK_COPY_LINE_LANES_A = (CHUNK_LINE_BYTES_A / sizeof(int4));\n    const int SHARED_OFFSET_A = (BLOCK_COL_TILES * M + SKEW_UINT8);\n\n    const int CHUNK_LINE_BYTES_B = (BLOCK_SIZE_K_SPARSE * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_B = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_B);\n    const int CHUNK_COPY_LINE_LANES_B = (CHUNK_LINE_BYTES_B / sizeof(int4));\n    const int SHARED_OFFSET_B = (BLOCK_SIZE_K_SPARSE + SKEW_UINT8);\n    const int SHARED_TO_GLOBAL_BYTES_PER_LINE = ((WARP_COL_TILES * M) * sizeof(int));\n    const int SHARED_TO_GLOBAL_BYTES_PER_WARP = (WARP_SIZE * sizeof(int));\n    const int SHARED_TO_GLOBAL_LINES_PER_WARP = (SHARED_TO_GLOBAL_BYTES_PER_WARP / SHARED_TO_GLOBAL_BYTES_PER_LINE);\n    const int SHARED_TO_GLOBAL_LANES_PER_LINE = (WARP_SIZE / SHARED_TO_GLOBAL_LINES_PER_WARP);\n    const int SHARED_TO_GLOBAL_ITERS = ((WARP_ROW_TILES * N) / SHARED_TO_GLOBAL_LINES_PER_WARP);\n\n    const int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const int WARP_STRIDE = (WARP_COL_TILES * M);\n    const int WARPS_PER_BLOCK = (BLOCK_ROW_WARPS * BLOCK_COL_WARPS);\n    const int THREADS_PER_BLOCK = (WARP_SIZE * WARPS_PER_BLOCK);\n    const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    const int GLOBAL_MEM_STRIDE = M_GLOBAL;\n    const int SHMEM_STRIDE = (M * BLOCK_COL_TILES);\n    const int SHMEM_OFFSET = (M * WARP_COL_TILES);\n    const int BLOCK_SIZE_M = (M * BLOCK_COL_TILES);\n    const int BLOCK_SIZE_N = (N * BLOCK_ROW_TILES);\n\n   extern __shared__ uint8_t shmem[];\n\n\t// Warp and lane identification.\n\tconst unsigned int warpId = threadIdx.x / WARP_SIZE;\n\tconst unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n\n\t// Offset in shared memory from which the B matrix is stored.\n\t// const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\tconst size_t shmem_idx_b_off = BLOCK_SIZE_K_SPARSE * SHARED_OFFSET_A;\n\n\n\t// Each CTA slides along the 128 x 128 tiles from the top left corner of the\n\t// matrix to the right and down, and selects the next tile to compute. Once\n\t// there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n\tconst unsigned int block_tile_i =\n\t\t((block_pos * BLOCK_COL_TILES) / M_TILES) * (BLOCK_ROW_TILES);\n\tconst unsigned int block_tile_j = (block_pos * BLOCK_COL_TILES) % M_TILES;\n\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n\twmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_ROW_TILES]\n\t\t\t\t\t\t\t\t\t\t\t\t\t [WARP_COL_TILES];\n\n    // Load the C matrix tiles into fragments from shared memory.\n\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t\twmma::fill_fragment(c[i][j], 0);\n\t\t}\n\t}\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    //for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n    for(int tile_k_idx_sparse = 0, tile_k_idx = 0; tile_k_idx_sparse < K_GLOBAL_SPARSE; tile_k_idx_sparse += BLOCK_SIZE_K_SPARSE, tile_k_idx += BLOCK_SIZE_K){\n\n\t\tsize_t shmem_idx = \n\t\twarpId < (WARPS_PER_BLOCK / 2)\n\t\t\t? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A\n\t\t\t: (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B + shmem_idx_b_off;\n\n\t\tint4 *lane_ptr = NULL;\n\t\tint *lane_ptr_index = NULL;\n\t\tconst uint8_t *warp_ptr = NULL;\n\n\n\t\tif(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\t//warp_ptr = &A[block_tile_j * M] +\n\t\t\t//\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * M_GLOBAL;\n\t\t\twarp_ptr = &A[block_tile_j * M];\n\t\t\t\n\t\t\tconst int *warp_ptr_index = &B_index[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t\t\t\t\t\t((warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A);\n\n\t\t\tlane_ptr_index = (int *)(warp_ptr_index + tile_k_idx_sparse + (laneId / CHUNK_COPY_LINE_LANES_A));\n\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_A) * SHARED_OFFSET_A;\n\t\t}else{\n\t\t\twarp_ptr = &B[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * K_GLOBAL_SPARSE;\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_sparse +\n\t\t\t\t\t\t\t\t(laneId / CHUNK_COPY_LINE_LANES_B) * K_GLOBAL_SPARSE) +\n\t\t\t\t\t\t\t\t(laneId % CHUNK_COPY_LINE_LANES_B);\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_B) * SHARED_OFFSET_B;\n\t\t}\n\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      // shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n\t  int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n\t  \t? BLOCK_SIZE_K_SPARSE / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A)\n\t\t: BLOCK_SIZE_N / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\n\t  /*\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\t  */\n\n\t  /*\n      int tile_k_idx_A;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n          tile_k_idx_A = *(lane_ptr_index);\n      }\n\t  */\n\n\t  #pragma unroll\n\t  for(int i = 0; i < iter_index; i += 1){\n\t\t  if(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\tint tile_k_idx_A = *(lane_ptr_index);\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_A * M_GLOBAL) + (laneId % CHUNK_COPY_LINE_LANES_A);\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_A)) =\n\t\t\t\t*lane_ptr;\n\t\t\t//warp_ptr = (uint8_t *)((uint8_t *)warp_ptr + M_GLOBAL * (WARPS_PER_BLOCK / 2) *CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tlane_ptr_index = (int *)((int *)lane_ptr_index +  (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A;\n\t\t  }else{\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_B)) =\n\t\t\t\t*lane_ptr;\n\t\t\tlane_ptr = (int4 *)((uint8_t *)lane_ptr + K_GLOBAL_SPARSE * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B;\n\t\t  }\n\t  }\n\n      __syncthreads();\n\n\t#pragma unroll\n      for (int k_step = 0; k_step < CHUNK_K_SPARSE; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::col_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n\t#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i += 1) {\n\t\t\tsize_t shmem_idx_a = (warpId % BLOCK_COL_WARPS) * M * WARP_COL_TILES + (i * M);\n\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_a + k_step * K * SHARED_OFFSET_A;\n\n\t\t\twmma::load_matrix_sync(a[i], tile_ptr, SHARED_OFFSET_A);\n\t\t#pragma unroll\n\t\t\tfor(int j = 0; j < WARP_ROW_TILES; j += 1){\n\t\t\t\tif(i == 0){\n\t\t\t\t\tsize_t shmem_idx_b = shmem_idx_b_off +\n\t\t\t\t\t\t\t\t\t\t\t(warpId / BLOCK_COL_WARPS) * (WARP_ROW_TILES * N) * SHARED_OFFSET_B +\n\t\t\t\t\t\t\t\t\t\t\t(j * N) * SHARED_OFFSET_B;\n\t\t\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_b + k_step * K;\n\t\t\t\t\twmma::load_matrix_sync(b[j], tile_ptr, SHARED_OFFSET_B);\n\t\t\t\t}\n\t\t\t\twmma::mma_sync(c[j][i], a[i], b[j], c[j][i]);\n\t\t\t}\n\n        }\n      }\n\n      __syncthreads();\n    }\n\n    // This pointer is used to access the C and D matrix tiles this warp computes.\n\tint *shmem_warp_tile_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * N * WARP_ROW_TILES * SHMEM_STRIDE +\n\t(warpId % BLOCK_COL_WARPS) * SHMEM_OFFSET;\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t#pragma unroll\n\t\t\tfor(int t = 0; t < c[i][j].num_elements; t += 1){\n\t\t\t\tc[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n\t\t\t}\n\t\t\tint *tile_ptr = shmem_warp_tile_ptr + i * N * SHMEM_STRIDE + j * M;\n\t\t\twmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, C_LAYOUT);\n\t\t}\n\t}\n\n    __syncthreads();\n\n\tint *shmem_warp_stream_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N * SHMEM_STRIDE\n\t\t\t\t\t\t\t\t\t+ (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tconst size_t gmem_idx =\n\t\t(block_tile_i * N + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N) * GLOBAL_MEM_STRIDE +\n\t\tblock_tile_j * M + (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tuint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n\tint *shmem_lane_stream_ptr =\n\t\tshmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * SHMEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\t\n\tuint8_t *dst_gmem_lane_stream_ptr =\n\t\tdst_gmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * GLOBAL_MEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\n\tfor(int i = 0; i < WARP_ROW_TILES * N; i += SHARED_TO_GLOBAL_LINES_PER_WARP){\n\t\t*(dst_gmem_lane_stream_ptr + i * GLOBAL_MEM_STRIDE) = (uint8_t)(*(shmem_lane_stream_ptr + i * SHMEM_STRIDE));\n\t}\n\n\t__syncthreads();\n}\n", "launch_config": {"dimBlock": [256, 1], "dimGrid": [256, 1]}}, "52": {"code": "\n__global__ void compute_gemm_imma(float *input0, float *input1, float *input2, float* input3, float* input4, float*input5, float* input6,\n                                  float *output0) {\n    // keys need to be replaced 4096, 1024, K_GLOBA_VALUEL, 0.75, 64\n\n\t//extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B = reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_index = reinterpret_cast<int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n\tconst int WARP_SIZE = 32;\n    const int M = 16;\n    const int K = 16;\n    const int N = 16;\n    const int WMMA_M = 16;\n    const int WMMA_K = 16;\n    const int WMMA_N = 16;\n    const int M_GLOBAL = 4096;\n    const int K_GLOBAL = 1024;\n    const int N_GLOBAL = 1024;\n    const float SPARSITY = 0.75;\n    const int K_GLOBAL_SPARSE = (int(K_GLOBAL * (1-SPARSITY)));\n    const int M_TILES = (M_GLOBAL / M);\n    const int N_TILES = (N_GLOBAL / N);\n    const int K_TILES = (K_GLOBAL / K);\n    const int BLOCK_ROW_WARPS = 2;\n    const int BLOCK_COL_WARPS = 4;\n    const int WARP_ROW_TILES = 4;\n    const int WARP_COL_TILES = 2;\n    const int SKEW_UINT8 = 16;\n    const int BANK_VAL = 32;\n    const int NUM_BANK = (K_GLOBAL / BANK_VAL);\n    /////////// BLOCK_ROW_TILES <= N_TILES ////////////\n    // const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    // ////////// BLOCK_COL_TILES <= M_TILES ////////////\n    // const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    auto C_LAYOUT = wmma::mem_col_major;\n    const int CHUNK_K = 64;\n    const int CHUNK_K_SPARSE = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K_SPARSE = (int((CHUNK_K * K) * (1 - SPARSITY)));\n    const int WARP_COPY_BYTES = (WARP_SIZE * sizeof(int4));\n    const int CHUNK_LINE_BYTES_A (BLOCK_COL_TILES * M * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_A = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_A);\n    const int CHUNK_COPY_LINE_LANES_A = (CHUNK_LINE_BYTES_A / sizeof(int4));\n    const int SHARED_OFFSET_A = (BLOCK_COL_TILES * M + SKEW_UINT8);\n\n    const int CHUNK_LINE_BYTES_B = (BLOCK_SIZE_K_SPARSE * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_B = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_B);\n    const int CHUNK_COPY_LINE_LANES_B = (CHUNK_LINE_BYTES_B / sizeof(int4));\n    const int SHARED_OFFSET_B = (BLOCK_SIZE_K_SPARSE + SKEW_UINT8);\n    const int SHARED_TO_GLOBAL_BYTES_PER_LINE = ((WARP_COL_TILES * M) * sizeof(int));\n    const int SHARED_TO_GLOBAL_BYTES_PER_WARP = (WARP_SIZE * sizeof(int));\n    const int SHARED_TO_GLOBAL_LINES_PER_WARP = (SHARED_TO_GLOBAL_BYTES_PER_WARP / SHARED_TO_GLOBAL_BYTES_PER_LINE);\n    const int SHARED_TO_GLOBAL_LANES_PER_LINE = (WARP_SIZE / SHARED_TO_GLOBAL_LINES_PER_WARP);\n    const int SHARED_TO_GLOBAL_ITERS = ((WARP_ROW_TILES * N) / SHARED_TO_GLOBAL_LINES_PER_WARP);\n\n    const int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const int WARP_STRIDE = (WARP_COL_TILES * M);\n    const int WARPS_PER_BLOCK = (BLOCK_ROW_WARPS * BLOCK_COL_WARPS);\n    const int THREADS_PER_BLOCK = (WARP_SIZE * WARPS_PER_BLOCK);\n    const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    const int GLOBAL_MEM_STRIDE = M_GLOBAL;\n    const int SHMEM_STRIDE = (M * BLOCK_COL_TILES);\n    const int SHMEM_OFFSET = (M * WARP_COL_TILES);\n    const int BLOCK_SIZE_M = (M * BLOCK_COL_TILES);\n    const int BLOCK_SIZE_N = (N * BLOCK_ROW_TILES);\n\n   extern __shared__ uint8_t shmem[];\n\n\t// Warp and lane identification.\n\tconst unsigned int warpId = threadIdx.x / WARP_SIZE;\n\tconst unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n\n\t// Offset in shared memory from which the B matrix is stored.\n\t// const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\tconst size_t shmem_idx_b_off = BLOCK_SIZE_K_SPARSE * SHARED_OFFSET_A;\n\n\n\t// Each CTA slides along the 128 x 128 tiles from the top left corner of the\n\t// matrix to the right and down, and selects the next tile to compute. Once\n\t// there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n\tconst unsigned int block_tile_i =\n\t\t((block_pos * BLOCK_COL_TILES) / M_TILES) * (BLOCK_ROW_TILES);\n\tconst unsigned int block_tile_j = (block_pos * BLOCK_COL_TILES) % M_TILES;\n\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n\twmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_ROW_TILES]\n\t\t\t\t\t\t\t\t\t\t\t\t\t [WARP_COL_TILES];\n\n    // Load the C matrix tiles into fragments from shared memory.\n\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t\twmma::fill_fragment(c[i][j], 0);\n\t\t}\n\t}\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    //for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n    for(int tile_k_idx_sparse = 0, tile_k_idx = 0; tile_k_idx_sparse < K_GLOBAL_SPARSE; tile_k_idx_sparse += BLOCK_SIZE_K_SPARSE, tile_k_idx += BLOCK_SIZE_K){\n\n\t\tsize_t shmem_idx = \n\t\twarpId < (WARPS_PER_BLOCK / 2)\n\t\t\t? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A\n\t\t\t: (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B + shmem_idx_b_off;\n\n\t\tint4 *lane_ptr = NULL;\n\t\tint *lane_ptr_index = NULL;\n\t\tconst uint8_t *warp_ptr = NULL;\n\n\n\t\tif(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\t//warp_ptr = &A[block_tile_j * M] +\n\t\t\t//\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * M_GLOBAL;\n\t\t\twarp_ptr = &A[block_tile_j * M];\n\t\t\t\n\t\t\tconst int *warp_ptr_index = &B_index[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t\t\t\t\t\t((warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A);\n\n\t\t\tlane_ptr_index = (int *)(warp_ptr_index + tile_k_idx_sparse + (laneId / CHUNK_COPY_LINE_LANES_A));\n\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_A) * SHARED_OFFSET_A;\n\t\t}else{\n\t\t\twarp_ptr = &B[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * K_GLOBAL_SPARSE;\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_sparse +\n\t\t\t\t\t\t\t\t(laneId / CHUNK_COPY_LINE_LANES_B) * K_GLOBAL_SPARSE) +\n\t\t\t\t\t\t\t\t(laneId % CHUNK_COPY_LINE_LANES_B);\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_B) * SHARED_OFFSET_B;\n\t\t}\n\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      // shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n\t  int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n\t  \t? BLOCK_SIZE_K_SPARSE / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A)\n\t\t: BLOCK_SIZE_N / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\n\t  /*\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\t  */\n\n\t  /*\n      int tile_k_idx_A;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n          tile_k_idx_A = *(lane_ptr_index);\n      }\n\t  */\n\n\t  #pragma unroll\n\t  for(int i = 0; i < iter_index; i += 1){\n\t\t  if(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\tint tile_k_idx_A = *(lane_ptr_index);\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_A * M_GLOBAL) + (laneId % CHUNK_COPY_LINE_LANES_A);\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_A)) =\n\t\t\t\t*lane_ptr;\n\t\t\t//warp_ptr = (uint8_t *)((uint8_t *)warp_ptr + M_GLOBAL * (WARPS_PER_BLOCK / 2) *CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tlane_ptr_index = (int *)((int *)lane_ptr_index +  (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A;\n\t\t  }else{\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_B)) =\n\t\t\t\t*lane_ptr;\n\t\t\tlane_ptr = (int4 *)((uint8_t *)lane_ptr + K_GLOBAL_SPARSE * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B;\n\t\t  }\n\t  }\n\n      __syncthreads();\n\n\t#pragma unroll\n      for (int k_step = 0; k_step < CHUNK_K_SPARSE; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::col_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n\t#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i += 1) {\n\t\t\tsize_t shmem_idx_a = (warpId % BLOCK_COL_WARPS) * M * WARP_COL_TILES + (i * M);\n\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_a + k_step * K * SHARED_OFFSET_A;\n\n\t\t\twmma::load_matrix_sync(a[i], tile_ptr, SHARED_OFFSET_A);\n\t\t#pragma unroll\n\t\t\tfor(int j = 0; j < WARP_ROW_TILES; j += 1){\n\t\t\t\tif(i == 0){\n\t\t\t\t\tsize_t shmem_idx_b = shmem_idx_b_off +\n\t\t\t\t\t\t\t\t\t\t\t(warpId / BLOCK_COL_WARPS) * (WARP_ROW_TILES * N) * SHARED_OFFSET_B +\n\t\t\t\t\t\t\t\t\t\t\t(j * N) * SHARED_OFFSET_B;\n\t\t\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_b + k_step * K;\n\t\t\t\t\twmma::load_matrix_sync(b[j], tile_ptr, SHARED_OFFSET_B);\n\t\t\t\t}\n\t\t\t\twmma::mma_sync(c[j][i], a[i], b[j], c[j][i]);\n\t\t\t}\n\n        }\n      }\n\n      __syncthreads();\n    }\n\n    // This pointer is used to access the C and D matrix tiles this warp computes.\n\tint *shmem_warp_tile_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * N * WARP_ROW_TILES * SHMEM_STRIDE +\n\t(warpId % BLOCK_COL_WARPS) * SHMEM_OFFSET;\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t#pragma unroll\n\t\t\tfor(int t = 0; t < c[i][j].num_elements; t += 1){\n\t\t\t\tc[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n\t\t\t}\n\t\t\tint *tile_ptr = shmem_warp_tile_ptr + i * N * SHMEM_STRIDE + j * M;\n\t\t\twmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, C_LAYOUT);\n\t\t}\n\t}\n\n    __syncthreads();\n\n\tint *shmem_warp_stream_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N * SHMEM_STRIDE\n\t\t\t\t\t\t\t\t\t+ (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tconst size_t gmem_idx =\n\t\t(block_tile_i * N + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N) * GLOBAL_MEM_STRIDE +\n\t\tblock_tile_j * M + (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tuint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n\tint *shmem_lane_stream_ptr =\n\t\tshmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * SHMEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\t\n\tuint8_t *dst_gmem_lane_stream_ptr =\n\t\tdst_gmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * GLOBAL_MEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\n\tfor(int i = 0; i < WARP_ROW_TILES * N; i += SHARED_TO_GLOBAL_LINES_PER_WARP){\n\t\t*(dst_gmem_lane_stream_ptr + i * GLOBAL_MEM_STRIDE) = (uint8_t)(*(shmem_lane_stream_ptr + i * SHMEM_STRIDE));\n\t}\n\n\t__syncthreads();\n}\n", "launch_config": {"dimBlock": [256, 1], "dimGrid": [256, 1]}}, "53": {"code": "\n__global__ void compute_gemm_imma(float *input0, float *input1, float *input2, float* input3, float* input4, float*input5, float* input6,\n                                  float *output0) {\n    // keys need to be replaced 4096, 4096, K_GLOBA_VALUEL, 0.75, 64\n\n\t//extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B = reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_index = reinterpret_cast<int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n\tconst int WARP_SIZE = 32;\n    const int M = 16;\n    const int K = 16;\n    const int N = 16;\n    const int WMMA_M = 16;\n    const int WMMA_K = 16;\n    const int WMMA_N = 16;\n    const int M_GLOBAL = 4096;\n    const int K_GLOBAL = 1024;\n    const int N_GLOBAL = 4096;\n    const float SPARSITY = 0.75;\n    const int K_GLOBAL_SPARSE = (int(K_GLOBAL * (1-SPARSITY)));\n    const int M_TILES = (M_GLOBAL / M);\n    const int N_TILES = (N_GLOBAL / N);\n    const int K_TILES = (K_GLOBAL / K);\n    const int BLOCK_ROW_WARPS = 2;\n    const int BLOCK_COL_WARPS = 4;\n    const int WARP_ROW_TILES = 4;\n    const int WARP_COL_TILES = 2;\n    const int SKEW_UINT8 = 16;\n    const int BANK_VAL = 32;\n    const int NUM_BANK = (K_GLOBAL / BANK_VAL);\n    /////////// BLOCK_ROW_TILES <= N_TILES ////////////\n    // const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    // ////////// BLOCK_COL_TILES <= M_TILES ////////////\n    // const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    auto C_LAYOUT = wmma::mem_col_major;\n    const int CHUNK_K = 64;\n    const int CHUNK_K_SPARSE = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K_SPARSE = (int((CHUNK_K * K) * (1 - SPARSITY)));\n    const int WARP_COPY_BYTES = (WARP_SIZE * sizeof(int4));\n    const int CHUNK_LINE_BYTES_A (BLOCK_COL_TILES * M * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_A = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_A);\n    const int CHUNK_COPY_LINE_LANES_A = (CHUNK_LINE_BYTES_A / sizeof(int4));\n    const int SHARED_OFFSET_A = (BLOCK_COL_TILES * M + SKEW_UINT8);\n\n    const int CHUNK_LINE_BYTES_B = (BLOCK_SIZE_K_SPARSE * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_B = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_B);\n    const int CHUNK_COPY_LINE_LANES_B = (CHUNK_LINE_BYTES_B / sizeof(int4));\n    const int SHARED_OFFSET_B = (BLOCK_SIZE_K_SPARSE + SKEW_UINT8);\n    const int SHARED_TO_GLOBAL_BYTES_PER_LINE = ((WARP_COL_TILES * M) * sizeof(int));\n    const int SHARED_TO_GLOBAL_BYTES_PER_WARP = (WARP_SIZE * sizeof(int));\n    const int SHARED_TO_GLOBAL_LINES_PER_WARP = (SHARED_TO_GLOBAL_BYTES_PER_WARP / SHARED_TO_GLOBAL_BYTES_PER_LINE);\n    const int SHARED_TO_GLOBAL_LANES_PER_LINE = (WARP_SIZE / SHARED_TO_GLOBAL_LINES_PER_WARP);\n    const int SHARED_TO_GLOBAL_ITERS = ((WARP_ROW_TILES * N) / SHARED_TO_GLOBAL_LINES_PER_WARP);\n\n    const int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const int WARP_STRIDE = (WARP_COL_TILES * M);\n    const int WARPS_PER_BLOCK = (BLOCK_ROW_WARPS * BLOCK_COL_WARPS);\n    const int THREADS_PER_BLOCK = (WARP_SIZE * WARPS_PER_BLOCK);\n    const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    const int GLOBAL_MEM_STRIDE = M_GLOBAL;\n    const int SHMEM_STRIDE = (M * BLOCK_COL_TILES);\n    const int SHMEM_OFFSET = (M * WARP_COL_TILES);\n    const int BLOCK_SIZE_M = (M * BLOCK_COL_TILES);\n    const int BLOCK_SIZE_N = (N * BLOCK_ROW_TILES);\n\n   extern __shared__ uint8_t shmem[];\n\n\t// Warp and lane identification.\n\tconst unsigned int warpId = threadIdx.x / WARP_SIZE;\n\tconst unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n\n\t// Offset in shared memory from which the B matrix is stored.\n\t// const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\tconst size_t shmem_idx_b_off = BLOCK_SIZE_K_SPARSE * SHARED_OFFSET_A;\n\n\n\t// Each CTA slides along the 128 x 128 tiles from the top left corner of the\n\t// matrix to the right and down, and selects the next tile to compute. Once\n\t// there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n\tconst unsigned int block_tile_i =\n\t\t((block_pos * BLOCK_COL_TILES) / M_TILES) * (BLOCK_ROW_TILES);\n\tconst unsigned int block_tile_j = (block_pos * BLOCK_COL_TILES) % M_TILES;\n\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n\twmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_ROW_TILES]\n\t\t\t\t\t\t\t\t\t\t\t\t\t [WARP_COL_TILES];\n\n    // Load the C matrix tiles into fragments from shared memory.\n\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t\twmma::fill_fragment(c[i][j], 0);\n\t\t}\n\t}\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    //for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n    for(int tile_k_idx_sparse = 0, tile_k_idx = 0; tile_k_idx_sparse < K_GLOBAL_SPARSE; tile_k_idx_sparse += BLOCK_SIZE_K_SPARSE, tile_k_idx += BLOCK_SIZE_K){\n\n\t\tsize_t shmem_idx = \n\t\twarpId < (WARPS_PER_BLOCK / 2)\n\t\t\t? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A\n\t\t\t: (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B + shmem_idx_b_off;\n\n\t\tint4 *lane_ptr = NULL;\n\t\tint *lane_ptr_index = NULL;\n\t\tconst uint8_t *warp_ptr = NULL;\n\n\n\t\tif(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\t//warp_ptr = &A[block_tile_j * M] +\n\t\t\t//\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * M_GLOBAL;\n\t\t\twarp_ptr = &A[block_tile_j * M];\n\t\t\t\n\t\t\tconst int *warp_ptr_index = &B_index[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t\t\t\t\t\t((warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A);\n\n\t\t\tlane_ptr_index = (int *)(warp_ptr_index + tile_k_idx_sparse + (laneId / CHUNK_COPY_LINE_LANES_A));\n\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_A) * SHARED_OFFSET_A;\n\t\t}else{\n\t\t\twarp_ptr = &B[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * K_GLOBAL_SPARSE;\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_sparse +\n\t\t\t\t\t\t\t\t(laneId / CHUNK_COPY_LINE_LANES_B) * K_GLOBAL_SPARSE) +\n\t\t\t\t\t\t\t\t(laneId % CHUNK_COPY_LINE_LANES_B);\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_B) * SHARED_OFFSET_B;\n\t\t}\n\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      // shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n\t  int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n\t  \t? BLOCK_SIZE_K_SPARSE / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A)\n\t\t: BLOCK_SIZE_N / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\n\t  /*\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\t  */\n\n\t  /*\n      int tile_k_idx_A;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n          tile_k_idx_A = *(lane_ptr_index);\n      }\n\t  */\n\n\t  #pragma unroll\n\t  for(int i = 0; i < iter_index; i += 1){\n\t\t  if(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\tint tile_k_idx_A = *(lane_ptr_index);\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_A * M_GLOBAL) + (laneId % CHUNK_COPY_LINE_LANES_A);\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_A)) =\n\t\t\t\t*lane_ptr;\n\t\t\t//warp_ptr = (uint8_t *)((uint8_t *)warp_ptr + M_GLOBAL * (WARPS_PER_BLOCK / 2) *CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tlane_ptr_index = (int *)((int *)lane_ptr_index +  (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A;\n\t\t  }else{\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_B)) =\n\t\t\t\t*lane_ptr;\n\t\t\tlane_ptr = (int4 *)((uint8_t *)lane_ptr + K_GLOBAL_SPARSE * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B;\n\t\t  }\n\t  }\n\n      __syncthreads();\n\n\t#pragma unroll\n      for (int k_step = 0; k_step < CHUNK_K_SPARSE; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::col_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n\t#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i += 1) {\n\t\t\tsize_t shmem_idx_a = (warpId % BLOCK_COL_WARPS) * M * WARP_COL_TILES + (i * M);\n\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_a + k_step * K * SHARED_OFFSET_A;\n\n\t\t\twmma::load_matrix_sync(a[i], tile_ptr, SHARED_OFFSET_A);\n\t\t#pragma unroll\n\t\t\tfor(int j = 0; j < WARP_ROW_TILES; j += 1){\n\t\t\t\tif(i == 0){\n\t\t\t\t\tsize_t shmem_idx_b = shmem_idx_b_off +\n\t\t\t\t\t\t\t\t\t\t\t(warpId / BLOCK_COL_WARPS) * (WARP_ROW_TILES * N) * SHARED_OFFSET_B +\n\t\t\t\t\t\t\t\t\t\t\t(j * N) * SHARED_OFFSET_B;\n\t\t\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_b + k_step * K;\n\t\t\t\t\twmma::load_matrix_sync(b[j], tile_ptr, SHARED_OFFSET_B);\n\t\t\t\t}\n\t\t\t\twmma::mma_sync(c[j][i], a[i], b[j], c[j][i]);\n\t\t\t}\n\n        }\n      }\n\n      __syncthreads();\n    }\n\n    // This pointer is used to access the C and D matrix tiles this warp computes.\n\tint *shmem_warp_tile_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * N * WARP_ROW_TILES * SHMEM_STRIDE +\n\t(warpId % BLOCK_COL_WARPS) * SHMEM_OFFSET;\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t#pragma unroll\n\t\t\tfor(int t = 0; t < c[i][j].num_elements; t += 1){\n\t\t\t\tc[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n\t\t\t}\n\t\t\tint *tile_ptr = shmem_warp_tile_ptr + i * N * SHMEM_STRIDE + j * M;\n\t\t\twmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, C_LAYOUT);\n\t\t}\n\t}\n\n    __syncthreads();\n\n\tint *shmem_warp_stream_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N * SHMEM_STRIDE\n\t\t\t\t\t\t\t\t\t+ (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tconst size_t gmem_idx =\n\t\t(block_tile_i * N + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N) * GLOBAL_MEM_STRIDE +\n\t\tblock_tile_j * M + (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tuint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n\tint *shmem_lane_stream_ptr =\n\t\tshmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * SHMEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\t\n\tuint8_t *dst_gmem_lane_stream_ptr =\n\t\tdst_gmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * GLOBAL_MEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\n\tfor(int i = 0; i < WARP_ROW_TILES * N; i += SHARED_TO_GLOBAL_LINES_PER_WARP){\n\t\t*(dst_gmem_lane_stream_ptr + i * GLOBAL_MEM_STRIDE) = (uint8_t)(*(shmem_lane_stream_ptr + i * SHMEM_STRIDE));\n\t}\n\n\t__syncthreads();\n}\n", "launch_config": {"dimBlock": [1024, 1], "dimGrid": [256, 1]}}, "54": {"code": "\n__global__ void compute_gemm_imma(float *input0, float *input1, float *input2, float* input3, float* input4, float*input5, float* input6,\n                                  float *output0) {\n    // keys need to be replaced 4096, 1024, K_GLOBA_VALUEL, 0.75, 64\n\n\t//extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B = reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_index = reinterpret_cast<int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n\tconst int WARP_SIZE = 32;\n    const int M = 16;\n    const int K = 16;\n    const int N = 16;\n    const int WMMA_M = 16;\n    const int WMMA_K = 16;\n    const int WMMA_N = 16;\n    const int M_GLOBAL = 4096;\n    const int K_GLOBAL = 4096;\n    const int N_GLOBAL = 1024;\n    const float SPARSITY = 0.75;\n    const int K_GLOBAL_SPARSE = (int(K_GLOBAL * (1-SPARSITY)));\n    const int M_TILES = (M_GLOBAL / M);\n    const int N_TILES = (N_GLOBAL / N);\n    const int K_TILES = (K_GLOBAL / K);\n    const int BLOCK_ROW_WARPS = 2;\n    const int BLOCK_COL_WARPS = 4;\n    const int WARP_ROW_TILES = 4;\n    const int WARP_COL_TILES = 2;\n    const int SKEW_UINT8 = 16;\n    const int BANK_VAL = 32;\n    const int NUM_BANK = (K_GLOBAL / BANK_VAL);\n    /////////// BLOCK_ROW_TILES <= N_TILES ////////////\n    // const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    // ////////// BLOCK_COL_TILES <= M_TILES ////////////\n    // const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    auto C_LAYOUT = wmma::mem_col_major;\n    const int CHUNK_K = 64;\n    const int CHUNK_K_SPARSE = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K_SPARSE = (int((CHUNK_K * K) * (1 - SPARSITY)));\n    const int WARP_COPY_BYTES = (WARP_SIZE * sizeof(int4));\n    const int CHUNK_LINE_BYTES_A (BLOCK_COL_TILES * M * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_A = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_A);\n    const int CHUNK_COPY_LINE_LANES_A = (CHUNK_LINE_BYTES_A / sizeof(int4));\n    const int SHARED_OFFSET_A = (BLOCK_COL_TILES * M + SKEW_UINT8);\n\n    const int CHUNK_LINE_BYTES_B = (BLOCK_SIZE_K_SPARSE * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_B = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_B);\n    const int CHUNK_COPY_LINE_LANES_B = (CHUNK_LINE_BYTES_B / sizeof(int4));\n    const int SHARED_OFFSET_B = (BLOCK_SIZE_K_SPARSE + SKEW_UINT8);\n    const int SHARED_TO_GLOBAL_BYTES_PER_LINE = ((WARP_COL_TILES * M) * sizeof(int));\n    const int SHARED_TO_GLOBAL_BYTES_PER_WARP = (WARP_SIZE * sizeof(int));\n    const int SHARED_TO_GLOBAL_LINES_PER_WARP = (SHARED_TO_GLOBAL_BYTES_PER_WARP / SHARED_TO_GLOBAL_BYTES_PER_LINE);\n    const int SHARED_TO_GLOBAL_LANES_PER_LINE = (WARP_SIZE / SHARED_TO_GLOBAL_LINES_PER_WARP);\n    const int SHARED_TO_GLOBAL_ITERS = ((WARP_ROW_TILES * N) / SHARED_TO_GLOBAL_LINES_PER_WARP);\n\n    const int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const int WARP_STRIDE = (WARP_COL_TILES * M);\n    const int WARPS_PER_BLOCK = (BLOCK_ROW_WARPS * BLOCK_COL_WARPS);\n    const int THREADS_PER_BLOCK = (WARP_SIZE * WARPS_PER_BLOCK);\n    const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    const int GLOBAL_MEM_STRIDE = M_GLOBAL;\n    const int SHMEM_STRIDE = (M * BLOCK_COL_TILES);\n    const int SHMEM_OFFSET = (M * WARP_COL_TILES);\n    const int BLOCK_SIZE_M = (M * BLOCK_COL_TILES);\n    const int BLOCK_SIZE_N = (N * BLOCK_ROW_TILES);\n\n   extern __shared__ uint8_t shmem[];\n\n\t// Warp and lane identification.\n\tconst unsigned int warpId = threadIdx.x / WARP_SIZE;\n\tconst unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n\n\t// Offset in shared memory from which the B matrix is stored.\n\t// const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\tconst size_t shmem_idx_b_off = BLOCK_SIZE_K_SPARSE * SHARED_OFFSET_A;\n\n\n\t// Each CTA slides along the 128 x 128 tiles from the top left corner of the\n\t// matrix to the right and down, and selects the next tile to compute. Once\n\t// there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n\tconst unsigned int block_tile_i =\n\t\t((block_pos * BLOCK_COL_TILES) / M_TILES) * (BLOCK_ROW_TILES);\n\tconst unsigned int block_tile_j = (block_pos * BLOCK_COL_TILES) % M_TILES;\n\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n\twmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_ROW_TILES]\n\t\t\t\t\t\t\t\t\t\t\t\t\t [WARP_COL_TILES];\n\n    // Load the C matrix tiles into fragments from shared memory.\n\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t\twmma::fill_fragment(c[i][j], 0);\n\t\t}\n\t}\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    //for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n    for(int tile_k_idx_sparse = 0, tile_k_idx = 0; tile_k_idx_sparse < K_GLOBAL_SPARSE; tile_k_idx_sparse += BLOCK_SIZE_K_SPARSE, tile_k_idx += BLOCK_SIZE_K){\n\n\t\tsize_t shmem_idx = \n\t\twarpId < (WARPS_PER_BLOCK / 2)\n\t\t\t? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A\n\t\t\t: (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B + shmem_idx_b_off;\n\n\t\tint4 *lane_ptr = NULL;\n\t\tint *lane_ptr_index = NULL;\n\t\tconst uint8_t *warp_ptr = NULL;\n\n\n\t\tif(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\t//warp_ptr = &A[block_tile_j * M] +\n\t\t\t//\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * M_GLOBAL;\n\t\t\twarp_ptr = &A[block_tile_j * M];\n\t\t\t\n\t\t\tconst int *warp_ptr_index = &B_index[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t\t\t\t\t\t((warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A);\n\n\t\t\tlane_ptr_index = (int *)(warp_ptr_index + tile_k_idx_sparse + (laneId / CHUNK_COPY_LINE_LANES_A));\n\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_A) * SHARED_OFFSET_A;\n\t\t}else{\n\t\t\twarp_ptr = &B[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * K_GLOBAL_SPARSE;\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_sparse +\n\t\t\t\t\t\t\t\t(laneId / CHUNK_COPY_LINE_LANES_B) * K_GLOBAL_SPARSE) +\n\t\t\t\t\t\t\t\t(laneId % CHUNK_COPY_LINE_LANES_B);\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_B) * SHARED_OFFSET_B;\n\t\t}\n\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      // shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n\t  int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n\t  \t? BLOCK_SIZE_K_SPARSE / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A)\n\t\t: BLOCK_SIZE_N / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\n\t  /*\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\t  */\n\n\t  /*\n      int tile_k_idx_A;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n          tile_k_idx_A = *(lane_ptr_index);\n      }\n\t  */\n\n\t  #pragma unroll\n\t  for(int i = 0; i < iter_index; i += 1){\n\t\t  if(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\tint tile_k_idx_A = *(lane_ptr_index);\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_A * M_GLOBAL) + (laneId % CHUNK_COPY_LINE_LANES_A);\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_A)) =\n\t\t\t\t*lane_ptr;\n\t\t\t//warp_ptr = (uint8_t *)((uint8_t *)warp_ptr + M_GLOBAL * (WARPS_PER_BLOCK / 2) *CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tlane_ptr_index = (int *)((int *)lane_ptr_index +  (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A;\n\t\t  }else{\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_B)) =\n\t\t\t\t*lane_ptr;\n\t\t\tlane_ptr = (int4 *)((uint8_t *)lane_ptr + K_GLOBAL_SPARSE * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B;\n\t\t  }\n\t  }\n\n      __syncthreads();\n\n\t#pragma unroll\n      for (int k_step = 0; k_step < CHUNK_K_SPARSE; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::col_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n\t#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i += 1) {\n\t\t\tsize_t shmem_idx_a = (warpId % BLOCK_COL_WARPS) * M * WARP_COL_TILES + (i * M);\n\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_a + k_step * K * SHARED_OFFSET_A;\n\n\t\t\twmma::load_matrix_sync(a[i], tile_ptr, SHARED_OFFSET_A);\n\t\t#pragma unroll\n\t\t\tfor(int j = 0; j < WARP_ROW_TILES; j += 1){\n\t\t\t\tif(i == 0){\n\t\t\t\t\tsize_t shmem_idx_b = shmem_idx_b_off +\n\t\t\t\t\t\t\t\t\t\t\t(warpId / BLOCK_COL_WARPS) * (WARP_ROW_TILES * N) * SHARED_OFFSET_B +\n\t\t\t\t\t\t\t\t\t\t\t(j * N) * SHARED_OFFSET_B;\n\t\t\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_b + k_step * K;\n\t\t\t\t\twmma::load_matrix_sync(b[j], tile_ptr, SHARED_OFFSET_B);\n\t\t\t\t}\n\t\t\t\twmma::mma_sync(c[j][i], a[i], b[j], c[j][i]);\n\t\t\t}\n\n        }\n      }\n\n      __syncthreads();\n    }\n\n    // This pointer is used to access the C and D matrix tiles this warp computes.\n\tint *shmem_warp_tile_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * N * WARP_ROW_TILES * SHMEM_STRIDE +\n\t(warpId % BLOCK_COL_WARPS) * SHMEM_OFFSET;\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t#pragma unroll\n\t\t\tfor(int t = 0; t < c[i][j].num_elements; t += 1){\n\t\t\t\tc[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n\t\t\t}\n\t\t\tint *tile_ptr = shmem_warp_tile_ptr + i * N * SHMEM_STRIDE + j * M;\n\t\t\twmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, C_LAYOUT);\n\t\t}\n\t}\n\n    __syncthreads();\n\n\tint *shmem_warp_stream_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N * SHMEM_STRIDE\n\t\t\t\t\t\t\t\t\t+ (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tconst size_t gmem_idx =\n\t\t(block_tile_i * N + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N) * GLOBAL_MEM_STRIDE +\n\t\tblock_tile_j * M + (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tuint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n\tint *shmem_lane_stream_ptr =\n\t\tshmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * SHMEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\t\n\tuint8_t *dst_gmem_lane_stream_ptr =\n\t\tdst_gmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * GLOBAL_MEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\n\tfor(int i = 0; i < WARP_ROW_TILES * N; i += SHARED_TO_GLOBAL_LINES_PER_WARP){\n\t\t*(dst_gmem_lane_stream_ptr + i * GLOBAL_MEM_STRIDE) = (uint8_t)(*(shmem_lane_stream_ptr + i * SHMEM_STRIDE));\n\t}\n\n\t__syncthreads();\n}\n", "launch_config": {"dimBlock": [256, 1], "dimGrid": [256, 1]}}, "55": {"code": "\n__global__ void compute_gemm_imma(float *input0, float *input1, float *input2, float* input3, float* input4, float*input5, float* input6,\n                                  float *output0) {\n    // keys need to be replaced 4096, 1024, K_GLOBA_VALUEL, 0.75, 64\n\n\t//extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B = reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_index = reinterpret_cast<int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n\tconst int WARP_SIZE = 32;\n    const int M = 16;\n    const int K = 16;\n    const int N = 16;\n    const int WMMA_M = 16;\n    const int WMMA_K = 16;\n    const int WMMA_N = 16;\n    const int M_GLOBAL = 4096;\n    const int K_GLOBAL = 1024;\n    const int N_GLOBAL = 1024;\n    const float SPARSITY = 0.75;\n    const int K_GLOBAL_SPARSE = (int(K_GLOBAL * (1-SPARSITY)));\n    const int M_TILES = (M_GLOBAL / M);\n    const int N_TILES = (N_GLOBAL / N);\n    const int K_TILES = (K_GLOBAL / K);\n    const int BLOCK_ROW_WARPS = 2;\n    const int BLOCK_COL_WARPS = 4;\n    const int WARP_ROW_TILES = 4;\n    const int WARP_COL_TILES = 2;\n    const int SKEW_UINT8 = 16;\n    const int BANK_VAL = 32;\n    const int NUM_BANK = (K_GLOBAL / BANK_VAL);\n    /////////// BLOCK_ROW_TILES <= N_TILES ////////////\n    // const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    // ////////// BLOCK_COL_TILES <= M_TILES ////////////\n    // const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    auto C_LAYOUT = wmma::mem_col_major;\n    const int CHUNK_K = 64;\n    const int CHUNK_K_SPARSE = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K_SPARSE = (int((CHUNK_K * K) * (1 - SPARSITY)));\n    const int WARP_COPY_BYTES = (WARP_SIZE * sizeof(int4));\n    const int CHUNK_LINE_BYTES_A (BLOCK_COL_TILES * M * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_A = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_A);\n    const int CHUNK_COPY_LINE_LANES_A = (CHUNK_LINE_BYTES_A / sizeof(int4));\n    const int SHARED_OFFSET_A = (BLOCK_COL_TILES * M + SKEW_UINT8);\n\n    const int CHUNK_LINE_BYTES_B = (BLOCK_SIZE_K_SPARSE * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_B = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_B);\n    const int CHUNK_COPY_LINE_LANES_B = (CHUNK_LINE_BYTES_B / sizeof(int4));\n    const int SHARED_OFFSET_B = (BLOCK_SIZE_K_SPARSE + SKEW_UINT8);\n    const int SHARED_TO_GLOBAL_BYTES_PER_LINE = ((WARP_COL_TILES * M) * sizeof(int));\n    const int SHARED_TO_GLOBAL_BYTES_PER_WARP = (WARP_SIZE * sizeof(int));\n    const int SHARED_TO_GLOBAL_LINES_PER_WARP = (SHARED_TO_GLOBAL_BYTES_PER_WARP / SHARED_TO_GLOBAL_BYTES_PER_LINE);\n    const int SHARED_TO_GLOBAL_LANES_PER_LINE = (WARP_SIZE / SHARED_TO_GLOBAL_LINES_PER_WARP);\n    const int SHARED_TO_GLOBAL_ITERS = ((WARP_ROW_TILES * N) / SHARED_TO_GLOBAL_LINES_PER_WARP);\n\n    const int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const int WARP_STRIDE = (WARP_COL_TILES * M);\n    const int WARPS_PER_BLOCK = (BLOCK_ROW_WARPS * BLOCK_COL_WARPS);\n    const int THREADS_PER_BLOCK = (WARP_SIZE * WARPS_PER_BLOCK);\n    const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    const int GLOBAL_MEM_STRIDE = M_GLOBAL;\n    const int SHMEM_STRIDE = (M * BLOCK_COL_TILES);\n    const int SHMEM_OFFSET = (M * WARP_COL_TILES);\n    const int BLOCK_SIZE_M = (M * BLOCK_COL_TILES);\n    const int BLOCK_SIZE_N = (N * BLOCK_ROW_TILES);\n\n   extern __shared__ uint8_t shmem[];\n\n\t// Warp and lane identification.\n\tconst unsigned int warpId = threadIdx.x / WARP_SIZE;\n\tconst unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n\n\t// Offset in shared memory from which the B matrix is stored.\n\t// const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\tconst size_t shmem_idx_b_off = BLOCK_SIZE_K_SPARSE * SHARED_OFFSET_A;\n\n\n\t// Each CTA slides along the 128 x 128 tiles from the top left corner of the\n\t// matrix to the right and down, and selects the next tile to compute. Once\n\t// there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n\tconst unsigned int block_tile_i =\n\t\t((block_pos * BLOCK_COL_TILES) / M_TILES) * (BLOCK_ROW_TILES);\n\tconst unsigned int block_tile_j = (block_pos * BLOCK_COL_TILES) % M_TILES;\n\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n\twmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_ROW_TILES]\n\t\t\t\t\t\t\t\t\t\t\t\t\t [WARP_COL_TILES];\n\n    // Load the C matrix tiles into fragments from shared memory.\n\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t\twmma::fill_fragment(c[i][j], 0);\n\t\t}\n\t}\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    //for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n    for(int tile_k_idx_sparse = 0, tile_k_idx = 0; tile_k_idx_sparse < K_GLOBAL_SPARSE; tile_k_idx_sparse += BLOCK_SIZE_K_SPARSE, tile_k_idx += BLOCK_SIZE_K){\n\n\t\tsize_t shmem_idx = \n\t\twarpId < (WARPS_PER_BLOCK / 2)\n\t\t\t? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A\n\t\t\t: (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B + shmem_idx_b_off;\n\n\t\tint4 *lane_ptr = NULL;\n\t\tint *lane_ptr_index = NULL;\n\t\tconst uint8_t *warp_ptr = NULL;\n\n\n\t\tif(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\t//warp_ptr = &A[block_tile_j * M] +\n\t\t\t//\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * M_GLOBAL;\n\t\t\twarp_ptr = &A[block_tile_j * M];\n\t\t\t\n\t\t\tconst int *warp_ptr_index = &B_index[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t\t\t\t\t\t((warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A);\n\n\t\t\tlane_ptr_index = (int *)(warp_ptr_index + tile_k_idx_sparse + (laneId / CHUNK_COPY_LINE_LANES_A));\n\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_A) * SHARED_OFFSET_A;\n\t\t}else{\n\t\t\twarp_ptr = &B[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * K_GLOBAL_SPARSE;\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_sparse +\n\t\t\t\t\t\t\t\t(laneId / CHUNK_COPY_LINE_LANES_B) * K_GLOBAL_SPARSE) +\n\t\t\t\t\t\t\t\t(laneId % CHUNK_COPY_LINE_LANES_B);\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_B) * SHARED_OFFSET_B;\n\t\t}\n\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      // shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n\t  int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n\t  \t? BLOCK_SIZE_K_SPARSE / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A)\n\t\t: BLOCK_SIZE_N / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\n\t  /*\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\t  */\n\n\t  /*\n      int tile_k_idx_A;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n          tile_k_idx_A = *(lane_ptr_index);\n      }\n\t  */\n\n\t  #pragma unroll\n\t  for(int i = 0; i < iter_index; i += 1){\n\t\t  if(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\tint tile_k_idx_A = *(lane_ptr_index);\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_A * M_GLOBAL) + (laneId % CHUNK_COPY_LINE_LANES_A);\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_A)) =\n\t\t\t\t*lane_ptr;\n\t\t\t//warp_ptr = (uint8_t *)((uint8_t *)warp_ptr + M_GLOBAL * (WARPS_PER_BLOCK / 2) *CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tlane_ptr_index = (int *)((int *)lane_ptr_index +  (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A;\n\t\t  }else{\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_B)) =\n\t\t\t\t*lane_ptr;\n\t\t\tlane_ptr = (int4 *)((uint8_t *)lane_ptr + K_GLOBAL_SPARSE * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B;\n\t\t  }\n\t  }\n\n      __syncthreads();\n\n\t#pragma unroll\n      for (int k_step = 0; k_step < CHUNK_K_SPARSE; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::col_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n\t#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i += 1) {\n\t\t\tsize_t shmem_idx_a = (warpId % BLOCK_COL_WARPS) * M * WARP_COL_TILES + (i * M);\n\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_a + k_step * K * SHARED_OFFSET_A;\n\n\t\t\twmma::load_matrix_sync(a[i], tile_ptr, SHARED_OFFSET_A);\n\t\t#pragma unroll\n\t\t\tfor(int j = 0; j < WARP_ROW_TILES; j += 1){\n\t\t\t\tif(i == 0){\n\t\t\t\t\tsize_t shmem_idx_b = shmem_idx_b_off +\n\t\t\t\t\t\t\t\t\t\t\t(warpId / BLOCK_COL_WARPS) * (WARP_ROW_TILES * N) * SHARED_OFFSET_B +\n\t\t\t\t\t\t\t\t\t\t\t(j * N) * SHARED_OFFSET_B;\n\t\t\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_b + k_step * K;\n\t\t\t\t\twmma::load_matrix_sync(b[j], tile_ptr, SHARED_OFFSET_B);\n\t\t\t\t}\n\t\t\t\twmma::mma_sync(c[j][i], a[i], b[j], c[j][i]);\n\t\t\t}\n\n        }\n      }\n\n      __syncthreads();\n    }\n\n    // This pointer is used to access the C and D matrix tiles this warp computes.\n\tint *shmem_warp_tile_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * N * WARP_ROW_TILES * SHMEM_STRIDE +\n\t(warpId % BLOCK_COL_WARPS) * SHMEM_OFFSET;\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t#pragma unroll\n\t\t\tfor(int t = 0; t < c[i][j].num_elements; t += 1){\n\t\t\t\tc[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n\t\t\t}\n\t\t\tint *tile_ptr = shmem_warp_tile_ptr + i * N * SHMEM_STRIDE + j * M;\n\t\t\twmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, C_LAYOUT);\n\t\t}\n\t}\n\n    __syncthreads();\n\n\tint *shmem_warp_stream_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N * SHMEM_STRIDE\n\t\t\t\t\t\t\t\t\t+ (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tconst size_t gmem_idx =\n\t\t(block_tile_i * N + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N) * GLOBAL_MEM_STRIDE +\n\t\tblock_tile_j * M + (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tuint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n\tint *shmem_lane_stream_ptr =\n\t\tshmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * SHMEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\t\n\tuint8_t *dst_gmem_lane_stream_ptr =\n\t\tdst_gmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * GLOBAL_MEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\n\tfor(int i = 0; i < WARP_ROW_TILES * N; i += SHARED_TO_GLOBAL_LINES_PER_WARP){\n\t\t*(dst_gmem_lane_stream_ptr + i * GLOBAL_MEM_STRIDE) = (uint8_t)(*(shmem_lane_stream_ptr + i * SHMEM_STRIDE));\n\t}\n\n\t__syncthreads();\n}\n", "launch_config": {"dimBlock": [256, 1], "dimGrid": [256, 1]}}, "56": {"code": "\n__global__ void compute_gemm_imma(float *input0, float *input1, float *input2, float* input3, float* input4, float*input5, float* input6,\n                                  float *output0) {\n    // keys need to be replaced 4096, 1024, K_GLOBA_VALUEL, 0.75, 64\n\n\t//extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B = reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_index = reinterpret_cast<int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n\tconst int WARP_SIZE = 32;\n    const int M = 16;\n    const int K = 16;\n    const int N = 16;\n    const int WMMA_M = 16;\n    const int WMMA_K = 16;\n    const int WMMA_N = 16;\n    const int M_GLOBAL = 4096;\n    const int K_GLOBAL = 1024;\n    const int N_GLOBAL = 1024;\n    const float SPARSITY = 0.75;\n    const int K_GLOBAL_SPARSE = (int(K_GLOBAL * (1-SPARSITY)));\n    const int M_TILES = (M_GLOBAL / M);\n    const int N_TILES = (N_GLOBAL / N);\n    const int K_TILES = (K_GLOBAL / K);\n    const int BLOCK_ROW_WARPS = 2;\n    const int BLOCK_COL_WARPS = 4;\n    const int WARP_ROW_TILES = 4;\n    const int WARP_COL_TILES = 2;\n    const int SKEW_UINT8 = 16;\n    const int BANK_VAL = 32;\n    const int NUM_BANK = (K_GLOBAL / BANK_VAL);\n    /////////// BLOCK_ROW_TILES <= N_TILES ////////////\n    // const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    // ////////// BLOCK_COL_TILES <= M_TILES ////////////\n    // const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    auto C_LAYOUT = wmma::mem_col_major;\n    const int CHUNK_K = 64;\n    const int CHUNK_K_SPARSE = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K_SPARSE = (int((CHUNK_K * K) * (1 - SPARSITY)));\n    const int WARP_COPY_BYTES = (WARP_SIZE * sizeof(int4));\n    const int CHUNK_LINE_BYTES_A (BLOCK_COL_TILES * M * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_A = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_A);\n    const int CHUNK_COPY_LINE_LANES_A = (CHUNK_LINE_BYTES_A / sizeof(int4));\n    const int SHARED_OFFSET_A = (BLOCK_COL_TILES * M + SKEW_UINT8);\n\n    const int CHUNK_LINE_BYTES_B = (BLOCK_SIZE_K_SPARSE * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_B = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_B);\n    const int CHUNK_COPY_LINE_LANES_B = (CHUNK_LINE_BYTES_B / sizeof(int4));\n    const int SHARED_OFFSET_B = (BLOCK_SIZE_K_SPARSE + SKEW_UINT8);\n    const int SHARED_TO_GLOBAL_BYTES_PER_LINE = ((WARP_COL_TILES * M) * sizeof(int));\n    const int SHARED_TO_GLOBAL_BYTES_PER_WARP = (WARP_SIZE * sizeof(int));\n    const int SHARED_TO_GLOBAL_LINES_PER_WARP = (SHARED_TO_GLOBAL_BYTES_PER_WARP / SHARED_TO_GLOBAL_BYTES_PER_LINE);\n    const int SHARED_TO_GLOBAL_LANES_PER_LINE = (WARP_SIZE / SHARED_TO_GLOBAL_LINES_PER_WARP);\n    const int SHARED_TO_GLOBAL_ITERS = ((WARP_ROW_TILES * N) / SHARED_TO_GLOBAL_LINES_PER_WARP);\n\n    const int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const int WARP_STRIDE = (WARP_COL_TILES * M);\n    const int WARPS_PER_BLOCK = (BLOCK_ROW_WARPS * BLOCK_COL_WARPS);\n    const int THREADS_PER_BLOCK = (WARP_SIZE * WARPS_PER_BLOCK);\n    const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    const int GLOBAL_MEM_STRIDE = M_GLOBAL;\n    const int SHMEM_STRIDE = (M * BLOCK_COL_TILES);\n    const int SHMEM_OFFSET = (M * WARP_COL_TILES);\n    const int BLOCK_SIZE_M = (M * BLOCK_COL_TILES);\n    const int BLOCK_SIZE_N = (N * BLOCK_ROW_TILES);\n\n   extern __shared__ uint8_t shmem[];\n\n\t// Warp and lane identification.\n\tconst unsigned int warpId = threadIdx.x / WARP_SIZE;\n\tconst unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n\n\t// Offset in shared memory from which the B matrix is stored.\n\t// const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\tconst size_t shmem_idx_b_off = BLOCK_SIZE_K_SPARSE * SHARED_OFFSET_A;\n\n\n\t// Each CTA slides along the 128 x 128 tiles from the top left corner of the\n\t// matrix to the right and down, and selects the next tile to compute. Once\n\t// there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n\tconst unsigned int block_tile_i =\n\t\t((block_pos * BLOCK_COL_TILES) / M_TILES) * (BLOCK_ROW_TILES);\n\tconst unsigned int block_tile_j = (block_pos * BLOCK_COL_TILES) % M_TILES;\n\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n\twmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_ROW_TILES]\n\t\t\t\t\t\t\t\t\t\t\t\t\t [WARP_COL_TILES];\n\n    // Load the C matrix tiles into fragments from shared memory.\n\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t\twmma::fill_fragment(c[i][j], 0);\n\t\t}\n\t}\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    //for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n    for(int tile_k_idx_sparse = 0, tile_k_idx = 0; tile_k_idx_sparse < K_GLOBAL_SPARSE; tile_k_idx_sparse += BLOCK_SIZE_K_SPARSE, tile_k_idx += BLOCK_SIZE_K){\n\n\t\tsize_t shmem_idx = \n\t\twarpId < (WARPS_PER_BLOCK / 2)\n\t\t\t? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A\n\t\t\t: (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B + shmem_idx_b_off;\n\n\t\tint4 *lane_ptr = NULL;\n\t\tint *lane_ptr_index = NULL;\n\t\tconst uint8_t *warp_ptr = NULL;\n\n\n\t\tif(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\t//warp_ptr = &A[block_tile_j * M] +\n\t\t\t//\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * M_GLOBAL;\n\t\t\twarp_ptr = &A[block_tile_j * M];\n\t\t\t\n\t\t\tconst int *warp_ptr_index = &B_index[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t\t\t\t\t\t((warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A);\n\n\t\t\tlane_ptr_index = (int *)(warp_ptr_index + tile_k_idx_sparse + (laneId / CHUNK_COPY_LINE_LANES_A));\n\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_A) * SHARED_OFFSET_A;\n\t\t}else{\n\t\t\twarp_ptr = &B[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * K_GLOBAL_SPARSE;\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_sparse +\n\t\t\t\t\t\t\t\t(laneId / CHUNK_COPY_LINE_LANES_B) * K_GLOBAL_SPARSE) +\n\t\t\t\t\t\t\t\t(laneId % CHUNK_COPY_LINE_LANES_B);\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_B) * SHARED_OFFSET_B;\n\t\t}\n\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      // shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n\t  int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n\t  \t? BLOCK_SIZE_K_SPARSE / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A)\n\t\t: BLOCK_SIZE_N / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\n\t  /*\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\t  */\n\n\t  /*\n      int tile_k_idx_A;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n          tile_k_idx_A = *(lane_ptr_index);\n      }\n\t  */\n\n\t  #pragma unroll\n\t  for(int i = 0; i < iter_index; i += 1){\n\t\t  if(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\tint tile_k_idx_A = *(lane_ptr_index);\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_A * M_GLOBAL) + (laneId % CHUNK_COPY_LINE_LANES_A);\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_A)) =\n\t\t\t\t*lane_ptr;\n\t\t\t//warp_ptr = (uint8_t *)((uint8_t *)warp_ptr + M_GLOBAL * (WARPS_PER_BLOCK / 2) *CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tlane_ptr_index = (int *)((int *)lane_ptr_index +  (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A;\n\t\t  }else{\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_B)) =\n\t\t\t\t*lane_ptr;\n\t\t\tlane_ptr = (int4 *)((uint8_t *)lane_ptr + K_GLOBAL_SPARSE * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B;\n\t\t  }\n\t  }\n\n      __syncthreads();\n\n\t#pragma unroll\n      for (int k_step = 0; k_step < CHUNK_K_SPARSE; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::col_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n\t#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i += 1) {\n\t\t\tsize_t shmem_idx_a = (warpId % BLOCK_COL_WARPS) * M * WARP_COL_TILES + (i * M);\n\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_a + k_step * K * SHARED_OFFSET_A;\n\n\t\t\twmma::load_matrix_sync(a[i], tile_ptr, SHARED_OFFSET_A);\n\t\t#pragma unroll\n\t\t\tfor(int j = 0; j < WARP_ROW_TILES; j += 1){\n\t\t\t\tif(i == 0){\n\t\t\t\t\tsize_t shmem_idx_b = shmem_idx_b_off +\n\t\t\t\t\t\t\t\t\t\t\t(warpId / BLOCK_COL_WARPS) * (WARP_ROW_TILES * N) * SHARED_OFFSET_B +\n\t\t\t\t\t\t\t\t\t\t\t(j * N) * SHARED_OFFSET_B;\n\t\t\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_b + k_step * K;\n\t\t\t\t\twmma::load_matrix_sync(b[j], tile_ptr, SHARED_OFFSET_B);\n\t\t\t\t}\n\t\t\t\twmma::mma_sync(c[j][i], a[i], b[j], c[j][i]);\n\t\t\t}\n\n        }\n      }\n\n      __syncthreads();\n    }\n\n    // This pointer is used to access the C and D matrix tiles this warp computes.\n\tint *shmem_warp_tile_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * N * WARP_ROW_TILES * SHMEM_STRIDE +\n\t(warpId % BLOCK_COL_WARPS) * SHMEM_OFFSET;\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t#pragma unroll\n\t\t\tfor(int t = 0; t < c[i][j].num_elements; t += 1){\n\t\t\t\tc[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n\t\t\t}\n\t\t\tint *tile_ptr = shmem_warp_tile_ptr + i * N * SHMEM_STRIDE + j * M;\n\t\t\twmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, C_LAYOUT);\n\t\t}\n\t}\n\n    __syncthreads();\n\n\tint *shmem_warp_stream_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N * SHMEM_STRIDE\n\t\t\t\t\t\t\t\t\t+ (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tconst size_t gmem_idx =\n\t\t(block_tile_i * N + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N) * GLOBAL_MEM_STRIDE +\n\t\tblock_tile_j * M + (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tuint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n\tint *shmem_lane_stream_ptr =\n\t\tshmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * SHMEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\t\n\tuint8_t *dst_gmem_lane_stream_ptr =\n\t\tdst_gmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * GLOBAL_MEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\n\tfor(int i = 0; i < WARP_ROW_TILES * N; i += SHARED_TO_GLOBAL_LINES_PER_WARP){\n\t\t*(dst_gmem_lane_stream_ptr + i * GLOBAL_MEM_STRIDE) = (uint8_t)(*(shmem_lane_stream_ptr + i * SHMEM_STRIDE));\n\t}\n\n\t__syncthreads();\n}\n", "launch_config": {"dimBlock": [256, 1], "dimGrid": [256, 1]}}, "57": {"code": "\n__global__ void compute_gemm_imma(float *input0, float *input1, float *input2, float* input3, float* input4, float*input5, float* input6,\n                                  float *output0) {\n    // keys need to be replaced 4096, 1024, K_GLOBA_VALUEL, 0.75, 64\n\n\t//extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B = reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_index = reinterpret_cast<int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n\tconst int WARP_SIZE = 32;\n    const int M = 16;\n    const int K = 16;\n    const int N = 16;\n    const int WMMA_M = 16;\n    const int WMMA_K = 16;\n    const int WMMA_N = 16;\n    const int M_GLOBAL = 4096;\n    const int K_GLOBAL = 1024;\n    const int N_GLOBAL = 1024;\n    const float SPARSITY = 0.75;\n    const int K_GLOBAL_SPARSE = (int(K_GLOBAL * (1-SPARSITY)));\n    const int M_TILES = (M_GLOBAL / M);\n    const int N_TILES = (N_GLOBAL / N);\n    const int K_TILES = (K_GLOBAL / K);\n    const int BLOCK_ROW_WARPS = 2;\n    const int BLOCK_COL_WARPS = 4;\n    const int WARP_ROW_TILES = 4;\n    const int WARP_COL_TILES = 2;\n    const int SKEW_UINT8 = 16;\n    const int BANK_VAL = 32;\n    const int NUM_BANK = (K_GLOBAL / BANK_VAL);\n    /////////// BLOCK_ROW_TILES <= N_TILES ////////////\n    // const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    // ////////// BLOCK_COL_TILES <= M_TILES ////////////\n    // const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    auto C_LAYOUT = wmma::mem_col_major;\n    const int CHUNK_K = 64;\n    const int CHUNK_K_SPARSE = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K_SPARSE = (int((CHUNK_K * K) * (1 - SPARSITY)));\n    const int WARP_COPY_BYTES = (WARP_SIZE * sizeof(int4));\n    const int CHUNK_LINE_BYTES_A (BLOCK_COL_TILES * M * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_A = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_A);\n    const int CHUNK_COPY_LINE_LANES_A = (CHUNK_LINE_BYTES_A / sizeof(int4));\n    const int SHARED_OFFSET_A = (BLOCK_COL_TILES * M + SKEW_UINT8);\n\n    const int CHUNK_LINE_BYTES_B = (BLOCK_SIZE_K_SPARSE * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_B = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_B);\n    const int CHUNK_COPY_LINE_LANES_B = (CHUNK_LINE_BYTES_B / sizeof(int4));\n    const int SHARED_OFFSET_B = (BLOCK_SIZE_K_SPARSE + SKEW_UINT8);\n    const int SHARED_TO_GLOBAL_BYTES_PER_LINE = ((WARP_COL_TILES * M) * sizeof(int));\n    const int SHARED_TO_GLOBAL_BYTES_PER_WARP = (WARP_SIZE * sizeof(int));\n    const int SHARED_TO_GLOBAL_LINES_PER_WARP = (SHARED_TO_GLOBAL_BYTES_PER_WARP / SHARED_TO_GLOBAL_BYTES_PER_LINE);\n    const int SHARED_TO_GLOBAL_LANES_PER_LINE = (WARP_SIZE / SHARED_TO_GLOBAL_LINES_PER_WARP);\n    const int SHARED_TO_GLOBAL_ITERS = ((WARP_ROW_TILES * N) / SHARED_TO_GLOBAL_LINES_PER_WARP);\n\n    const int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const int WARP_STRIDE = (WARP_COL_TILES * M);\n    const int WARPS_PER_BLOCK = (BLOCK_ROW_WARPS * BLOCK_COL_WARPS);\n    const int THREADS_PER_BLOCK = (WARP_SIZE * WARPS_PER_BLOCK);\n    const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    const int GLOBAL_MEM_STRIDE = M_GLOBAL;\n    const int SHMEM_STRIDE = (M * BLOCK_COL_TILES);\n    const int SHMEM_OFFSET = (M * WARP_COL_TILES);\n    const int BLOCK_SIZE_M = (M * BLOCK_COL_TILES);\n    const int BLOCK_SIZE_N = (N * BLOCK_ROW_TILES);\n\n   extern __shared__ uint8_t shmem[];\n\n\t// Warp and lane identification.\n\tconst unsigned int warpId = threadIdx.x / WARP_SIZE;\n\tconst unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n\n\t// Offset in shared memory from which the B matrix is stored.\n\t// const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\tconst size_t shmem_idx_b_off = BLOCK_SIZE_K_SPARSE * SHARED_OFFSET_A;\n\n\n\t// Each CTA slides along the 128 x 128 tiles from the top left corner of the\n\t// matrix to the right and down, and selects the next tile to compute. Once\n\t// there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n\tconst unsigned int block_tile_i =\n\t\t((block_pos * BLOCK_COL_TILES) / M_TILES) * (BLOCK_ROW_TILES);\n\tconst unsigned int block_tile_j = (block_pos * BLOCK_COL_TILES) % M_TILES;\n\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n\twmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_ROW_TILES]\n\t\t\t\t\t\t\t\t\t\t\t\t\t [WARP_COL_TILES];\n\n    // Load the C matrix tiles into fragments from shared memory.\n\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t\twmma::fill_fragment(c[i][j], 0);\n\t\t}\n\t}\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    //for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n    for(int tile_k_idx_sparse = 0, tile_k_idx = 0; tile_k_idx_sparse < K_GLOBAL_SPARSE; tile_k_idx_sparse += BLOCK_SIZE_K_SPARSE, tile_k_idx += BLOCK_SIZE_K){\n\n\t\tsize_t shmem_idx = \n\t\twarpId < (WARPS_PER_BLOCK / 2)\n\t\t\t? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A\n\t\t\t: (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B + shmem_idx_b_off;\n\n\t\tint4 *lane_ptr = NULL;\n\t\tint *lane_ptr_index = NULL;\n\t\tconst uint8_t *warp_ptr = NULL;\n\n\n\t\tif(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\t//warp_ptr = &A[block_tile_j * M] +\n\t\t\t//\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * M_GLOBAL;\n\t\t\twarp_ptr = &A[block_tile_j * M];\n\t\t\t\n\t\t\tconst int *warp_ptr_index = &B_index[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t\t\t\t\t\t((warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A);\n\n\t\t\tlane_ptr_index = (int *)(warp_ptr_index + tile_k_idx_sparse + (laneId / CHUNK_COPY_LINE_LANES_A));\n\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_A) * SHARED_OFFSET_A;\n\t\t}else{\n\t\t\twarp_ptr = &B[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * K_GLOBAL_SPARSE;\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_sparse +\n\t\t\t\t\t\t\t\t(laneId / CHUNK_COPY_LINE_LANES_B) * K_GLOBAL_SPARSE) +\n\t\t\t\t\t\t\t\t(laneId % CHUNK_COPY_LINE_LANES_B);\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_B) * SHARED_OFFSET_B;\n\t\t}\n\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      // shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n\t  int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n\t  \t? BLOCK_SIZE_K_SPARSE / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A)\n\t\t: BLOCK_SIZE_N / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\n\t  /*\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\t  */\n\n\t  /*\n      int tile_k_idx_A;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n          tile_k_idx_A = *(lane_ptr_index);\n      }\n\t  */\n\n\t  #pragma unroll\n\t  for(int i = 0; i < iter_index; i += 1){\n\t\t  if(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\tint tile_k_idx_A = *(lane_ptr_index);\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_A * M_GLOBAL) + (laneId % CHUNK_COPY_LINE_LANES_A);\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_A)) =\n\t\t\t\t*lane_ptr;\n\t\t\t//warp_ptr = (uint8_t *)((uint8_t *)warp_ptr + M_GLOBAL * (WARPS_PER_BLOCK / 2) *CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tlane_ptr_index = (int *)((int *)lane_ptr_index +  (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A;\n\t\t  }else{\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_B)) =\n\t\t\t\t*lane_ptr;\n\t\t\tlane_ptr = (int4 *)((uint8_t *)lane_ptr + K_GLOBAL_SPARSE * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B;\n\t\t  }\n\t  }\n\n      __syncthreads();\n\n\t#pragma unroll\n      for (int k_step = 0; k_step < CHUNK_K_SPARSE; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::col_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n\t#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i += 1) {\n\t\t\tsize_t shmem_idx_a = (warpId % BLOCK_COL_WARPS) * M * WARP_COL_TILES + (i * M);\n\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_a + k_step * K * SHARED_OFFSET_A;\n\n\t\t\twmma::load_matrix_sync(a[i], tile_ptr, SHARED_OFFSET_A);\n\t\t#pragma unroll\n\t\t\tfor(int j = 0; j < WARP_ROW_TILES; j += 1){\n\t\t\t\tif(i == 0){\n\t\t\t\t\tsize_t shmem_idx_b = shmem_idx_b_off +\n\t\t\t\t\t\t\t\t\t\t\t(warpId / BLOCK_COL_WARPS) * (WARP_ROW_TILES * N) * SHARED_OFFSET_B +\n\t\t\t\t\t\t\t\t\t\t\t(j * N) * SHARED_OFFSET_B;\n\t\t\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_b + k_step * K;\n\t\t\t\t\twmma::load_matrix_sync(b[j], tile_ptr, SHARED_OFFSET_B);\n\t\t\t\t}\n\t\t\t\twmma::mma_sync(c[j][i], a[i], b[j], c[j][i]);\n\t\t\t}\n\n        }\n      }\n\n      __syncthreads();\n    }\n\n    // This pointer is used to access the C and D matrix tiles this warp computes.\n\tint *shmem_warp_tile_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * N * WARP_ROW_TILES * SHMEM_STRIDE +\n\t(warpId % BLOCK_COL_WARPS) * SHMEM_OFFSET;\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t#pragma unroll\n\t\t\tfor(int t = 0; t < c[i][j].num_elements; t += 1){\n\t\t\t\tc[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n\t\t\t}\n\t\t\tint *tile_ptr = shmem_warp_tile_ptr + i * N * SHMEM_STRIDE + j * M;\n\t\t\twmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, C_LAYOUT);\n\t\t}\n\t}\n\n    __syncthreads();\n\n\tint *shmem_warp_stream_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N * SHMEM_STRIDE\n\t\t\t\t\t\t\t\t\t+ (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tconst size_t gmem_idx =\n\t\t(block_tile_i * N + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N) * GLOBAL_MEM_STRIDE +\n\t\tblock_tile_j * M + (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tuint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n\tint *shmem_lane_stream_ptr =\n\t\tshmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * SHMEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\t\n\tuint8_t *dst_gmem_lane_stream_ptr =\n\t\tdst_gmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * GLOBAL_MEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\n\tfor(int i = 0; i < WARP_ROW_TILES * N; i += SHARED_TO_GLOBAL_LINES_PER_WARP){\n\t\t*(dst_gmem_lane_stream_ptr + i * GLOBAL_MEM_STRIDE) = (uint8_t)(*(shmem_lane_stream_ptr + i * SHMEM_STRIDE));\n\t}\n\n\t__syncthreads();\n}\n", "launch_config": {"dimBlock": [256, 1], "dimGrid": [256, 1]}}, "58": {"code": "\n__global__ void compute_gemm_imma(float *input0, float *input1, float *input2, float* input3, float* input4, float*input5, float* input6,\n                                  float *output0) {\n    // keys need to be replaced 4096, 1024, K_GLOBA_VALUEL, 0.75, 64\n\n\t//extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B = reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_index = reinterpret_cast<int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n\tconst int WARP_SIZE = 32;\n    const int M = 16;\n    const int K = 16;\n    const int N = 16;\n    const int WMMA_M = 16;\n    const int WMMA_K = 16;\n    const int WMMA_N = 16;\n    const int M_GLOBAL = 4096;\n    const int K_GLOBAL = 1024;\n    const int N_GLOBAL = 1024;\n    const float SPARSITY = 0.75;\n    const int K_GLOBAL_SPARSE = (int(K_GLOBAL * (1-SPARSITY)));\n    const int M_TILES = (M_GLOBAL / M);\n    const int N_TILES = (N_GLOBAL / N);\n    const int K_TILES = (K_GLOBAL / K);\n    const int BLOCK_ROW_WARPS = 2;\n    const int BLOCK_COL_WARPS = 4;\n    const int WARP_ROW_TILES = 4;\n    const int WARP_COL_TILES = 2;\n    const int SKEW_UINT8 = 16;\n    const int BANK_VAL = 32;\n    const int NUM_BANK = (K_GLOBAL / BANK_VAL);\n    /////////// BLOCK_ROW_TILES <= N_TILES ////////////\n    // const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    // ////////// BLOCK_COL_TILES <= M_TILES ////////////\n    // const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    auto C_LAYOUT = wmma::mem_col_major;\n    const int CHUNK_K = 64;\n    const int CHUNK_K_SPARSE = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K_SPARSE = (int((CHUNK_K * K) * (1 - SPARSITY)));\n    const int WARP_COPY_BYTES = (WARP_SIZE * sizeof(int4));\n    const int CHUNK_LINE_BYTES_A (BLOCK_COL_TILES * M * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_A = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_A);\n    const int CHUNK_COPY_LINE_LANES_A = (CHUNK_LINE_BYTES_A / sizeof(int4));\n    const int SHARED_OFFSET_A = (BLOCK_COL_TILES * M + SKEW_UINT8);\n\n    const int CHUNK_LINE_BYTES_B = (BLOCK_SIZE_K_SPARSE * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_B = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_B);\n    const int CHUNK_COPY_LINE_LANES_B = (CHUNK_LINE_BYTES_B / sizeof(int4));\n    const int SHARED_OFFSET_B = (BLOCK_SIZE_K_SPARSE + SKEW_UINT8);\n    const int SHARED_TO_GLOBAL_BYTES_PER_LINE = ((WARP_COL_TILES * M) * sizeof(int));\n    const int SHARED_TO_GLOBAL_BYTES_PER_WARP = (WARP_SIZE * sizeof(int));\n    const int SHARED_TO_GLOBAL_LINES_PER_WARP = (SHARED_TO_GLOBAL_BYTES_PER_WARP / SHARED_TO_GLOBAL_BYTES_PER_LINE);\n    const int SHARED_TO_GLOBAL_LANES_PER_LINE = (WARP_SIZE / SHARED_TO_GLOBAL_LINES_PER_WARP);\n    const int SHARED_TO_GLOBAL_ITERS = ((WARP_ROW_TILES * N) / SHARED_TO_GLOBAL_LINES_PER_WARP);\n\n    const int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const int WARP_STRIDE = (WARP_COL_TILES * M);\n    const int WARPS_PER_BLOCK = (BLOCK_ROW_WARPS * BLOCK_COL_WARPS);\n    const int THREADS_PER_BLOCK = (WARP_SIZE * WARPS_PER_BLOCK);\n    const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    const int GLOBAL_MEM_STRIDE = M_GLOBAL;\n    const int SHMEM_STRIDE = (M * BLOCK_COL_TILES);\n    const int SHMEM_OFFSET = (M * WARP_COL_TILES);\n    const int BLOCK_SIZE_M = (M * BLOCK_COL_TILES);\n    const int BLOCK_SIZE_N = (N * BLOCK_ROW_TILES);\n\n   extern __shared__ uint8_t shmem[];\n\n\t// Warp and lane identification.\n\tconst unsigned int warpId = threadIdx.x / WARP_SIZE;\n\tconst unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n\n\t// Offset in shared memory from which the B matrix is stored.\n\t// const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\tconst size_t shmem_idx_b_off = BLOCK_SIZE_K_SPARSE * SHARED_OFFSET_A;\n\n\n\t// Each CTA slides along the 128 x 128 tiles from the top left corner of the\n\t// matrix to the right and down, and selects the next tile to compute. Once\n\t// there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n\tconst unsigned int block_tile_i =\n\t\t((block_pos * BLOCK_COL_TILES) / M_TILES) * (BLOCK_ROW_TILES);\n\tconst unsigned int block_tile_j = (block_pos * BLOCK_COL_TILES) % M_TILES;\n\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n\twmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_ROW_TILES]\n\t\t\t\t\t\t\t\t\t\t\t\t\t [WARP_COL_TILES];\n\n    // Load the C matrix tiles into fragments from shared memory.\n\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t\twmma::fill_fragment(c[i][j], 0);\n\t\t}\n\t}\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    //for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n    for(int tile_k_idx_sparse = 0, tile_k_idx = 0; tile_k_idx_sparse < K_GLOBAL_SPARSE; tile_k_idx_sparse += BLOCK_SIZE_K_SPARSE, tile_k_idx += BLOCK_SIZE_K){\n\n\t\tsize_t shmem_idx = \n\t\twarpId < (WARPS_PER_BLOCK / 2)\n\t\t\t? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A\n\t\t\t: (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B + shmem_idx_b_off;\n\n\t\tint4 *lane_ptr = NULL;\n\t\tint *lane_ptr_index = NULL;\n\t\tconst uint8_t *warp_ptr = NULL;\n\n\n\t\tif(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\t//warp_ptr = &A[block_tile_j * M] +\n\t\t\t//\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * M_GLOBAL;\n\t\t\twarp_ptr = &A[block_tile_j * M];\n\t\t\t\n\t\t\tconst int *warp_ptr_index = &B_index[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t\t\t\t\t\t((warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A);\n\n\t\t\tlane_ptr_index = (int *)(warp_ptr_index + tile_k_idx_sparse + (laneId / CHUNK_COPY_LINE_LANES_A));\n\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_A) * SHARED_OFFSET_A;\n\t\t}else{\n\t\t\twarp_ptr = &B[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * K_GLOBAL_SPARSE;\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_sparse +\n\t\t\t\t\t\t\t\t(laneId / CHUNK_COPY_LINE_LANES_B) * K_GLOBAL_SPARSE) +\n\t\t\t\t\t\t\t\t(laneId % CHUNK_COPY_LINE_LANES_B);\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_B) * SHARED_OFFSET_B;\n\t\t}\n\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      // shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n\t  int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n\t  \t? BLOCK_SIZE_K_SPARSE / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A)\n\t\t: BLOCK_SIZE_N / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\n\t  /*\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\t  */\n\n\t  /*\n      int tile_k_idx_A;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n          tile_k_idx_A = *(lane_ptr_index);\n      }\n\t  */\n\n\t  #pragma unroll\n\t  for(int i = 0; i < iter_index; i += 1){\n\t\t  if(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\tint tile_k_idx_A = *(lane_ptr_index);\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_A * M_GLOBAL) + (laneId % CHUNK_COPY_LINE_LANES_A);\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_A)) =\n\t\t\t\t*lane_ptr;\n\t\t\t//warp_ptr = (uint8_t *)((uint8_t *)warp_ptr + M_GLOBAL * (WARPS_PER_BLOCK / 2) *CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tlane_ptr_index = (int *)((int *)lane_ptr_index +  (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A;\n\t\t  }else{\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_B)) =\n\t\t\t\t*lane_ptr;\n\t\t\tlane_ptr = (int4 *)((uint8_t *)lane_ptr + K_GLOBAL_SPARSE * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B;\n\t\t  }\n\t  }\n\n      __syncthreads();\n\n\t#pragma unroll\n      for (int k_step = 0; k_step < CHUNK_K_SPARSE; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::col_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n\t#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i += 1) {\n\t\t\tsize_t shmem_idx_a = (warpId % BLOCK_COL_WARPS) * M * WARP_COL_TILES + (i * M);\n\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_a + k_step * K * SHARED_OFFSET_A;\n\n\t\t\twmma::load_matrix_sync(a[i], tile_ptr, SHARED_OFFSET_A);\n\t\t#pragma unroll\n\t\t\tfor(int j = 0; j < WARP_ROW_TILES; j += 1){\n\t\t\t\tif(i == 0){\n\t\t\t\t\tsize_t shmem_idx_b = shmem_idx_b_off +\n\t\t\t\t\t\t\t\t\t\t\t(warpId / BLOCK_COL_WARPS) * (WARP_ROW_TILES * N) * SHARED_OFFSET_B +\n\t\t\t\t\t\t\t\t\t\t\t(j * N) * SHARED_OFFSET_B;\n\t\t\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_b + k_step * K;\n\t\t\t\t\twmma::load_matrix_sync(b[j], tile_ptr, SHARED_OFFSET_B);\n\t\t\t\t}\n\t\t\t\twmma::mma_sync(c[j][i], a[i], b[j], c[j][i]);\n\t\t\t}\n\n        }\n      }\n\n      __syncthreads();\n    }\n\n    // This pointer is used to access the C and D matrix tiles this warp computes.\n\tint *shmem_warp_tile_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * N * WARP_ROW_TILES * SHMEM_STRIDE +\n\t(warpId % BLOCK_COL_WARPS) * SHMEM_OFFSET;\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t#pragma unroll\n\t\t\tfor(int t = 0; t < c[i][j].num_elements; t += 1){\n\t\t\t\tc[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n\t\t\t}\n\t\t\tint *tile_ptr = shmem_warp_tile_ptr + i * N * SHMEM_STRIDE + j * M;\n\t\t\twmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, C_LAYOUT);\n\t\t}\n\t}\n\n    __syncthreads();\n\n\tint *shmem_warp_stream_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N * SHMEM_STRIDE\n\t\t\t\t\t\t\t\t\t+ (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tconst size_t gmem_idx =\n\t\t(block_tile_i * N + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N) * GLOBAL_MEM_STRIDE +\n\t\tblock_tile_j * M + (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tuint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n\tint *shmem_lane_stream_ptr =\n\t\tshmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * SHMEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\t\n\tuint8_t *dst_gmem_lane_stream_ptr =\n\t\tdst_gmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * GLOBAL_MEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\n\tfor(int i = 0; i < WARP_ROW_TILES * N; i += SHARED_TO_GLOBAL_LINES_PER_WARP){\n\t\t*(dst_gmem_lane_stream_ptr + i * GLOBAL_MEM_STRIDE) = (uint8_t)(*(shmem_lane_stream_ptr + i * SHMEM_STRIDE));\n\t}\n\n\t__syncthreads();\n}\n", "launch_config": {"dimBlock": [256, 1], "dimGrid": [256, 1]}}, "59": {"code": "\n__global__ void compute_gemm_imma(float *input0, float *input1, float *input2, float* input3, float* input4, float*input5, float* input6,\n                                  float *output0) {\n    // keys need to be replaced 4096, 4096, K_GLOBA_VALUEL, 0.75, 64\n\n\t//extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B = reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_index = reinterpret_cast<int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n\tconst int WARP_SIZE = 32;\n    const int M = 16;\n    const int K = 16;\n    const int N = 16;\n    const int WMMA_M = 16;\n    const int WMMA_K = 16;\n    const int WMMA_N = 16;\n    const int M_GLOBAL = 4096;\n    const int K_GLOBAL = 1024;\n    const int N_GLOBAL = 4096;\n    const float SPARSITY = 0.75;\n    const int K_GLOBAL_SPARSE = (int(K_GLOBAL * (1-SPARSITY)));\n    const int M_TILES = (M_GLOBAL / M);\n    const int N_TILES = (N_GLOBAL / N);\n    const int K_TILES = (K_GLOBAL / K);\n    const int BLOCK_ROW_WARPS = 2;\n    const int BLOCK_COL_WARPS = 4;\n    const int WARP_ROW_TILES = 4;\n    const int WARP_COL_TILES = 2;\n    const int SKEW_UINT8 = 16;\n    const int BANK_VAL = 32;\n    const int NUM_BANK = (K_GLOBAL / BANK_VAL);\n    /////////// BLOCK_ROW_TILES <= N_TILES ////////////\n    // const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    // ////////// BLOCK_COL_TILES <= M_TILES ////////////\n    // const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    auto C_LAYOUT = wmma::mem_col_major;\n    const int CHUNK_K = 64;\n    const int CHUNK_K_SPARSE = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K_SPARSE = (int((CHUNK_K * K) * (1 - SPARSITY)));\n    const int WARP_COPY_BYTES = (WARP_SIZE * sizeof(int4));\n    const int CHUNK_LINE_BYTES_A (BLOCK_COL_TILES * M * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_A = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_A);\n    const int CHUNK_COPY_LINE_LANES_A = (CHUNK_LINE_BYTES_A / sizeof(int4));\n    const int SHARED_OFFSET_A = (BLOCK_COL_TILES * M + SKEW_UINT8);\n\n    const int CHUNK_LINE_BYTES_B = (BLOCK_SIZE_K_SPARSE * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_B = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_B);\n    const int CHUNK_COPY_LINE_LANES_B = (CHUNK_LINE_BYTES_B / sizeof(int4));\n    const int SHARED_OFFSET_B = (BLOCK_SIZE_K_SPARSE + SKEW_UINT8);\n    const int SHARED_TO_GLOBAL_BYTES_PER_LINE = ((WARP_COL_TILES * M) * sizeof(int));\n    const int SHARED_TO_GLOBAL_BYTES_PER_WARP = (WARP_SIZE * sizeof(int));\n    const int SHARED_TO_GLOBAL_LINES_PER_WARP = (SHARED_TO_GLOBAL_BYTES_PER_WARP / SHARED_TO_GLOBAL_BYTES_PER_LINE);\n    const int SHARED_TO_GLOBAL_LANES_PER_LINE = (WARP_SIZE / SHARED_TO_GLOBAL_LINES_PER_WARP);\n    const int SHARED_TO_GLOBAL_ITERS = ((WARP_ROW_TILES * N) / SHARED_TO_GLOBAL_LINES_PER_WARP);\n\n    const int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const int WARP_STRIDE = (WARP_COL_TILES * M);\n    const int WARPS_PER_BLOCK = (BLOCK_ROW_WARPS * BLOCK_COL_WARPS);\n    const int THREADS_PER_BLOCK = (WARP_SIZE * WARPS_PER_BLOCK);\n    const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    const int GLOBAL_MEM_STRIDE = M_GLOBAL;\n    const int SHMEM_STRIDE = (M * BLOCK_COL_TILES);\n    const int SHMEM_OFFSET = (M * WARP_COL_TILES);\n    const int BLOCK_SIZE_M = (M * BLOCK_COL_TILES);\n    const int BLOCK_SIZE_N = (N * BLOCK_ROW_TILES);\n\n   extern __shared__ uint8_t shmem[];\n\n\t// Warp and lane identification.\n\tconst unsigned int warpId = threadIdx.x / WARP_SIZE;\n\tconst unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n\n\t// Offset in shared memory from which the B matrix is stored.\n\t// const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\tconst size_t shmem_idx_b_off = BLOCK_SIZE_K_SPARSE * SHARED_OFFSET_A;\n\n\n\t// Each CTA slides along the 128 x 128 tiles from the top left corner of the\n\t// matrix to the right and down, and selects the next tile to compute. Once\n\t// there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n\tconst unsigned int block_tile_i =\n\t\t((block_pos * BLOCK_COL_TILES) / M_TILES) * (BLOCK_ROW_TILES);\n\tconst unsigned int block_tile_j = (block_pos * BLOCK_COL_TILES) % M_TILES;\n\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n\twmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_ROW_TILES]\n\t\t\t\t\t\t\t\t\t\t\t\t\t [WARP_COL_TILES];\n\n    // Load the C matrix tiles into fragments from shared memory.\n\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t\twmma::fill_fragment(c[i][j], 0);\n\t\t}\n\t}\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    //for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n    for(int tile_k_idx_sparse = 0, tile_k_idx = 0; tile_k_idx_sparse < K_GLOBAL_SPARSE; tile_k_idx_sparse += BLOCK_SIZE_K_SPARSE, tile_k_idx += BLOCK_SIZE_K){\n\n\t\tsize_t shmem_idx = \n\t\twarpId < (WARPS_PER_BLOCK / 2)\n\t\t\t? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A\n\t\t\t: (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B + shmem_idx_b_off;\n\n\t\tint4 *lane_ptr = NULL;\n\t\tint *lane_ptr_index = NULL;\n\t\tconst uint8_t *warp_ptr = NULL;\n\n\n\t\tif(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\t//warp_ptr = &A[block_tile_j * M] +\n\t\t\t//\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * M_GLOBAL;\n\t\t\twarp_ptr = &A[block_tile_j * M];\n\t\t\t\n\t\t\tconst int *warp_ptr_index = &B_index[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t\t\t\t\t\t((warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A);\n\n\t\t\tlane_ptr_index = (int *)(warp_ptr_index + tile_k_idx_sparse + (laneId / CHUNK_COPY_LINE_LANES_A));\n\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_A) * SHARED_OFFSET_A;\n\t\t}else{\n\t\t\twarp_ptr = &B[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * K_GLOBAL_SPARSE;\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_sparse +\n\t\t\t\t\t\t\t\t(laneId / CHUNK_COPY_LINE_LANES_B) * K_GLOBAL_SPARSE) +\n\t\t\t\t\t\t\t\t(laneId % CHUNK_COPY_LINE_LANES_B);\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_B) * SHARED_OFFSET_B;\n\t\t}\n\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      // shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n\t  int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n\t  \t? BLOCK_SIZE_K_SPARSE / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A)\n\t\t: BLOCK_SIZE_N / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\n\t  /*\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\t  */\n\n\t  /*\n      int tile_k_idx_A;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n          tile_k_idx_A = *(lane_ptr_index);\n      }\n\t  */\n\n\t  #pragma unroll\n\t  for(int i = 0; i < iter_index; i += 1){\n\t\t  if(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\tint tile_k_idx_A = *(lane_ptr_index);\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_A * M_GLOBAL) + (laneId % CHUNK_COPY_LINE_LANES_A);\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_A)) =\n\t\t\t\t*lane_ptr;\n\t\t\t//warp_ptr = (uint8_t *)((uint8_t *)warp_ptr + M_GLOBAL * (WARPS_PER_BLOCK / 2) *CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tlane_ptr_index = (int *)((int *)lane_ptr_index +  (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A;\n\t\t  }else{\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_B)) =\n\t\t\t\t*lane_ptr;\n\t\t\tlane_ptr = (int4 *)((uint8_t *)lane_ptr + K_GLOBAL_SPARSE * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B;\n\t\t  }\n\t  }\n\n      __syncthreads();\n\n\t#pragma unroll\n      for (int k_step = 0; k_step < CHUNK_K_SPARSE; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::col_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n\t#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i += 1) {\n\t\t\tsize_t shmem_idx_a = (warpId % BLOCK_COL_WARPS) * M * WARP_COL_TILES + (i * M);\n\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_a + k_step * K * SHARED_OFFSET_A;\n\n\t\t\twmma::load_matrix_sync(a[i], tile_ptr, SHARED_OFFSET_A);\n\t\t#pragma unroll\n\t\t\tfor(int j = 0; j < WARP_ROW_TILES; j += 1){\n\t\t\t\tif(i == 0){\n\t\t\t\t\tsize_t shmem_idx_b = shmem_idx_b_off +\n\t\t\t\t\t\t\t\t\t\t\t(warpId / BLOCK_COL_WARPS) * (WARP_ROW_TILES * N) * SHARED_OFFSET_B +\n\t\t\t\t\t\t\t\t\t\t\t(j * N) * SHARED_OFFSET_B;\n\t\t\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_b + k_step * K;\n\t\t\t\t\twmma::load_matrix_sync(b[j], tile_ptr, SHARED_OFFSET_B);\n\t\t\t\t}\n\t\t\t\twmma::mma_sync(c[j][i], a[i], b[j], c[j][i]);\n\t\t\t}\n\n        }\n      }\n\n      __syncthreads();\n    }\n\n    // This pointer is used to access the C and D matrix tiles this warp computes.\n\tint *shmem_warp_tile_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * N * WARP_ROW_TILES * SHMEM_STRIDE +\n\t(warpId % BLOCK_COL_WARPS) * SHMEM_OFFSET;\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t#pragma unroll\n\t\t\tfor(int t = 0; t < c[i][j].num_elements; t += 1){\n\t\t\t\tc[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n\t\t\t}\n\t\t\tint *tile_ptr = shmem_warp_tile_ptr + i * N * SHMEM_STRIDE + j * M;\n\t\t\twmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, C_LAYOUT);\n\t\t}\n\t}\n\n    __syncthreads();\n\n\tint *shmem_warp_stream_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N * SHMEM_STRIDE\n\t\t\t\t\t\t\t\t\t+ (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tconst size_t gmem_idx =\n\t\t(block_tile_i * N + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N) * GLOBAL_MEM_STRIDE +\n\t\tblock_tile_j * M + (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tuint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n\tint *shmem_lane_stream_ptr =\n\t\tshmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * SHMEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\t\n\tuint8_t *dst_gmem_lane_stream_ptr =\n\t\tdst_gmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * GLOBAL_MEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\n\tfor(int i = 0; i < WARP_ROW_TILES * N; i += SHARED_TO_GLOBAL_LINES_PER_WARP){\n\t\t*(dst_gmem_lane_stream_ptr + i * GLOBAL_MEM_STRIDE) = (uint8_t)(*(shmem_lane_stream_ptr + i * SHMEM_STRIDE));\n\t}\n\n\t__syncthreads();\n}\n", "launch_config": {"dimBlock": [1024, 1], "dimGrid": [256, 1]}}, "60": {"code": "\n__global__ void compute_gemm_imma(float *input0, float *input1, float *input2, float* input3, float* input4, float*input5, float* input6,\n                                  float *output0) {\n    // keys need to be replaced 4096, 1024, K_GLOBA_VALUEL, 0.75, 64\n\n\t//extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B = reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_index = reinterpret_cast<int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n\tconst int WARP_SIZE = 32;\n    const int M = 16;\n    const int K = 16;\n    const int N = 16;\n    const int WMMA_M = 16;\n    const int WMMA_K = 16;\n    const int WMMA_N = 16;\n    const int M_GLOBAL = 4096;\n    const int K_GLOBAL = 4096;\n    const int N_GLOBAL = 1024;\n    const float SPARSITY = 0.75;\n    const int K_GLOBAL_SPARSE = (int(K_GLOBAL * (1-SPARSITY)));\n    const int M_TILES = (M_GLOBAL / M);\n    const int N_TILES = (N_GLOBAL / N);\n    const int K_TILES = (K_GLOBAL / K);\n    const int BLOCK_ROW_WARPS = 2;\n    const int BLOCK_COL_WARPS = 4;\n    const int WARP_ROW_TILES = 4;\n    const int WARP_COL_TILES = 2;\n    const int SKEW_UINT8 = 16;\n    const int BANK_VAL = 32;\n    const int NUM_BANK = (K_GLOBAL / BANK_VAL);\n    /////////// BLOCK_ROW_TILES <= N_TILES ////////////\n    // const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    // ////////// BLOCK_COL_TILES <= M_TILES ////////////\n    // const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    auto C_LAYOUT = wmma::mem_col_major;\n    const int CHUNK_K = 64;\n    const int CHUNK_K_SPARSE = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K_SPARSE = (int((CHUNK_K * K) * (1 - SPARSITY)));\n    const int WARP_COPY_BYTES = (WARP_SIZE * sizeof(int4));\n    const int CHUNK_LINE_BYTES_A (BLOCK_COL_TILES * M * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_A = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_A);\n    const int CHUNK_COPY_LINE_LANES_A = (CHUNK_LINE_BYTES_A / sizeof(int4));\n    const int SHARED_OFFSET_A = (BLOCK_COL_TILES * M + SKEW_UINT8);\n\n    const int CHUNK_LINE_BYTES_B = (BLOCK_SIZE_K_SPARSE * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_B = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_B);\n    const int CHUNK_COPY_LINE_LANES_B = (CHUNK_LINE_BYTES_B / sizeof(int4));\n    const int SHARED_OFFSET_B = (BLOCK_SIZE_K_SPARSE + SKEW_UINT8);\n    const int SHARED_TO_GLOBAL_BYTES_PER_LINE = ((WARP_COL_TILES * M) * sizeof(int));\n    const int SHARED_TO_GLOBAL_BYTES_PER_WARP = (WARP_SIZE * sizeof(int));\n    const int SHARED_TO_GLOBAL_LINES_PER_WARP = (SHARED_TO_GLOBAL_BYTES_PER_WARP / SHARED_TO_GLOBAL_BYTES_PER_LINE);\n    const int SHARED_TO_GLOBAL_LANES_PER_LINE = (WARP_SIZE / SHARED_TO_GLOBAL_LINES_PER_WARP);\n    const int SHARED_TO_GLOBAL_ITERS = ((WARP_ROW_TILES * N) / SHARED_TO_GLOBAL_LINES_PER_WARP);\n\n    const int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const int WARP_STRIDE = (WARP_COL_TILES * M);\n    const int WARPS_PER_BLOCK = (BLOCK_ROW_WARPS * BLOCK_COL_WARPS);\n    const int THREADS_PER_BLOCK = (WARP_SIZE * WARPS_PER_BLOCK);\n    const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    const int GLOBAL_MEM_STRIDE = M_GLOBAL;\n    const int SHMEM_STRIDE = (M * BLOCK_COL_TILES);\n    const int SHMEM_OFFSET = (M * WARP_COL_TILES);\n    const int BLOCK_SIZE_M = (M * BLOCK_COL_TILES);\n    const int BLOCK_SIZE_N = (N * BLOCK_ROW_TILES);\n\n   extern __shared__ uint8_t shmem[];\n\n\t// Warp and lane identification.\n\tconst unsigned int warpId = threadIdx.x / WARP_SIZE;\n\tconst unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n\n\t// Offset in shared memory from which the B matrix is stored.\n\t// const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\tconst size_t shmem_idx_b_off = BLOCK_SIZE_K_SPARSE * SHARED_OFFSET_A;\n\n\n\t// Each CTA slides along the 128 x 128 tiles from the top left corner of the\n\t// matrix to the right and down, and selects the next tile to compute. Once\n\t// there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n\tconst unsigned int block_tile_i =\n\t\t((block_pos * BLOCK_COL_TILES) / M_TILES) * (BLOCK_ROW_TILES);\n\tconst unsigned int block_tile_j = (block_pos * BLOCK_COL_TILES) % M_TILES;\n\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n\twmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_ROW_TILES]\n\t\t\t\t\t\t\t\t\t\t\t\t\t [WARP_COL_TILES];\n\n    // Load the C matrix tiles into fragments from shared memory.\n\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t\twmma::fill_fragment(c[i][j], 0);\n\t\t}\n\t}\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    //for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n    for(int tile_k_idx_sparse = 0, tile_k_idx = 0; tile_k_idx_sparse < K_GLOBAL_SPARSE; tile_k_idx_sparse += BLOCK_SIZE_K_SPARSE, tile_k_idx += BLOCK_SIZE_K){\n\n\t\tsize_t shmem_idx = \n\t\twarpId < (WARPS_PER_BLOCK / 2)\n\t\t\t? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A\n\t\t\t: (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B + shmem_idx_b_off;\n\n\t\tint4 *lane_ptr = NULL;\n\t\tint *lane_ptr_index = NULL;\n\t\tconst uint8_t *warp_ptr = NULL;\n\n\n\t\tif(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\t//warp_ptr = &A[block_tile_j * M] +\n\t\t\t//\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * M_GLOBAL;\n\t\t\twarp_ptr = &A[block_tile_j * M];\n\t\t\t\n\t\t\tconst int *warp_ptr_index = &B_index[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t\t\t\t\t\t((warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A);\n\n\t\t\tlane_ptr_index = (int *)(warp_ptr_index + tile_k_idx_sparse + (laneId / CHUNK_COPY_LINE_LANES_A));\n\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_A) * SHARED_OFFSET_A;\n\t\t}else{\n\t\t\twarp_ptr = &B[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * K_GLOBAL_SPARSE;\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_sparse +\n\t\t\t\t\t\t\t\t(laneId / CHUNK_COPY_LINE_LANES_B) * K_GLOBAL_SPARSE) +\n\t\t\t\t\t\t\t\t(laneId % CHUNK_COPY_LINE_LANES_B);\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_B) * SHARED_OFFSET_B;\n\t\t}\n\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      // shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n\t  int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n\t  \t? BLOCK_SIZE_K_SPARSE / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A)\n\t\t: BLOCK_SIZE_N / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\n\t  /*\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\t  */\n\n\t  /*\n      int tile_k_idx_A;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n          tile_k_idx_A = *(lane_ptr_index);\n      }\n\t  */\n\n\t  #pragma unroll\n\t  for(int i = 0; i < iter_index; i += 1){\n\t\t  if(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\tint tile_k_idx_A = *(lane_ptr_index);\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_A * M_GLOBAL) + (laneId % CHUNK_COPY_LINE_LANES_A);\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_A)) =\n\t\t\t\t*lane_ptr;\n\t\t\t//warp_ptr = (uint8_t *)((uint8_t *)warp_ptr + M_GLOBAL * (WARPS_PER_BLOCK / 2) *CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tlane_ptr_index = (int *)((int *)lane_ptr_index +  (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A;\n\t\t  }else{\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_B)) =\n\t\t\t\t*lane_ptr;\n\t\t\tlane_ptr = (int4 *)((uint8_t *)lane_ptr + K_GLOBAL_SPARSE * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B;\n\t\t  }\n\t  }\n\n      __syncthreads();\n\n\t#pragma unroll\n      for (int k_step = 0; k_step < CHUNK_K_SPARSE; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::col_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n\t#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i += 1) {\n\t\t\tsize_t shmem_idx_a = (warpId % BLOCK_COL_WARPS) * M * WARP_COL_TILES + (i * M);\n\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_a + k_step * K * SHARED_OFFSET_A;\n\n\t\t\twmma::load_matrix_sync(a[i], tile_ptr, SHARED_OFFSET_A);\n\t\t#pragma unroll\n\t\t\tfor(int j = 0; j < WARP_ROW_TILES; j += 1){\n\t\t\t\tif(i == 0){\n\t\t\t\t\tsize_t shmem_idx_b = shmem_idx_b_off +\n\t\t\t\t\t\t\t\t\t\t\t(warpId / BLOCK_COL_WARPS) * (WARP_ROW_TILES * N) * SHARED_OFFSET_B +\n\t\t\t\t\t\t\t\t\t\t\t(j * N) * SHARED_OFFSET_B;\n\t\t\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_b + k_step * K;\n\t\t\t\t\twmma::load_matrix_sync(b[j], tile_ptr, SHARED_OFFSET_B);\n\t\t\t\t}\n\t\t\t\twmma::mma_sync(c[j][i], a[i], b[j], c[j][i]);\n\t\t\t}\n\n        }\n      }\n\n      __syncthreads();\n    }\n\n    // This pointer is used to access the C and D matrix tiles this warp computes.\n\tint *shmem_warp_tile_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * N * WARP_ROW_TILES * SHMEM_STRIDE +\n\t(warpId % BLOCK_COL_WARPS) * SHMEM_OFFSET;\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t#pragma unroll\n\t\t\tfor(int t = 0; t < c[i][j].num_elements; t += 1){\n\t\t\t\tc[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n\t\t\t}\n\t\t\tint *tile_ptr = shmem_warp_tile_ptr + i * N * SHMEM_STRIDE + j * M;\n\t\t\twmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, C_LAYOUT);\n\t\t}\n\t}\n\n    __syncthreads();\n\n\tint *shmem_warp_stream_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N * SHMEM_STRIDE\n\t\t\t\t\t\t\t\t\t+ (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tconst size_t gmem_idx =\n\t\t(block_tile_i * N + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N) * GLOBAL_MEM_STRIDE +\n\t\tblock_tile_j * M + (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tuint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n\tint *shmem_lane_stream_ptr =\n\t\tshmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * SHMEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\t\n\tuint8_t *dst_gmem_lane_stream_ptr =\n\t\tdst_gmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * GLOBAL_MEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\n\tfor(int i = 0; i < WARP_ROW_TILES * N; i += SHARED_TO_GLOBAL_LINES_PER_WARP){\n\t\t*(dst_gmem_lane_stream_ptr + i * GLOBAL_MEM_STRIDE) = (uint8_t)(*(shmem_lane_stream_ptr + i * SHMEM_STRIDE));\n\t}\n\n\t__syncthreads();\n}\n", "launch_config": {"dimBlock": [256, 1], "dimGrid": [256, 1]}}, "61": {"code": "\n__global__ void compute_gemm_imma(float *input0, float *input1, float *input2, float* input3, float* input4, float*input5, float* input6,\n                                  float *output0) {\n    // keys need to be replaced 4096, 1024, K_GLOBA_VALUEL, 0.75, 64\n\n\t//extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B = reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_index = reinterpret_cast<int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n\tconst int WARP_SIZE = 32;\n    const int M = 16;\n    const int K = 16;\n    const int N = 16;\n    const int WMMA_M = 16;\n    const int WMMA_K = 16;\n    const int WMMA_N = 16;\n    const int M_GLOBAL = 4096;\n    const int K_GLOBAL = 1024;\n    const int N_GLOBAL = 1024;\n    const float SPARSITY = 0.75;\n    const int K_GLOBAL_SPARSE = (int(K_GLOBAL * (1-SPARSITY)));\n    const int M_TILES = (M_GLOBAL / M);\n    const int N_TILES = (N_GLOBAL / N);\n    const int K_TILES = (K_GLOBAL / K);\n    const int BLOCK_ROW_WARPS = 2;\n    const int BLOCK_COL_WARPS = 4;\n    const int WARP_ROW_TILES = 4;\n    const int WARP_COL_TILES = 2;\n    const int SKEW_UINT8 = 16;\n    const int BANK_VAL = 32;\n    const int NUM_BANK = (K_GLOBAL / BANK_VAL);\n    /////////// BLOCK_ROW_TILES <= N_TILES ////////////\n    // const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    // ////////// BLOCK_COL_TILES <= M_TILES ////////////\n    // const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    auto C_LAYOUT = wmma::mem_col_major;\n    const int CHUNK_K = 64;\n    const int CHUNK_K_SPARSE = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K_SPARSE = (int((CHUNK_K * K) * (1 - SPARSITY)));\n    const int WARP_COPY_BYTES = (WARP_SIZE * sizeof(int4));\n    const int CHUNK_LINE_BYTES_A (BLOCK_COL_TILES * M * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_A = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_A);\n    const int CHUNK_COPY_LINE_LANES_A = (CHUNK_LINE_BYTES_A / sizeof(int4));\n    const int SHARED_OFFSET_A = (BLOCK_COL_TILES * M + SKEW_UINT8);\n\n    const int CHUNK_LINE_BYTES_B = (BLOCK_SIZE_K_SPARSE * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_B = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_B);\n    const int CHUNK_COPY_LINE_LANES_B = (CHUNK_LINE_BYTES_B / sizeof(int4));\n    const int SHARED_OFFSET_B = (BLOCK_SIZE_K_SPARSE + SKEW_UINT8);\n    const int SHARED_TO_GLOBAL_BYTES_PER_LINE = ((WARP_COL_TILES * M) * sizeof(int));\n    const int SHARED_TO_GLOBAL_BYTES_PER_WARP = (WARP_SIZE * sizeof(int));\n    const int SHARED_TO_GLOBAL_LINES_PER_WARP = (SHARED_TO_GLOBAL_BYTES_PER_WARP / SHARED_TO_GLOBAL_BYTES_PER_LINE);\n    const int SHARED_TO_GLOBAL_LANES_PER_LINE = (WARP_SIZE / SHARED_TO_GLOBAL_LINES_PER_WARP);\n    const int SHARED_TO_GLOBAL_ITERS = ((WARP_ROW_TILES * N) / SHARED_TO_GLOBAL_LINES_PER_WARP);\n\n    const int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const int WARP_STRIDE = (WARP_COL_TILES * M);\n    const int WARPS_PER_BLOCK = (BLOCK_ROW_WARPS * BLOCK_COL_WARPS);\n    const int THREADS_PER_BLOCK = (WARP_SIZE * WARPS_PER_BLOCK);\n    const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    const int GLOBAL_MEM_STRIDE = M_GLOBAL;\n    const int SHMEM_STRIDE = (M * BLOCK_COL_TILES);\n    const int SHMEM_OFFSET = (M * WARP_COL_TILES);\n    const int BLOCK_SIZE_M = (M * BLOCK_COL_TILES);\n    const int BLOCK_SIZE_N = (N * BLOCK_ROW_TILES);\n\n   extern __shared__ uint8_t shmem[];\n\n\t// Warp and lane identification.\n\tconst unsigned int warpId = threadIdx.x / WARP_SIZE;\n\tconst unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n\n\t// Offset in shared memory from which the B matrix is stored.\n\t// const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\tconst size_t shmem_idx_b_off = BLOCK_SIZE_K_SPARSE * SHARED_OFFSET_A;\n\n\n\t// Each CTA slides along the 128 x 128 tiles from the top left corner of the\n\t// matrix to the right and down, and selects the next tile to compute. Once\n\t// there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n\tconst unsigned int block_tile_i =\n\t\t((block_pos * BLOCK_COL_TILES) / M_TILES) * (BLOCK_ROW_TILES);\n\tconst unsigned int block_tile_j = (block_pos * BLOCK_COL_TILES) % M_TILES;\n\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n\twmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_ROW_TILES]\n\t\t\t\t\t\t\t\t\t\t\t\t\t [WARP_COL_TILES];\n\n    // Load the C matrix tiles into fragments from shared memory.\n\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t\twmma::fill_fragment(c[i][j], 0);\n\t\t}\n\t}\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    //for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n    for(int tile_k_idx_sparse = 0, tile_k_idx = 0; tile_k_idx_sparse < K_GLOBAL_SPARSE; tile_k_idx_sparse += BLOCK_SIZE_K_SPARSE, tile_k_idx += BLOCK_SIZE_K){\n\n\t\tsize_t shmem_idx = \n\t\twarpId < (WARPS_PER_BLOCK / 2)\n\t\t\t? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A\n\t\t\t: (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B + shmem_idx_b_off;\n\n\t\tint4 *lane_ptr = NULL;\n\t\tint *lane_ptr_index = NULL;\n\t\tconst uint8_t *warp_ptr = NULL;\n\n\n\t\tif(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\t//warp_ptr = &A[block_tile_j * M] +\n\t\t\t//\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * M_GLOBAL;\n\t\t\twarp_ptr = &A[block_tile_j * M];\n\t\t\t\n\t\t\tconst int *warp_ptr_index = &B_index[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t\t\t\t\t\t((warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A);\n\n\t\t\tlane_ptr_index = (int *)(warp_ptr_index + tile_k_idx_sparse + (laneId / CHUNK_COPY_LINE_LANES_A));\n\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_A) * SHARED_OFFSET_A;\n\t\t}else{\n\t\t\twarp_ptr = &B[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * K_GLOBAL_SPARSE;\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_sparse +\n\t\t\t\t\t\t\t\t(laneId / CHUNK_COPY_LINE_LANES_B) * K_GLOBAL_SPARSE) +\n\t\t\t\t\t\t\t\t(laneId % CHUNK_COPY_LINE_LANES_B);\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_B) * SHARED_OFFSET_B;\n\t\t}\n\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      // shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n\t  int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n\t  \t? BLOCK_SIZE_K_SPARSE / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A)\n\t\t: BLOCK_SIZE_N / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\n\t  /*\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\t  */\n\n\t  /*\n      int tile_k_idx_A;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n          tile_k_idx_A = *(lane_ptr_index);\n      }\n\t  */\n\n\t  #pragma unroll\n\t  for(int i = 0; i < iter_index; i += 1){\n\t\t  if(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\tint tile_k_idx_A = *(lane_ptr_index);\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_A * M_GLOBAL) + (laneId % CHUNK_COPY_LINE_LANES_A);\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_A)) =\n\t\t\t\t*lane_ptr;\n\t\t\t//warp_ptr = (uint8_t *)((uint8_t *)warp_ptr + M_GLOBAL * (WARPS_PER_BLOCK / 2) *CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tlane_ptr_index = (int *)((int *)lane_ptr_index +  (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A;\n\t\t  }else{\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_B)) =\n\t\t\t\t*lane_ptr;\n\t\t\tlane_ptr = (int4 *)((uint8_t *)lane_ptr + K_GLOBAL_SPARSE * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B;\n\t\t  }\n\t  }\n\n      __syncthreads();\n\n\t#pragma unroll\n      for (int k_step = 0; k_step < CHUNK_K_SPARSE; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::col_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n\t#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i += 1) {\n\t\t\tsize_t shmem_idx_a = (warpId % BLOCK_COL_WARPS) * M * WARP_COL_TILES + (i * M);\n\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_a + k_step * K * SHARED_OFFSET_A;\n\n\t\t\twmma::load_matrix_sync(a[i], tile_ptr, SHARED_OFFSET_A);\n\t\t#pragma unroll\n\t\t\tfor(int j = 0; j < WARP_ROW_TILES; j += 1){\n\t\t\t\tif(i == 0){\n\t\t\t\t\tsize_t shmem_idx_b = shmem_idx_b_off +\n\t\t\t\t\t\t\t\t\t\t\t(warpId / BLOCK_COL_WARPS) * (WARP_ROW_TILES * N) * SHARED_OFFSET_B +\n\t\t\t\t\t\t\t\t\t\t\t(j * N) * SHARED_OFFSET_B;\n\t\t\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_b + k_step * K;\n\t\t\t\t\twmma::load_matrix_sync(b[j], tile_ptr, SHARED_OFFSET_B);\n\t\t\t\t}\n\t\t\t\twmma::mma_sync(c[j][i], a[i], b[j], c[j][i]);\n\t\t\t}\n\n        }\n      }\n\n      __syncthreads();\n    }\n\n    // This pointer is used to access the C and D matrix tiles this warp computes.\n\tint *shmem_warp_tile_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * N * WARP_ROW_TILES * SHMEM_STRIDE +\n\t(warpId % BLOCK_COL_WARPS) * SHMEM_OFFSET;\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t#pragma unroll\n\t\t\tfor(int t = 0; t < c[i][j].num_elements; t += 1){\n\t\t\t\tc[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n\t\t\t}\n\t\t\tint *tile_ptr = shmem_warp_tile_ptr + i * N * SHMEM_STRIDE + j * M;\n\t\t\twmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, C_LAYOUT);\n\t\t}\n\t}\n\n    __syncthreads();\n\n\tint *shmem_warp_stream_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N * SHMEM_STRIDE\n\t\t\t\t\t\t\t\t\t+ (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tconst size_t gmem_idx =\n\t\t(block_tile_i * N + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N) * GLOBAL_MEM_STRIDE +\n\t\tblock_tile_j * M + (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tuint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n\tint *shmem_lane_stream_ptr =\n\t\tshmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * SHMEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\t\n\tuint8_t *dst_gmem_lane_stream_ptr =\n\t\tdst_gmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * GLOBAL_MEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\n\tfor(int i = 0; i < WARP_ROW_TILES * N; i += SHARED_TO_GLOBAL_LINES_PER_WARP){\n\t\t*(dst_gmem_lane_stream_ptr + i * GLOBAL_MEM_STRIDE) = (uint8_t)(*(shmem_lane_stream_ptr + i * SHMEM_STRIDE));\n\t}\n\n\t__syncthreads();\n}\n", "launch_config": {"dimBlock": [256, 1], "dimGrid": [256, 1]}}, "62": {"code": "\n__global__ void compute_gemm_imma(float *input0, float *input1, float *input2, float* input3, float* input4, float*input5, float* input6,\n                                  float *output0) {\n    // keys need to be replaced 4096, 1024, K_GLOBA_VALUEL, 0.75, 64\n\n\t//extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B = reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_index = reinterpret_cast<int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n\tconst int WARP_SIZE = 32;\n    const int M = 16;\n    const int K = 16;\n    const int N = 16;\n    const int WMMA_M = 16;\n    const int WMMA_K = 16;\n    const int WMMA_N = 16;\n    const int M_GLOBAL = 4096;\n    const int K_GLOBAL = 1024;\n    const int N_GLOBAL = 1024;\n    const float SPARSITY = 0.75;\n    const int K_GLOBAL_SPARSE = (int(K_GLOBAL * (1-SPARSITY)));\n    const int M_TILES = (M_GLOBAL / M);\n    const int N_TILES = (N_GLOBAL / N);\n    const int K_TILES = (K_GLOBAL / K);\n    const int BLOCK_ROW_WARPS = 2;\n    const int BLOCK_COL_WARPS = 4;\n    const int WARP_ROW_TILES = 4;\n    const int WARP_COL_TILES = 2;\n    const int SKEW_UINT8 = 16;\n    const int BANK_VAL = 32;\n    const int NUM_BANK = (K_GLOBAL / BANK_VAL);\n    /////////// BLOCK_ROW_TILES <= N_TILES ////////////\n    // const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    // ////////// BLOCK_COL_TILES <= M_TILES ////////////\n    // const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    auto C_LAYOUT = wmma::mem_col_major;\n    const int CHUNK_K = 64;\n    const int CHUNK_K_SPARSE = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K_SPARSE = (int((CHUNK_K * K) * (1 - SPARSITY)));\n    const int WARP_COPY_BYTES = (WARP_SIZE * sizeof(int4));\n    const int CHUNK_LINE_BYTES_A (BLOCK_COL_TILES * M * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_A = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_A);\n    const int CHUNK_COPY_LINE_LANES_A = (CHUNK_LINE_BYTES_A / sizeof(int4));\n    const int SHARED_OFFSET_A = (BLOCK_COL_TILES * M + SKEW_UINT8);\n\n    const int CHUNK_LINE_BYTES_B = (BLOCK_SIZE_K_SPARSE * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_B = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_B);\n    const int CHUNK_COPY_LINE_LANES_B = (CHUNK_LINE_BYTES_B / sizeof(int4));\n    const int SHARED_OFFSET_B = (BLOCK_SIZE_K_SPARSE + SKEW_UINT8);\n    const int SHARED_TO_GLOBAL_BYTES_PER_LINE = ((WARP_COL_TILES * M) * sizeof(int));\n    const int SHARED_TO_GLOBAL_BYTES_PER_WARP = (WARP_SIZE * sizeof(int));\n    const int SHARED_TO_GLOBAL_LINES_PER_WARP = (SHARED_TO_GLOBAL_BYTES_PER_WARP / SHARED_TO_GLOBAL_BYTES_PER_LINE);\n    const int SHARED_TO_GLOBAL_LANES_PER_LINE = (WARP_SIZE / SHARED_TO_GLOBAL_LINES_PER_WARP);\n    const int SHARED_TO_GLOBAL_ITERS = ((WARP_ROW_TILES * N) / SHARED_TO_GLOBAL_LINES_PER_WARP);\n\n    const int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const int WARP_STRIDE = (WARP_COL_TILES * M);\n    const int WARPS_PER_BLOCK = (BLOCK_ROW_WARPS * BLOCK_COL_WARPS);\n    const int THREADS_PER_BLOCK = (WARP_SIZE * WARPS_PER_BLOCK);\n    const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    const int GLOBAL_MEM_STRIDE = M_GLOBAL;\n    const int SHMEM_STRIDE = (M * BLOCK_COL_TILES);\n    const int SHMEM_OFFSET = (M * WARP_COL_TILES);\n    const int BLOCK_SIZE_M = (M * BLOCK_COL_TILES);\n    const int BLOCK_SIZE_N = (N * BLOCK_ROW_TILES);\n\n   extern __shared__ uint8_t shmem[];\n\n\t// Warp and lane identification.\n\tconst unsigned int warpId = threadIdx.x / WARP_SIZE;\n\tconst unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n\n\t// Offset in shared memory from which the B matrix is stored.\n\t// const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\tconst size_t shmem_idx_b_off = BLOCK_SIZE_K_SPARSE * SHARED_OFFSET_A;\n\n\n\t// Each CTA slides along the 128 x 128 tiles from the top left corner of the\n\t// matrix to the right and down, and selects the next tile to compute. Once\n\t// there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n\tconst unsigned int block_tile_i =\n\t\t((block_pos * BLOCK_COL_TILES) / M_TILES) * (BLOCK_ROW_TILES);\n\tconst unsigned int block_tile_j = (block_pos * BLOCK_COL_TILES) % M_TILES;\n\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n\twmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_ROW_TILES]\n\t\t\t\t\t\t\t\t\t\t\t\t\t [WARP_COL_TILES];\n\n    // Load the C matrix tiles into fragments from shared memory.\n\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t\twmma::fill_fragment(c[i][j], 0);\n\t\t}\n\t}\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    //for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n    for(int tile_k_idx_sparse = 0, tile_k_idx = 0; tile_k_idx_sparse < K_GLOBAL_SPARSE; tile_k_idx_sparse += BLOCK_SIZE_K_SPARSE, tile_k_idx += BLOCK_SIZE_K){\n\n\t\tsize_t shmem_idx = \n\t\twarpId < (WARPS_PER_BLOCK / 2)\n\t\t\t? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A\n\t\t\t: (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B + shmem_idx_b_off;\n\n\t\tint4 *lane_ptr = NULL;\n\t\tint *lane_ptr_index = NULL;\n\t\tconst uint8_t *warp_ptr = NULL;\n\n\n\t\tif(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\t//warp_ptr = &A[block_tile_j * M] +\n\t\t\t//\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * M_GLOBAL;\n\t\t\twarp_ptr = &A[block_tile_j * M];\n\t\t\t\n\t\t\tconst int *warp_ptr_index = &B_index[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t\t\t\t\t\t((warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A);\n\n\t\t\tlane_ptr_index = (int *)(warp_ptr_index + tile_k_idx_sparse + (laneId / CHUNK_COPY_LINE_LANES_A));\n\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_A) * SHARED_OFFSET_A;\n\t\t}else{\n\t\t\twarp_ptr = &B[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * K_GLOBAL_SPARSE;\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_sparse +\n\t\t\t\t\t\t\t\t(laneId / CHUNK_COPY_LINE_LANES_B) * K_GLOBAL_SPARSE) +\n\t\t\t\t\t\t\t\t(laneId % CHUNK_COPY_LINE_LANES_B);\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_B) * SHARED_OFFSET_B;\n\t\t}\n\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      // shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n\t  int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n\t  \t? BLOCK_SIZE_K_SPARSE / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A)\n\t\t: BLOCK_SIZE_N / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\n\t  /*\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\t  */\n\n\t  /*\n      int tile_k_idx_A;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n          tile_k_idx_A = *(lane_ptr_index);\n      }\n\t  */\n\n\t  #pragma unroll\n\t  for(int i = 0; i < iter_index; i += 1){\n\t\t  if(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\tint tile_k_idx_A = *(lane_ptr_index);\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_A * M_GLOBAL) + (laneId % CHUNK_COPY_LINE_LANES_A);\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_A)) =\n\t\t\t\t*lane_ptr;\n\t\t\t//warp_ptr = (uint8_t *)((uint8_t *)warp_ptr + M_GLOBAL * (WARPS_PER_BLOCK / 2) *CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tlane_ptr_index = (int *)((int *)lane_ptr_index +  (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A;\n\t\t  }else{\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_B)) =\n\t\t\t\t*lane_ptr;\n\t\t\tlane_ptr = (int4 *)((uint8_t *)lane_ptr + K_GLOBAL_SPARSE * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B;\n\t\t  }\n\t  }\n\n      __syncthreads();\n\n\t#pragma unroll\n      for (int k_step = 0; k_step < CHUNK_K_SPARSE; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::col_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n\t#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i += 1) {\n\t\t\tsize_t shmem_idx_a = (warpId % BLOCK_COL_WARPS) * M * WARP_COL_TILES + (i * M);\n\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_a + k_step * K * SHARED_OFFSET_A;\n\n\t\t\twmma::load_matrix_sync(a[i], tile_ptr, SHARED_OFFSET_A);\n\t\t#pragma unroll\n\t\t\tfor(int j = 0; j < WARP_ROW_TILES; j += 1){\n\t\t\t\tif(i == 0){\n\t\t\t\t\tsize_t shmem_idx_b = shmem_idx_b_off +\n\t\t\t\t\t\t\t\t\t\t\t(warpId / BLOCK_COL_WARPS) * (WARP_ROW_TILES * N) * SHARED_OFFSET_B +\n\t\t\t\t\t\t\t\t\t\t\t(j * N) * SHARED_OFFSET_B;\n\t\t\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_b + k_step * K;\n\t\t\t\t\twmma::load_matrix_sync(b[j], tile_ptr, SHARED_OFFSET_B);\n\t\t\t\t}\n\t\t\t\twmma::mma_sync(c[j][i], a[i], b[j], c[j][i]);\n\t\t\t}\n\n        }\n      }\n\n      __syncthreads();\n    }\n\n    // This pointer is used to access the C and D matrix tiles this warp computes.\n\tint *shmem_warp_tile_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * N * WARP_ROW_TILES * SHMEM_STRIDE +\n\t(warpId % BLOCK_COL_WARPS) * SHMEM_OFFSET;\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t#pragma unroll\n\t\t\tfor(int t = 0; t < c[i][j].num_elements; t += 1){\n\t\t\t\tc[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n\t\t\t}\n\t\t\tint *tile_ptr = shmem_warp_tile_ptr + i * N * SHMEM_STRIDE + j * M;\n\t\t\twmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, C_LAYOUT);\n\t\t}\n\t}\n\n    __syncthreads();\n\n\tint *shmem_warp_stream_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N * SHMEM_STRIDE\n\t\t\t\t\t\t\t\t\t+ (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tconst size_t gmem_idx =\n\t\t(block_tile_i * N + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N) * GLOBAL_MEM_STRIDE +\n\t\tblock_tile_j * M + (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tuint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n\tint *shmem_lane_stream_ptr =\n\t\tshmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * SHMEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\t\n\tuint8_t *dst_gmem_lane_stream_ptr =\n\t\tdst_gmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * GLOBAL_MEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\n\tfor(int i = 0; i < WARP_ROW_TILES * N; i += SHARED_TO_GLOBAL_LINES_PER_WARP){\n\t\t*(dst_gmem_lane_stream_ptr + i * GLOBAL_MEM_STRIDE) = (uint8_t)(*(shmem_lane_stream_ptr + i * SHMEM_STRIDE));\n\t}\n\n\t__syncthreads();\n}\n", "launch_config": {"dimBlock": [256, 1], "dimGrid": [256, 1]}}, "63": {"code": "\n__global__ void compute_gemm_imma(float *input0, float *input1, float *input2, float* input3, float* input4, float*input5, float* input6,\n                                  float *output0) {\n    // keys need to be replaced 4096, 1024, K_GLOBA_VALUEL, 0.75, 64\n\n\t//extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B = reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_index = reinterpret_cast<int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n\tconst int WARP_SIZE = 32;\n    const int M = 16;\n    const int K = 16;\n    const int N = 16;\n    const int WMMA_M = 16;\n    const int WMMA_K = 16;\n    const int WMMA_N = 16;\n    const int M_GLOBAL = 4096;\n    const int K_GLOBAL = 1024;\n    const int N_GLOBAL = 1024;\n    const float SPARSITY = 0.75;\n    const int K_GLOBAL_SPARSE = (int(K_GLOBAL * (1-SPARSITY)));\n    const int M_TILES = (M_GLOBAL / M);\n    const int N_TILES = (N_GLOBAL / N);\n    const int K_TILES = (K_GLOBAL / K);\n    const int BLOCK_ROW_WARPS = 2;\n    const int BLOCK_COL_WARPS = 4;\n    const int WARP_ROW_TILES = 4;\n    const int WARP_COL_TILES = 2;\n    const int SKEW_UINT8 = 16;\n    const int BANK_VAL = 32;\n    const int NUM_BANK = (K_GLOBAL / BANK_VAL);\n    /////////// BLOCK_ROW_TILES <= N_TILES ////////////\n    // const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    // ////////// BLOCK_COL_TILES <= M_TILES ////////////\n    // const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    auto C_LAYOUT = wmma::mem_col_major;\n    const int CHUNK_K = 64;\n    const int CHUNK_K_SPARSE = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K_SPARSE = (int((CHUNK_K * K) * (1 - SPARSITY)));\n    const int WARP_COPY_BYTES = (WARP_SIZE * sizeof(int4));\n    const int CHUNK_LINE_BYTES_A (BLOCK_COL_TILES * M * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_A = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_A);\n    const int CHUNK_COPY_LINE_LANES_A = (CHUNK_LINE_BYTES_A / sizeof(int4));\n    const int SHARED_OFFSET_A = (BLOCK_COL_TILES * M + SKEW_UINT8);\n\n    const int CHUNK_LINE_BYTES_B = (BLOCK_SIZE_K_SPARSE * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_B = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_B);\n    const int CHUNK_COPY_LINE_LANES_B = (CHUNK_LINE_BYTES_B / sizeof(int4));\n    const int SHARED_OFFSET_B = (BLOCK_SIZE_K_SPARSE + SKEW_UINT8);\n    const int SHARED_TO_GLOBAL_BYTES_PER_LINE = ((WARP_COL_TILES * M) * sizeof(int));\n    const int SHARED_TO_GLOBAL_BYTES_PER_WARP = (WARP_SIZE * sizeof(int));\n    const int SHARED_TO_GLOBAL_LINES_PER_WARP = (SHARED_TO_GLOBAL_BYTES_PER_WARP / SHARED_TO_GLOBAL_BYTES_PER_LINE);\n    const int SHARED_TO_GLOBAL_LANES_PER_LINE = (WARP_SIZE / SHARED_TO_GLOBAL_LINES_PER_WARP);\n    const int SHARED_TO_GLOBAL_ITERS = ((WARP_ROW_TILES * N) / SHARED_TO_GLOBAL_LINES_PER_WARP);\n\n    const int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const int WARP_STRIDE = (WARP_COL_TILES * M);\n    const int WARPS_PER_BLOCK = (BLOCK_ROW_WARPS * BLOCK_COL_WARPS);\n    const int THREADS_PER_BLOCK = (WARP_SIZE * WARPS_PER_BLOCK);\n    const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    const int GLOBAL_MEM_STRIDE = M_GLOBAL;\n    const int SHMEM_STRIDE = (M * BLOCK_COL_TILES);\n    const int SHMEM_OFFSET = (M * WARP_COL_TILES);\n    const int BLOCK_SIZE_M = (M * BLOCK_COL_TILES);\n    const int BLOCK_SIZE_N = (N * BLOCK_ROW_TILES);\n\n   extern __shared__ uint8_t shmem[];\n\n\t// Warp and lane identification.\n\tconst unsigned int warpId = threadIdx.x / WARP_SIZE;\n\tconst unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n\n\t// Offset in shared memory from which the B matrix is stored.\n\t// const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\tconst size_t shmem_idx_b_off = BLOCK_SIZE_K_SPARSE * SHARED_OFFSET_A;\n\n\n\t// Each CTA slides along the 128 x 128 tiles from the top left corner of the\n\t// matrix to the right and down, and selects the next tile to compute. Once\n\t// there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n\tconst unsigned int block_tile_i =\n\t\t((block_pos * BLOCK_COL_TILES) / M_TILES) * (BLOCK_ROW_TILES);\n\tconst unsigned int block_tile_j = (block_pos * BLOCK_COL_TILES) % M_TILES;\n\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n\twmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_ROW_TILES]\n\t\t\t\t\t\t\t\t\t\t\t\t\t [WARP_COL_TILES];\n\n    // Load the C matrix tiles into fragments from shared memory.\n\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t\twmma::fill_fragment(c[i][j], 0);\n\t\t}\n\t}\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    //for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n    for(int tile_k_idx_sparse = 0, tile_k_idx = 0; tile_k_idx_sparse < K_GLOBAL_SPARSE; tile_k_idx_sparse += BLOCK_SIZE_K_SPARSE, tile_k_idx += BLOCK_SIZE_K){\n\n\t\tsize_t shmem_idx = \n\t\twarpId < (WARPS_PER_BLOCK / 2)\n\t\t\t? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A\n\t\t\t: (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B + shmem_idx_b_off;\n\n\t\tint4 *lane_ptr = NULL;\n\t\tint *lane_ptr_index = NULL;\n\t\tconst uint8_t *warp_ptr = NULL;\n\n\n\t\tif(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\t//warp_ptr = &A[block_tile_j * M] +\n\t\t\t//\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * M_GLOBAL;\n\t\t\twarp_ptr = &A[block_tile_j * M];\n\t\t\t\n\t\t\tconst int *warp_ptr_index = &B_index[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t\t\t\t\t\t((warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A);\n\n\t\t\tlane_ptr_index = (int *)(warp_ptr_index + tile_k_idx_sparse + (laneId / CHUNK_COPY_LINE_LANES_A));\n\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_A) * SHARED_OFFSET_A;\n\t\t}else{\n\t\t\twarp_ptr = &B[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * K_GLOBAL_SPARSE;\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_sparse +\n\t\t\t\t\t\t\t\t(laneId / CHUNK_COPY_LINE_LANES_B) * K_GLOBAL_SPARSE) +\n\t\t\t\t\t\t\t\t(laneId % CHUNK_COPY_LINE_LANES_B);\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_B) * SHARED_OFFSET_B;\n\t\t}\n\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      // shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n\t  int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n\t  \t? BLOCK_SIZE_K_SPARSE / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A)\n\t\t: BLOCK_SIZE_N / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\n\t  /*\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\t  */\n\n\t  /*\n      int tile_k_idx_A;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n          tile_k_idx_A = *(lane_ptr_index);\n      }\n\t  */\n\n\t  #pragma unroll\n\t  for(int i = 0; i < iter_index; i += 1){\n\t\t  if(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\tint tile_k_idx_A = *(lane_ptr_index);\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_A * M_GLOBAL) + (laneId % CHUNK_COPY_LINE_LANES_A);\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_A)) =\n\t\t\t\t*lane_ptr;\n\t\t\t//warp_ptr = (uint8_t *)((uint8_t *)warp_ptr + M_GLOBAL * (WARPS_PER_BLOCK / 2) *CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tlane_ptr_index = (int *)((int *)lane_ptr_index +  (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A;\n\t\t  }else{\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_B)) =\n\t\t\t\t*lane_ptr;\n\t\t\tlane_ptr = (int4 *)((uint8_t *)lane_ptr + K_GLOBAL_SPARSE * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B;\n\t\t  }\n\t  }\n\n      __syncthreads();\n\n\t#pragma unroll\n      for (int k_step = 0; k_step < CHUNK_K_SPARSE; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::col_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n\t#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i += 1) {\n\t\t\tsize_t shmem_idx_a = (warpId % BLOCK_COL_WARPS) * M * WARP_COL_TILES + (i * M);\n\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_a + k_step * K * SHARED_OFFSET_A;\n\n\t\t\twmma::load_matrix_sync(a[i], tile_ptr, SHARED_OFFSET_A);\n\t\t#pragma unroll\n\t\t\tfor(int j = 0; j < WARP_ROW_TILES; j += 1){\n\t\t\t\tif(i == 0){\n\t\t\t\t\tsize_t shmem_idx_b = shmem_idx_b_off +\n\t\t\t\t\t\t\t\t\t\t\t(warpId / BLOCK_COL_WARPS) * (WARP_ROW_TILES * N) * SHARED_OFFSET_B +\n\t\t\t\t\t\t\t\t\t\t\t(j * N) * SHARED_OFFSET_B;\n\t\t\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_b + k_step * K;\n\t\t\t\t\twmma::load_matrix_sync(b[j], tile_ptr, SHARED_OFFSET_B);\n\t\t\t\t}\n\t\t\t\twmma::mma_sync(c[j][i], a[i], b[j], c[j][i]);\n\t\t\t}\n\n        }\n      }\n\n      __syncthreads();\n    }\n\n    // This pointer is used to access the C and D matrix tiles this warp computes.\n\tint *shmem_warp_tile_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * N * WARP_ROW_TILES * SHMEM_STRIDE +\n\t(warpId % BLOCK_COL_WARPS) * SHMEM_OFFSET;\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t#pragma unroll\n\t\t\tfor(int t = 0; t < c[i][j].num_elements; t += 1){\n\t\t\t\tc[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n\t\t\t}\n\t\t\tint *tile_ptr = shmem_warp_tile_ptr + i * N * SHMEM_STRIDE + j * M;\n\t\t\twmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, C_LAYOUT);\n\t\t}\n\t}\n\n    __syncthreads();\n\n\tint *shmem_warp_stream_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N * SHMEM_STRIDE\n\t\t\t\t\t\t\t\t\t+ (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tconst size_t gmem_idx =\n\t\t(block_tile_i * N + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N) * GLOBAL_MEM_STRIDE +\n\t\tblock_tile_j * M + (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tuint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n\tint *shmem_lane_stream_ptr =\n\t\tshmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * SHMEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\t\n\tuint8_t *dst_gmem_lane_stream_ptr =\n\t\tdst_gmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * GLOBAL_MEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\n\tfor(int i = 0; i < WARP_ROW_TILES * N; i += SHARED_TO_GLOBAL_LINES_PER_WARP){\n\t\t*(dst_gmem_lane_stream_ptr + i * GLOBAL_MEM_STRIDE) = (uint8_t)(*(shmem_lane_stream_ptr + i * SHMEM_STRIDE));\n\t}\n\n\t__syncthreads();\n}\n", "launch_config": {"dimBlock": [256, 1], "dimGrid": [256, 1]}}, "64": {"code": "\n__global__ void compute_gemm_imma(float *input0, float *input1, float *input2, float* input3, float* input4, float*input5, float* input6,\n                                  float *output0) {\n    // keys need to be replaced 4096, 1024, K_GLOBA_VALUEL, 0.75, 64\n\n\t//extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B = reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_index = reinterpret_cast<int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n\tconst int WARP_SIZE = 32;\n    const int M = 16;\n    const int K = 16;\n    const int N = 16;\n    const int WMMA_M = 16;\n    const int WMMA_K = 16;\n    const int WMMA_N = 16;\n    const int M_GLOBAL = 4096;\n    const int K_GLOBAL = 1024;\n    const int N_GLOBAL = 1024;\n    const float SPARSITY = 0.75;\n    const int K_GLOBAL_SPARSE = (int(K_GLOBAL * (1-SPARSITY)));\n    const int M_TILES = (M_GLOBAL / M);\n    const int N_TILES = (N_GLOBAL / N);\n    const int K_TILES = (K_GLOBAL / K);\n    const int BLOCK_ROW_WARPS = 2;\n    const int BLOCK_COL_WARPS = 4;\n    const int WARP_ROW_TILES = 4;\n    const int WARP_COL_TILES = 2;\n    const int SKEW_UINT8 = 16;\n    const int BANK_VAL = 32;\n    const int NUM_BANK = (K_GLOBAL / BANK_VAL);\n    /////////// BLOCK_ROW_TILES <= N_TILES ////////////\n    // const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    // ////////// BLOCK_COL_TILES <= M_TILES ////////////\n    // const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    auto C_LAYOUT = wmma::mem_col_major;\n    const int CHUNK_K = 64;\n    const int CHUNK_K_SPARSE = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K_SPARSE = (int((CHUNK_K * K) * (1 - SPARSITY)));\n    const int WARP_COPY_BYTES = (WARP_SIZE * sizeof(int4));\n    const int CHUNK_LINE_BYTES_A (BLOCK_COL_TILES * M * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_A = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_A);\n    const int CHUNK_COPY_LINE_LANES_A = (CHUNK_LINE_BYTES_A / sizeof(int4));\n    const int SHARED_OFFSET_A = (BLOCK_COL_TILES * M + SKEW_UINT8);\n\n    const int CHUNK_LINE_BYTES_B = (BLOCK_SIZE_K_SPARSE * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_B = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_B);\n    const int CHUNK_COPY_LINE_LANES_B = (CHUNK_LINE_BYTES_B / sizeof(int4));\n    const int SHARED_OFFSET_B = (BLOCK_SIZE_K_SPARSE + SKEW_UINT8);\n    const int SHARED_TO_GLOBAL_BYTES_PER_LINE = ((WARP_COL_TILES * M) * sizeof(int));\n    const int SHARED_TO_GLOBAL_BYTES_PER_WARP = (WARP_SIZE * sizeof(int));\n    const int SHARED_TO_GLOBAL_LINES_PER_WARP = (SHARED_TO_GLOBAL_BYTES_PER_WARP / SHARED_TO_GLOBAL_BYTES_PER_LINE);\n    const int SHARED_TO_GLOBAL_LANES_PER_LINE = (WARP_SIZE / SHARED_TO_GLOBAL_LINES_PER_WARP);\n    const int SHARED_TO_GLOBAL_ITERS = ((WARP_ROW_TILES * N) / SHARED_TO_GLOBAL_LINES_PER_WARP);\n\n    const int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const int WARP_STRIDE = (WARP_COL_TILES * M);\n    const int WARPS_PER_BLOCK = (BLOCK_ROW_WARPS * BLOCK_COL_WARPS);\n    const int THREADS_PER_BLOCK = (WARP_SIZE * WARPS_PER_BLOCK);\n    const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    const int GLOBAL_MEM_STRIDE = M_GLOBAL;\n    const int SHMEM_STRIDE = (M * BLOCK_COL_TILES);\n    const int SHMEM_OFFSET = (M * WARP_COL_TILES);\n    const int BLOCK_SIZE_M = (M * BLOCK_COL_TILES);\n    const int BLOCK_SIZE_N = (N * BLOCK_ROW_TILES);\n\n   extern __shared__ uint8_t shmem[];\n\n\t// Warp and lane identification.\n\tconst unsigned int warpId = threadIdx.x / WARP_SIZE;\n\tconst unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n\n\t// Offset in shared memory from which the B matrix is stored.\n\t// const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\tconst size_t shmem_idx_b_off = BLOCK_SIZE_K_SPARSE * SHARED_OFFSET_A;\n\n\n\t// Each CTA slides along the 128 x 128 tiles from the top left corner of the\n\t// matrix to the right and down, and selects the next tile to compute. Once\n\t// there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n\tconst unsigned int block_tile_i =\n\t\t((block_pos * BLOCK_COL_TILES) / M_TILES) * (BLOCK_ROW_TILES);\n\tconst unsigned int block_tile_j = (block_pos * BLOCK_COL_TILES) % M_TILES;\n\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n\twmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_ROW_TILES]\n\t\t\t\t\t\t\t\t\t\t\t\t\t [WARP_COL_TILES];\n\n    // Load the C matrix tiles into fragments from shared memory.\n\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t\twmma::fill_fragment(c[i][j], 0);\n\t\t}\n\t}\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    //for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n    for(int tile_k_idx_sparse = 0, tile_k_idx = 0; tile_k_idx_sparse < K_GLOBAL_SPARSE; tile_k_idx_sparse += BLOCK_SIZE_K_SPARSE, tile_k_idx += BLOCK_SIZE_K){\n\n\t\tsize_t shmem_idx = \n\t\twarpId < (WARPS_PER_BLOCK / 2)\n\t\t\t? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A\n\t\t\t: (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B + shmem_idx_b_off;\n\n\t\tint4 *lane_ptr = NULL;\n\t\tint *lane_ptr_index = NULL;\n\t\tconst uint8_t *warp_ptr = NULL;\n\n\n\t\tif(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\t//warp_ptr = &A[block_tile_j * M] +\n\t\t\t//\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * M_GLOBAL;\n\t\t\twarp_ptr = &A[block_tile_j * M];\n\t\t\t\n\t\t\tconst int *warp_ptr_index = &B_index[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t\t\t\t\t\t((warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A);\n\n\t\t\tlane_ptr_index = (int *)(warp_ptr_index + tile_k_idx_sparse + (laneId / CHUNK_COPY_LINE_LANES_A));\n\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_A) * SHARED_OFFSET_A;\n\t\t}else{\n\t\t\twarp_ptr = &B[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * K_GLOBAL_SPARSE;\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_sparse +\n\t\t\t\t\t\t\t\t(laneId / CHUNK_COPY_LINE_LANES_B) * K_GLOBAL_SPARSE) +\n\t\t\t\t\t\t\t\t(laneId % CHUNK_COPY_LINE_LANES_B);\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_B) * SHARED_OFFSET_B;\n\t\t}\n\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      // shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n\t  int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n\t  \t? BLOCK_SIZE_K_SPARSE / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A)\n\t\t: BLOCK_SIZE_N / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\n\t  /*\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\t  */\n\n\t  /*\n      int tile_k_idx_A;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n          tile_k_idx_A = *(lane_ptr_index);\n      }\n\t  */\n\n\t  #pragma unroll\n\t  for(int i = 0; i < iter_index; i += 1){\n\t\t  if(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\tint tile_k_idx_A = *(lane_ptr_index);\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_A * M_GLOBAL) + (laneId % CHUNK_COPY_LINE_LANES_A);\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_A)) =\n\t\t\t\t*lane_ptr;\n\t\t\t//warp_ptr = (uint8_t *)((uint8_t *)warp_ptr + M_GLOBAL * (WARPS_PER_BLOCK / 2) *CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tlane_ptr_index = (int *)((int *)lane_ptr_index +  (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A;\n\t\t  }else{\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_B)) =\n\t\t\t\t*lane_ptr;\n\t\t\tlane_ptr = (int4 *)((uint8_t *)lane_ptr + K_GLOBAL_SPARSE * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B;\n\t\t  }\n\t  }\n\n      __syncthreads();\n\n\t#pragma unroll\n      for (int k_step = 0; k_step < CHUNK_K_SPARSE; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::col_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n\t#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i += 1) {\n\t\t\tsize_t shmem_idx_a = (warpId % BLOCK_COL_WARPS) * M * WARP_COL_TILES + (i * M);\n\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_a + k_step * K * SHARED_OFFSET_A;\n\n\t\t\twmma::load_matrix_sync(a[i], tile_ptr, SHARED_OFFSET_A);\n\t\t#pragma unroll\n\t\t\tfor(int j = 0; j < WARP_ROW_TILES; j += 1){\n\t\t\t\tif(i == 0){\n\t\t\t\t\tsize_t shmem_idx_b = shmem_idx_b_off +\n\t\t\t\t\t\t\t\t\t\t\t(warpId / BLOCK_COL_WARPS) * (WARP_ROW_TILES * N) * SHARED_OFFSET_B +\n\t\t\t\t\t\t\t\t\t\t\t(j * N) * SHARED_OFFSET_B;\n\t\t\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_b + k_step * K;\n\t\t\t\t\twmma::load_matrix_sync(b[j], tile_ptr, SHARED_OFFSET_B);\n\t\t\t\t}\n\t\t\t\twmma::mma_sync(c[j][i], a[i], b[j], c[j][i]);\n\t\t\t}\n\n        }\n      }\n\n      __syncthreads();\n    }\n\n    // This pointer is used to access the C and D matrix tiles this warp computes.\n\tint *shmem_warp_tile_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * N * WARP_ROW_TILES * SHMEM_STRIDE +\n\t(warpId % BLOCK_COL_WARPS) * SHMEM_OFFSET;\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t#pragma unroll\n\t\t\tfor(int t = 0; t < c[i][j].num_elements; t += 1){\n\t\t\t\tc[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n\t\t\t}\n\t\t\tint *tile_ptr = shmem_warp_tile_ptr + i * N * SHMEM_STRIDE + j * M;\n\t\t\twmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, C_LAYOUT);\n\t\t}\n\t}\n\n    __syncthreads();\n\n\tint *shmem_warp_stream_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N * SHMEM_STRIDE\n\t\t\t\t\t\t\t\t\t+ (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tconst size_t gmem_idx =\n\t\t(block_tile_i * N + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N) * GLOBAL_MEM_STRIDE +\n\t\tblock_tile_j * M + (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tuint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n\tint *shmem_lane_stream_ptr =\n\t\tshmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * SHMEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\t\n\tuint8_t *dst_gmem_lane_stream_ptr =\n\t\tdst_gmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * GLOBAL_MEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\n\tfor(int i = 0; i < WARP_ROW_TILES * N; i += SHARED_TO_GLOBAL_LINES_PER_WARP){\n\t\t*(dst_gmem_lane_stream_ptr + i * GLOBAL_MEM_STRIDE) = (uint8_t)(*(shmem_lane_stream_ptr + i * SHMEM_STRIDE));\n\t}\n\n\t__syncthreads();\n}\n", "launch_config": {"dimBlock": [256, 1], "dimGrid": [256, 1]}}, "65": {"code": "\n__global__ void compute_gemm_imma(float *input0, float *input1, float *input2, float* input3, float* input4, float*input5, float* input6,\n                                  float *output0) {\n    // keys need to be replaced 4096, 4096, K_GLOBA_VALUEL, 0.75, 64\n\n\t//extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B = reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_index = reinterpret_cast<int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n\tconst int WARP_SIZE = 32;\n    const int M = 16;\n    const int K = 16;\n    const int N = 16;\n    const int WMMA_M = 16;\n    const int WMMA_K = 16;\n    const int WMMA_N = 16;\n    const int M_GLOBAL = 4096;\n    const int K_GLOBAL = 1024;\n    const int N_GLOBAL = 4096;\n    const float SPARSITY = 0.75;\n    const int K_GLOBAL_SPARSE = (int(K_GLOBAL * (1-SPARSITY)));\n    const int M_TILES = (M_GLOBAL / M);\n    const int N_TILES = (N_GLOBAL / N);\n    const int K_TILES = (K_GLOBAL / K);\n    const int BLOCK_ROW_WARPS = 2;\n    const int BLOCK_COL_WARPS = 4;\n    const int WARP_ROW_TILES = 4;\n    const int WARP_COL_TILES = 2;\n    const int SKEW_UINT8 = 16;\n    const int BANK_VAL = 32;\n    const int NUM_BANK = (K_GLOBAL / BANK_VAL);\n    /////////// BLOCK_ROW_TILES <= N_TILES ////////////\n    // const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    // ////////// BLOCK_COL_TILES <= M_TILES ////////////\n    // const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    auto C_LAYOUT = wmma::mem_col_major;\n    const int CHUNK_K = 64;\n    const int CHUNK_K_SPARSE = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K_SPARSE = (int((CHUNK_K * K) * (1 - SPARSITY)));\n    const int WARP_COPY_BYTES = (WARP_SIZE * sizeof(int4));\n    const int CHUNK_LINE_BYTES_A (BLOCK_COL_TILES * M * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_A = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_A);\n    const int CHUNK_COPY_LINE_LANES_A = (CHUNK_LINE_BYTES_A / sizeof(int4));\n    const int SHARED_OFFSET_A = (BLOCK_COL_TILES * M + SKEW_UINT8);\n\n    const int CHUNK_LINE_BYTES_B = (BLOCK_SIZE_K_SPARSE * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_B = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_B);\n    const int CHUNK_COPY_LINE_LANES_B = (CHUNK_LINE_BYTES_B / sizeof(int4));\n    const int SHARED_OFFSET_B = (BLOCK_SIZE_K_SPARSE + SKEW_UINT8);\n    const int SHARED_TO_GLOBAL_BYTES_PER_LINE = ((WARP_COL_TILES * M) * sizeof(int));\n    const int SHARED_TO_GLOBAL_BYTES_PER_WARP = (WARP_SIZE * sizeof(int));\n    const int SHARED_TO_GLOBAL_LINES_PER_WARP = (SHARED_TO_GLOBAL_BYTES_PER_WARP / SHARED_TO_GLOBAL_BYTES_PER_LINE);\n    const int SHARED_TO_GLOBAL_LANES_PER_LINE = (WARP_SIZE / SHARED_TO_GLOBAL_LINES_PER_WARP);\n    const int SHARED_TO_GLOBAL_ITERS = ((WARP_ROW_TILES * N) / SHARED_TO_GLOBAL_LINES_PER_WARP);\n\n    const int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const int WARP_STRIDE = (WARP_COL_TILES * M);\n    const int WARPS_PER_BLOCK = (BLOCK_ROW_WARPS * BLOCK_COL_WARPS);\n    const int THREADS_PER_BLOCK = (WARP_SIZE * WARPS_PER_BLOCK);\n    const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    const int GLOBAL_MEM_STRIDE = M_GLOBAL;\n    const int SHMEM_STRIDE = (M * BLOCK_COL_TILES);\n    const int SHMEM_OFFSET = (M * WARP_COL_TILES);\n    const int BLOCK_SIZE_M = (M * BLOCK_COL_TILES);\n    const int BLOCK_SIZE_N = (N * BLOCK_ROW_TILES);\n\n   extern __shared__ uint8_t shmem[];\n\n\t// Warp and lane identification.\n\tconst unsigned int warpId = threadIdx.x / WARP_SIZE;\n\tconst unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n\n\t// Offset in shared memory from which the B matrix is stored.\n\t// const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\tconst size_t shmem_idx_b_off = BLOCK_SIZE_K_SPARSE * SHARED_OFFSET_A;\n\n\n\t// Each CTA slides along the 128 x 128 tiles from the top left corner of the\n\t// matrix to the right and down, and selects the next tile to compute. Once\n\t// there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n\tconst unsigned int block_tile_i =\n\t\t((block_pos * BLOCK_COL_TILES) / M_TILES) * (BLOCK_ROW_TILES);\n\tconst unsigned int block_tile_j = (block_pos * BLOCK_COL_TILES) % M_TILES;\n\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n\twmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_ROW_TILES]\n\t\t\t\t\t\t\t\t\t\t\t\t\t [WARP_COL_TILES];\n\n    // Load the C matrix tiles into fragments from shared memory.\n\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t\twmma::fill_fragment(c[i][j], 0);\n\t\t}\n\t}\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    //for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n    for(int tile_k_idx_sparse = 0, tile_k_idx = 0; tile_k_idx_sparse < K_GLOBAL_SPARSE; tile_k_idx_sparse += BLOCK_SIZE_K_SPARSE, tile_k_idx += BLOCK_SIZE_K){\n\n\t\tsize_t shmem_idx = \n\t\twarpId < (WARPS_PER_BLOCK / 2)\n\t\t\t? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A\n\t\t\t: (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B + shmem_idx_b_off;\n\n\t\tint4 *lane_ptr = NULL;\n\t\tint *lane_ptr_index = NULL;\n\t\tconst uint8_t *warp_ptr = NULL;\n\n\n\t\tif(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\t//warp_ptr = &A[block_tile_j * M] +\n\t\t\t//\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * M_GLOBAL;\n\t\t\twarp_ptr = &A[block_tile_j * M];\n\t\t\t\n\t\t\tconst int *warp_ptr_index = &B_index[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t\t\t\t\t\t((warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A);\n\n\t\t\tlane_ptr_index = (int *)(warp_ptr_index + tile_k_idx_sparse + (laneId / CHUNK_COPY_LINE_LANES_A));\n\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_A) * SHARED_OFFSET_A;\n\t\t}else{\n\t\t\twarp_ptr = &B[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * K_GLOBAL_SPARSE;\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_sparse +\n\t\t\t\t\t\t\t\t(laneId / CHUNK_COPY_LINE_LANES_B) * K_GLOBAL_SPARSE) +\n\t\t\t\t\t\t\t\t(laneId % CHUNK_COPY_LINE_LANES_B);\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_B) * SHARED_OFFSET_B;\n\t\t}\n\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      // shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n\t  int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n\t  \t? BLOCK_SIZE_K_SPARSE / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A)\n\t\t: BLOCK_SIZE_N / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\n\t  /*\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\t  */\n\n\t  /*\n      int tile_k_idx_A;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n          tile_k_idx_A = *(lane_ptr_index);\n      }\n\t  */\n\n\t  #pragma unroll\n\t  for(int i = 0; i < iter_index; i += 1){\n\t\t  if(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\tint tile_k_idx_A = *(lane_ptr_index);\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_A * M_GLOBAL) + (laneId % CHUNK_COPY_LINE_LANES_A);\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_A)) =\n\t\t\t\t*lane_ptr;\n\t\t\t//warp_ptr = (uint8_t *)((uint8_t *)warp_ptr + M_GLOBAL * (WARPS_PER_BLOCK / 2) *CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tlane_ptr_index = (int *)((int *)lane_ptr_index +  (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A;\n\t\t  }else{\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_B)) =\n\t\t\t\t*lane_ptr;\n\t\t\tlane_ptr = (int4 *)((uint8_t *)lane_ptr + K_GLOBAL_SPARSE * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B;\n\t\t  }\n\t  }\n\n      __syncthreads();\n\n\t#pragma unroll\n      for (int k_step = 0; k_step < CHUNK_K_SPARSE; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::col_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n\t#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i += 1) {\n\t\t\tsize_t shmem_idx_a = (warpId % BLOCK_COL_WARPS) * M * WARP_COL_TILES + (i * M);\n\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_a + k_step * K * SHARED_OFFSET_A;\n\n\t\t\twmma::load_matrix_sync(a[i], tile_ptr, SHARED_OFFSET_A);\n\t\t#pragma unroll\n\t\t\tfor(int j = 0; j < WARP_ROW_TILES; j += 1){\n\t\t\t\tif(i == 0){\n\t\t\t\t\tsize_t shmem_idx_b = shmem_idx_b_off +\n\t\t\t\t\t\t\t\t\t\t\t(warpId / BLOCK_COL_WARPS) * (WARP_ROW_TILES * N) * SHARED_OFFSET_B +\n\t\t\t\t\t\t\t\t\t\t\t(j * N) * SHARED_OFFSET_B;\n\t\t\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_b + k_step * K;\n\t\t\t\t\twmma::load_matrix_sync(b[j], tile_ptr, SHARED_OFFSET_B);\n\t\t\t\t}\n\t\t\t\twmma::mma_sync(c[j][i], a[i], b[j], c[j][i]);\n\t\t\t}\n\n        }\n      }\n\n      __syncthreads();\n    }\n\n    // This pointer is used to access the C and D matrix tiles this warp computes.\n\tint *shmem_warp_tile_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * N * WARP_ROW_TILES * SHMEM_STRIDE +\n\t(warpId % BLOCK_COL_WARPS) * SHMEM_OFFSET;\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t#pragma unroll\n\t\t\tfor(int t = 0; t < c[i][j].num_elements; t += 1){\n\t\t\t\tc[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n\t\t\t}\n\t\t\tint *tile_ptr = shmem_warp_tile_ptr + i * N * SHMEM_STRIDE + j * M;\n\t\t\twmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, C_LAYOUT);\n\t\t}\n\t}\n\n    __syncthreads();\n\n\tint *shmem_warp_stream_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N * SHMEM_STRIDE\n\t\t\t\t\t\t\t\t\t+ (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tconst size_t gmem_idx =\n\t\t(block_tile_i * N + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N) * GLOBAL_MEM_STRIDE +\n\t\tblock_tile_j * M + (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tuint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n\tint *shmem_lane_stream_ptr =\n\t\tshmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * SHMEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\t\n\tuint8_t *dst_gmem_lane_stream_ptr =\n\t\tdst_gmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * GLOBAL_MEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\n\tfor(int i = 0; i < WARP_ROW_TILES * N; i += SHARED_TO_GLOBAL_LINES_PER_WARP){\n\t\t*(dst_gmem_lane_stream_ptr + i * GLOBAL_MEM_STRIDE) = (uint8_t)(*(shmem_lane_stream_ptr + i * SHMEM_STRIDE));\n\t}\n\n\t__syncthreads();\n}\n", "launch_config": {"dimBlock": [1024, 1], "dimGrid": [256, 1]}}, "66": {"code": "\n__global__ void compute_gemm_imma(float *input0, float *input1, float *input2, float* input3, float* input4, float*input5, float* input6,\n                                  float *output0) {\n    // keys need to be replaced 4096, 1024, K_GLOBA_VALUEL, 0.75, 64\n\n\t//extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B = reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_index = reinterpret_cast<int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n\tconst int WARP_SIZE = 32;\n    const int M = 16;\n    const int K = 16;\n    const int N = 16;\n    const int WMMA_M = 16;\n    const int WMMA_K = 16;\n    const int WMMA_N = 16;\n    const int M_GLOBAL = 4096;\n    const int K_GLOBAL = 4096;\n    const int N_GLOBAL = 1024;\n    const float SPARSITY = 0.75;\n    const int K_GLOBAL_SPARSE = (int(K_GLOBAL * (1-SPARSITY)));\n    const int M_TILES = (M_GLOBAL / M);\n    const int N_TILES = (N_GLOBAL / N);\n    const int K_TILES = (K_GLOBAL / K);\n    const int BLOCK_ROW_WARPS = 2;\n    const int BLOCK_COL_WARPS = 4;\n    const int WARP_ROW_TILES = 4;\n    const int WARP_COL_TILES = 2;\n    const int SKEW_UINT8 = 16;\n    const int BANK_VAL = 32;\n    const int NUM_BANK = (K_GLOBAL / BANK_VAL);\n    /////////// BLOCK_ROW_TILES <= N_TILES ////////////\n    // const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    // ////////// BLOCK_COL_TILES <= M_TILES ////////////\n    // const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    auto C_LAYOUT = wmma::mem_col_major;\n    const int CHUNK_K = 64;\n    const int CHUNK_K_SPARSE = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K_SPARSE = (int((CHUNK_K * K) * (1 - SPARSITY)));\n    const int WARP_COPY_BYTES = (WARP_SIZE * sizeof(int4));\n    const int CHUNK_LINE_BYTES_A (BLOCK_COL_TILES * M * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_A = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_A);\n    const int CHUNK_COPY_LINE_LANES_A = (CHUNK_LINE_BYTES_A / sizeof(int4));\n    const int SHARED_OFFSET_A = (BLOCK_COL_TILES * M + SKEW_UINT8);\n\n    const int CHUNK_LINE_BYTES_B = (BLOCK_SIZE_K_SPARSE * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_B = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_B);\n    const int CHUNK_COPY_LINE_LANES_B = (CHUNK_LINE_BYTES_B / sizeof(int4));\n    const int SHARED_OFFSET_B = (BLOCK_SIZE_K_SPARSE + SKEW_UINT8);\n    const int SHARED_TO_GLOBAL_BYTES_PER_LINE = ((WARP_COL_TILES * M) * sizeof(int));\n    const int SHARED_TO_GLOBAL_BYTES_PER_WARP = (WARP_SIZE * sizeof(int));\n    const int SHARED_TO_GLOBAL_LINES_PER_WARP = (SHARED_TO_GLOBAL_BYTES_PER_WARP / SHARED_TO_GLOBAL_BYTES_PER_LINE);\n    const int SHARED_TO_GLOBAL_LANES_PER_LINE = (WARP_SIZE / SHARED_TO_GLOBAL_LINES_PER_WARP);\n    const int SHARED_TO_GLOBAL_ITERS = ((WARP_ROW_TILES * N) / SHARED_TO_GLOBAL_LINES_PER_WARP);\n\n    const int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const int WARP_STRIDE = (WARP_COL_TILES * M);\n    const int WARPS_PER_BLOCK = (BLOCK_ROW_WARPS * BLOCK_COL_WARPS);\n    const int THREADS_PER_BLOCK = (WARP_SIZE * WARPS_PER_BLOCK);\n    const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    const int GLOBAL_MEM_STRIDE = M_GLOBAL;\n    const int SHMEM_STRIDE = (M * BLOCK_COL_TILES);\n    const int SHMEM_OFFSET = (M * WARP_COL_TILES);\n    const int BLOCK_SIZE_M = (M * BLOCK_COL_TILES);\n    const int BLOCK_SIZE_N = (N * BLOCK_ROW_TILES);\n\n   extern __shared__ uint8_t shmem[];\n\n\t// Warp and lane identification.\n\tconst unsigned int warpId = threadIdx.x / WARP_SIZE;\n\tconst unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n\n\t// Offset in shared memory from which the B matrix is stored.\n\t// const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\tconst size_t shmem_idx_b_off = BLOCK_SIZE_K_SPARSE * SHARED_OFFSET_A;\n\n\n\t// Each CTA slides along the 128 x 128 tiles from the top left corner of the\n\t// matrix to the right and down, and selects the next tile to compute. Once\n\t// there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n\tconst unsigned int block_tile_i =\n\t\t((block_pos * BLOCK_COL_TILES) / M_TILES) * (BLOCK_ROW_TILES);\n\tconst unsigned int block_tile_j = (block_pos * BLOCK_COL_TILES) % M_TILES;\n\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n\twmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_ROW_TILES]\n\t\t\t\t\t\t\t\t\t\t\t\t\t [WARP_COL_TILES];\n\n    // Load the C matrix tiles into fragments from shared memory.\n\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t\twmma::fill_fragment(c[i][j], 0);\n\t\t}\n\t}\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    //for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n    for(int tile_k_idx_sparse = 0, tile_k_idx = 0; tile_k_idx_sparse < K_GLOBAL_SPARSE; tile_k_idx_sparse += BLOCK_SIZE_K_SPARSE, tile_k_idx += BLOCK_SIZE_K){\n\n\t\tsize_t shmem_idx = \n\t\twarpId < (WARPS_PER_BLOCK / 2)\n\t\t\t? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A\n\t\t\t: (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B + shmem_idx_b_off;\n\n\t\tint4 *lane_ptr = NULL;\n\t\tint *lane_ptr_index = NULL;\n\t\tconst uint8_t *warp_ptr = NULL;\n\n\n\t\tif(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\t//warp_ptr = &A[block_tile_j * M] +\n\t\t\t//\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * M_GLOBAL;\n\t\t\twarp_ptr = &A[block_tile_j * M];\n\t\t\t\n\t\t\tconst int *warp_ptr_index = &B_index[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t\t\t\t\t\t((warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A);\n\n\t\t\tlane_ptr_index = (int *)(warp_ptr_index + tile_k_idx_sparse + (laneId / CHUNK_COPY_LINE_LANES_A));\n\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_A) * SHARED_OFFSET_A;\n\t\t}else{\n\t\t\twarp_ptr = &B[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * K_GLOBAL_SPARSE;\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_sparse +\n\t\t\t\t\t\t\t\t(laneId / CHUNK_COPY_LINE_LANES_B) * K_GLOBAL_SPARSE) +\n\t\t\t\t\t\t\t\t(laneId % CHUNK_COPY_LINE_LANES_B);\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_B) * SHARED_OFFSET_B;\n\t\t}\n\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      // shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n\t  int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n\t  \t? BLOCK_SIZE_K_SPARSE / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A)\n\t\t: BLOCK_SIZE_N / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\n\t  /*\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\t  */\n\n\t  /*\n      int tile_k_idx_A;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n          tile_k_idx_A = *(lane_ptr_index);\n      }\n\t  */\n\n\t  #pragma unroll\n\t  for(int i = 0; i < iter_index; i += 1){\n\t\t  if(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\tint tile_k_idx_A = *(lane_ptr_index);\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_A * M_GLOBAL) + (laneId % CHUNK_COPY_LINE_LANES_A);\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_A)) =\n\t\t\t\t*lane_ptr;\n\t\t\t//warp_ptr = (uint8_t *)((uint8_t *)warp_ptr + M_GLOBAL * (WARPS_PER_BLOCK / 2) *CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tlane_ptr_index = (int *)((int *)lane_ptr_index +  (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A;\n\t\t  }else{\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_B)) =\n\t\t\t\t*lane_ptr;\n\t\t\tlane_ptr = (int4 *)((uint8_t *)lane_ptr + K_GLOBAL_SPARSE * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B;\n\t\t  }\n\t  }\n\n      __syncthreads();\n\n\t#pragma unroll\n      for (int k_step = 0; k_step < CHUNK_K_SPARSE; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::col_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n\t#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i += 1) {\n\t\t\tsize_t shmem_idx_a = (warpId % BLOCK_COL_WARPS) * M * WARP_COL_TILES + (i * M);\n\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_a + k_step * K * SHARED_OFFSET_A;\n\n\t\t\twmma::load_matrix_sync(a[i], tile_ptr, SHARED_OFFSET_A);\n\t\t#pragma unroll\n\t\t\tfor(int j = 0; j < WARP_ROW_TILES; j += 1){\n\t\t\t\tif(i == 0){\n\t\t\t\t\tsize_t shmem_idx_b = shmem_idx_b_off +\n\t\t\t\t\t\t\t\t\t\t\t(warpId / BLOCK_COL_WARPS) * (WARP_ROW_TILES * N) * SHARED_OFFSET_B +\n\t\t\t\t\t\t\t\t\t\t\t(j * N) * SHARED_OFFSET_B;\n\t\t\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_b + k_step * K;\n\t\t\t\t\twmma::load_matrix_sync(b[j], tile_ptr, SHARED_OFFSET_B);\n\t\t\t\t}\n\t\t\t\twmma::mma_sync(c[j][i], a[i], b[j], c[j][i]);\n\t\t\t}\n\n        }\n      }\n\n      __syncthreads();\n    }\n\n    // This pointer is used to access the C and D matrix tiles this warp computes.\n\tint *shmem_warp_tile_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * N * WARP_ROW_TILES * SHMEM_STRIDE +\n\t(warpId % BLOCK_COL_WARPS) * SHMEM_OFFSET;\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t#pragma unroll\n\t\t\tfor(int t = 0; t < c[i][j].num_elements; t += 1){\n\t\t\t\tc[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n\t\t\t}\n\t\t\tint *tile_ptr = shmem_warp_tile_ptr + i * N * SHMEM_STRIDE + j * M;\n\t\t\twmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, C_LAYOUT);\n\t\t}\n\t}\n\n    __syncthreads();\n\n\tint *shmem_warp_stream_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N * SHMEM_STRIDE\n\t\t\t\t\t\t\t\t\t+ (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tconst size_t gmem_idx =\n\t\t(block_tile_i * N + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N) * GLOBAL_MEM_STRIDE +\n\t\tblock_tile_j * M + (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tuint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n\tint *shmem_lane_stream_ptr =\n\t\tshmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * SHMEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\t\n\tuint8_t *dst_gmem_lane_stream_ptr =\n\t\tdst_gmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * GLOBAL_MEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\n\tfor(int i = 0; i < WARP_ROW_TILES * N; i += SHARED_TO_GLOBAL_LINES_PER_WARP){\n\t\t*(dst_gmem_lane_stream_ptr + i * GLOBAL_MEM_STRIDE) = (uint8_t)(*(shmem_lane_stream_ptr + i * SHMEM_STRIDE));\n\t}\n\n\t__syncthreads();\n}\n", "launch_config": {"dimBlock": [256, 1], "dimGrid": [256, 1]}}, "67": {"code": "\n__global__ void compute_gemm_imma(float *input0, float *input1, float *input2, float* input3, float* input4, float*input5, float* input6,\n                                  float *output0) {\n    // keys need to be replaced 4096, 1024, K_GLOBA_VALUEL, 0.75, 64\n\n\t//extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B = reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_index = reinterpret_cast<int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n\tconst int WARP_SIZE = 32;\n    const int M = 16;\n    const int K = 16;\n    const int N = 16;\n    const int WMMA_M = 16;\n    const int WMMA_K = 16;\n    const int WMMA_N = 16;\n    const int M_GLOBAL = 4096;\n    const int K_GLOBAL = 1024;\n    const int N_GLOBAL = 1024;\n    const float SPARSITY = 0.75;\n    const int K_GLOBAL_SPARSE = (int(K_GLOBAL * (1-SPARSITY)));\n    const int M_TILES = (M_GLOBAL / M);\n    const int N_TILES = (N_GLOBAL / N);\n    const int K_TILES = (K_GLOBAL / K);\n    const int BLOCK_ROW_WARPS = 2;\n    const int BLOCK_COL_WARPS = 4;\n    const int WARP_ROW_TILES = 4;\n    const int WARP_COL_TILES = 2;\n    const int SKEW_UINT8 = 16;\n    const int BANK_VAL = 32;\n    const int NUM_BANK = (K_GLOBAL / BANK_VAL);\n    /////////// BLOCK_ROW_TILES <= N_TILES ////////////\n    // const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    // ////////// BLOCK_COL_TILES <= M_TILES ////////////\n    // const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    auto C_LAYOUT = wmma::mem_col_major;\n    const int CHUNK_K = 64;\n    const int CHUNK_K_SPARSE = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K_SPARSE = (int((CHUNK_K * K) * (1 - SPARSITY)));\n    const int WARP_COPY_BYTES = (WARP_SIZE * sizeof(int4));\n    const int CHUNK_LINE_BYTES_A (BLOCK_COL_TILES * M * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_A = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_A);\n    const int CHUNK_COPY_LINE_LANES_A = (CHUNK_LINE_BYTES_A / sizeof(int4));\n    const int SHARED_OFFSET_A = (BLOCK_COL_TILES * M + SKEW_UINT8);\n\n    const int CHUNK_LINE_BYTES_B = (BLOCK_SIZE_K_SPARSE * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_B = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_B);\n    const int CHUNK_COPY_LINE_LANES_B = (CHUNK_LINE_BYTES_B / sizeof(int4));\n    const int SHARED_OFFSET_B = (BLOCK_SIZE_K_SPARSE + SKEW_UINT8);\n    const int SHARED_TO_GLOBAL_BYTES_PER_LINE = ((WARP_COL_TILES * M) * sizeof(int));\n    const int SHARED_TO_GLOBAL_BYTES_PER_WARP = (WARP_SIZE * sizeof(int));\n    const int SHARED_TO_GLOBAL_LINES_PER_WARP = (SHARED_TO_GLOBAL_BYTES_PER_WARP / SHARED_TO_GLOBAL_BYTES_PER_LINE);\n    const int SHARED_TO_GLOBAL_LANES_PER_LINE = (WARP_SIZE / SHARED_TO_GLOBAL_LINES_PER_WARP);\n    const int SHARED_TO_GLOBAL_ITERS = ((WARP_ROW_TILES * N) / SHARED_TO_GLOBAL_LINES_PER_WARP);\n\n    const int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const int WARP_STRIDE = (WARP_COL_TILES * M);\n    const int WARPS_PER_BLOCK = (BLOCK_ROW_WARPS * BLOCK_COL_WARPS);\n    const int THREADS_PER_BLOCK = (WARP_SIZE * WARPS_PER_BLOCK);\n    const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    const int GLOBAL_MEM_STRIDE = M_GLOBAL;\n    const int SHMEM_STRIDE = (M * BLOCK_COL_TILES);\n    const int SHMEM_OFFSET = (M * WARP_COL_TILES);\n    const int BLOCK_SIZE_M = (M * BLOCK_COL_TILES);\n    const int BLOCK_SIZE_N = (N * BLOCK_ROW_TILES);\n\n   extern __shared__ uint8_t shmem[];\n\n\t// Warp and lane identification.\n\tconst unsigned int warpId = threadIdx.x / WARP_SIZE;\n\tconst unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n\n\t// Offset in shared memory from which the B matrix is stored.\n\t// const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\tconst size_t shmem_idx_b_off = BLOCK_SIZE_K_SPARSE * SHARED_OFFSET_A;\n\n\n\t// Each CTA slides along the 128 x 128 tiles from the top left corner of the\n\t// matrix to the right and down, and selects the next tile to compute. Once\n\t// there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n\tconst unsigned int block_tile_i =\n\t\t((block_pos * BLOCK_COL_TILES) / M_TILES) * (BLOCK_ROW_TILES);\n\tconst unsigned int block_tile_j = (block_pos * BLOCK_COL_TILES) % M_TILES;\n\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n\twmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_ROW_TILES]\n\t\t\t\t\t\t\t\t\t\t\t\t\t [WARP_COL_TILES];\n\n    // Load the C matrix tiles into fragments from shared memory.\n\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t\twmma::fill_fragment(c[i][j], 0);\n\t\t}\n\t}\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    //for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n    for(int tile_k_idx_sparse = 0, tile_k_idx = 0; tile_k_idx_sparse < K_GLOBAL_SPARSE; tile_k_idx_sparse += BLOCK_SIZE_K_SPARSE, tile_k_idx += BLOCK_SIZE_K){\n\n\t\tsize_t shmem_idx = \n\t\twarpId < (WARPS_PER_BLOCK / 2)\n\t\t\t? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A\n\t\t\t: (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B + shmem_idx_b_off;\n\n\t\tint4 *lane_ptr = NULL;\n\t\tint *lane_ptr_index = NULL;\n\t\tconst uint8_t *warp_ptr = NULL;\n\n\n\t\tif(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\t//warp_ptr = &A[block_tile_j * M] +\n\t\t\t//\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * M_GLOBAL;\n\t\t\twarp_ptr = &A[block_tile_j * M];\n\t\t\t\n\t\t\tconst int *warp_ptr_index = &B_index[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t\t\t\t\t\t((warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A);\n\n\t\t\tlane_ptr_index = (int *)(warp_ptr_index + tile_k_idx_sparse + (laneId / CHUNK_COPY_LINE_LANES_A));\n\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_A) * SHARED_OFFSET_A;\n\t\t}else{\n\t\t\twarp_ptr = &B[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * K_GLOBAL_SPARSE;\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_sparse +\n\t\t\t\t\t\t\t\t(laneId / CHUNK_COPY_LINE_LANES_B) * K_GLOBAL_SPARSE) +\n\t\t\t\t\t\t\t\t(laneId % CHUNK_COPY_LINE_LANES_B);\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_B) * SHARED_OFFSET_B;\n\t\t}\n\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      // shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n\t  int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n\t  \t? BLOCK_SIZE_K_SPARSE / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A)\n\t\t: BLOCK_SIZE_N / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\n\t  /*\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\t  */\n\n\t  /*\n      int tile_k_idx_A;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n          tile_k_idx_A = *(lane_ptr_index);\n      }\n\t  */\n\n\t  #pragma unroll\n\t  for(int i = 0; i < iter_index; i += 1){\n\t\t  if(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\tint tile_k_idx_A = *(lane_ptr_index);\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_A * M_GLOBAL) + (laneId % CHUNK_COPY_LINE_LANES_A);\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_A)) =\n\t\t\t\t*lane_ptr;\n\t\t\t//warp_ptr = (uint8_t *)((uint8_t *)warp_ptr + M_GLOBAL * (WARPS_PER_BLOCK / 2) *CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tlane_ptr_index = (int *)((int *)lane_ptr_index +  (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A;\n\t\t  }else{\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_B)) =\n\t\t\t\t*lane_ptr;\n\t\t\tlane_ptr = (int4 *)((uint8_t *)lane_ptr + K_GLOBAL_SPARSE * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B;\n\t\t  }\n\t  }\n\n      __syncthreads();\n\n\t#pragma unroll\n      for (int k_step = 0; k_step < CHUNK_K_SPARSE; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::col_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n\t#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i += 1) {\n\t\t\tsize_t shmem_idx_a = (warpId % BLOCK_COL_WARPS) * M * WARP_COL_TILES + (i * M);\n\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_a + k_step * K * SHARED_OFFSET_A;\n\n\t\t\twmma::load_matrix_sync(a[i], tile_ptr, SHARED_OFFSET_A);\n\t\t#pragma unroll\n\t\t\tfor(int j = 0; j < WARP_ROW_TILES; j += 1){\n\t\t\t\tif(i == 0){\n\t\t\t\t\tsize_t shmem_idx_b = shmem_idx_b_off +\n\t\t\t\t\t\t\t\t\t\t\t(warpId / BLOCK_COL_WARPS) * (WARP_ROW_TILES * N) * SHARED_OFFSET_B +\n\t\t\t\t\t\t\t\t\t\t\t(j * N) * SHARED_OFFSET_B;\n\t\t\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_b + k_step * K;\n\t\t\t\t\twmma::load_matrix_sync(b[j], tile_ptr, SHARED_OFFSET_B);\n\t\t\t\t}\n\t\t\t\twmma::mma_sync(c[j][i], a[i], b[j], c[j][i]);\n\t\t\t}\n\n        }\n      }\n\n      __syncthreads();\n    }\n\n    // This pointer is used to access the C and D matrix tiles this warp computes.\n\tint *shmem_warp_tile_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * N * WARP_ROW_TILES * SHMEM_STRIDE +\n\t(warpId % BLOCK_COL_WARPS) * SHMEM_OFFSET;\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t#pragma unroll\n\t\t\tfor(int t = 0; t < c[i][j].num_elements; t += 1){\n\t\t\t\tc[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n\t\t\t}\n\t\t\tint *tile_ptr = shmem_warp_tile_ptr + i * N * SHMEM_STRIDE + j * M;\n\t\t\twmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, C_LAYOUT);\n\t\t}\n\t}\n\n    __syncthreads();\n\n\tint *shmem_warp_stream_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N * SHMEM_STRIDE\n\t\t\t\t\t\t\t\t\t+ (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tconst size_t gmem_idx =\n\t\t(block_tile_i * N + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N) * GLOBAL_MEM_STRIDE +\n\t\tblock_tile_j * M + (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tuint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n\tint *shmem_lane_stream_ptr =\n\t\tshmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * SHMEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\t\n\tuint8_t *dst_gmem_lane_stream_ptr =\n\t\tdst_gmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * GLOBAL_MEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\n\tfor(int i = 0; i < WARP_ROW_TILES * N; i += SHARED_TO_GLOBAL_LINES_PER_WARP){\n\t\t*(dst_gmem_lane_stream_ptr + i * GLOBAL_MEM_STRIDE) = (uint8_t)(*(shmem_lane_stream_ptr + i * SHMEM_STRIDE));\n\t}\n\n\t__syncthreads();\n}\n", "launch_config": {"dimBlock": [256, 1], "dimGrid": [256, 1]}}, "68": {"code": "\n__global__ void compute_gemm_imma(float *input0, float *input1, float *input2, float* input3, float* input4, float*input5, float* input6,\n                                  float *output0) {\n    // keys need to be replaced 4096, 1024, K_GLOBA_VALUEL, 0.75, 64\n\n\t//extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B = reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_index = reinterpret_cast<int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n\tconst int WARP_SIZE = 32;\n    const int M = 16;\n    const int K = 16;\n    const int N = 16;\n    const int WMMA_M = 16;\n    const int WMMA_K = 16;\n    const int WMMA_N = 16;\n    const int M_GLOBAL = 4096;\n    const int K_GLOBAL = 1024;\n    const int N_GLOBAL = 1024;\n    const float SPARSITY = 0.75;\n    const int K_GLOBAL_SPARSE = (int(K_GLOBAL * (1-SPARSITY)));\n    const int M_TILES = (M_GLOBAL / M);\n    const int N_TILES = (N_GLOBAL / N);\n    const int K_TILES = (K_GLOBAL / K);\n    const int BLOCK_ROW_WARPS = 2;\n    const int BLOCK_COL_WARPS = 4;\n    const int WARP_ROW_TILES = 4;\n    const int WARP_COL_TILES = 2;\n    const int SKEW_UINT8 = 16;\n    const int BANK_VAL = 32;\n    const int NUM_BANK = (K_GLOBAL / BANK_VAL);\n    /////////// BLOCK_ROW_TILES <= N_TILES ////////////\n    // const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    // ////////// BLOCK_COL_TILES <= M_TILES ////////////\n    // const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    auto C_LAYOUT = wmma::mem_col_major;\n    const int CHUNK_K = 64;\n    const int CHUNK_K_SPARSE = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K_SPARSE = (int((CHUNK_K * K) * (1 - SPARSITY)));\n    const int WARP_COPY_BYTES = (WARP_SIZE * sizeof(int4));\n    const int CHUNK_LINE_BYTES_A (BLOCK_COL_TILES * M * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_A = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_A);\n    const int CHUNK_COPY_LINE_LANES_A = (CHUNK_LINE_BYTES_A / sizeof(int4));\n    const int SHARED_OFFSET_A = (BLOCK_COL_TILES * M + SKEW_UINT8);\n\n    const int CHUNK_LINE_BYTES_B = (BLOCK_SIZE_K_SPARSE * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_B = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_B);\n    const int CHUNK_COPY_LINE_LANES_B = (CHUNK_LINE_BYTES_B / sizeof(int4));\n    const int SHARED_OFFSET_B = (BLOCK_SIZE_K_SPARSE + SKEW_UINT8);\n    const int SHARED_TO_GLOBAL_BYTES_PER_LINE = ((WARP_COL_TILES * M) * sizeof(int));\n    const int SHARED_TO_GLOBAL_BYTES_PER_WARP = (WARP_SIZE * sizeof(int));\n    const int SHARED_TO_GLOBAL_LINES_PER_WARP = (SHARED_TO_GLOBAL_BYTES_PER_WARP / SHARED_TO_GLOBAL_BYTES_PER_LINE);\n    const int SHARED_TO_GLOBAL_LANES_PER_LINE = (WARP_SIZE / SHARED_TO_GLOBAL_LINES_PER_WARP);\n    const int SHARED_TO_GLOBAL_ITERS = ((WARP_ROW_TILES * N) / SHARED_TO_GLOBAL_LINES_PER_WARP);\n\n    const int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const int WARP_STRIDE = (WARP_COL_TILES * M);\n    const int WARPS_PER_BLOCK = (BLOCK_ROW_WARPS * BLOCK_COL_WARPS);\n    const int THREADS_PER_BLOCK = (WARP_SIZE * WARPS_PER_BLOCK);\n    const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    const int GLOBAL_MEM_STRIDE = M_GLOBAL;\n    const int SHMEM_STRIDE = (M * BLOCK_COL_TILES);\n    const int SHMEM_OFFSET = (M * WARP_COL_TILES);\n    const int BLOCK_SIZE_M = (M * BLOCK_COL_TILES);\n    const int BLOCK_SIZE_N = (N * BLOCK_ROW_TILES);\n\n   extern __shared__ uint8_t shmem[];\n\n\t// Warp and lane identification.\n\tconst unsigned int warpId = threadIdx.x / WARP_SIZE;\n\tconst unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n\n\t// Offset in shared memory from which the B matrix is stored.\n\t// const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\tconst size_t shmem_idx_b_off = BLOCK_SIZE_K_SPARSE * SHARED_OFFSET_A;\n\n\n\t// Each CTA slides along the 128 x 128 tiles from the top left corner of the\n\t// matrix to the right and down, and selects the next tile to compute. Once\n\t// there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n\tconst unsigned int block_tile_i =\n\t\t((block_pos * BLOCK_COL_TILES) / M_TILES) * (BLOCK_ROW_TILES);\n\tconst unsigned int block_tile_j = (block_pos * BLOCK_COL_TILES) % M_TILES;\n\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n\twmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_ROW_TILES]\n\t\t\t\t\t\t\t\t\t\t\t\t\t [WARP_COL_TILES];\n\n    // Load the C matrix tiles into fragments from shared memory.\n\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t\twmma::fill_fragment(c[i][j], 0);\n\t\t}\n\t}\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    //for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n    for(int tile_k_idx_sparse = 0, tile_k_idx = 0; tile_k_idx_sparse < K_GLOBAL_SPARSE; tile_k_idx_sparse += BLOCK_SIZE_K_SPARSE, tile_k_idx += BLOCK_SIZE_K){\n\n\t\tsize_t shmem_idx = \n\t\twarpId < (WARPS_PER_BLOCK / 2)\n\t\t\t? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A\n\t\t\t: (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B + shmem_idx_b_off;\n\n\t\tint4 *lane_ptr = NULL;\n\t\tint *lane_ptr_index = NULL;\n\t\tconst uint8_t *warp_ptr = NULL;\n\n\n\t\tif(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\t//warp_ptr = &A[block_tile_j * M] +\n\t\t\t//\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * M_GLOBAL;\n\t\t\twarp_ptr = &A[block_tile_j * M];\n\t\t\t\n\t\t\tconst int *warp_ptr_index = &B_index[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t\t\t\t\t\t((warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A);\n\n\t\t\tlane_ptr_index = (int *)(warp_ptr_index + tile_k_idx_sparse + (laneId / CHUNK_COPY_LINE_LANES_A));\n\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_A) * SHARED_OFFSET_A;\n\t\t}else{\n\t\t\twarp_ptr = &B[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * K_GLOBAL_SPARSE;\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_sparse +\n\t\t\t\t\t\t\t\t(laneId / CHUNK_COPY_LINE_LANES_B) * K_GLOBAL_SPARSE) +\n\t\t\t\t\t\t\t\t(laneId % CHUNK_COPY_LINE_LANES_B);\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_B) * SHARED_OFFSET_B;\n\t\t}\n\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      // shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n\t  int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n\t  \t? BLOCK_SIZE_K_SPARSE / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A)\n\t\t: BLOCK_SIZE_N / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\n\t  /*\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\t  */\n\n\t  /*\n      int tile_k_idx_A;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n          tile_k_idx_A = *(lane_ptr_index);\n      }\n\t  */\n\n\t  #pragma unroll\n\t  for(int i = 0; i < iter_index; i += 1){\n\t\t  if(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\tint tile_k_idx_A = *(lane_ptr_index);\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_A * M_GLOBAL) + (laneId % CHUNK_COPY_LINE_LANES_A);\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_A)) =\n\t\t\t\t*lane_ptr;\n\t\t\t//warp_ptr = (uint8_t *)((uint8_t *)warp_ptr + M_GLOBAL * (WARPS_PER_BLOCK / 2) *CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tlane_ptr_index = (int *)((int *)lane_ptr_index +  (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A;\n\t\t  }else{\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_B)) =\n\t\t\t\t*lane_ptr;\n\t\t\tlane_ptr = (int4 *)((uint8_t *)lane_ptr + K_GLOBAL_SPARSE * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B;\n\t\t  }\n\t  }\n\n      __syncthreads();\n\n\t#pragma unroll\n      for (int k_step = 0; k_step < CHUNK_K_SPARSE; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::col_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n\t#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i += 1) {\n\t\t\tsize_t shmem_idx_a = (warpId % BLOCK_COL_WARPS) * M * WARP_COL_TILES + (i * M);\n\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_a + k_step * K * SHARED_OFFSET_A;\n\n\t\t\twmma::load_matrix_sync(a[i], tile_ptr, SHARED_OFFSET_A);\n\t\t#pragma unroll\n\t\t\tfor(int j = 0; j < WARP_ROW_TILES; j += 1){\n\t\t\t\tif(i == 0){\n\t\t\t\t\tsize_t shmem_idx_b = shmem_idx_b_off +\n\t\t\t\t\t\t\t\t\t\t\t(warpId / BLOCK_COL_WARPS) * (WARP_ROW_TILES * N) * SHARED_OFFSET_B +\n\t\t\t\t\t\t\t\t\t\t\t(j * N) * SHARED_OFFSET_B;\n\t\t\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_b + k_step * K;\n\t\t\t\t\twmma::load_matrix_sync(b[j], tile_ptr, SHARED_OFFSET_B);\n\t\t\t\t}\n\t\t\t\twmma::mma_sync(c[j][i], a[i], b[j], c[j][i]);\n\t\t\t}\n\n        }\n      }\n\n      __syncthreads();\n    }\n\n    // This pointer is used to access the C and D matrix tiles this warp computes.\n\tint *shmem_warp_tile_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * N * WARP_ROW_TILES * SHMEM_STRIDE +\n\t(warpId % BLOCK_COL_WARPS) * SHMEM_OFFSET;\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t#pragma unroll\n\t\t\tfor(int t = 0; t < c[i][j].num_elements; t += 1){\n\t\t\t\tc[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n\t\t\t}\n\t\t\tint *tile_ptr = shmem_warp_tile_ptr + i * N * SHMEM_STRIDE + j * M;\n\t\t\twmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, C_LAYOUT);\n\t\t}\n\t}\n\n    __syncthreads();\n\n\tint *shmem_warp_stream_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N * SHMEM_STRIDE\n\t\t\t\t\t\t\t\t\t+ (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tconst size_t gmem_idx =\n\t\t(block_tile_i * N + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N) * GLOBAL_MEM_STRIDE +\n\t\tblock_tile_j * M + (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tuint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n\tint *shmem_lane_stream_ptr =\n\t\tshmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * SHMEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\t\n\tuint8_t *dst_gmem_lane_stream_ptr =\n\t\tdst_gmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * GLOBAL_MEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\n\tfor(int i = 0; i < WARP_ROW_TILES * N; i += SHARED_TO_GLOBAL_LINES_PER_WARP){\n\t\t*(dst_gmem_lane_stream_ptr + i * GLOBAL_MEM_STRIDE) = (uint8_t)(*(shmem_lane_stream_ptr + i * SHMEM_STRIDE));\n\t}\n\n\t__syncthreads();\n}\n", "launch_config": {"dimBlock": [256, 1], "dimGrid": [256, 1]}}, "69": {"code": "\n__global__ void compute_gemm_imma(float *input0, float *input1, float *input2, float* input3, float* input4, float*input5, float* input6,\n                                  float *output0) {\n    // keys need to be replaced 4096, 1024, K_GLOBA_VALUEL, 0.75, 64\n\n\t//extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B = reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_index = reinterpret_cast<int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n\tconst int WARP_SIZE = 32;\n    const int M = 16;\n    const int K = 16;\n    const int N = 16;\n    const int WMMA_M = 16;\n    const int WMMA_K = 16;\n    const int WMMA_N = 16;\n    const int M_GLOBAL = 4096;\n    const int K_GLOBAL = 1024;\n    const int N_GLOBAL = 1024;\n    const float SPARSITY = 0.75;\n    const int K_GLOBAL_SPARSE = (int(K_GLOBAL * (1-SPARSITY)));\n    const int M_TILES = (M_GLOBAL / M);\n    const int N_TILES = (N_GLOBAL / N);\n    const int K_TILES = (K_GLOBAL / K);\n    const int BLOCK_ROW_WARPS = 2;\n    const int BLOCK_COL_WARPS = 4;\n    const int WARP_ROW_TILES = 4;\n    const int WARP_COL_TILES = 2;\n    const int SKEW_UINT8 = 16;\n    const int BANK_VAL = 32;\n    const int NUM_BANK = (K_GLOBAL / BANK_VAL);\n    /////////// BLOCK_ROW_TILES <= N_TILES ////////////\n    // const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    // ////////// BLOCK_COL_TILES <= M_TILES ////////////\n    // const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    auto C_LAYOUT = wmma::mem_col_major;\n    const int CHUNK_K = 64;\n    const int CHUNK_K_SPARSE = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K_SPARSE = (int((CHUNK_K * K) * (1 - SPARSITY)));\n    const int WARP_COPY_BYTES = (WARP_SIZE * sizeof(int4));\n    const int CHUNK_LINE_BYTES_A (BLOCK_COL_TILES * M * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_A = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_A);\n    const int CHUNK_COPY_LINE_LANES_A = (CHUNK_LINE_BYTES_A / sizeof(int4));\n    const int SHARED_OFFSET_A = (BLOCK_COL_TILES * M + SKEW_UINT8);\n\n    const int CHUNK_LINE_BYTES_B = (BLOCK_SIZE_K_SPARSE * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_B = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_B);\n    const int CHUNK_COPY_LINE_LANES_B = (CHUNK_LINE_BYTES_B / sizeof(int4));\n    const int SHARED_OFFSET_B = (BLOCK_SIZE_K_SPARSE + SKEW_UINT8);\n    const int SHARED_TO_GLOBAL_BYTES_PER_LINE = ((WARP_COL_TILES * M) * sizeof(int));\n    const int SHARED_TO_GLOBAL_BYTES_PER_WARP = (WARP_SIZE * sizeof(int));\n    const int SHARED_TO_GLOBAL_LINES_PER_WARP = (SHARED_TO_GLOBAL_BYTES_PER_WARP / SHARED_TO_GLOBAL_BYTES_PER_LINE);\n    const int SHARED_TO_GLOBAL_LANES_PER_LINE = (WARP_SIZE / SHARED_TO_GLOBAL_LINES_PER_WARP);\n    const int SHARED_TO_GLOBAL_ITERS = ((WARP_ROW_TILES * N) / SHARED_TO_GLOBAL_LINES_PER_WARP);\n\n    const int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const int WARP_STRIDE = (WARP_COL_TILES * M);\n    const int WARPS_PER_BLOCK = (BLOCK_ROW_WARPS * BLOCK_COL_WARPS);\n    const int THREADS_PER_BLOCK = (WARP_SIZE * WARPS_PER_BLOCK);\n    const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    const int GLOBAL_MEM_STRIDE = M_GLOBAL;\n    const int SHMEM_STRIDE = (M * BLOCK_COL_TILES);\n    const int SHMEM_OFFSET = (M * WARP_COL_TILES);\n    const int BLOCK_SIZE_M = (M * BLOCK_COL_TILES);\n    const int BLOCK_SIZE_N = (N * BLOCK_ROW_TILES);\n\n   extern __shared__ uint8_t shmem[];\n\n\t// Warp and lane identification.\n\tconst unsigned int warpId = threadIdx.x / WARP_SIZE;\n\tconst unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n\n\t// Offset in shared memory from which the B matrix is stored.\n\t// const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\tconst size_t shmem_idx_b_off = BLOCK_SIZE_K_SPARSE * SHARED_OFFSET_A;\n\n\n\t// Each CTA slides along the 128 x 128 tiles from the top left corner of the\n\t// matrix to the right and down, and selects the next tile to compute. Once\n\t// there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n\tconst unsigned int block_tile_i =\n\t\t((block_pos * BLOCK_COL_TILES) / M_TILES) * (BLOCK_ROW_TILES);\n\tconst unsigned int block_tile_j = (block_pos * BLOCK_COL_TILES) % M_TILES;\n\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n\twmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_ROW_TILES]\n\t\t\t\t\t\t\t\t\t\t\t\t\t [WARP_COL_TILES];\n\n    // Load the C matrix tiles into fragments from shared memory.\n\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t\twmma::fill_fragment(c[i][j], 0);\n\t\t}\n\t}\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    //for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n    for(int tile_k_idx_sparse = 0, tile_k_idx = 0; tile_k_idx_sparse < K_GLOBAL_SPARSE; tile_k_idx_sparse += BLOCK_SIZE_K_SPARSE, tile_k_idx += BLOCK_SIZE_K){\n\n\t\tsize_t shmem_idx = \n\t\twarpId < (WARPS_PER_BLOCK / 2)\n\t\t\t? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A\n\t\t\t: (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B + shmem_idx_b_off;\n\n\t\tint4 *lane_ptr = NULL;\n\t\tint *lane_ptr_index = NULL;\n\t\tconst uint8_t *warp_ptr = NULL;\n\n\n\t\tif(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\t//warp_ptr = &A[block_tile_j * M] +\n\t\t\t//\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * M_GLOBAL;\n\t\t\twarp_ptr = &A[block_tile_j * M];\n\t\t\t\n\t\t\tconst int *warp_ptr_index = &B_index[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t\t\t\t\t\t((warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A);\n\n\t\t\tlane_ptr_index = (int *)(warp_ptr_index + tile_k_idx_sparse + (laneId / CHUNK_COPY_LINE_LANES_A));\n\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_A) * SHARED_OFFSET_A;\n\t\t}else{\n\t\t\twarp_ptr = &B[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * K_GLOBAL_SPARSE;\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_sparse +\n\t\t\t\t\t\t\t\t(laneId / CHUNK_COPY_LINE_LANES_B) * K_GLOBAL_SPARSE) +\n\t\t\t\t\t\t\t\t(laneId % CHUNK_COPY_LINE_LANES_B);\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_B) * SHARED_OFFSET_B;\n\t\t}\n\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      // shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n\t  int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n\t  \t? BLOCK_SIZE_K_SPARSE / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A)\n\t\t: BLOCK_SIZE_N / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\n\t  /*\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\t  */\n\n\t  /*\n      int tile_k_idx_A;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n          tile_k_idx_A = *(lane_ptr_index);\n      }\n\t  */\n\n\t  #pragma unroll\n\t  for(int i = 0; i < iter_index; i += 1){\n\t\t  if(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\tint tile_k_idx_A = *(lane_ptr_index);\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_A * M_GLOBAL) + (laneId % CHUNK_COPY_LINE_LANES_A);\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_A)) =\n\t\t\t\t*lane_ptr;\n\t\t\t//warp_ptr = (uint8_t *)((uint8_t *)warp_ptr + M_GLOBAL * (WARPS_PER_BLOCK / 2) *CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tlane_ptr_index = (int *)((int *)lane_ptr_index +  (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A;\n\t\t  }else{\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_B)) =\n\t\t\t\t*lane_ptr;\n\t\t\tlane_ptr = (int4 *)((uint8_t *)lane_ptr + K_GLOBAL_SPARSE * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B;\n\t\t  }\n\t  }\n\n      __syncthreads();\n\n\t#pragma unroll\n      for (int k_step = 0; k_step < CHUNK_K_SPARSE; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::col_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n\t#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i += 1) {\n\t\t\tsize_t shmem_idx_a = (warpId % BLOCK_COL_WARPS) * M * WARP_COL_TILES + (i * M);\n\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_a + k_step * K * SHARED_OFFSET_A;\n\n\t\t\twmma::load_matrix_sync(a[i], tile_ptr, SHARED_OFFSET_A);\n\t\t#pragma unroll\n\t\t\tfor(int j = 0; j < WARP_ROW_TILES; j += 1){\n\t\t\t\tif(i == 0){\n\t\t\t\t\tsize_t shmem_idx_b = shmem_idx_b_off +\n\t\t\t\t\t\t\t\t\t\t\t(warpId / BLOCK_COL_WARPS) * (WARP_ROW_TILES * N) * SHARED_OFFSET_B +\n\t\t\t\t\t\t\t\t\t\t\t(j * N) * SHARED_OFFSET_B;\n\t\t\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_b + k_step * K;\n\t\t\t\t\twmma::load_matrix_sync(b[j], tile_ptr, SHARED_OFFSET_B);\n\t\t\t\t}\n\t\t\t\twmma::mma_sync(c[j][i], a[i], b[j], c[j][i]);\n\t\t\t}\n\n        }\n      }\n\n      __syncthreads();\n    }\n\n    // This pointer is used to access the C and D matrix tiles this warp computes.\n\tint *shmem_warp_tile_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * N * WARP_ROW_TILES * SHMEM_STRIDE +\n\t(warpId % BLOCK_COL_WARPS) * SHMEM_OFFSET;\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t#pragma unroll\n\t\t\tfor(int t = 0; t < c[i][j].num_elements; t += 1){\n\t\t\t\tc[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n\t\t\t}\n\t\t\tint *tile_ptr = shmem_warp_tile_ptr + i * N * SHMEM_STRIDE + j * M;\n\t\t\twmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, C_LAYOUT);\n\t\t}\n\t}\n\n    __syncthreads();\n\n\tint *shmem_warp_stream_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N * SHMEM_STRIDE\n\t\t\t\t\t\t\t\t\t+ (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tconst size_t gmem_idx =\n\t\t(block_tile_i * N + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N) * GLOBAL_MEM_STRIDE +\n\t\tblock_tile_j * M + (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tuint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n\tint *shmem_lane_stream_ptr =\n\t\tshmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * SHMEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\t\n\tuint8_t *dst_gmem_lane_stream_ptr =\n\t\tdst_gmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * GLOBAL_MEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\n\tfor(int i = 0; i < WARP_ROW_TILES * N; i += SHARED_TO_GLOBAL_LINES_PER_WARP){\n\t\t*(dst_gmem_lane_stream_ptr + i * GLOBAL_MEM_STRIDE) = (uint8_t)(*(shmem_lane_stream_ptr + i * SHMEM_STRIDE));\n\t}\n\n\t__syncthreads();\n}\n", "launch_config": {"dimBlock": [256, 1], "dimGrid": [256, 1]}}, "70": {"code": "\n__global__ void compute_gemm_imma(float *input0, float *input1, float *input2, float* input3, float* input4, float*input5, float* input6,\n                                  float *output0) {\n    // keys need to be replaced 4096, 1024, K_GLOBA_VALUEL, 0.75, 64\n\n\t//extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B = reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_index = reinterpret_cast<int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n\tconst int WARP_SIZE = 32;\n    const int M = 16;\n    const int K = 16;\n    const int N = 16;\n    const int WMMA_M = 16;\n    const int WMMA_K = 16;\n    const int WMMA_N = 16;\n    const int M_GLOBAL = 4096;\n    const int K_GLOBAL = 1024;\n    const int N_GLOBAL = 1024;\n    const float SPARSITY = 0.75;\n    const int K_GLOBAL_SPARSE = (int(K_GLOBAL * (1-SPARSITY)));\n    const int M_TILES = (M_GLOBAL / M);\n    const int N_TILES = (N_GLOBAL / N);\n    const int K_TILES = (K_GLOBAL / K);\n    const int BLOCK_ROW_WARPS = 2;\n    const int BLOCK_COL_WARPS = 4;\n    const int WARP_ROW_TILES = 4;\n    const int WARP_COL_TILES = 2;\n    const int SKEW_UINT8 = 16;\n    const int BANK_VAL = 32;\n    const int NUM_BANK = (K_GLOBAL / BANK_VAL);\n    /////////// BLOCK_ROW_TILES <= N_TILES ////////////\n    // const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    // ////////// BLOCK_COL_TILES <= M_TILES ////////////\n    // const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    auto C_LAYOUT = wmma::mem_col_major;\n    const int CHUNK_K = 64;\n    const int CHUNK_K_SPARSE = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K_SPARSE = (int((CHUNK_K * K) * (1 - SPARSITY)));\n    const int WARP_COPY_BYTES = (WARP_SIZE * sizeof(int4));\n    const int CHUNK_LINE_BYTES_A (BLOCK_COL_TILES * M * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_A = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_A);\n    const int CHUNK_COPY_LINE_LANES_A = (CHUNK_LINE_BYTES_A / sizeof(int4));\n    const int SHARED_OFFSET_A = (BLOCK_COL_TILES * M + SKEW_UINT8);\n\n    const int CHUNK_LINE_BYTES_B = (BLOCK_SIZE_K_SPARSE * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_B = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_B);\n    const int CHUNK_COPY_LINE_LANES_B = (CHUNK_LINE_BYTES_B / sizeof(int4));\n    const int SHARED_OFFSET_B = (BLOCK_SIZE_K_SPARSE + SKEW_UINT8);\n    const int SHARED_TO_GLOBAL_BYTES_PER_LINE = ((WARP_COL_TILES * M) * sizeof(int));\n    const int SHARED_TO_GLOBAL_BYTES_PER_WARP = (WARP_SIZE * sizeof(int));\n    const int SHARED_TO_GLOBAL_LINES_PER_WARP = (SHARED_TO_GLOBAL_BYTES_PER_WARP / SHARED_TO_GLOBAL_BYTES_PER_LINE);\n    const int SHARED_TO_GLOBAL_LANES_PER_LINE = (WARP_SIZE / SHARED_TO_GLOBAL_LINES_PER_WARP);\n    const int SHARED_TO_GLOBAL_ITERS = ((WARP_ROW_TILES * N) / SHARED_TO_GLOBAL_LINES_PER_WARP);\n\n    const int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const int WARP_STRIDE = (WARP_COL_TILES * M);\n    const int WARPS_PER_BLOCK = (BLOCK_ROW_WARPS * BLOCK_COL_WARPS);\n    const int THREADS_PER_BLOCK = (WARP_SIZE * WARPS_PER_BLOCK);\n    const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    const int GLOBAL_MEM_STRIDE = M_GLOBAL;\n    const int SHMEM_STRIDE = (M * BLOCK_COL_TILES);\n    const int SHMEM_OFFSET = (M * WARP_COL_TILES);\n    const int BLOCK_SIZE_M = (M * BLOCK_COL_TILES);\n    const int BLOCK_SIZE_N = (N * BLOCK_ROW_TILES);\n\n   extern __shared__ uint8_t shmem[];\n\n\t// Warp and lane identification.\n\tconst unsigned int warpId = threadIdx.x / WARP_SIZE;\n\tconst unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n\n\t// Offset in shared memory from which the B matrix is stored.\n\t// const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\tconst size_t shmem_idx_b_off = BLOCK_SIZE_K_SPARSE * SHARED_OFFSET_A;\n\n\n\t// Each CTA slides along the 128 x 128 tiles from the top left corner of the\n\t// matrix to the right and down, and selects the next tile to compute. Once\n\t// there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n\tconst unsigned int block_tile_i =\n\t\t((block_pos * BLOCK_COL_TILES) / M_TILES) * (BLOCK_ROW_TILES);\n\tconst unsigned int block_tile_j = (block_pos * BLOCK_COL_TILES) % M_TILES;\n\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n\twmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_ROW_TILES]\n\t\t\t\t\t\t\t\t\t\t\t\t\t [WARP_COL_TILES];\n\n    // Load the C matrix tiles into fragments from shared memory.\n\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t\twmma::fill_fragment(c[i][j], 0);\n\t\t}\n\t}\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    //for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n    for(int tile_k_idx_sparse = 0, tile_k_idx = 0; tile_k_idx_sparse < K_GLOBAL_SPARSE; tile_k_idx_sparse += BLOCK_SIZE_K_SPARSE, tile_k_idx += BLOCK_SIZE_K){\n\n\t\tsize_t shmem_idx = \n\t\twarpId < (WARPS_PER_BLOCK / 2)\n\t\t\t? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A\n\t\t\t: (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B + shmem_idx_b_off;\n\n\t\tint4 *lane_ptr = NULL;\n\t\tint *lane_ptr_index = NULL;\n\t\tconst uint8_t *warp_ptr = NULL;\n\n\n\t\tif(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\t//warp_ptr = &A[block_tile_j * M] +\n\t\t\t//\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * M_GLOBAL;\n\t\t\twarp_ptr = &A[block_tile_j * M];\n\t\t\t\n\t\t\tconst int *warp_ptr_index = &B_index[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t\t\t\t\t\t((warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A);\n\n\t\t\tlane_ptr_index = (int *)(warp_ptr_index + tile_k_idx_sparse + (laneId / CHUNK_COPY_LINE_LANES_A));\n\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_A) * SHARED_OFFSET_A;\n\t\t}else{\n\t\t\twarp_ptr = &B[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * K_GLOBAL_SPARSE;\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_sparse +\n\t\t\t\t\t\t\t\t(laneId / CHUNK_COPY_LINE_LANES_B) * K_GLOBAL_SPARSE) +\n\t\t\t\t\t\t\t\t(laneId % CHUNK_COPY_LINE_LANES_B);\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_B) * SHARED_OFFSET_B;\n\t\t}\n\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      // shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n\t  int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n\t  \t? BLOCK_SIZE_K_SPARSE / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A)\n\t\t: BLOCK_SIZE_N / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\n\t  /*\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\t  */\n\n\t  /*\n      int tile_k_idx_A;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n          tile_k_idx_A = *(lane_ptr_index);\n      }\n\t  */\n\n\t  #pragma unroll\n\t  for(int i = 0; i < iter_index; i += 1){\n\t\t  if(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\tint tile_k_idx_A = *(lane_ptr_index);\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_A * M_GLOBAL) + (laneId % CHUNK_COPY_LINE_LANES_A);\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_A)) =\n\t\t\t\t*lane_ptr;\n\t\t\t//warp_ptr = (uint8_t *)((uint8_t *)warp_ptr + M_GLOBAL * (WARPS_PER_BLOCK / 2) *CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tlane_ptr_index = (int *)((int *)lane_ptr_index +  (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A;\n\t\t  }else{\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_B)) =\n\t\t\t\t*lane_ptr;\n\t\t\tlane_ptr = (int4 *)((uint8_t *)lane_ptr + K_GLOBAL_SPARSE * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B;\n\t\t  }\n\t  }\n\n      __syncthreads();\n\n\t#pragma unroll\n      for (int k_step = 0; k_step < CHUNK_K_SPARSE; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::col_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n\t#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i += 1) {\n\t\t\tsize_t shmem_idx_a = (warpId % BLOCK_COL_WARPS) * M * WARP_COL_TILES + (i * M);\n\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_a + k_step * K * SHARED_OFFSET_A;\n\n\t\t\twmma::load_matrix_sync(a[i], tile_ptr, SHARED_OFFSET_A);\n\t\t#pragma unroll\n\t\t\tfor(int j = 0; j < WARP_ROW_TILES; j += 1){\n\t\t\t\tif(i == 0){\n\t\t\t\t\tsize_t shmem_idx_b = shmem_idx_b_off +\n\t\t\t\t\t\t\t\t\t\t\t(warpId / BLOCK_COL_WARPS) * (WARP_ROW_TILES * N) * SHARED_OFFSET_B +\n\t\t\t\t\t\t\t\t\t\t\t(j * N) * SHARED_OFFSET_B;\n\t\t\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_b + k_step * K;\n\t\t\t\t\twmma::load_matrix_sync(b[j], tile_ptr, SHARED_OFFSET_B);\n\t\t\t\t}\n\t\t\t\twmma::mma_sync(c[j][i], a[i], b[j], c[j][i]);\n\t\t\t}\n\n        }\n      }\n\n      __syncthreads();\n    }\n\n    // This pointer is used to access the C and D matrix tiles this warp computes.\n\tint *shmem_warp_tile_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * N * WARP_ROW_TILES * SHMEM_STRIDE +\n\t(warpId % BLOCK_COL_WARPS) * SHMEM_OFFSET;\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t#pragma unroll\n\t\t\tfor(int t = 0; t < c[i][j].num_elements; t += 1){\n\t\t\t\tc[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n\t\t\t}\n\t\t\tint *tile_ptr = shmem_warp_tile_ptr + i * N * SHMEM_STRIDE + j * M;\n\t\t\twmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, C_LAYOUT);\n\t\t}\n\t}\n\n    __syncthreads();\n\n\tint *shmem_warp_stream_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N * SHMEM_STRIDE\n\t\t\t\t\t\t\t\t\t+ (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tconst size_t gmem_idx =\n\t\t(block_tile_i * N + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N) * GLOBAL_MEM_STRIDE +\n\t\tblock_tile_j * M + (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tuint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n\tint *shmem_lane_stream_ptr =\n\t\tshmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * SHMEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\t\n\tuint8_t *dst_gmem_lane_stream_ptr =\n\t\tdst_gmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * GLOBAL_MEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\n\tfor(int i = 0; i < WARP_ROW_TILES * N; i += SHARED_TO_GLOBAL_LINES_PER_WARP){\n\t\t*(dst_gmem_lane_stream_ptr + i * GLOBAL_MEM_STRIDE) = (uint8_t)(*(shmem_lane_stream_ptr + i * SHMEM_STRIDE));\n\t}\n\n\t__syncthreads();\n}\n", "launch_config": {"dimBlock": [256, 1], "dimGrid": [256, 1]}}, "71": {"code": "\n__global__ void compute_gemm_imma(float *input0, float *input1, float *input2, float* input3, float* input4, float*input5, float* input6,\n                                  float *output0) {\n    // keys need to be replaced 4096, 4096, K_GLOBA_VALUEL, 0.75, 64\n\n\t//extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B = reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_index = reinterpret_cast<int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n\tconst int WARP_SIZE = 32;\n    const int M = 16;\n    const int K = 16;\n    const int N = 16;\n    const int WMMA_M = 16;\n    const int WMMA_K = 16;\n    const int WMMA_N = 16;\n    const int M_GLOBAL = 4096;\n    const int K_GLOBAL = 1024;\n    const int N_GLOBAL = 4096;\n    const float SPARSITY = 0.75;\n    const int K_GLOBAL_SPARSE = (int(K_GLOBAL * (1-SPARSITY)));\n    const int M_TILES = (M_GLOBAL / M);\n    const int N_TILES = (N_GLOBAL / N);\n    const int K_TILES = (K_GLOBAL / K);\n    const int BLOCK_ROW_WARPS = 2;\n    const int BLOCK_COL_WARPS = 4;\n    const int WARP_ROW_TILES = 4;\n    const int WARP_COL_TILES = 2;\n    const int SKEW_UINT8 = 16;\n    const int BANK_VAL = 32;\n    const int NUM_BANK = (K_GLOBAL / BANK_VAL);\n    /////////// BLOCK_ROW_TILES <= N_TILES ////////////\n    // const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    // ////////// BLOCK_COL_TILES <= M_TILES ////////////\n    // const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    auto C_LAYOUT = wmma::mem_col_major;\n    const int CHUNK_K = 64;\n    const int CHUNK_K_SPARSE = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K_SPARSE = (int((CHUNK_K * K) * (1 - SPARSITY)));\n    const int WARP_COPY_BYTES = (WARP_SIZE * sizeof(int4));\n    const int CHUNK_LINE_BYTES_A (BLOCK_COL_TILES * M * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_A = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_A);\n    const int CHUNK_COPY_LINE_LANES_A = (CHUNK_LINE_BYTES_A / sizeof(int4));\n    const int SHARED_OFFSET_A = (BLOCK_COL_TILES * M + SKEW_UINT8);\n\n    const int CHUNK_LINE_BYTES_B = (BLOCK_SIZE_K_SPARSE * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_B = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_B);\n    const int CHUNK_COPY_LINE_LANES_B = (CHUNK_LINE_BYTES_B / sizeof(int4));\n    const int SHARED_OFFSET_B = (BLOCK_SIZE_K_SPARSE + SKEW_UINT8);\n    const int SHARED_TO_GLOBAL_BYTES_PER_LINE = ((WARP_COL_TILES * M) * sizeof(int));\n    const int SHARED_TO_GLOBAL_BYTES_PER_WARP = (WARP_SIZE * sizeof(int));\n    const int SHARED_TO_GLOBAL_LINES_PER_WARP = (SHARED_TO_GLOBAL_BYTES_PER_WARP / SHARED_TO_GLOBAL_BYTES_PER_LINE);\n    const int SHARED_TO_GLOBAL_LANES_PER_LINE = (WARP_SIZE / SHARED_TO_GLOBAL_LINES_PER_WARP);\n    const int SHARED_TO_GLOBAL_ITERS = ((WARP_ROW_TILES * N) / SHARED_TO_GLOBAL_LINES_PER_WARP);\n\n    const int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const int WARP_STRIDE = (WARP_COL_TILES * M);\n    const int WARPS_PER_BLOCK = (BLOCK_ROW_WARPS * BLOCK_COL_WARPS);\n    const int THREADS_PER_BLOCK = (WARP_SIZE * WARPS_PER_BLOCK);\n    const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    const int GLOBAL_MEM_STRIDE = M_GLOBAL;\n    const int SHMEM_STRIDE = (M * BLOCK_COL_TILES);\n    const int SHMEM_OFFSET = (M * WARP_COL_TILES);\n    const int BLOCK_SIZE_M = (M * BLOCK_COL_TILES);\n    const int BLOCK_SIZE_N = (N * BLOCK_ROW_TILES);\n\n   extern __shared__ uint8_t shmem[];\n\n\t// Warp and lane identification.\n\tconst unsigned int warpId = threadIdx.x / WARP_SIZE;\n\tconst unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n\n\t// Offset in shared memory from which the B matrix is stored.\n\t// const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\tconst size_t shmem_idx_b_off = BLOCK_SIZE_K_SPARSE * SHARED_OFFSET_A;\n\n\n\t// Each CTA slides along the 128 x 128 tiles from the top left corner of the\n\t// matrix to the right and down, and selects the next tile to compute. Once\n\t// there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n\tconst unsigned int block_tile_i =\n\t\t((block_pos * BLOCK_COL_TILES) / M_TILES) * (BLOCK_ROW_TILES);\n\tconst unsigned int block_tile_j = (block_pos * BLOCK_COL_TILES) % M_TILES;\n\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n\twmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_ROW_TILES]\n\t\t\t\t\t\t\t\t\t\t\t\t\t [WARP_COL_TILES];\n\n    // Load the C matrix tiles into fragments from shared memory.\n\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t\twmma::fill_fragment(c[i][j], 0);\n\t\t}\n\t}\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    //for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n    for(int tile_k_idx_sparse = 0, tile_k_idx = 0; tile_k_idx_sparse < K_GLOBAL_SPARSE; tile_k_idx_sparse += BLOCK_SIZE_K_SPARSE, tile_k_idx += BLOCK_SIZE_K){\n\n\t\tsize_t shmem_idx = \n\t\twarpId < (WARPS_PER_BLOCK / 2)\n\t\t\t? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A\n\t\t\t: (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B + shmem_idx_b_off;\n\n\t\tint4 *lane_ptr = NULL;\n\t\tint *lane_ptr_index = NULL;\n\t\tconst uint8_t *warp_ptr = NULL;\n\n\n\t\tif(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\t//warp_ptr = &A[block_tile_j * M] +\n\t\t\t//\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * M_GLOBAL;\n\t\t\twarp_ptr = &A[block_tile_j * M];\n\t\t\t\n\t\t\tconst int *warp_ptr_index = &B_index[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t\t\t\t\t\t((warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A);\n\n\t\t\tlane_ptr_index = (int *)(warp_ptr_index + tile_k_idx_sparse + (laneId / CHUNK_COPY_LINE_LANES_A));\n\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_A) * SHARED_OFFSET_A;\n\t\t}else{\n\t\t\twarp_ptr = &B[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * K_GLOBAL_SPARSE;\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_sparse +\n\t\t\t\t\t\t\t\t(laneId / CHUNK_COPY_LINE_LANES_B) * K_GLOBAL_SPARSE) +\n\t\t\t\t\t\t\t\t(laneId % CHUNK_COPY_LINE_LANES_B);\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_B) * SHARED_OFFSET_B;\n\t\t}\n\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      // shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n\t  int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n\t  \t? BLOCK_SIZE_K_SPARSE / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A)\n\t\t: BLOCK_SIZE_N / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\n\t  /*\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\t  */\n\n\t  /*\n      int tile_k_idx_A;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n          tile_k_idx_A = *(lane_ptr_index);\n      }\n\t  */\n\n\t  #pragma unroll\n\t  for(int i = 0; i < iter_index; i += 1){\n\t\t  if(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\tint tile_k_idx_A = *(lane_ptr_index);\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_A * M_GLOBAL) + (laneId % CHUNK_COPY_LINE_LANES_A);\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_A)) =\n\t\t\t\t*lane_ptr;\n\t\t\t//warp_ptr = (uint8_t *)((uint8_t *)warp_ptr + M_GLOBAL * (WARPS_PER_BLOCK / 2) *CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tlane_ptr_index = (int *)((int *)lane_ptr_index +  (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A;\n\t\t  }else{\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_B)) =\n\t\t\t\t*lane_ptr;\n\t\t\tlane_ptr = (int4 *)((uint8_t *)lane_ptr + K_GLOBAL_SPARSE * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B;\n\t\t  }\n\t  }\n\n      __syncthreads();\n\n\t#pragma unroll\n      for (int k_step = 0; k_step < CHUNK_K_SPARSE; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::col_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n\t#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i += 1) {\n\t\t\tsize_t shmem_idx_a = (warpId % BLOCK_COL_WARPS) * M * WARP_COL_TILES + (i * M);\n\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_a + k_step * K * SHARED_OFFSET_A;\n\n\t\t\twmma::load_matrix_sync(a[i], tile_ptr, SHARED_OFFSET_A);\n\t\t#pragma unroll\n\t\t\tfor(int j = 0; j < WARP_ROW_TILES; j += 1){\n\t\t\t\tif(i == 0){\n\t\t\t\t\tsize_t shmem_idx_b = shmem_idx_b_off +\n\t\t\t\t\t\t\t\t\t\t\t(warpId / BLOCK_COL_WARPS) * (WARP_ROW_TILES * N) * SHARED_OFFSET_B +\n\t\t\t\t\t\t\t\t\t\t\t(j * N) * SHARED_OFFSET_B;\n\t\t\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_b + k_step * K;\n\t\t\t\t\twmma::load_matrix_sync(b[j], tile_ptr, SHARED_OFFSET_B);\n\t\t\t\t}\n\t\t\t\twmma::mma_sync(c[j][i], a[i], b[j], c[j][i]);\n\t\t\t}\n\n        }\n      }\n\n      __syncthreads();\n    }\n\n    // This pointer is used to access the C and D matrix tiles this warp computes.\n\tint *shmem_warp_tile_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * N * WARP_ROW_TILES * SHMEM_STRIDE +\n\t(warpId % BLOCK_COL_WARPS) * SHMEM_OFFSET;\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t#pragma unroll\n\t\t\tfor(int t = 0; t < c[i][j].num_elements; t += 1){\n\t\t\t\tc[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n\t\t\t}\n\t\t\tint *tile_ptr = shmem_warp_tile_ptr + i * N * SHMEM_STRIDE + j * M;\n\t\t\twmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, C_LAYOUT);\n\t\t}\n\t}\n\n    __syncthreads();\n\n\tint *shmem_warp_stream_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N * SHMEM_STRIDE\n\t\t\t\t\t\t\t\t\t+ (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tconst size_t gmem_idx =\n\t\t(block_tile_i * N + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N) * GLOBAL_MEM_STRIDE +\n\t\tblock_tile_j * M + (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tuint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n\tint *shmem_lane_stream_ptr =\n\t\tshmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * SHMEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\t\n\tuint8_t *dst_gmem_lane_stream_ptr =\n\t\tdst_gmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * GLOBAL_MEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\n\tfor(int i = 0; i < WARP_ROW_TILES * N; i += SHARED_TO_GLOBAL_LINES_PER_WARP){\n\t\t*(dst_gmem_lane_stream_ptr + i * GLOBAL_MEM_STRIDE) = (uint8_t)(*(shmem_lane_stream_ptr + i * SHMEM_STRIDE));\n\t}\n\n\t__syncthreads();\n}\n", "launch_config": {"dimBlock": [1024, 1], "dimGrid": [256, 1]}}, "72": {"code": "\n__global__ void compute_gemm_imma(float *input0, float *input1, float *input2, float* input3, float* input4, float*input5, float* input6,\n                                  float *output0) {\n    // keys need to be replaced 4096, 1024, K_GLOBA_VALUEL, 0.75, 64\n\n\t//extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B = reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_index = reinterpret_cast<int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n\tconst int WARP_SIZE = 32;\n    const int M = 16;\n    const int K = 16;\n    const int N = 16;\n    const int WMMA_M = 16;\n    const int WMMA_K = 16;\n    const int WMMA_N = 16;\n    const int M_GLOBAL = 4096;\n    const int K_GLOBAL = 4096;\n    const int N_GLOBAL = 1024;\n    const float SPARSITY = 0.75;\n    const int K_GLOBAL_SPARSE = (int(K_GLOBAL * (1-SPARSITY)));\n    const int M_TILES = (M_GLOBAL / M);\n    const int N_TILES = (N_GLOBAL / N);\n    const int K_TILES = (K_GLOBAL / K);\n    const int BLOCK_ROW_WARPS = 2;\n    const int BLOCK_COL_WARPS = 4;\n    const int WARP_ROW_TILES = 4;\n    const int WARP_COL_TILES = 2;\n    const int SKEW_UINT8 = 16;\n    const int BANK_VAL = 32;\n    const int NUM_BANK = (K_GLOBAL / BANK_VAL);\n    /////////// BLOCK_ROW_TILES <= N_TILES ////////////\n    // const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    // ////////// BLOCK_COL_TILES <= M_TILES ////////////\n    // const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    auto C_LAYOUT = wmma::mem_col_major;\n    const int CHUNK_K = 64;\n    const int CHUNK_K_SPARSE = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K_SPARSE = (int((CHUNK_K * K) * (1 - SPARSITY)));\n    const int WARP_COPY_BYTES = (WARP_SIZE * sizeof(int4));\n    const int CHUNK_LINE_BYTES_A (BLOCK_COL_TILES * M * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_A = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_A);\n    const int CHUNK_COPY_LINE_LANES_A = (CHUNK_LINE_BYTES_A / sizeof(int4));\n    const int SHARED_OFFSET_A = (BLOCK_COL_TILES * M + SKEW_UINT8);\n\n    const int CHUNK_LINE_BYTES_B = (BLOCK_SIZE_K_SPARSE * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_B = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_B);\n    const int CHUNK_COPY_LINE_LANES_B = (CHUNK_LINE_BYTES_B / sizeof(int4));\n    const int SHARED_OFFSET_B = (BLOCK_SIZE_K_SPARSE + SKEW_UINT8);\n    const int SHARED_TO_GLOBAL_BYTES_PER_LINE = ((WARP_COL_TILES * M) * sizeof(int));\n    const int SHARED_TO_GLOBAL_BYTES_PER_WARP = (WARP_SIZE * sizeof(int));\n    const int SHARED_TO_GLOBAL_LINES_PER_WARP = (SHARED_TO_GLOBAL_BYTES_PER_WARP / SHARED_TO_GLOBAL_BYTES_PER_LINE);\n    const int SHARED_TO_GLOBAL_LANES_PER_LINE = (WARP_SIZE / SHARED_TO_GLOBAL_LINES_PER_WARP);\n    const int SHARED_TO_GLOBAL_ITERS = ((WARP_ROW_TILES * N) / SHARED_TO_GLOBAL_LINES_PER_WARP);\n\n    const int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const int WARP_STRIDE = (WARP_COL_TILES * M);\n    const int WARPS_PER_BLOCK = (BLOCK_ROW_WARPS * BLOCK_COL_WARPS);\n    const int THREADS_PER_BLOCK = (WARP_SIZE * WARPS_PER_BLOCK);\n    const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    const int GLOBAL_MEM_STRIDE = M_GLOBAL;\n    const int SHMEM_STRIDE = (M * BLOCK_COL_TILES);\n    const int SHMEM_OFFSET = (M * WARP_COL_TILES);\n    const int BLOCK_SIZE_M = (M * BLOCK_COL_TILES);\n    const int BLOCK_SIZE_N = (N * BLOCK_ROW_TILES);\n\n   extern __shared__ uint8_t shmem[];\n\n\t// Warp and lane identification.\n\tconst unsigned int warpId = threadIdx.x / WARP_SIZE;\n\tconst unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n\n\t// Offset in shared memory from which the B matrix is stored.\n\t// const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\tconst size_t shmem_idx_b_off = BLOCK_SIZE_K_SPARSE * SHARED_OFFSET_A;\n\n\n\t// Each CTA slides along the 128 x 128 tiles from the top left corner of the\n\t// matrix to the right and down, and selects the next tile to compute. Once\n\t// there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n\tconst unsigned int block_tile_i =\n\t\t((block_pos * BLOCK_COL_TILES) / M_TILES) * (BLOCK_ROW_TILES);\n\tconst unsigned int block_tile_j = (block_pos * BLOCK_COL_TILES) % M_TILES;\n\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n\twmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_ROW_TILES]\n\t\t\t\t\t\t\t\t\t\t\t\t\t [WARP_COL_TILES];\n\n    // Load the C matrix tiles into fragments from shared memory.\n\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t\twmma::fill_fragment(c[i][j], 0);\n\t\t}\n\t}\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    //for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n    for(int tile_k_idx_sparse = 0, tile_k_idx = 0; tile_k_idx_sparse < K_GLOBAL_SPARSE; tile_k_idx_sparse += BLOCK_SIZE_K_SPARSE, tile_k_idx += BLOCK_SIZE_K){\n\n\t\tsize_t shmem_idx = \n\t\twarpId < (WARPS_PER_BLOCK / 2)\n\t\t\t? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A\n\t\t\t: (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B + shmem_idx_b_off;\n\n\t\tint4 *lane_ptr = NULL;\n\t\tint *lane_ptr_index = NULL;\n\t\tconst uint8_t *warp_ptr = NULL;\n\n\n\t\tif(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\t//warp_ptr = &A[block_tile_j * M] +\n\t\t\t//\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * M_GLOBAL;\n\t\t\twarp_ptr = &A[block_tile_j * M];\n\t\t\t\n\t\t\tconst int *warp_ptr_index = &B_index[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t\t\t\t\t\t((warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A);\n\n\t\t\tlane_ptr_index = (int *)(warp_ptr_index + tile_k_idx_sparse + (laneId / CHUNK_COPY_LINE_LANES_A));\n\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_A) * SHARED_OFFSET_A;\n\t\t}else{\n\t\t\twarp_ptr = &B[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * K_GLOBAL_SPARSE;\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_sparse +\n\t\t\t\t\t\t\t\t(laneId / CHUNK_COPY_LINE_LANES_B) * K_GLOBAL_SPARSE) +\n\t\t\t\t\t\t\t\t(laneId % CHUNK_COPY_LINE_LANES_B);\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_B) * SHARED_OFFSET_B;\n\t\t}\n\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      // shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n\t  int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n\t  \t? BLOCK_SIZE_K_SPARSE / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A)\n\t\t: BLOCK_SIZE_N / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\n\t  /*\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\t  */\n\n\t  /*\n      int tile_k_idx_A;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n          tile_k_idx_A = *(lane_ptr_index);\n      }\n\t  */\n\n\t  #pragma unroll\n\t  for(int i = 0; i < iter_index; i += 1){\n\t\t  if(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\tint tile_k_idx_A = *(lane_ptr_index);\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_A * M_GLOBAL) + (laneId % CHUNK_COPY_LINE_LANES_A);\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_A)) =\n\t\t\t\t*lane_ptr;\n\t\t\t//warp_ptr = (uint8_t *)((uint8_t *)warp_ptr + M_GLOBAL * (WARPS_PER_BLOCK / 2) *CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tlane_ptr_index = (int *)((int *)lane_ptr_index +  (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A;\n\t\t  }else{\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_B)) =\n\t\t\t\t*lane_ptr;\n\t\t\tlane_ptr = (int4 *)((uint8_t *)lane_ptr + K_GLOBAL_SPARSE * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B;\n\t\t  }\n\t  }\n\n      __syncthreads();\n\n\t#pragma unroll\n      for (int k_step = 0; k_step < CHUNK_K_SPARSE; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::col_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n\t#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i += 1) {\n\t\t\tsize_t shmem_idx_a = (warpId % BLOCK_COL_WARPS) * M * WARP_COL_TILES + (i * M);\n\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_a + k_step * K * SHARED_OFFSET_A;\n\n\t\t\twmma::load_matrix_sync(a[i], tile_ptr, SHARED_OFFSET_A);\n\t\t#pragma unroll\n\t\t\tfor(int j = 0; j < WARP_ROW_TILES; j += 1){\n\t\t\t\tif(i == 0){\n\t\t\t\t\tsize_t shmem_idx_b = shmem_idx_b_off +\n\t\t\t\t\t\t\t\t\t\t\t(warpId / BLOCK_COL_WARPS) * (WARP_ROW_TILES * N) * SHARED_OFFSET_B +\n\t\t\t\t\t\t\t\t\t\t\t(j * N) * SHARED_OFFSET_B;\n\t\t\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_b + k_step * K;\n\t\t\t\t\twmma::load_matrix_sync(b[j], tile_ptr, SHARED_OFFSET_B);\n\t\t\t\t}\n\t\t\t\twmma::mma_sync(c[j][i], a[i], b[j], c[j][i]);\n\t\t\t}\n\n        }\n      }\n\n      __syncthreads();\n    }\n\n    // This pointer is used to access the C and D matrix tiles this warp computes.\n\tint *shmem_warp_tile_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * N * WARP_ROW_TILES * SHMEM_STRIDE +\n\t(warpId % BLOCK_COL_WARPS) * SHMEM_OFFSET;\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t#pragma unroll\n\t\t\tfor(int t = 0; t < c[i][j].num_elements; t += 1){\n\t\t\t\tc[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n\t\t\t}\n\t\t\tint *tile_ptr = shmem_warp_tile_ptr + i * N * SHMEM_STRIDE + j * M;\n\t\t\twmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, C_LAYOUT);\n\t\t}\n\t}\n\n    __syncthreads();\n\n\tint *shmem_warp_stream_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N * SHMEM_STRIDE\n\t\t\t\t\t\t\t\t\t+ (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tconst size_t gmem_idx =\n\t\t(block_tile_i * N + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N) * GLOBAL_MEM_STRIDE +\n\t\tblock_tile_j * M + (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tuint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n\tint *shmem_lane_stream_ptr =\n\t\tshmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * SHMEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\t\n\tuint8_t *dst_gmem_lane_stream_ptr =\n\t\tdst_gmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * GLOBAL_MEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\n\tfor(int i = 0; i < WARP_ROW_TILES * N; i += SHARED_TO_GLOBAL_LINES_PER_WARP){\n\t\t*(dst_gmem_lane_stream_ptr + i * GLOBAL_MEM_STRIDE) = (uint8_t)(*(shmem_lane_stream_ptr + i * SHMEM_STRIDE));\n\t}\n\n\t__syncthreads();\n}\n", "launch_config": {"dimBlock": [256, 1], "dimGrid": [256, 1]}}, "73": {"code": "\n__global__ void compute_gemm_imma(float *input0, float *input1, float *input2, float* input3, float* input4, float*input5, float* input6,\n                                  float *output0) {\n    // keys need to be replaced 4096, 1024, K_GLOBA_VALUEL, 0.75, 64\n\n\t//extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B = reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_index = reinterpret_cast<int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n\tconst int WARP_SIZE = 32;\n    const int M = 16;\n    const int K = 16;\n    const int N = 16;\n    const int WMMA_M = 16;\n    const int WMMA_K = 16;\n    const int WMMA_N = 16;\n    const int M_GLOBAL = 4096;\n    const int K_GLOBAL = 1024;\n    const int N_GLOBAL = 1024;\n    const float SPARSITY = 0.75;\n    const int K_GLOBAL_SPARSE = (int(K_GLOBAL * (1-SPARSITY)));\n    const int M_TILES = (M_GLOBAL / M);\n    const int N_TILES = (N_GLOBAL / N);\n    const int K_TILES = (K_GLOBAL / K);\n    const int BLOCK_ROW_WARPS = 2;\n    const int BLOCK_COL_WARPS = 4;\n    const int WARP_ROW_TILES = 4;\n    const int WARP_COL_TILES = 2;\n    const int SKEW_UINT8 = 16;\n    const int BANK_VAL = 32;\n    const int NUM_BANK = (K_GLOBAL / BANK_VAL);\n    /////////// BLOCK_ROW_TILES <= N_TILES ////////////\n    // const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    // ////////// BLOCK_COL_TILES <= M_TILES ////////////\n    // const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    auto C_LAYOUT = wmma::mem_col_major;\n    const int CHUNK_K = 64;\n    const int CHUNK_K_SPARSE = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K_SPARSE = (int((CHUNK_K * K) * (1 - SPARSITY)));\n    const int WARP_COPY_BYTES = (WARP_SIZE * sizeof(int4));\n    const int CHUNK_LINE_BYTES_A (BLOCK_COL_TILES * M * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_A = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_A);\n    const int CHUNK_COPY_LINE_LANES_A = (CHUNK_LINE_BYTES_A / sizeof(int4));\n    const int SHARED_OFFSET_A = (BLOCK_COL_TILES * M + SKEW_UINT8);\n\n    const int CHUNK_LINE_BYTES_B = (BLOCK_SIZE_K_SPARSE * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_B = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_B);\n    const int CHUNK_COPY_LINE_LANES_B = (CHUNK_LINE_BYTES_B / sizeof(int4));\n    const int SHARED_OFFSET_B = (BLOCK_SIZE_K_SPARSE + SKEW_UINT8);\n    const int SHARED_TO_GLOBAL_BYTES_PER_LINE = ((WARP_COL_TILES * M) * sizeof(int));\n    const int SHARED_TO_GLOBAL_BYTES_PER_WARP = (WARP_SIZE * sizeof(int));\n    const int SHARED_TO_GLOBAL_LINES_PER_WARP = (SHARED_TO_GLOBAL_BYTES_PER_WARP / SHARED_TO_GLOBAL_BYTES_PER_LINE);\n    const int SHARED_TO_GLOBAL_LANES_PER_LINE = (WARP_SIZE / SHARED_TO_GLOBAL_LINES_PER_WARP);\n    const int SHARED_TO_GLOBAL_ITERS = ((WARP_ROW_TILES * N) / SHARED_TO_GLOBAL_LINES_PER_WARP);\n\n    const int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const int WARP_STRIDE = (WARP_COL_TILES * M);\n    const int WARPS_PER_BLOCK = (BLOCK_ROW_WARPS * BLOCK_COL_WARPS);\n    const int THREADS_PER_BLOCK = (WARP_SIZE * WARPS_PER_BLOCK);\n    const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    const int GLOBAL_MEM_STRIDE = M_GLOBAL;\n    const int SHMEM_STRIDE = (M * BLOCK_COL_TILES);\n    const int SHMEM_OFFSET = (M * WARP_COL_TILES);\n    const int BLOCK_SIZE_M = (M * BLOCK_COL_TILES);\n    const int BLOCK_SIZE_N = (N * BLOCK_ROW_TILES);\n\n   extern __shared__ uint8_t shmem[];\n\n\t// Warp and lane identification.\n\tconst unsigned int warpId = threadIdx.x / WARP_SIZE;\n\tconst unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n\n\t// Offset in shared memory from which the B matrix is stored.\n\t// const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\tconst size_t shmem_idx_b_off = BLOCK_SIZE_K_SPARSE * SHARED_OFFSET_A;\n\n\n\t// Each CTA slides along the 128 x 128 tiles from the top left corner of the\n\t// matrix to the right and down, and selects the next tile to compute. Once\n\t// there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n\tconst unsigned int block_tile_i =\n\t\t((block_pos * BLOCK_COL_TILES) / M_TILES) * (BLOCK_ROW_TILES);\n\tconst unsigned int block_tile_j = (block_pos * BLOCK_COL_TILES) % M_TILES;\n\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n\twmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_ROW_TILES]\n\t\t\t\t\t\t\t\t\t\t\t\t\t [WARP_COL_TILES];\n\n    // Load the C matrix tiles into fragments from shared memory.\n\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t\twmma::fill_fragment(c[i][j], 0);\n\t\t}\n\t}\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    //for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n    for(int tile_k_idx_sparse = 0, tile_k_idx = 0; tile_k_idx_sparse < K_GLOBAL_SPARSE; tile_k_idx_sparse += BLOCK_SIZE_K_SPARSE, tile_k_idx += BLOCK_SIZE_K){\n\n\t\tsize_t shmem_idx = \n\t\twarpId < (WARPS_PER_BLOCK / 2)\n\t\t\t? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A\n\t\t\t: (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B + shmem_idx_b_off;\n\n\t\tint4 *lane_ptr = NULL;\n\t\tint *lane_ptr_index = NULL;\n\t\tconst uint8_t *warp_ptr = NULL;\n\n\n\t\tif(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\t//warp_ptr = &A[block_tile_j * M] +\n\t\t\t//\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * M_GLOBAL;\n\t\t\twarp_ptr = &A[block_tile_j * M];\n\t\t\t\n\t\t\tconst int *warp_ptr_index = &B_index[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t\t\t\t\t\t((warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A);\n\n\t\t\tlane_ptr_index = (int *)(warp_ptr_index + tile_k_idx_sparse + (laneId / CHUNK_COPY_LINE_LANES_A));\n\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_A) * SHARED_OFFSET_A;\n\t\t}else{\n\t\t\twarp_ptr = &B[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * K_GLOBAL_SPARSE;\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_sparse +\n\t\t\t\t\t\t\t\t(laneId / CHUNK_COPY_LINE_LANES_B) * K_GLOBAL_SPARSE) +\n\t\t\t\t\t\t\t\t(laneId % CHUNK_COPY_LINE_LANES_B);\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_B) * SHARED_OFFSET_B;\n\t\t}\n\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      // shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n\t  int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n\t  \t? BLOCK_SIZE_K_SPARSE / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A)\n\t\t: BLOCK_SIZE_N / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\n\t  /*\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\t  */\n\n\t  /*\n      int tile_k_idx_A;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n          tile_k_idx_A = *(lane_ptr_index);\n      }\n\t  */\n\n\t  #pragma unroll\n\t  for(int i = 0; i < iter_index; i += 1){\n\t\t  if(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\tint tile_k_idx_A = *(lane_ptr_index);\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_A * M_GLOBAL) + (laneId % CHUNK_COPY_LINE_LANES_A);\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_A)) =\n\t\t\t\t*lane_ptr;\n\t\t\t//warp_ptr = (uint8_t *)((uint8_t *)warp_ptr + M_GLOBAL * (WARPS_PER_BLOCK / 2) *CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tlane_ptr_index = (int *)((int *)lane_ptr_index +  (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A;\n\t\t  }else{\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_B)) =\n\t\t\t\t*lane_ptr;\n\t\t\tlane_ptr = (int4 *)((uint8_t *)lane_ptr + K_GLOBAL_SPARSE * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B;\n\t\t  }\n\t  }\n\n      __syncthreads();\n\n\t#pragma unroll\n      for (int k_step = 0; k_step < CHUNK_K_SPARSE; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::col_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n\t#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i += 1) {\n\t\t\tsize_t shmem_idx_a = (warpId % BLOCK_COL_WARPS) * M * WARP_COL_TILES + (i * M);\n\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_a + k_step * K * SHARED_OFFSET_A;\n\n\t\t\twmma::load_matrix_sync(a[i], tile_ptr, SHARED_OFFSET_A);\n\t\t#pragma unroll\n\t\t\tfor(int j = 0; j < WARP_ROW_TILES; j += 1){\n\t\t\t\tif(i == 0){\n\t\t\t\t\tsize_t shmem_idx_b = shmem_idx_b_off +\n\t\t\t\t\t\t\t\t\t\t\t(warpId / BLOCK_COL_WARPS) * (WARP_ROW_TILES * N) * SHARED_OFFSET_B +\n\t\t\t\t\t\t\t\t\t\t\t(j * N) * SHARED_OFFSET_B;\n\t\t\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_b + k_step * K;\n\t\t\t\t\twmma::load_matrix_sync(b[j], tile_ptr, SHARED_OFFSET_B);\n\t\t\t\t}\n\t\t\t\twmma::mma_sync(c[j][i], a[i], b[j], c[j][i]);\n\t\t\t}\n\n        }\n      }\n\n      __syncthreads();\n    }\n\n    // This pointer is used to access the C and D matrix tiles this warp computes.\n\tint *shmem_warp_tile_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * N * WARP_ROW_TILES * SHMEM_STRIDE +\n\t(warpId % BLOCK_COL_WARPS) * SHMEM_OFFSET;\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t#pragma unroll\n\t\t\tfor(int t = 0; t < c[i][j].num_elements; t += 1){\n\t\t\t\tc[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n\t\t\t}\n\t\t\tint *tile_ptr = shmem_warp_tile_ptr + i * N * SHMEM_STRIDE + j * M;\n\t\t\twmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, C_LAYOUT);\n\t\t}\n\t}\n\n    __syncthreads();\n\n\tint *shmem_warp_stream_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N * SHMEM_STRIDE\n\t\t\t\t\t\t\t\t\t+ (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tconst size_t gmem_idx =\n\t\t(block_tile_i * N + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N) * GLOBAL_MEM_STRIDE +\n\t\tblock_tile_j * M + (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tuint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n\tint *shmem_lane_stream_ptr =\n\t\tshmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * SHMEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\t\n\tuint8_t *dst_gmem_lane_stream_ptr =\n\t\tdst_gmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * GLOBAL_MEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\n\tfor(int i = 0; i < WARP_ROW_TILES * N; i += SHARED_TO_GLOBAL_LINES_PER_WARP){\n\t\t*(dst_gmem_lane_stream_ptr + i * GLOBAL_MEM_STRIDE) = (uint8_t)(*(shmem_lane_stream_ptr + i * SHMEM_STRIDE));\n\t}\n\n\t__syncthreads();\n}\n", "launch_config": {"dimBlock": [256, 1], "dimGrid": [256, 1]}}, "74": {"code": "\n__global__ void compute_gemm_imma(float *input0, float *input1, float *input2, float* input3, float* input4, float*input5, float* input6,\n                                  float *output0) {\n    // keys need to be replaced 4096, 1024, K_GLOBA_VALUEL, 0.75, 64\n\n\t//extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B = reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_index = reinterpret_cast<int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n\tconst int WARP_SIZE = 32;\n    const int M = 16;\n    const int K = 16;\n    const int N = 16;\n    const int WMMA_M = 16;\n    const int WMMA_K = 16;\n    const int WMMA_N = 16;\n    const int M_GLOBAL = 4096;\n    const int K_GLOBAL = 1024;\n    const int N_GLOBAL = 1024;\n    const float SPARSITY = 0.75;\n    const int K_GLOBAL_SPARSE = (int(K_GLOBAL * (1-SPARSITY)));\n    const int M_TILES = (M_GLOBAL / M);\n    const int N_TILES = (N_GLOBAL / N);\n    const int K_TILES = (K_GLOBAL / K);\n    const int BLOCK_ROW_WARPS = 2;\n    const int BLOCK_COL_WARPS = 4;\n    const int WARP_ROW_TILES = 4;\n    const int WARP_COL_TILES = 2;\n    const int SKEW_UINT8 = 16;\n    const int BANK_VAL = 32;\n    const int NUM_BANK = (K_GLOBAL / BANK_VAL);\n    /////////// BLOCK_ROW_TILES <= N_TILES ////////////\n    // const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    // ////////// BLOCK_COL_TILES <= M_TILES ////////////\n    // const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    auto C_LAYOUT = wmma::mem_col_major;\n    const int CHUNK_K = 64;\n    const int CHUNK_K_SPARSE = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K_SPARSE = (int((CHUNK_K * K) * (1 - SPARSITY)));\n    const int WARP_COPY_BYTES = (WARP_SIZE * sizeof(int4));\n    const int CHUNK_LINE_BYTES_A (BLOCK_COL_TILES * M * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_A = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_A);\n    const int CHUNK_COPY_LINE_LANES_A = (CHUNK_LINE_BYTES_A / sizeof(int4));\n    const int SHARED_OFFSET_A = (BLOCK_COL_TILES * M + SKEW_UINT8);\n\n    const int CHUNK_LINE_BYTES_B = (BLOCK_SIZE_K_SPARSE * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_B = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_B);\n    const int CHUNK_COPY_LINE_LANES_B = (CHUNK_LINE_BYTES_B / sizeof(int4));\n    const int SHARED_OFFSET_B = (BLOCK_SIZE_K_SPARSE + SKEW_UINT8);\n    const int SHARED_TO_GLOBAL_BYTES_PER_LINE = ((WARP_COL_TILES * M) * sizeof(int));\n    const int SHARED_TO_GLOBAL_BYTES_PER_WARP = (WARP_SIZE * sizeof(int));\n    const int SHARED_TO_GLOBAL_LINES_PER_WARP = (SHARED_TO_GLOBAL_BYTES_PER_WARP / SHARED_TO_GLOBAL_BYTES_PER_LINE);\n    const int SHARED_TO_GLOBAL_LANES_PER_LINE = (WARP_SIZE / SHARED_TO_GLOBAL_LINES_PER_WARP);\n    const int SHARED_TO_GLOBAL_ITERS = ((WARP_ROW_TILES * N) / SHARED_TO_GLOBAL_LINES_PER_WARP);\n\n    const int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const int WARP_STRIDE = (WARP_COL_TILES * M);\n    const int WARPS_PER_BLOCK = (BLOCK_ROW_WARPS * BLOCK_COL_WARPS);\n    const int THREADS_PER_BLOCK = (WARP_SIZE * WARPS_PER_BLOCK);\n    const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    const int GLOBAL_MEM_STRIDE = M_GLOBAL;\n    const int SHMEM_STRIDE = (M * BLOCK_COL_TILES);\n    const int SHMEM_OFFSET = (M * WARP_COL_TILES);\n    const int BLOCK_SIZE_M = (M * BLOCK_COL_TILES);\n    const int BLOCK_SIZE_N = (N * BLOCK_ROW_TILES);\n\n   extern __shared__ uint8_t shmem[];\n\n\t// Warp and lane identification.\n\tconst unsigned int warpId = threadIdx.x / WARP_SIZE;\n\tconst unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n\n\t// Offset in shared memory from which the B matrix is stored.\n\t// const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\tconst size_t shmem_idx_b_off = BLOCK_SIZE_K_SPARSE * SHARED_OFFSET_A;\n\n\n\t// Each CTA slides along the 128 x 128 tiles from the top left corner of the\n\t// matrix to the right and down, and selects the next tile to compute. Once\n\t// there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n\tconst unsigned int block_tile_i =\n\t\t((block_pos * BLOCK_COL_TILES) / M_TILES) * (BLOCK_ROW_TILES);\n\tconst unsigned int block_tile_j = (block_pos * BLOCK_COL_TILES) % M_TILES;\n\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n\twmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_ROW_TILES]\n\t\t\t\t\t\t\t\t\t\t\t\t\t [WARP_COL_TILES];\n\n    // Load the C matrix tiles into fragments from shared memory.\n\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t\twmma::fill_fragment(c[i][j], 0);\n\t\t}\n\t}\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    //for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n    for(int tile_k_idx_sparse = 0, tile_k_idx = 0; tile_k_idx_sparse < K_GLOBAL_SPARSE; tile_k_idx_sparse += BLOCK_SIZE_K_SPARSE, tile_k_idx += BLOCK_SIZE_K){\n\n\t\tsize_t shmem_idx = \n\t\twarpId < (WARPS_PER_BLOCK / 2)\n\t\t\t? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A\n\t\t\t: (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B + shmem_idx_b_off;\n\n\t\tint4 *lane_ptr = NULL;\n\t\tint *lane_ptr_index = NULL;\n\t\tconst uint8_t *warp_ptr = NULL;\n\n\n\t\tif(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\t//warp_ptr = &A[block_tile_j * M] +\n\t\t\t//\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * M_GLOBAL;\n\t\t\twarp_ptr = &A[block_tile_j * M];\n\t\t\t\n\t\t\tconst int *warp_ptr_index = &B_index[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t\t\t\t\t\t((warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A);\n\n\t\t\tlane_ptr_index = (int *)(warp_ptr_index + tile_k_idx_sparse + (laneId / CHUNK_COPY_LINE_LANES_A));\n\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_A) * SHARED_OFFSET_A;\n\t\t}else{\n\t\t\twarp_ptr = &B[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * K_GLOBAL_SPARSE;\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_sparse +\n\t\t\t\t\t\t\t\t(laneId / CHUNK_COPY_LINE_LANES_B) * K_GLOBAL_SPARSE) +\n\t\t\t\t\t\t\t\t(laneId % CHUNK_COPY_LINE_LANES_B);\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_B) * SHARED_OFFSET_B;\n\t\t}\n\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      // shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n\t  int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n\t  \t? BLOCK_SIZE_K_SPARSE / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A)\n\t\t: BLOCK_SIZE_N / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\n\t  /*\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\t  */\n\n\t  /*\n      int tile_k_idx_A;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n          tile_k_idx_A = *(lane_ptr_index);\n      }\n\t  */\n\n\t  #pragma unroll\n\t  for(int i = 0; i < iter_index; i += 1){\n\t\t  if(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\tint tile_k_idx_A = *(lane_ptr_index);\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_A * M_GLOBAL) + (laneId % CHUNK_COPY_LINE_LANES_A);\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_A)) =\n\t\t\t\t*lane_ptr;\n\t\t\t//warp_ptr = (uint8_t *)((uint8_t *)warp_ptr + M_GLOBAL * (WARPS_PER_BLOCK / 2) *CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tlane_ptr_index = (int *)((int *)lane_ptr_index +  (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A;\n\t\t  }else{\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_B)) =\n\t\t\t\t*lane_ptr;\n\t\t\tlane_ptr = (int4 *)((uint8_t *)lane_ptr + K_GLOBAL_SPARSE * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B;\n\t\t  }\n\t  }\n\n      __syncthreads();\n\n\t#pragma unroll\n      for (int k_step = 0; k_step < CHUNK_K_SPARSE; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::col_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n\t#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i += 1) {\n\t\t\tsize_t shmem_idx_a = (warpId % BLOCK_COL_WARPS) * M * WARP_COL_TILES + (i * M);\n\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_a + k_step * K * SHARED_OFFSET_A;\n\n\t\t\twmma::load_matrix_sync(a[i], tile_ptr, SHARED_OFFSET_A);\n\t\t#pragma unroll\n\t\t\tfor(int j = 0; j < WARP_ROW_TILES; j += 1){\n\t\t\t\tif(i == 0){\n\t\t\t\t\tsize_t shmem_idx_b = shmem_idx_b_off +\n\t\t\t\t\t\t\t\t\t\t\t(warpId / BLOCK_COL_WARPS) * (WARP_ROW_TILES * N) * SHARED_OFFSET_B +\n\t\t\t\t\t\t\t\t\t\t\t(j * N) * SHARED_OFFSET_B;\n\t\t\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_b + k_step * K;\n\t\t\t\t\twmma::load_matrix_sync(b[j], tile_ptr, SHARED_OFFSET_B);\n\t\t\t\t}\n\t\t\t\twmma::mma_sync(c[j][i], a[i], b[j], c[j][i]);\n\t\t\t}\n\n        }\n      }\n\n      __syncthreads();\n    }\n\n    // This pointer is used to access the C and D matrix tiles this warp computes.\n\tint *shmem_warp_tile_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * N * WARP_ROW_TILES * SHMEM_STRIDE +\n\t(warpId % BLOCK_COL_WARPS) * SHMEM_OFFSET;\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t#pragma unroll\n\t\t\tfor(int t = 0; t < c[i][j].num_elements; t += 1){\n\t\t\t\tc[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n\t\t\t}\n\t\t\tint *tile_ptr = shmem_warp_tile_ptr + i * N * SHMEM_STRIDE + j * M;\n\t\t\twmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, C_LAYOUT);\n\t\t}\n\t}\n\n    __syncthreads();\n\n\tint *shmem_warp_stream_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N * SHMEM_STRIDE\n\t\t\t\t\t\t\t\t\t+ (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tconst size_t gmem_idx =\n\t\t(block_tile_i * N + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N) * GLOBAL_MEM_STRIDE +\n\t\tblock_tile_j * M + (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tuint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n\tint *shmem_lane_stream_ptr =\n\t\tshmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * SHMEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\t\n\tuint8_t *dst_gmem_lane_stream_ptr =\n\t\tdst_gmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * GLOBAL_MEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\n\tfor(int i = 0; i < WARP_ROW_TILES * N; i += SHARED_TO_GLOBAL_LINES_PER_WARP){\n\t\t*(dst_gmem_lane_stream_ptr + i * GLOBAL_MEM_STRIDE) = (uint8_t)(*(shmem_lane_stream_ptr + i * SHMEM_STRIDE));\n\t}\n\n\t__syncthreads();\n}\n", "launch_config": {"dimBlock": [256, 1], "dimGrid": [256, 1]}}, "75": {"code": "\n__global__ void compute_gemm_imma(float *input0, float *input1, float *input2, float* input3, float* input4, float*input5, float* input6,\n                                  float *output0) {\n    // keys need to be replaced 4096, 1024, K_GLOBA_VALUEL, 0.75, 64\n\n\t//extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B = reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_index = reinterpret_cast<int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n\tconst int WARP_SIZE = 32;\n    const int M = 16;\n    const int K = 16;\n    const int N = 16;\n    const int WMMA_M = 16;\n    const int WMMA_K = 16;\n    const int WMMA_N = 16;\n    const int M_GLOBAL = 4096;\n    const int K_GLOBAL = 1024;\n    const int N_GLOBAL = 1024;\n    const float SPARSITY = 0.75;\n    const int K_GLOBAL_SPARSE = (int(K_GLOBAL * (1-SPARSITY)));\n    const int M_TILES = (M_GLOBAL / M);\n    const int N_TILES = (N_GLOBAL / N);\n    const int K_TILES = (K_GLOBAL / K);\n    const int BLOCK_ROW_WARPS = 2;\n    const int BLOCK_COL_WARPS = 4;\n    const int WARP_ROW_TILES = 4;\n    const int WARP_COL_TILES = 2;\n    const int SKEW_UINT8 = 16;\n    const int BANK_VAL = 32;\n    const int NUM_BANK = (K_GLOBAL / BANK_VAL);\n    /////////// BLOCK_ROW_TILES <= N_TILES ////////////\n    // const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    // ////////// BLOCK_COL_TILES <= M_TILES ////////////\n    // const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    auto C_LAYOUT = wmma::mem_col_major;\n    const int CHUNK_K = 64;\n    const int CHUNK_K_SPARSE = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K_SPARSE = (int((CHUNK_K * K) * (1 - SPARSITY)));\n    const int WARP_COPY_BYTES = (WARP_SIZE * sizeof(int4));\n    const int CHUNK_LINE_BYTES_A (BLOCK_COL_TILES * M * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_A = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_A);\n    const int CHUNK_COPY_LINE_LANES_A = (CHUNK_LINE_BYTES_A / sizeof(int4));\n    const int SHARED_OFFSET_A = (BLOCK_COL_TILES * M + SKEW_UINT8);\n\n    const int CHUNK_LINE_BYTES_B = (BLOCK_SIZE_K_SPARSE * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_B = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_B);\n    const int CHUNK_COPY_LINE_LANES_B = (CHUNK_LINE_BYTES_B / sizeof(int4));\n    const int SHARED_OFFSET_B = (BLOCK_SIZE_K_SPARSE + SKEW_UINT8);\n    const int SHARED_TO_GLOBAL_BYTES_PER_LINE = ((WARP_COL_TILES * M) * sizeof(int));\n    const int SHARED_TO_GLOBAL_BYTES_PER_WARP = (WARP_SIZE * sizeof(int));\n    const int SHARED_TO_GLOBAL_LINES_PER_WARP = (SHARED_TO_GLOBAL_BYTES_PER_WARP / SHARED_TO_GLOBAL_BYTES_PER_LINE);\n    const int SHARED_TO_GLOBAL_LANES_PER_LINE = (WARP_SIZE / SHARED_TO_GLOBAL_LINES_PER_WARP);\n    const int SHARED_TO_GLOBAL_ITERS = ((WARP_ROW_TILES * N) / SHARED_TO_GLOBAL_LINES_PER_WARP);\n\n    const int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const int WARP_STRIDE = (WARP_COL_TILES * M);\n    const int WARPS_PER_BLOCK = (BLOCK_ROW_WARPS * BLOCK_COL_WARPS);\n    const int THREADS_PER_BLOCK = (WARP_SIZE * WARPS_PER_BLOCK);\n    const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    const int GLOBAL_MEM_STRIDE = M_GLOBAL;\n    const int SHMEM_STRIDE = (M * BLOCK_COL_TILES);\n    const int SHMEM_OFFSET = (M * WARP_COL_TILES);\n    const int BLOCK_SIZE_M = (M * BLOCK_COL_TILES);\n    const int BLOCK_SIZE_N = (N * BLOCK_ROW_TILES);\n\n   extern __shared__ uint8_t shmem[];\n\n\t// Warp and lane identification.\n\tconst unsigned int warpId = threadIdx.x / WARP_SIZE;\n\tconst unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n\n\t// Offset in shared memory from which the B matrix is stored.\n\t// const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\tconst size_t shmem_idx_b_off = BLOCK_SIZE_K_SPARSE * SHARED_OFFSET_A;\n\n\n\t// Each CTA slides along the 128 x 128 tiles from the top left corner of the\n\t// matrix to the right and down, and selects the next tile to compute. Once\n\t// there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n\tconst unsigned int block_tile_i =\n\t\t((block_pos * BLOCK_COL_TILES) / M_TILES) * (BLOCK_ROW_TILES);\n\tconst unsigned int block_tile_j = (block_pos * BLOCK_COL_TILES) % M_TILES;\n\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n\twmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_ROW_TILES]\n\t\t\t\t\t\t\t\t\t\t\t\t\t [WARP_COL_TILES];\n\n    // Load the C matrix tiles into fragments from shared memory.\n\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t\twmma::fill_fragment(c[i][j], 0);\n\t\t}\n\t}\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    //for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n    for(int tile_k_idx_sparse = 0, tile_k_idx = 0; tile_k_idx_sparse < K_GLOBAL_SPARSE; tile_k_idx_sparse += BLOCK_SIZE_K_SPARSE, tile_k_idx += BLOCK_SIZE_K){\n\n\t\tsize_t shmem_idx = \n\t\twarpId < (WARPS_PER_BLOCK / 2)\n\t\t\t? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A\n\t\t\t: (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B + shmem_idx_b_off;\n\n\t\tint4 *lane_ptr = NULL;\n\t\tint *lane_ptr_index = NULL;\n\t\tconst uint8_t *warp_ptr = NULL;\n\n\n\t\tif(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\t//warp_ptr = &A[block_tile_j * M] +\n\t\t\t//\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * M_GLOBAL;\n\t\t\twarp_ptr = &A[block_tile_j * M];\n\t\t\t\n\t\t\tconst int *warp_ptr_index = &B_index[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t\t\t\t\t\t((warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A);\n\n\t\t\tlane_ptr_index = (int *)(warp_ptr_index + tile_k_idx_sparse + (laneId / CHUNK_COPY_LINE_LANES_A));\n\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_A) * SHARED_OFFSET_A;\n\t\t}else{\n\t\t\twarp_ptr = &B[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * K_GLOBAL_SPARSE;\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_sparse +\n\t\t\t\t\t\t\t\t(laneId / CHUNK_COPY_LINE_LANES_B) * K_GLOBAL_SPARSE) +\n\t\t\t\t\t\t\t\t(laneId % CHUNK_COPY_LINE_LANES_B);\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_B) * SHARED_OFFSET_B;\n\t\t}\n\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      // shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n\t  int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n\t  \t? BLOCK_SIZE_K_SPARSE / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A)\n\t\t: BLOCK_SIZE_N / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\n\t  /*\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\t  */\n\n\t  /*\n      int tile_k_idx_A;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n          tile_k_idx_A = *(lane_ptr_index);\n      }\n\t  */\n\n\t  #pragma unroll\n\t  for(int i = 0; i < iter_index; i += 1){\n\t\t  if(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\tint tile_k_idx_A = *(lane_ptr_index);\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_A * M_GLOBAL) + (laneId % CHUNK_COPY_LINE_LANES_A);\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_A)) =\n\t\t\t\t*lane_ptr;\n\t\t\t//warp_ptr = (uint8_t *)((uint8_t *)warp_ptr + M_GLOBAL * (WARPS_PER_BLOCK / 2) *CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tlane_ptr_index = (int *)((int *)lane_ptr_index +  (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A;\n\t\t  }else{\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_B)) =\n\t\t\t\t*lane_ptr;\n\t\t\tlane_ptr = (int4 *)((uint8_t *)lane_ptr + K_GLOBAL_SPARSE * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B;\n\t\t  }\n\t  }\n\n      __syncthreads();\n\n\t#pragma unroll\n      for (int k_step = 0; k_step < CHUNK_K_SPARSE; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::col_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n\t#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i += 1) {\n\t\t\tsize_t shmem_idx_a = (warpId % BLOCK_COL_WARPS) * M * WARP_COL_TILES + (i * M);\n\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_a + k_step * K * SHARED_OFFSET_A;\n\n\t\t\twmma::load_matrix_sync(a[i], tile_ptr, SHARED_OFFSET_A);\n\t\t#pragma unroll\n\t\t\tfor(int j = 0; j < WARP_ROW_TILES; j += 1){\n\t\t\t\tif(i == 0){\n\t\t\t\t\tsize_t shmem_idx_b = shmem_idx_b_off +\n\t\t\t\t\t\t\t\t\t\t\t(warpId / BLOCK_COL_WARPS) * (WARP_ROW_TILES * N) * SHARED_OFFSET_B +\n\t\t\t\t\t\t\t\t\t\t\t(j * N) * SHARED_OFFSET_B;\n\t\t\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_b + k_step * K;\n\t\t\t\t\twmma::load_matrix_sync(b[j], tile_ptr, SHARED_OFFSET_B);\n\t\t\t\t}\n\t\t\t\twmma::mma_sync(c[j][i], a[i], b[j], c[j][i]);\n\t\t\t}\n\n        }\n      }\n\n      __syncthreads();\n    }\n\n    // This pointer is used to access the C and D matrix tiles this warp computes.\n\tint *shmem_warp_tile_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * N * WARP_ROW_TILES * SHMEM_STRIDE +\n\t(warpId % BLOCK_COL_WARPS) * SHMEM_OFFSET;\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t#pragma unroll\n\t\t\tfor(int t = 0; t < c[i][j].num_elements; t += 1){\n\t\t\t\tc[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n\t\t\t}\n\t\t\tint *tile_ptr = shmem_warp_tile_ptr + i * N * SHMEM_STRIDE + j * M;\n\t\t\twmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, C_LAYOUT);\n\t\t}\n\t}\n\n    __syncthreads();\n\n\tint *shmem_warp_stream_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N * SHMEM_STRIDE\n\t\t\t\t\t\t\t\t\t+ (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tconst size_t gmem_idx =\n\t\t(block_tile_i * N + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N) * GLOBAL_MEM_STRIDE +\n\t\tblock_tile_j * M + (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tuint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n\tint *shmem_lane_stream_ptr =\n\t\tshmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * SHMEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\t\n\tuint8_t *dst_gmem_lane_stream_ptr =\n\t\tdst_gmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * GLOBAL_MEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\n\tfor(int i = 0; i < WARP_ROW_TILES * N; i += SHARED_TO_GLOBAL_LINES_PER_WARP){\n\t\t*(dst_gmem_lane_stream_ptr + i * GLOBAL_MEM_STRIDE) = (uint8_t)(*(shmem_lane_stream_ptr + i * SHMEM_STRIDE));\n\t}\n\n\t__syncthreads();\n}\n", "launch_config": {"dimBlock": [256, 1], "dimGrid": [256, 1]}}, "76": {"code": "\n__global__ void compute_gemm_imma(float *input0, float *input1, float *input2, float* input3, float* input4, float*input5, float* input6,\n                                  float *output0) {\n    // keys need to be replaced 4096, 1024, K_GLOBA_VALUEL, 0.75, 64\n\n\t//extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B = reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_index = reinterpret_cast<int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n\tconst int WARP_SIZE = 32;\n    const int M = 16;\n    const int K = 16;\n    const int N = 16;\n    const int WMMA_M = 16;\n    const int WMMA_K = 16;\n    const int WMMA_N = 16;\n    const int M_GLOBAL = 4096;\n    const int K_GLOBAL = 1024;\n    const int N_GLOBAL = 1024;\n    const float SPARSITY = 0.75;\n    const int K_GLOBAL_SPARSE = (int(K_GLOBAL * (1-SPARSITY)));\n    const int M_TILES = (M_GLOBAL / M);\n    const int N_TILES = (N_GLOBAL / N);\n    const int K_TILES = (K_GLOBAL / K);\n    const int BLOCK_ROW_WARPS = 2;\n    const int BLOCK_COL_WARPS = 4;\n    const int WARP_ROW_TILES = 4;\n    const int WARP_COL_TILES = 2;\n    const int SKEW_UINT8 = 16;\n    const int BANK_VAL = 32;\n    const int NUM_BANK = (K_GLOBAL / BANK_VAL);\n    /////////// BLOCK_ROW_TILES <= N_TILES ////////////\n    // const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    // ////////// BLOCK_COL_TILES <= M_TILES ////////////\n    // const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    auto C_LAYOUT = wmma::mem_col_major;\n    const int CHUNK_K = 64;\n    const int CHUNK_K_SPARSE = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K_SPARSE = (int((CHUNK_K * K) * (1 - SPARSITY)));\n    const int WARP_COPY_BYTES = (WARP_SIZE * sizeof(int4));\n    const int CHUNK_LINE_BYTES_A (BLOCK_COL_TILES * M * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_A = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_A);\n    const int CHUNK_COPY_LINE_LANES_A = (CHUNK_LINE_BYTES_A / sizeof(int4));\n    const int SHARED_OFFSET_A = (BLOCK_COL_TILES * M + SKEW_UINT8);\n\n    const int CHUNK_LINE_BYTES_B = (BLOCK_SIZE_K_SPARSE * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_B = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_B);\n    const int CHUNK_COPY_LINE_LANES_B = (CHUNK_LINE_BYTES_B / sizeof(int4));\n    const int SHARED_OFFSET_B = (BLOCK_SIZE_K_SPARSE + SKEW_UINT8);\n    const int SHARED_TO_GLOBAL_BYTES_PER_LINE = ((WARP_COL_TILES * M) * sizeof(int));\n    const int SHARED_TO_GLOBAL_BYTES_PER_WARP = (WARP_SIZE * sizeof(int));\n    const int SHARED_TO_GLOBAL_LINES_PER_WARP = (SHARED_TO_GLOBAL_BYTES_PER_WARP / SHARED_TO_GLOBAL_BYTES_PER_LINE);\n    const int SHARED_TO_GLOBAL_LANES_PER_LINE = (WARP_SIZE / SHARED_TO_GLOBAL_LINES_PER_WARP);\n    const int SHARED_TO_GLOBAL_ITERS = ((WARP_ROW_TILES * N) / SHARED_TO_GLOBAL_LINES_PER_WARP);\n\n    const int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const int WARP_STRIDE = (WARP_COL_TILES * M);\n    const int WARPS_PER_BLOCK = (BLOCK_ROW_WARPS * BLOCK_COL_WARPS);\n    const int THREADS_PER_BLOCK = (WARP_SIZE * WARPS_PER_BLOCK);\n    const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    const int GLOBAL_MEM_STRIDE = M_GLOBAL;\n    const int SHMEM_STRIDE = (M * BLOCK_COL_TILES);\n    const int SHMEM_OFFSET = (M * WARP_COL_TILES);\n    const int BLOCK_SIZE_M = (M * BLOCK_COL_TILES);\n    const int BLOCK_SIZE_N = (N * BLOCK_ROW_TILES);\n\n   extern __shared__ uint8_t shmem[];\n\n\t// Warp and lane identification.\n\tconst unsigned int warpId = threadIdx.x / WARP_SIZE;\n\tconst unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n\n\t// Offset in shared memory from which the B matrix is stored.\n\t// const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\tconst size_t shmem_idx_b_off = BLOCK_SIZE_K_SPARSE * SHARED_OFFSET_A;\n\n\n\t// Each CTA slides along the 128 x 128 tiles from the top left corner of the\n\t// matrix to the right and down, and selects the next tile to compute. Once\n\t// there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n\tconst unsigned int block_tile_i =\n\t\t((block_pos * BLOCK_COL_TILES) / M_TILES) * (BLOCK_ROW_TILES);\n\tconst unsigned int block_tile_j = (block_pos * BLOCK_COL_TILES) % M_TILES;\n\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n\twmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_ROW_TILES]\n\t\t\t\t\t\t\t\t\t\t\t\t\t [WARP_COL_TILES];\n\n    // Load the C matrix tiles into fragments from shared memory.\n\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t\twmma::fill_fragment(c[i][j], 0);\n\t\t}\n\t}\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    //for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n    for(int tile_k_idx_sparse = 0, tile_k_idx = 0; tile_k_idx_sparse < K_GLOBAL_SPARSE; tile_k_idx_sparse += BLOCK_SIZE_K_SPARSE, tile_k_idx += BLOCK_SIZE_K){\n\n\t\tsize_t shmem_idx = \n\t\twarpId < (WARPS_PER_BLOCK / 2)\n\t\t\t? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A\n\t\t\t: (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B + shmem_idx_b_off;\n\n\t\tint4 *lane_ptr = NULL;\n\t\tint *lane_ptr_index = NULL;\n\t\tconst uint8_t *warp_ptr = NULL;\n\n\n\t\tif(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\t//warp_ptr = &A[block_tile_j * M] +\n\t\t\t//\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * M_GLOBAL;\n\t\t\twarp_ptr = &A[block_tile_j * M];\n\t\t\t\n\t\t\tconst int *warp_ptr_index = &B_index[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t\t\t\t\t\t((warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A);\n\n\t\t\tlane_ptr_index = (int *)(warp_ptr_index + tile_k_idx_sparse + (laneId / CHUNK_COPY_LINE_LANES_A));\n\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_A) * SHARED_OFFSET_A;\n\t\t}else{\n\t\t\twarp_ptr = &B[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * K_GLOBAL_SPARSE;\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_sparse +\n\t\t\t\t\t\t\t\t(laneId / CHUNK_COPY_LINE_LANES_B) * K_GLOBAL_SPARSE) +\n\t\t\t\t\t\t\t\t(laneId % CHUNK_COPY_LINE_LANES_B);\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_B) * SHARED_OFFSET_B;\n\t\t}\n\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      // shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n\t  int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n\t  \t? BLOCK_SIZE_K_SPARSE / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A)\n\t\t: BLOCK_SIZE_N / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\n\t  /*\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\t  */\n\n\t  /*\n      int tile_k_idx_A;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n          tile_k_idx_A = *(lane_ptr_index);\n      }\n\t  */\n\n\t  #pragma unroll\n\t  for(int i = 0; i < iter_index; i += 1){\n\t\t  if(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\tint tile_k_idx_A = *(lane_ptr_index);\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_A * M_GLOBAL) + (laneId % CHUNK_COPY_LINE_LANES_A);\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_A)) =\n\t\t\t\t*lane_ptr;\n\t\t\t//warp_ptr = (uint8_t *)((uint8_t *)warp_ptr + M_GLOBAL * (WARPS_PER_BLOCK / 2) *CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tlane_ptr_index = (int *)((int *)lane_ptr_index +  (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A;\n\t\t  }else{\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_B)) =\n\t\t\t\t*lane_ptr;\n\t\t\tlane_ptr = (int4 *)((uint8_t *)lane_ptr + K_GLOBAL_SPARSE * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B;\n\t\t  }\n\t  }\n\n      __syncthreads();\n\n\t#pragma unroll\n      for (int k_step = 0; k_step < CHUNK_K_SPARSE; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::col_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n\t#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i += 1) {\n\t\t\tsize_t shmem_idx_a = (warpId % BLOCK_COL_WARPS) * M * WARP_COL_TILES + (i * M);\n\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_a + k_step * K * SHARED_OFFSET_A;\n\n\t\t\twmma::load_matrix_sync(a[i], tile_ptr, SHARED_OFFSET_A);\n\t\t#pragma unroll\n\t\t\tfor(int j = 0; j < WARP_ROW_TILES; j += 1){\n\t\t\t\tif(i == 0){\n\t\t\t\t\tsize_t shmem_idx_b = shmem_idx_b_off +\n\t\t\t\t\t\t\t\t\t\t\t(warpId / BLOCK_COL_WARPS) * (WARP_ROW_TILES * N) * SHARED_OFFSET_B +\n\t\t\t\t\t\t\t\t\t\t\t(j * N) * SHARED_OFFSET_B;\n\t\t\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_b + k_step * K;\n\t\t\t\t\twmma::load_matrix_sync(b[j], tile_ptr, SHARED_OFFSET_B);\n\t\t\t\t}\n\t\t\t\twmma::mma_sync(c[j][i], a[i], b[j], c[j][i]);\n\t\t\t}\n\n        }\n      }\n\n      __syncthreads();\n    }\n\n    // This pointer is used to access the C and D matrix tiles this warp computes.\n\tint *shmem_warp_tile_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * N * WARP_ROW_TILES * SHMEM_STRIDE +\n\t(warpId % BLOCK_COL_WARPS) * SHMEM_OFFSET;\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t#pragma unroll\n\t\t\tfor(int t = 0; t < c[i][j].num_elements; t += 1){\n\t\t\t\tc[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n\t\t\t}\n\t\t\tint *tile_ptr = shmem_warp_tile_ptr + i * N * SHMEM_STRIDE + j * M;\n\t\t\twmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, C_LAYOUT);\n\t\t}\n\t}\n\n    __syncthreads();\n\n\tint *shmem_warp_stream_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N * SHMEM_STRIDE\n\t\t\t\t\t\t\t\t\t+ (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tconst size_t gmem_idx =\n\t\t(block_tile_i * N + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N) * GLOBAL_MEM_STRIDE +\n\t\tblock_tile_j * M + (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tuint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n\tint *shmem_lane_stream_ptr =\n\t\tshmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * SHMEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\t\n\tuint8_t *dst_gmem_lane_stream_ptr =\n\t\tdst_gmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * GLOBAL_MEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\n\tfor(int i = 0; i < WARP_ROW_TILES * N; i += SHARED_TO_GLOBAL_LINES_PER_WARP){\n\t\t*(dst_gmem_lane_stream_ptr + i * GLOBAL_MEM_STRIDE) = (uint8_t)(*(shmem_lane_stream_ptr + i * SHMEM_STRIDE));\n\t}\n\n\t__syncthreads();\n}\n", "launch_config": {"dimBlock": [256, 1], "dimGrid": [256, 1]}}, "77": {"code": "\n__global__ void compute_gemm_imma(float *input0, float *input1, float *input2, float* input3, float* input4, float*input5, float* input6,\n                                  float *output0) {\n    // keys need to be replaced 4096, 4096, K_GLOBA_VALUEL, 0.75, 64\n\n\t//extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B = reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_index = reinterpret_cast<int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n\tconst int WARP_SIZE = 32;\n    const int M = 16;\n    const int K = 16;\n    const int N = 16;\n    const int WMMA_M = 16;\n    const int WMMA_K = 16;\n    const int WMMA_N = 16;\n    const int M_GLOBAL = 4096;\n    const int K_GLOBAL = 1024;\n    const int N_GLOBAL = 4096;\n    const float SPARSITY = 0.75;\n    const int K_GLOBAL_SPARSE = (int(K_GLOBAL * (1-SPARSITY)));\n    const int M_TILES = (M_GLOBAL / M);\n    const int N_TILES = (N_GLOBAL / N);\n    const int K_TILES = (K_GLOBAL / K);\n    const int BLOCK_ROW_WARPS = 2;\n    const int BLOCK_COL_WARPS = 4;\n    const int WARP_ROW_TILES = 4;\n    const int WARP_COL_TILES = 2;\n    const int SKEW_UINT8 = 16;\n    const int BANK_VAL = 32;\n    const int NUM_BANK = (K_GLOBAL / BANK_VAL);\n    /////////// BLOCK_ROW_TILES <= N_TILES ////////////\n    // const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    // ////////// BLOCK_COL_TILES <= M_TILES ////////////\n    // const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    auto C_LAYOUT = wmma::mem_col_major;\n    const int CHUNK_K = 64;\n    const int CHUNK_K_SPARSE = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K_SPARSE = (int((CHUNK_K * K) * (1 - SPARSITY)));\n    const int WARP_COPY_BYTES = (WARP_SIZE * sizeof(int4));\n    const int CHUNK_LINE_BYTES_A (BLOCK_COL_TILES * M * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_A = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_A);\n    const int CHUNK_COPY_LINE_LANES_A = (CHUNK_LINE_BYTES_A / sizeof(int4));\n    const int SHARED_OFFSET_A = (BLOCK_COL_TILES * M + SKEW_UINT8);\n\n    const int CHUNK_LINE_BYTES_B = (BLOCK_SIZE_K_SPARSE * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_B = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_B);\n    const int CHUNK_COPY_LINE_LANES_B = (CHUNK_LINE_BYTES_B / sizeof(int4));\n    const int SHARED_OFFSET_B = (BLOCK_SIZE_K_SPARSE + SKEW_UINT8);\n    const int SHARED_TO_GLOBAL_BYTES_PER_LINE = ((WARP_COL_TILES * M) * sizeof(int));\n    const int SHARED_TO_GLOBAL_BYTES_PER_WARP = (WARP_SIZE * sizeof(int));\n    const int SHARED_TO_GLOBAL_LINES_PER_WARP = (SHARED_TO_GLOBAL_BYTES_PER_WARP / SHARED_TO_GLOBAL_BYTES_PER_LINE);\n    const int SHARED_TO_GLOBAL_LANES_PER_LINE = (WARP_SIZE / SHARED_TO_GLOBAL_LINES_PER_WARP);\n    const int SHARED_TO_GLOBAL_ITERS = ((WARP_ROW_TILES * N) / SHARED_TO_GLOBAL_LINES_PER_WARP);\n\n    const int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const int WARP_STRIDE = (WARP_COL_TILES * M);\n    const int WARPS_PER_BLOCK = (BLOCK_ROW_WARPS * BLOCK_COL_WARPS);\n    const int THREADS_PER_BLOCK = (WARP_SIZE * WARPS_PER_BLOCK);\n    const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    const int GLOBAL_MEM_STRIDE = M_GLOBAL;\n    const int SHMEM_STRIDE = (M * BLOCK_COL_TILES);\n    const int SHMEM_OFFSET = (M * WARP_COL_TILES);\n    const int BLOCK_SIZE_M = (M * BLOCK_COL_TILES);\n    const int BLOCK_SIZE_N = (N * BLOCK_ROW_TILES);\n\n   extern __shared__ uint8_t shmem[];\n\n\t// Warp and lane identification.\n\tconst unsigned int warpId = threadIdx.x / WARP_SIZE;\n\tconst unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n\n\t// Offset in shared memory from which the B matrix is stored.\n\t// const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\tconst size_t shmem_idx_b_off = BLOCK_SIZE_K_SPARSE * SHARED_OFFSET_A;\n\n\n\t// Each CTA slides along the 128 x 128 tiles from the top left corner of the\n\t// matrix to the right and down, and selects the next tile to compute. Once\n\t// there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n\tconst unsigned int block_tile_i =\n\t\t((block_pos * BLOCK_COL_TILES) / M_TILES) * (BLOCK_ROW_TILES);\n\tconst unsigned int block_tile_j = (block_pos * BLOCK_COL_TILES) % M_TILES;\n\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n\twmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_ROW_TILES]\n\t\t\t\t\t\t\t\t\t\t\t\t\t [WARP_COL_TILES];\n\n    // Load the C matrix tiles into fragments from shared memory.\n\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t\twmma::fill_fragment(c[i][j], 0);\n\t\t}\n\t}\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    //for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n    for(int tile_k_idx_sparse = 0, tile_k_idx = 0; tile_k_idx_sparse < K_GLOBAL_SPARSE; tile_k_idx_sparse += BLOCK_SIZE_K_SPARSE, tile_k_idx += BLOCK_SIZE_K){\n\n\t\tsize_t shmem_idx = \n\t\twarpId < (WARPS_PER_BLOCK / 2)\n\t\t\t? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A\n\t\t\t: (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B + shmem_idx_b_off;\n\n\t\tint4 *lane_ptr = NULL;\n\t\tint *lane_ptr_index = NULL;\n\t\tconst uint8_t *warp_ptr = NULL;\n\n\n\t\tif(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\t//warp_ptr = &A[block_tile_j * M] +\n\t\t\t//\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * M_GLOBAL;\n\t\t\twarp_ptr = &A[block_tile_j * M];\n\t\t\t\n\t\t\tconst int *warp_ptr_index = &B_index[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t\t\t\t\t\t((warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A);\n\n\t\t\tlane_ptr_index = (int *)(warp_ptr_index + tile_k_idx_sparse + (laneId / CHUNK_COPY_LINE_LANES_A));\n\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_A) * SHARED_OFFSET_A;\n\t\t}else{\n\t\t\twarp_ptr = &B[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * K_GLOBAL_SPARSE;\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_sparse +\n\t\t\t\t\t\t\t\t(laneId / CHUNK_COPY_LINE_LANES_B) * K_GLOBAL_SPARSE) +\n\t\t\t\t\t\t\t\t(laneId % CHUNK_COPY_LINE_LANES_B);\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_B) * SHARED_OFFSET_B;\n\t\t}\n\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      // shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n\t  int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n\t  \t? BLOCK_SIZE_K_SPARSE / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A)\n\t\t: BLOCK_SIZE_N / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\n\t  /*\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\t  */\n\n\t  /*\n      int tile_k_idx_A;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n          tile_k_idx_A = *(lane_ptr_index);\n      }\n\t  */\n\n\t  #pragma unroll\n\t  for(int i = 0; i < iter_index; i += 1){\n\t\t  if(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\tint tile_k_idx_A = *(lane_ptr_index);\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_A * M_GLOBAL) + (laneId % CHUNK_COPY_LINE_LANES_A);\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_A)) =\n\t\t\t\t*lane_ptr;\n\t\t\t//warp_ptr = (uint8_t *)((uint8_t *)warp_ptr + M_GLOBAL * (WARPS_PER_BLOCK / 2) *CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tlane_ptr_index = (int *)((int *)lane_ptr_index +  (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A;\n\t\t  }else{\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_B)) =\n\t\t\t\t*lane_ptr;\n\t\t\tlane_ptr = (int4 *)((uint8_t *)lane_ptr + K_GLOBAL_SPARSE * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B;\n\t\t  }\n\t  }\n\n      __syncthreads();\n\n\t#pragma unroll\n      for (int k_step = 0; k_step < CHUNK_K_SPARSE; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::col_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n\t#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i += 1) {\n\t\t\tsize_t shmem_idx_a = (warpId % BLOCK_COL_WARPS) * M * WARP_COL_TILES + (i * M);\n\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_a + k_step * K * SHARED_OFFSET_A;\n\n\t\t\twmma::load_matrix_sync(a[i], tile_ptr, SHARED_OFFSET_A);\n\t\t#pragma unroll\n\t\t\tfor(int j = 0; j < WARP_ROW_TILES; j += 1){\n\t\t\t\tif(i == 0){\n\t\t\t\t\tsize_t shmem_idx_b = shmem_idx_b_off +\n\t\t\t\t\t\t\t\t\t\t\t(warpId / BLOCK_COL_WARPS) * (WARP_ROW_TILES * N) * SHARED_OFFSET_B +\n\t\t\t\t\t\t\t\t\t\t\t(j * N) * SHARED_OFFSET_B;\n\t\t\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_b + k_step * K;\n\t\t\t\t\twmma::load_matrix_sync(b[j], tile_ptr, SHARED_OFFSET_B);\n\t\t\t\t}\n\t\t\t\twmma::mma_sync(c[j][i], a[i], b[j], c[j][i]);\n\t\t\t}\n\n        }\n      }\n\n      __syncthreads();\n    }\n\n    // This pointer is used to access the C and D matrix tiles this warp computes.\n\tint *shmem_warp_tile_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * N * WARP_ROW_TILES * SHMEM_STRIDE +\n\t(warpId % BLOCK_COL_WARPS) * SHMEM_OFFSET;\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t#pragma unroll\n\t\t\tfor(int t = 0; t < c[i][j].num_elements; t += 1){\n\t\t\t\tc[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n\t\t\t}\n\t\t\tint *tile_ptr = shmem_warp_tile_ptr + i * N * SHMEM_STRIDE + j * M;\n\t\t\twmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, C_LAYOUT);\n\t\t}\n\t}\n\n    __syncthreads();\n\n\tint *shmem_warp_stream_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N * SHMEM_STRIDE\n\t\t\t\t\t\t\t\t\t+ (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tconst size_t gmem_idx =\n\t\t(block_tile_i * N + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N) * GLOBAL_MEM_STRIDE +\n\t\tblock_tile_j * M + (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tuint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n\tint *shmem_lane_stream_ptr =\n\t\tshmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * SHMEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\t\n\tuint8_t *dst_gmem_lane_stream_ptr =\n\t\tdst_gmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * GLOBAL_MEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\n\tfor(int i = 0; i < WARP_ROW_TILES * N; i += SHARED_TO_GLOBAL_LINES_PER_WARP){\n\t\t*(dst_gmem_lane_stream_ptr + i * GLOBAL_MEM_STRIDE) = (uint8_t)(*(shmem_lane_stream_ptr + i * SHMEM_STRIDE));\n\t}\n\n\t__syncthreads();\n}\n", "launch_config": {"dimBlock": [1024, 1], "dimGrid": [256, 1]}}, "78": {"code": "\n__global__ void compute_gemm_imma(float *input0, float *input1, float *input2, float* input3, float* input4, float*input5, float* input6,\n                                  float *output0) {\n    // keys need to be replaced 4096, 1024, K_GLOBA_VALUEL, 0.75, 64\n\n\t//extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B = reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_index = reinterpret_cast<int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n\tconst int WARP_SIZE = 32;\n    const int M = 16;\n    const int K = 16;\n    const int N = 16;\n    const int WMMA_M = 16;\n    const int WMMA_K = 16;\n    const int WMMA_N = 16;\n    const int M_GLOBAL = 4096;\n    const int K_GLOBAL = 4096;\n    const int N_GLOBAL = 1024;\n    const float SPARSITY = 0.75;\n    const int K_GLOBAL_SPARSE = (int(K_GLOBAL * (1-SPARSITY)));\n    const int M_TILES = (M_GLOBAL / M);\n    const int N_TILES = (N_GLOBAL / N);\n    const int K_TILES = (K_GLOBAL / K);\n    const int BLOCK_ROW_WARPS = 2;\n    const int BLOCK_COL_WARPS = 4;\n    const int WARP_ROW_TILES = 4;\n    const int WARP_COL_TILES = 2;\n    const int SKEW_UINT8 = 16;\n    const int BANK_VAL = 32;\n    const int NUM_BANK = (K_GLOBAL / BANK_VAL);\n    /////////// BLOCK_ROW_TILES <= N_TILES ////////////\n    // const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    // ////////// BLOCK_COL_TILES <= M_TILES ////////////\n    // const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    auto C_LAYOUT = wmma::mem_col_major;\n    const int CHUNK_K = 64;\n    const int CHUNK_K_SPARSE = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K_SPARSE = (int((CHUNK_K * K) * (1 - SPARSITY)));\n    const int WARP_COPY_BYTES = (WARP_SIZE * sizeof(int4));\n    const int CHUNK_LINE_BYTES_A (BLOCK_COL_TILES * M * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_A = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_A);\n    const int CHUNK_COPY_LINE_LANES_A = (CHUNK_LINE_BYTES_A / sizeof(int4));\n    const int SHARED_OFFSET_A = (BLOCK_COL_TILES * M + SKEW_UINT8);\n\n    const int CHUNK_LINE_BYTES_B = (BLOCK_SIZE_K_SPARSE * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_B = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_B);\n    const int CHUNK_COPY_LINE_LANES_B = (CHUNK_LINE_BYTES_B / sizeof(int4));\n    const int SHARED_OFFSET_B = (BLOCK_SIZE_K_SPARSE + SKEW_UINT8);\n    const int SHARED_TO_GLOBAL_BYTES_PER_LINE = ((WARP_COL_TILES * M) * sizeof(int));\n    const int SHARED_TO_GLOBAL_BYTES_PER_WARP = (WARP_SIZE * sizeof(int));\n    const int SHARED_TO_GLOBAL_LINES_PER_WARP = (SHARED_TO_GLOBAL_BYTES_PER_WARP / SHARED_TO_GLOBAL_BYTES_PER_LINE);\n    const int SHARED_TO_GLOBAL_LANES_PER_LINE = (WARP_SIZE / SHARED_TO_GLOBAL_LINES_PER_WARP);\n    const int SHARED_TO_GLOBAL_ITERS = ((WARP_ROW_TILES * N) / SHARED_TO_GLOBAL_LINES_PER_WARP);\n\n    const int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const int WARP_STRIDE = (WARP_COL_TILES * M);\n    const int WARPS_PER_BLOCK = (BLOCK_ROW_WARPS * BLOCK_COL_WARPS);\n    const int THREADS_PER_BLOCK = (WARP_SIZE * WARPS_PER_BLOCK);\n    const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    const int GLOBAL_MEM_STRIDE = M_GLOBAL;\n    const int SHMEM_STRIDE = (M * BLOCK_COL_TILES);\n    const int SHMEM_OFFSET = (M * WARP_COL_TILES);\n    const int BLOCK_SIZE_M = (M * BLOCK_COL_TILES);\n    const int BLOCK_SIZE_N = (N * BLOCK_ROW_TILES);\n\n   extern __shared__ uint8_t shmem[];\n\n\t// Warp and lane identification.\n\tconst unsigned int warpId = threadIdx.x / WARP_SIZE;\n\tconst unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n\n\t// Offset in shared memory from which the B matrix is stored.\n\t// const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\tconst size_t shmem_idx_b_off = BLOCK_SIZE_K_SPARSE * SHARED_OFFSET_A;\n\n\n\t// Each CTA slides along the 128 x 128 tiles from the top left corner of the\n\t// matrix to the right and down, and selects the next tile to compute. Once\n\t// there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n\tconst unsigned int block_tile_i =\n\t\t((block_pos * BLOCK_COL_TILES) / M_TILES) * (BLOCK_ROW_TILES);\n\tconst unsigned int block_tile_j = (block_pos * BLOCK_COL_TILES) % M_TILES;\n\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n\twmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_ROW_TILES]\n\t\t\t\t\t\t\t\t\t\t\t\t\t [WARP_COL_TILES];\n\n    // Load the C matrix tiles into fragments from shared memory.\n\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t\twmma::fill_fragment(c[i][j], 0);\n\t\t}\n\t}\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    //for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n    for(int tile_k_idx_sparse = 0, tile_k_idx = 0; tile_k_idx_sparse < K_GLOBAL_SPARSE; tile_k_idx_sparse += BLOCK_SIZE_K_SPARSE, tile_k_idx += BLOCK_SIZE_K){\n\n\t\tsize_t shmem_idx = \n\t\twarpId < (WARPS_PER_BLOCK / 2)\n\t\t\t? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A\n\t\t\t: (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B + shmem_idx_b_off;\n\n\t\tint4 *lane_ptr = NULL;\n\t\tint *lane_ptr_index = NULL;\n\t\tconst uint8_t *warp_ptr = NULL;\n\n\n\t\tif(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\t//warp_ptr = &A[block_tile_j * M] +\n\t\t\t//\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * M_GLOBAL;\n\t\t\twarp_ptr = &A[block_tile_j * M];\n\t\t\t\n\t\t\tconst int *warp_ptr_index = &B_index[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t\t\t\t\t\t((warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A);\n\n\t\t\tlane_ptr_index = (int *)(warp_ptr_index + tile_k_idx_sparse + (laneId / CHUNK_COPY_LINE_LANES_A));\n\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_A) * SHARED_OFFSET_A;\n\t\t}else{\n\t\t\twarp_ptr = &B[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * K_GLOBAL_SPARSE;\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_sparse +\n\t\t\t\t\t\t\t\t(laneId / CHUNK_COPY_LINE_LANES_B) * K_GLOBAL_SPARSE) +\n\t\t\t\t\t\t\t\t(laneId % CHUNK_COPY_LINE_LANES_B);\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_B) * SHARED_OFFSET_B;\n\t\t}\n\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      // shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n\t  int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n\t  \t? BLOCK_SIZE_K_SPARSE / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A)\n\t\t: BLOCK_SIZE_N / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\n\t  /*\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\t  */\n\n\t  /*\n      int tile_k_idx_A;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n          tile_k_idx_A = *(lane_ptr_index);\n      }\n\t  */\n\n\t  #pragma unroll\n\t  for(int i = 0; i < iter_index; i += 1){\n\t\t  if(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\tint tile_k_idx_A = *(lane_ptr_index);\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_A * M_GLOBAL) + (laneId % CHUNK_COPY_LINE_LANES_A);\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_A)) =\n\t\t\t\t*lane_ptr;\n\t\t\t//warp_ptr = (uint8_t *)((uint8_t *)warp_ptr + M_GLOBAL * (WARPS_PER_BLOCK / 2) *CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tlane_ptr_index = (int *)((int *)lane_ptr_index +  (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A;\n\t\t  }else{\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_B)) =\n\t\t\t\t*lane_ptr;\n\t\t\tlane_ptr = (int4 *)((uint8_t *)lane_ptr + K_GLOBAL_SPARSE * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B;\n\t\t  }\n\t  }\n\n      __syncthreads();\n\n\t#pragma unroll\n      for (int k_step = 0; k_step < CHUNK_K_SPARSE; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::col_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n\t#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i += 1) {\n\t\t\tsize_t shmem_idx_a = (warpId % BLOCK_COL_WARPS) * M * WARP_COL_TILES + (i * M);\n\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_a + k_step * K * SHARED_OFFSET_A;\n\n\t\t\twmma::load_matrix_sync(a[i], tile_ptr, SHARED_OFFSET_A);\n\t\t#pragma unroll\n\t\t\tfor(int j = 0; j < WARP_ROW_TILES; j += 1){\n\t\t\t\tif(i == 0){\n\t\t\t\t\tsize_t shmem_idx_b = shmem_idx_b_off +\n\t\t\t\t\t\t\t\t\t\t\t(warpId / BLOCK_COL_WARPS) * (WARP_ROW_TILES * N) * SHARED_OFFSET_B +\n\t\t\t\t\t\t\t\t\t\t\t(j * N) * SHARED_OFFSET_B;\n\t\t\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_b + k_step * K;\n\t\t\t\t\twmma::load_matrix_sync(b[j], tile_ptr, SHARED_OFFSET_B);\n\t\t\t\t}\n\t\t\t\twmma::mma_sync(c[j][i], a[i], b[j], c[j][i]);\n\t\t\t}\n\n        }\n      }\n\n      __syncthreads();\n    }\n\n    // This pointer is used to access the C and D matrix tiles this warp computes.\n\tint *shmem_warp_tile_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * N * WARP_ROW_TILES * SHMEM_STRIDE +\n\t(warpId % BLOCK_COL_WARPS) * SHMEM_OFFSET;\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t#pragma unroll\n\t\t\tfor(int t = 0; t < c[i][j].num_elements; t += 1){\n\t\t\t\tc[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n\t\t\t}\n\t\t\tint *tile_ptr = shmem_warp_tile_ptr + i * N * SHMEM_STRIDE + j * M;\n\t\t\twmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, C_LAYOUT);\n\t\t}\n\t}\n\n    __syncthreads();\n\n\tint *shmem_warp_stream_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N * SHMEM_STRIDE\n\t\t\t\t\t\t\t\t\t+ (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tconst size_t gmem_idx =\n\t\t(block_tile_i * N + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N) * GLOBAL_MEM_STRIDE +\n\t\tblock_tile_j * M + (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tuint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n\tint *shmem_lane_stream_ptr =\n\t\tshmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * SHMEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\t\n\tuint8_t *dst_gmem_lane_stream_ptr =\n\t\tdst_gmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * GLOBAL_MEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\n\tfor(int i = 0; i < WARP_ROW_TILES * N; i += SHARED_TO_GLOBAL_LINES_PER_WARP){\n\t\t*(dst_gmem_lane_stream_ptr + i * GLOBAL_MEM_STRIDE) = (uint8_t)(*(shmem_lane_stream_ptr + i * SHMEM_STRIDE));\n\t}\n\n\t__syncthreads();\n}\n", "launch_config": {"dimBlock": [256, 1], "dimGrid": [256, 1]}}, "79": {"code": "\n__global__ void compute_gemm_imma(float *input0, float *input1, float *input2, float* input3, float* input4, float*input5, float* input6,\n                                  float *output0) {\n    // keys need to be replaced 4096, 1024, K_GLOBA_VALUEL, 0.75, 64\n\n\t//extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B = reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_index = reinterpret_cast<int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n\tconst int WARP_SIZE = 32;\n    const int M = 16;\n    const int K = 16;\n    const int N = 16;\n    const int WMMA_M = 16;\n    const int WMMA_K = 16;\n    const int WMMA_N = 16;\n    const int M_GLOBAL = 4096;\n    const int K_GLOBAL = 1024;\n    const int N_GLOBAL = 1024;\n    const float SPARSITY = 0.75;\n    const int K_GLOBAL_SPARSE = (int(K_GLOBAL * (1-SPARSITY)));\n    const int M_TILES = (M_GLOBAL / M);\n    const int N_TILES = (N_GLOBAL / N);\n    const int K_TILES = (K_GLOBAL / K);\n    const int BLOCK_ROW_WARPS = 2;\n    const int BLOCK_COL_WARPS = 4;\n    const int WARP_ROW_TILES = 4;\n    const int WARP_COL_TILES = 2;\n    const int SKEW_UINT8 = 16;\n    const int BANK_VAL = 32;\n    const int NUM_BANK = (K_GLOBAL / BANK_VAL);\n    /////////// BLOCK_ROW_TILES <= N_TILES ////////////\n    // const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    // ////////// BLOCK_COL_TILES <= M_TILES ////////////\n    // const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    auto C_LAYOUT = wmma::mem_col_major;\n    const int CHUNK_K = 64;\n    const int CHUNK_K_SPARSE = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K_SPARSE = (int((CHUNK_K * K) * (1 - SPARSITY)));\n    const int WARP_COPY_BYTES = (WARP_SIZE * sizeof(int4));\n    const int CHUNK_LINE_BYTES_A (BLOCK_COL_TILES * M * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_A = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_A);\n    const int CHUNK_COPY_LINE_LANES_A = (CHUNK_LINE_BYTES_A / sizeof(int4));\n    const int SHARED_OFFSET_A = (BLOCK_COL_TILES * M + SKEW_UINT8);\n\n    const int CHUNK_LINE_BYTES_B = (BLOCK_SIZE_K_SPARSE * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_B = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_B);\n    const int CHUNK_COPY_LINE_LANES_B = (CHUNK_LINE_BYTES_B / sizeof(int4));\n    const int SHARED_OFFSET_B = (BLOCK_SIZE_K_SPARSE + SKEW_UINT8);\n    const int SHARED_TO_GLOBAL_BYTES_PER_LINE = ((WARP_COL_TILES * M) * sizeof(int));\n    const int SHARED_TO_GLOBAL_BYTES_PER_WARP = (WARP_SIZE * sizeof(int));\n    const int SHARED_TO_GLOBAL_LINES_PER_WARP = (SHARED_TO_GLOBAL_BYTES_PER_WARP / SHARED_TO_GLOBAL_BYTES_PER_LINE);\n    const int SHARED_TO_GLOBAL_LANES_PER_LINE = (WARP_SIZE / SHARED_TO_GLOBAL_LINES_PER_WARP);\n    const int SHARED_TO_GLOBAL_ITERS = ((WARP_ROW_TILES * N) / SHARED_TO_GLOBAL_LINES_PER_WARP);\n\n    const int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const int WARP_STRIDE = (WARP_COL_TILES * M);\n    const int WARPS_PER_BLOCK = (BLOCK_ROW_WARPS * BLOCK_COL_WARPS);\n    const int THREADS_PER_BLOCK = (WARP_SIZE * WARPS_PER_BLOCK);\n    const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    const int GLOBAL_MEM_STRIDE = M_GLOBAL;\n    const int SHMEM_STRIDE = (M * BLOCK_COL_TILES);\n    const int SHMEM_OFFSET = (M * WARP_COL_TILES);\n    const int BLOCK_SIZE_M = (M * BLOCK_COL_TILES);\n    const int BLOCK_SIZE_N = (N * BLOCK_ROW_TILES);\n\n   extern __shared__ uint8_t shmem[];\n\n\t// Warp and lane identification.\n\tconst unsigned int warpId = threadIdx.x / WARP_SIZE;\n\tconst unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n\n\t// Offset in shared memory from which the B matrix is stored.\n\t// const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\tconst size_t shmem_idx_b_off = BLOCK_SIZE_K_SPARSE * SHARED_OFFSET_A;\n\n\n\t// Each CTA slides along the 128 x 128 tiles from the top left corner of the\n\t// matrix to the right and down, and selects the next tile to compute. Once\n\t// there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n\tconst unsigned int block_tile_i =\n\t\t((block_pos * BLOCK_COL_TILES) / M_TILES) * (BLOCK_ROW_TILES);\n\tconst unsigned int block_tile_j = (block_pos * BLOCK_COL_TILES) % M_TILES;\n\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n\twmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_ROW_TILES]\n\t\t\t\t\t\t\t\t\t\t\t\t\t [WARP_COL_TILES];\n\n    // Load the C matrix tiles into fragments from shared memory.\n\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t\twmma::fill_fragment(c[i][j], 0);\n\t\t}\n\t}\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    //for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n    for(int tile_k_idx_sparse = 0, tile_k_idx = 0; tile_k_idx_sparse < K_GLOBAL_SPARSE; tile_k_idx_sparse += BLOCK_SIZE_K_SPARSE, tile_k_idx += BLOCK_SIZE_K){\n\n\t\tsize_t shmem_idx = \n\t\twarpId < (WARPS_PER_BLOCK / 2)\n\t\t\t? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A\n\t\t\t: (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B + shmem_idx_b_off;\n\n\t\tint4 *lane_ptr = NULL;\n\t\tint *lane_ptr_index = NULL;\n\t\tconst uint8_t *warp_ptr = NULL;\n\n\n\t\tif(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\t//warp_ptr = &A[block_tile_j * M] +\n\t\t\t//\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * M_GLOBAL;\n\t\t\twarp_ptr = &A[block_tile_j * M];\n\t\t\t\n\t\t\tconst int *warp_ptr_index = &B_index[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t\t\t\t\t\t((warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A);\n\n\t\t\tlane_ptr_index = (int *)(warp_ptr_index + tile_k_idx_sparse + (laneId / CHUNK_COPY_LINE_LANES_A));\n\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_A) * SHARED_OFFSET_A;\n\t\t}else{\n\t\t\twarp_ptr = &B[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * K_GLOBAL_SPARSE;\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_sparse +\n\t\t\t\t\t\t\t\t(laneId / CHUNK_COPY_LINE_LANES_B) * K_GLOBAL_SPARSE) +\n\t\t\t\t\t\t\t\t(laneId % CHUNK_COPY_LINE_LANES_B);\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_B) * SHARED_OFFSET_B;\n\t\t}\n\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      // shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n\t  int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n\t  \t? BLOCK_SIZE_K_SPARSE / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A)\n\t\t: BLOCK_SIZE_N / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\n\t  /*\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\t  */\n\n\t  /*\n      int tile_k_idx_A;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n          tile_k_idx_A = *(lane_ptr_index);\n      }\n\t  */\n\n\t  #pragma unroll\n\t  for(int i = 0; i < iter_index; i += 1){\n\t\t  if(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\tint tile_k_idx_A = *(lane_ptr_index);\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_A * M_GLOBAL) + (laneId % CHUNK_COPY_LINE_LANES_A);\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_A)) =\n\t\t\t\t*lane_ptr;\n\t\t\t//warp_ptr = (uint8_t *)((uint8_t *)warp_ptr + M_GLOBAL * (WARPS_PER_BLOCK / 2) *CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tlane_ptr_index = (int *)((int *)lane_ptr_index +  (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A;\n\t\t  }else{\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_B)) =\n\t\t\t\t*lane_ptr;\n\t\t\tlane_ptr = (int4 *)((uint8_t *)lane_ptr + K_GLOBAL_SPARSE * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B;\n\t\t  }\n\t  }\n\n      __syncthreads();\n\n\t#pragma unroll\n      for (int k_step = 0; k_step < CHUNK_K_SPARSE; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::col_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n\t#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i += 1) {\n\t\t\tsize_t shmem_idx_a = (warpId % BLOCK_COL_WARPS) * M * WARP_COL_TILES + (i * M);\n\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_a + k_step * K * SHARED_OFFSET_A;\n\n\t\t\twmma::load_matrix_sync(a[i], tile_ptr, SHARED_OFFSET_A);\n\t\t#pragma unroll\n\t\t\tfor(int j = 0; j < WARP_ROW_TILES; j += 1){\n\t\t\t\tif(i == 0){\n\t\t\t\t\tsize_t shmem_idx_b = shmem_idx_b_off +\n\t\t\t\t\t\t\t\t\t\t\t(warpId / BLOCK_COL_WARPS) * (WARP_ROW_TILES * N) * SHARED_OFFSET_B +\n\t\t\t\t\t\t\t\t\t\t\t(j * N) * SHARED_OFFSET_B;\n\t\t\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_b + k_step * K;\n\t\t\t\t\twmma::load_matrix_sync(b[j], tile_ptr, SHARED_OFFSET_B);\n\t\t\t\t}\n\t\t\t\twmma::mma_sync(c[j][i], a[i], b[j], c[j][i]);\n\t\t\t}\n\n        }\n      }\n\n      __syncthreads();\n    }\n\n    // This pointer is used to access the C and D matrix tiles this warp computes.\n\tint *shmem_warp_tile_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * N * WARP_ROW_TILES * SHMEM_STRIDE +\n\t(warpId % BLOCK_COL_WARPS) * SHMEM_OFFSET;\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t#pragma unroll\n\t\t\tfor(int t = 0; t < c[i][j].num_elements; t += 1){\n\t\t\t\tc[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n\t\t\t}\n\t\t\tint *tile_ptr = shmem_warp_tile_ptr + i * N * SHMEM_STRIDE + j * M;\n\t\t\twmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, C_LAYOUT);\n\t\t}\n\t}\n\n    __syncthreads();\n\n\tint *shmem_warp_stream_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N * SHMEM_STRIDE\n\t\t\t\t\t\t\t\t\t+ (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tconst size_t gmem_idx =\n\t\t(block_tile_i * N + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N) * GLOBAL_MEM_STRIDE +\n\t\tblock_tile_j * M + (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tuint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n\tint *shmem_lane_stream_ptr =\n\t\tshmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * SHMEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\t\n\tuint8_t *dst_gmem_lane_stream_ptr =\n\t\tdst_gmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * GLOBAL_MEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\n\tfor(int i = 0; i < WARP_ROW_TILES * N; i += SHARED_TO_GLOBAL_LINES_PER_WARP){\n\t\t*(dst_gmem_lane_stream_ptr + i * GLOBAL_MEM_STRIDE) = (uint8_t)(*(shmem_lane_stream_ptr + i * SHMEM_STRIDE));\n\t}\n\n\t__syncthreads();\n}\n", "launch_config": {"dimBlock": [256, 1], "dimGrid": [256, 1]}}, "80": {"code": "\n__global__ void compute_gemm_imma(float *input0, float *input1, float *input2, float* input3, float* input4, float*input5, float* input6,\n                                  float *output0) {\n    // keys need to be replaced 4096, 1024, K_GLOBA_VALUEL, 0.75, 64\n\n\t//extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B = reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_index = reinterpret_cast<int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n\tconst int WARP_SIZE = 32;\n    const int M = 16;\n    const int K = 16;\n    const int N = 16;\n    const int WMMA_M = 16;\n    const int WMMA_K = 16;\n    const int WMMA_N = 16;\n    const int M_GLOBAL = 4096;\n    const int K_GLOBAL = 1024;\n    const int N_GLOBAL = 1024;\n    const float SPARSITY = 0.75;\n    const int K_GLOBAL_SPARSE = (int(K_GLOBAL * (1-SPARSITY)));\n    const int M_TILES = (M_GLOBAL / M);\n    const int N_TILES = (N_GLOBAL / N);\n    const int K_TILES = (K_GLOBAL / K);\n    const int BLOCK_ROW_WARPS = 2;\n    const int BLOCK_COL_WARPS = 4;\n    const int WARP_ROW_TILES = 4;\n    const int WARP_COL_TILES = 2;\n    const int SKEW_UINT8 = 16;\n    const int BANK_VAL = 32;\n    const int NUM_BANK = (K_GLOBAL / BANK_VAL);\n    /////////// BLOCK_ROW_TILES <= N_TILES ////////////\n    // const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    // ////////// BLOCK_COL_TILES <= M_TILES ////////////\n    // const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    auto C_LAYOUT = wmma::mem_col_major;\n    const int CHUNK_K = 64;\n    const int CHUNK_K_SPARSE = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K_SPARSE = (int((CHUNK_K * K) * (1 - SPARSITY)));\n    const int WARP_COPY_BYTES = (WARP_SIZE * sizeof(int4));\n    const int CHUNK_LINE_BYTES_A (BLOCK_COL_TILES * M * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_A = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_A);\n    const int CHUNK_COPY_LINE_LANES_A = (CHUNK_LINE_BYTES_A / sizeof(int4));\n    const int SHARED_OFFSET_A = (BLOCK_COL_TILES * M + SKEW_UINT8);\n\n    const int CHUNK_LINE_BYTES_B = (BLOCK_SIZE_K_SPARSE * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_B = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_B);\n    const int CHUNK_COPY_LINE_LANES_B = (CHUNK_LINE_BYTES_B / sizeof(int4));\n    const int SHARED_OFFSET_B = (BLOCK_SIZE_K_SPARSE + SKEW_UINT8);\n    const int SHARED_TO_GLOBAL_BYTES_PER_LINE = ((WARP_COL_TILES * M) * sizeof(int));\n    const int SHARED_TO_GLOBAL_BYTES_PER_WARP = (WARP_SIZE * sizeof(int));\n    const int SHARED_TO_GLOBAL_LINES_PER_WARP = (SHARED_TO_GLOBAL_BYTES_PER_WARP / SHARED_TO_GLOBAL_BYTES_PER_LINE);\n    const int SHARED_TO_GLOBAL_LANES_PER_LINE = (WARP_SIZE / SHARED_TO_GLOBAL_LINES_PER_WARP);\n    const int SHARED_TO_GLOBAL_ITERS = ((WARP_ROW_TILES * N) / SHARED_TO_GLOBAL_LINES_PER_WARP);\n\n    const int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const int WARP_STRIDE = (WARP_COL_TILES * M);\n    const int WARPS_PER_BLOCK = (BLOCK_ROW_WARPS * BLOCK_COL_WARPS);\n    const int THREADS_PER_BLOCK = (WARP_SIZE * WARPS_PER_BLOCK);\n    const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    const int GLOBAL_MEM_STRIDE = M_GLOBAL;\n    const int SHMEM_STRIDE = (M * BLOCK_COL_TILES);\n    const int SHMEM_OFFSET = (M * WARP_COL_TILES);\n    const int BLOCK_SIZE_M = (M * BLOCK_COL_TILES);\n    const int BLOCK_SIZE_N = (N * BLOCK_ROW_TILES);\n\n   extern __shared__ uint8_t shmem[];\n\n\t// Warp and lane identification.\n\tconst unsigned int warpId = threadIdx.x / WARP_SIZE;\n\tconst unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n\n\t// Offset in shared memory from which the B matrix is stored.\n\t// const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\tconst size_t shmem_idx_b_off = BLOCK_SIZE_K_SPARSE * SHARED_OFFSET_A;\n\n\n\t// Each CTA slides along the 128 x 128 tiles from the top left corner of the\n\t// matrix to the right and down, and selects the next tile to compute. Once\n\t// there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n\tconst unsigned int block_tile_i =\n\t\t((block_pos * BLOCK_COL_TILES) / M_TILES) * (BLOCK_ROW_TILES);\n\tconst unsigned int block_tile_j = (block_pos * BLOCK_COL_TILES) % M_TILES;\n\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n\twmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_ROW_TILES]\n\t\t\t\t\t\t\t\t\t\t\t\t\t [WARP_COL_TILES];\n\n    // Load the C matrix tiles into fragments from shared memory.\n\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t\twmma::fill_fragment(c[i][j], 0);\n\t\t}\n\t}\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    //for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n    for(int tile_k_idx_sparse = 0, tile_k_idx = 0; tile_k_idx_sparse < K_GLOBAL_SPARSE; tile_k_idx_sparse += BLOCK_SIZE_K_SPARSE, tile_k_idx += BLOCK_SIZE_K){\n\n\t\tsize_t shmem_idx = \n\t\twarpId < (WARPS_PER_BLOCK / 2)\n\t\t\t? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A\n\t\t\t: (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B + shmem_idx_b_off;\n\n\t\tint4 *lane_ptr = NULL;\n\t\tint *lane_ptr_index = NULL;\n\t\tconst uint8_t *warp_ptr = NULL;\n\n\n\t\tif(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\t//warp_ptr = &A[block_tile_j * M] +\n\t\t\t//\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * M_GLOBAL;\n\t\t\twarp_ptr = &A[block_tile_j * M];\n\t\t\t\n\t\t\tconst int *warp_ptr_index = &B_index[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t\t\t\t\t\t((warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A);\n\n\t\t\tlane_ptr_index = (int *)(warp_ptr_index + tile_k_idx_sparse + (laneId / CHUNK_COPY_LINE_LANES_A));\n\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_A) * SHARED_OFFSET_A;\n\t\t}else{\n\t\t\twarp_ptr = &B[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * K_GLOBAL_SPARSE;\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_sparse +\n\t\t\t\t\t\t\t\t(laneId / CHUNK_COPY_LINE_LANES_B) * K_GLOBAL_SPARSE) +\n\t\t\t\t\t\t\t\t(laneId % CHUNK_COPY_LINE_LANES_B);\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_B) * SHARED_OFFSET_B;\n\t\t}\n\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      // shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n\t  int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n\t  \t? BLOCK_SIZE_K_SPARSE / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A)\n\t\t: BLOCK_SIZE_N / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\n\t  /*\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\t  */\n\n\t  /*\n      int tile_k_idx_A;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n          tile_k_idx_A = *(lane_ptr_index);\n      }\n\t  */\n\n\t  #pragma unroll\n\t  for(int i = 0; i < iter_index; i += 1){\n\t\t  if(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\tint tile_k_idx_A = *(lane_ptr_index);\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_A * M_GLOBAL) + (laneId % CHUNK_COPY_LINE_LANES_A);\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_A)) =\n\t\t\t\t*lane_ptr;\n\t\t\t//warp_ptr = (uint8_t *)((uint8_t *)warp_ptr + M_GLOBAL * (WARPS_PER_BLOCK / 2) *CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tlane_ptr_index = (int *)((int *)lane_ptr_index +  (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A;\n\t\t  }else{\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_B)) =\n\t\t\t\t*lane_ptr;\n\t\t\tlane_ptr = (int4 *)((uint8_t *)lane_ptr + K_GLOBAL_SPARSE * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B;\n\t\t  }\n\t  }\n\n      __syncthreads();\n\n\t#pragma unroll\n      for (int k_step = 0; k_step < CHUNK_K_SPARSE; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::col_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n\t#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i += 1) {\n\t\t\tsize_t shmem_idx_a = (warpId % BLOCK_COL_WARPS) * M * WARP_COL_TILES + (i * M);\n\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_a + k_step * K * SHARED_OFFSET_A;\n\n\t\t\twmma::load_matrix_sync(a[i], tile_ptr, SHARED_OFFSET_A);\n\t\t#pragma unroll\n\t\t\tfor(int j = 0; j < WARP_ROW_TILES; j += 1){\n\t\t\t\tif(i == 0){\n\t\t\t\t\tsize_t shmem_idx_b = shmem_idx_b_off +\n\t\t\t\t\t\t\t\t\t\t\t(warpId / BLOCK_COL_WARPS) * (WARP_ROW_TILES * N) * SHARED_OFFSET_B +\n\t\t\t\t\t\t\t\t\t\t\t(j * N) * SHARED_OFFSET_B;\n\t\t\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_b + k_step * K;\n\t\t\t\t\twmma::load_matrix_sync(b[j], tile_ptr, SHARED_OFFSET_B);\n\t\t\t\t}\n\t\t\t\twmma::mma_sync(c[j][i], a[i], b[j], c[j][i]);\n\t\t\t}\n\n        }\n      }\n\n      __syncthreads();\n    }\n\n    // This pointer is used to access the C and D matrix tiles this warp computes.\n\tint *shmem_warp_tile_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * N * WARP_ROW_TILES * SHMEM_STRIDE +\n\t(warpId % BLOCK_COL_WARPS) * SHMEM_OFFSET;\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t#pragma unroll\n\t\t\tfor(int t = 0; t < c[i][j].num_elements; t += 1){\n\t\t\t\tc[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n\t\t\t}\n\t\t\tint *tile_ptr = shmem_warp_tile_ptr + i * N * SHMEM_STRIDE + j * M;\n\t\t\twmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, C_LAYOUT);\n\t\t}\n\t}\n\n    __syncthreads();\n\n\tint *shmem_warp_stream_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N * SHMEM_STRIDE\n\t\t\t\t\t\t\t\t\t+ (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tconst size_t gmem_idx =\n\t\t(block_tile_i * N + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N) * GLOBAL_MEM_STRIDE +\n\t\tblock_tile_j * M + (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tuint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n\tint *shmem_lane_stream_ptr =\n\t\tshmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * SHMEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\t\n\tuint8_t *dst_gmem_lane_stream_ptr =\n\t\tdst_gmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * GLOBAL_MEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\n\tfor(int i = 0; i < WARP_ROW_TILES * N; i += SHARED_TO_GLOBAL_LINES_PER_WARP){\n\t\t*(dst_gmem_lane_stream_ptr + i * GLOBAL_MEM_STRIDE) = (uint8_t)(*(shmem_lane_stream_ptr + i * SHMEM_STRIDE));\n\t}\n\n\t__syncthreads();\n}\n", "launch_config": {"dimBlock": [256, 1], "dimGrid": [256, 1]}}, "81": {"code": "\n__global__ void compute_gemm_imma(float *input0, float *input1, float *input2, float* input3, float* input4, float*input5, float* input6,\n                                  float *output0) {\n    // keys need to be replaced 4096, 1024, K_GLOBA_VALUEL, 0.75, 64\n\n\t//extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B = reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_index = reinterpret_cast<int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n\tconst int WARP_SIZE = 32;\n    const int M = 16;\n    const int K = 16;\n    const int N = 16;\n    const int WMMA_M = 16;\n    const int WMMA_K = 16;\n    const int WMMA_N = 16;\n    const int M_GLOBAL = 4096;\n    const int K_GLOBAL = 1024;\n    const int N_GLOBAL = 1024;\n    const float SPARSITY = 0.75;\n    const int K_GLOBAL_SPARSE = (int(K_GLOBAL * (1-SPARSITY)));\n    const int M_TILES = (M_GLOBAL / M);\n    const int N_TILES = (N_GLOBAL / N);\n    const int K_TILES = (K_GLOBAL / K);\n    const int BLOCK_ROW_WARPS = 2;\n    const int BLOCK_COL_WARPS = 4;\n    const int WARP_ROW_TILES = 4;\n    const int WARP_COL_TILES = 2;\n    const int SKEW_UINT8 = 16;\n    const int BANK_VAL = 32;\n    const int NUM_BANK = (K_GLOBAL / BANK_VAL);\n    /////////// BLOCK_ROW_TILES <= N_TILES ////////////\n    // const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    // ////////// BLOCK_COL_TILES <= M_TILES ////////////\n    // const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    auto C_LAYOUT = wmma::mem_col_major;\n    const int CHUNK_K = 64;\n    const int CHUNK_K_SPARSE = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K_SPARSE = (int((CHUNK_K * K) * (1 - SPARSITY)));\n    const int WARP_COPY_BYTES = (WARP_SIZE * sizeof(int4));\n    const int CHUNK_LINE_BYTES_A (BLOCK_COL_TILES * M * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_A = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_A);\n    const int CHUNK_COPY_LINE_LANES_A = (CHUNK_LINE_BYTES_A / sizeof(int4));\n    const int SHARED_OFFSET_A = (BLOCK_COL_TILES * M + SKEW_UINT8);\n\n    const int CHUNK_LINE_BYTES_B = (BLOCK_SIZE_K_SPARSE * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_B = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_B);\n    const int CHUNK_COPY_LINE_LANES_B = (CHUNK_LINE_BYTES_B / sizeof(int4));\n    const int SHARED_OFFSET_B = (BLOCK_SIZE_K_SPARSE + SKEW_UINT8);\n    const int SHARED_TO_GLOBAL_BYTES_PER_LINE = ((WARP_COL_TILES * M) * sizeof(int));\n    const int SHARED_TO_GLOBAL_BYTES_PER_WARP = (WARP_SIZE * sizeof(int));\n    const int SHARED_TO_GLOBAL_LINES_PER_WARP = (SHARED_TO_GLOBAL_BYTES_PER_WARP / SHARED_TO_GLOBAL_BYTES_PER_LINE);\n    const int SHARED_TO_GLOBAL_LANES_PER_LINE = (WARP_SIZE / SHARED_TO_GLOBAL_LINES_PER_WARP);\n    const int SHARED_TO_GLOBAL_ITERS = ((WARP_ROW_TILES * N) / SHARED_TO_GLOBAL_LINES_PER_WARP);\n\n    const int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const int WARP_STRIDE = (WARP_COL_TILES * M);\n    const int WARPS_PER_BLOCK = (BLOCK_ROW_WARPS * BLOCK_COL_WARPS);\n    const int THREADS_PER_BLOCK = (WARP_SIZE * WARPS_PER_BLOCK);\n    const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    const int GLOBAL_MEM_STRIDE = M_GLOBAL;\n    const int SHMEM_STRIDE = (M * BLOCK_COL_TILES);\n    const int SHMEM_OFFSET = (M * WARP_COL_TILES);\n    const int BLOCK_SIZE_M = (M * BLOCK_COL_TILES);\n    const int BLOCK_SIZE_N = (N * BLOCK_ROW_TILES);\n\n   extern __shared__ uint8_t shmem[];\n\n\t// Warp and lane identification.\n\tconst unsigned int warpId = threadIdx.x / WARP_SIZE;\n\tconst unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n\n\t// Offset in shared memory from which the B matrix is stored.\n\t// const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\tconst size_t shmem_idx_b_off = BLOCK_SIZE_K_SPARSE * SHARED_OFFSET_A;\n\n\n\t// Each CTA slides along the 128 x 128 tiles from the top left corner of the\n\t// matrix to the right and down, and selects the next tile to compute. Once\n\t// there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n\tconst unsigned int block_tile_i =\n\t\t((block_pos * BLOCK_COL_TILES) / M_TILES) * (BLOCK_ROW_TILES);\n\tconst unsigned int block_tile_j = (block_pos * BLOCK_COL_TILES) % M_TILES;\n\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n\twmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_ROW_TILES]\n\t\t\t\t\t\t\t\t\t\t\t\t\t [WARP_COL_TILES];\n\n    // Load the C matrix tiles into fragments from shared memory.\n\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t\twmma::fill_fragment(c[i][j], 0);\n\t\t}\n\t}\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    //for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n    for(int tile_k_idx_sparse = 0, tile_k_idx = 0; tile_k_idx_sparse < K_GLOBAL_SPARSE; tile_k_idx_sparse += BLOCK_SIZE_K_SPARSE, tile_k_idx += BLOCK_SIZE_K){\n\n\t\tsize_t shmem_idx = \n\t\twarpId < (WARPS_PER_BLOCK / 2)\n\t\t\t? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A\n\t\t\t: (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B + shmem_idx_b_off;\n\n\t\tint4 *lane_ptr = NULL;\n\t\tint *lane_ptr_index = NULL;\n\t\tconst uint8_t *warp_ptr = NULL;\n\n\n\t\tif(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\t//warp_ptr = &A[block_tile_j * M] +\n\t\t\t//\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * M_GLOBAL;\n\t\t\twarp_ptr = &A[block_tile_j * M];\n\t\t\t\n\t\t\tconst int *warp_ptr_index = &B_index[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t\t\t\t\t\t((warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A);\n\n\t\t\tlane_ptr_index = (int *)(warp_ptr_index + tile_k_idx_sparse + (laneId / CHUNK_COPY_LINE_LANES_A));\n\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_A) * SHARED_OFFSET_A;\n\t\t}else{\n\t\t\twarp_ptr = &B[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * K_GLOBAL_SPARSE;\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_sparse +\n\t\t\t\t\t\t\t\t(laneId / CHUNK_COPY_LINE_LANES_B) * K_GLOBAL_SPARSE) +\n\t\t\t\t\t\t\t\t(laneId % CHUNK_COPY_LINE_LANES_B);\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_B) * SHARED_OFFSET_B;\n\t\t}\n\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      // shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n\t  int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n\t  \t? BLOCK_SIZE_K_SPARSE / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A)\n\t\t: BLOCK_SIZE_N / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\n\t  /*\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\t  */\n\n\t  /*\n      int tile_k_idx_A;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n          tile_k_idx_A = *(lane_ptr_index);\n      }\n\t  */\n\n\t  #pragma unroll\n\t  for(int i = 0; i < iter_index; i += 1){\n\t\t  if(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\tint tile_k_idx_A = *(lane_ptr_index);\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_A * M_GLOBAL) + (laneId % CHUNK_COPY_LINE_LANES_A);\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_A)) =\n\t\t\t\t*lane_ptr;\n\t\t\t//warp_ptr = (uint8_t *)((uint8_t *)warp_ptr + M_GLOBAL * (WARPS_PER_BLOCK / 2) *CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tlane_ptr_index = (int *)((int *)lane_ptr_index +  (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A;\n\t\t  }else{\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_B)) =\n\t\t\t\t*lane_ptr;\n\t\t\tlane_ptr = (int4 *)((uint8_t *)lane_ptr + K_GLOBAL_SPARSE * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B;\n\t\t  }\n\t  }\n\n      __syncthreads();\n\n\t#pragma unroll\n      for (int k_step = 0; k_step < CHUNK_K_SPARSE; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::col_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n\t#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i += 1) {\n\t\t\tsize_t shmem_idx_a = (warpId % BLOCK_COL_WARPS) * M * WARP_COL_TILES + (i * M);\n\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_a + k_step * K * SHARED_OFFSET_A;\n\n\t\t\twmma::load_matrix_sync(a[i], tile_ptr, SHARED_OFFSET_A);\n\t\t#pragma unroll\n\t\t\tfor(int j = 0; j < WARP_ROW_TILES; j += 1){\n\t\t\t\tif(i == 0){\n\t\t\t\t\tsize_t shmem_idx_b = shmem_idx_b_off +\n\t\t\t\t\t\t\t\t\t\t\t(warpId / BLOCK_COL_WARPS) * (WARP_ROW_TILES * N) * SHARED_OFFSET_B +\n\t\t\t\t\t\t\t\t\t\t\t(j * N) * SHARED_OFFSET_B;\n\t\t\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_b + k_step * K;\n\t\t\t\t\twmma::load_matrix_sync(b[j], tile_ptr, SHARED_OFFSET_B);\n\t\t\t\t}\n\t\t\t\twmma::mma_sync(c[j][i], a[i], b[j], c[j][i]);\n\t\t\t}\n\n        }\n      }\n\n      __syncthreads();\n    }\n\n    // This pointer is used to access the C and D matrix tiles this warp computes.\n\tint *shmem_warp_tile_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * N * WARP_ROW_TILES * SHMEM_STRIDE +\n\t(warpId % BLOCK_COL_WARPS) * SHMEM_OFFSET;\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t#pragma unroll\n\t\t\tfor(int t = 0; t < c[i][j].num_elements; t += 1){\n\t\t\t\tc[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n\t\t\t}\n\t\t\tint *tile_ptr = shmem_warp_tile_ptr + i * N * SHMEM_STRIDE + j * M;\n\t\t\twmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, C_LAYOUT);\n\t\t}\n\t}\n\n    __syncthreads();\n\n\tint *shmem_warp_stream_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N * SHMEM_STRIDE\n\t\t\t\t\t\t\t\t\t+ (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tconst size_t gmem_idx =\n\t\t(block_tile_i * N + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N) * GLOBAL_MEM_STRIDE +\n\t\tblock_tile_j * M + (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tuint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n\tint *shmem_lane_stream_ptr =\n\t\tshmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * SHMEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\t\n\tuint8_t *dst_gmem_lane_stream_ptr =\n\t\tdst_gmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * GLOBAL_MEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\n\tfor(int i = 0; i < WARP_ROW_TILES * N; i += SHARED_TO_GLOBAL_LINES_PER_WARP){\n\t\t*(dst_gmem_lane_stream_ptr + i * GLOBAL_MEM_STRIDE) = (uint8_t)(*(shmem_lane_stream_ptr + i * SHMEM_STRIDE));\n\t}\n\n\t__syncthreads();\n}\n", "launch_config": {"dimBlock": [256, 1], "dimGrid": [256, 1]}}, "82": {"code": "\n__global__ void compute_gemm_imma(float *input0, float *input1, float *input2, float* input3, float* input4, float*input5, float* input6,\n                                  float *output0) {\n    // keys need to be replaced 4096, 1024, K_GLOBA_VALUEL, 0.75, 64\n\n\t//extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B = reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_index = reinterpret_cast<int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n\tconst int WARP_SIZE = 32;\n    const int M = 16;\n    const int K = 16;\n    const int N = 16;\n    const int WMMA_M = 16;\n    const int WMMA_K = 16;\n    const int WMMA_N = 16;\n    const int M_GLOBAL = 4096;\n    const int K_GLOBAL = 1024;\n    const int N_GLOBAL = 1024;\n    const float SPARSITY = 0.75;\n    const int K_GLOBAL_SPARSE = (int(K_GLOBAL * (1-SPARSITY)));\n    const int M_TILES = (M_GLOBAL / M);\n    const int N_TILES = (N_GLOBAL / N);\n    const int K_TILES = (K_GLOBAL / K);\n    const int BLOCK_ROW_WARPS = 2;\n    const int BLOCK_COL_WARPS = 4;\n    const int WARP_ROW_TILES = 4;\n    const int WARP_COL_TILES = 2;\n    const int SKEW_UINT8 = 16;\n    const int BANK_VAL = 32;\n    const int NUM_BANK = (K_GLOBAL / BANK_VAL);\n    /////////// BLOCK_ROW_TILES <= N_TILES ////////////\n    // const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    // ////////// BLOCK_COL_TILES <= M_TILES ////////////\n    // const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    auto C_LAYOUT = wmma::mem_col_major;\n    const int CHUNK_K = 64;\n    const int CHUNK_K_SPARSE = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K_SPARSE = (int((CHUNK_K * K) * (1 - SPARSITY)));\n    const int WARP_COPY_BYTES = (WARP_SIZE * sizeof(int4));\n    const int CHUNK_LINE_BYTES_A (BLOCK_COL_TILES * M * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_A = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_A);\n    const int CHUNK_COPY_LINE_LANES_A = (CHUNK_LINE_BYTES_A / sizeof(int4));\n    const int SHARED_OFFSET_A = (BLOCK_COL_TILES * M + SKEW_UINT8);\n\n    const int CHUNK_LINE_BYTES_B = (BLOCK_SIZE_K_SPARSE * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_B = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_B);\n    const int CHUNK_COPY_LINE_LANES_B = (CHUNK_LINE_BYTES_B / sizeof(int4));\n    const int SHARED_OFFSET_B = (BLOCK_SIZE_K_SPARSE + SKEW_UINT8);\n    const int SHARED_TO_GLOBAL_BYTES_PER_LINE = ((WARP_COL_TILES * M) * sizeof(int));\n    const int SHARED_TO_GLOBAL_BYTES_PER_WARP = (WARP_SIZE * sizeof(int));\n    const int SHARED_TO_GLOBAL_LINES_PER_WARP = (SHARED_TO_GLOBAL_BYTES_PER_WARP / SHARED_TO_GLOBAL_BYTES_PER_LINE);\n    const int SHARED_TO_GLOBAL_LANES_PER_LINE = (WARP_SIZE / SHARED_TO_GLOBAL_LINES_PER_WARP);\n    const int SHARED_TO_GLOBAL_ITERS = ((WARP_ROW_TILES * N) / SHARED_TO_GLOBAL_LINES_PER_WARP);\n\n    const int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const int WARP_STRIDE = (WARP_COL_TILES * M);\n    const int WARPS_PER_BLOCK = (BLOCK_ROW_WARPS * BLOCK_COL_WARPS);\n    const int THREADS_PER_BLOCK = (WARP_SIZE * WARPS_PER_BLOCK);\n    const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    const int GLOBAL_MEM_STRIDE = M_GLOBAL;\n    const int SHMEM_STRIDE = (M * BLOCK_COL_TILES);\n    const int SHMEM_OFFSET = (M * WARP_COL_TILES);\n    const int BLOCK_SIZE_M = (M * BLOCK_COL_TILES);\n    const int BLOCK_SIZE_N = (N * BLOCK_ROW_TILES);\n\n   extern __shared__ uint8_t shmem[];\n\n\t// Warp and lane identification.\n\tconst unsigned int warpId = threadIdx.x / WARP_SIZE;\n\tconst unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n\n\t// Offset in shared memory from which the B matrix is stored.\n\t// const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\tconst size_t shmem_idx_b_off = BLOCK_SIZE_K_SPARSE * SHARED_OFFSET_A;\n\n\n\t// Each CTA slides along the 128 x 128 tiles from the top left corner of the\n\t// matrix to the right and down, and selects the next tile to compute. Once\n\t// there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n\tconst unsigned int block_tile_i =\n\t\t((block_pos * BLOCK_COL_TILES) / M_TILES) * (BLOCK_ROW_TILES);\n\tconst unsigned int block_tile_j = (block_pos * BLOCK_COL_TILES) % M_TILES;\n\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n\twmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_ROW_TILES]\n\t\t\t\t\t\t\t\t\t\t\t\t\t [WARP_COL_TILES];\n\n    // Load the C matrix tiles into fragments from shared memory.\n\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t\twmma::fill_fragment(c[i][j], 0);\n\t\t}\n\t}\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    //for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n    for(int tile_k_idx_sparse = 0, tile_k_idx = 0; tile_k_idx_sparse < K_GLOBAL_SPARSE; tile_k_idx_sparse += BLOCK_SIZE_K_SPARSE, tile_k_idx += BLOCK_SIZE_K){\n\n\t\tsize_t shmem_idx = \n\t\twarpId < (WARPS_PER_BLOCK / 2)\n\t\t\t? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A\n\t\t\t: (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B + shmem_idx_b_off;\n\n\t\tint4 *lane_ptr = NULL;\n\t\tint *lane_ptr_index = NULL;\n\t\tconst uint8_t *warp_ptr = NULL;\n\n\n\t\tif(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\t//warp_ptr = &A[block_tile_j * M] +\n\t\t\t//\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * M_GLOBAL;\n\t\t\twarp_ptr = &A[block_tile_j * M];\n\t\t\t\n\t\t\tconst int *warp_ptr_index = &B_index[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t\t\t\t\t\t((warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A);\n\n\t\t\tlane_ptr_index = (int *)(warp_ptr_index + tile_k_idx_sparse + (laneId / CHUNK_COPY_LINE_LANES_A));\n\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_A) * SHARED_OFFSET_A;\n\t\t}else{\n\t\t\twarp_ptr = &B[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * K_GLOBAL_SPARSE;\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_sparse +\n\t\t\t\t\t\t\t\t(laneId / CHUNK_COPY_LINE_LANES_B) * K_GLOBAL_SPARSE) +\n\t\t\t\t\t\t\t\t(laneId % CHUNK_COPY_LINE_LANES_B);\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_B) * SHARED_OFFSET_B;\n\t\t}\n\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      // shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n\t  int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n\t  \t? BLOCK_SIZE_K_SPARSE / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A)\n\t\t: BLOCK_SIZE_N / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\n\t  /*\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\t  */\n\n\t  /*\n      int tile_k_idx_A;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n          tile_k_idx_A = *(lane_ptr_index);\n      }\n\t  */\n\n\t  #pragma unroll\n\t  for(int i = 0; i < iter_index; i += 1){\n\t\t  if(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\tint tile_k_idx_A = *(lane_ptr_index);\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_A * M_GLOBAL) + (laneId % CHUNK_COPY_LINE_LANES_A);\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_A)) =\n\t\t\t\t*lane_ptr;\n\t\t\t//warp_ptr = (uint8_t *)((uint8_t *)warp_ptr + M_GLOBAL * (WARPS_PER_BLOCK / 2) *CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tlane_ptr_index = (int *)((int *)lane_ptr_index +  (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A;\n\t\t  }else{\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_B)) =\n\t\t\t\t*lane_ptr;\n\t\t\tlane_ptr = (int4 *)((uint8_t *)lane_ptr + K_GLOBAL_SPARSE * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B;\n\t\t  }\n\t  }\n\n      __syncthreads();\n\n\t#pragma unroll\n      for (int k_step = 0; k_step < CHUNK_K_SPARSE; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::col_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n\t#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i += 1) {\n\t\t\tsize_t shmem_idx_a = (warpId % BLOCK_COL_WARPS) * M * WARP_COL_TILES + (i * M);\n\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_a + k_step * K * SHARED_OFFSET_A;\n\n\t\t\twmma::load_matrix_sync(a[i], tile_ptr, SHARED_OFFSET_A);\n\t\t#pragma unroll\n\t\t\tfor(int j = 0; j < WARP_ROW_TILES; j += 1){\n\t\t\t\tif(i == 0){\n\t\t\t\t\tsize_t shmem_idx_b = shmem_idx_b_off +\n\t\t\t\t\t\t\t\t\t\t\t(warpId / BLOCK_COL_WARPS) * (WARP_ROW_TILES * N) * SHARED_OFFSET_B +\n\t\t\t\t\t\t\t\t\t\t\t(j * N) * SHARED_OFFSET_B;\n\t\t\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_b + k_step * K;\n\t\t\t\t\twmma::load_matrix_sync(b[j], tile_ptr, SHARED_OFFSET_B);\n\t\t\t\t}\n\t\t\t\twmma::mma_sync(c[j][i], a[i], b[j], c[j][i]);\n\t\t\t}\n\n        }\n      }\n\n      __syncthreads();\n    }\n\n    // This pointer is used to access the C and D matrix tiles this warp computes.\n\tint *shmem_warp_tile_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * N * WARP_ROW_TILES * SHMEM_STRIDE +\n\t(warpId % BLOCK_COL_WARPS) * SHMEM_OFFSET;\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t#pragma unroll\n\t\t\tfor(int t = 0; t < c[i][j].num_elements; t += 1){\n\t\t\t\tc[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n\t\t\t}\n\t\t\tint *tile_ptr = shmem_warp_tile_ptr + i * N * SHMEM_STRIDE + j * M;\n\t\t\twmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, C_LAYOUT);\n\t\t}\n\t}\n\n    __syncthreads();\n\n\tint *shmem_warp_stream_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N * SHMEM_STRIDE\n\t\t\t\t\t\t\t\t\t+ (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tconst size_t gmem_idx =\n\t\t(block_tile_i * N + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N) * GLOBAL_MEM_STRIDE +\n\t\tblock_tile_j * M + (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tuint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n\tint *shmem_lane_stream_ptr =\n\t\tshmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * SHMEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\t\n\tuint8_t *dst_gmem_lane_stream_ptr =\n\t\tdst_gmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * GLOBAL_MEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\n\tfor(int i = 0; i < WARP_ROW_TILES * N; i += SHARED_TO_GLOBAL_LINES_PER_WARP){\n\t\t*(dst_gmem_lane_stream_ptr + i * GLOBAL_MEM_STRIDE) = (uint8_t)(*(shmem_lane_stream_ptr + i * SHMEM_STRIDE));\n\t}\n\n\t__syncthreads();\n}\n", "launch_config": {"dimBlock": [256, 1], "dimGrid": [256, 1]}}, "83": {"code": "\n__global__ void compute_gemm_imma(float *input0, float *input1, float *input2, float* input3, float* input4, float*input5, float* input6,\n                                  float *output0) {\n    // keys need to be replaced 4096, 4096, K_GLOBA_VALUEL, 0.75, 64\n\n\t//extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B = reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_index = reinterpret_cast<int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n\tconst int WARP_SIZE = 32;\n    const int M = 16;\n    const int K = 16;\n    const int N = 16;\n    const int WMMA_M = 16;\n    const int WMMA_K = 16;\n    const int WMMA_N = 16;\n    const int M_GLOBAL = 4096;\n    const int K_GLOBAL = 1024;\n    const int N_GLOBAL = 4096;\n    const float SPARSITY = 0.75;\n    const int K_GLOBAL_SPARSE = (int(K_GLOBAL * (1-SPARSITY)));\n    const int M_TILES = (M_GLOBAL / M);\n    const int N_TILES = (N_GLOBAL / N);\n    const int K_TILES = (K_GLOBAL / K);\n    const int BLOCK_ROW_WARPS = 2;\n    const int BLOCK_COL_WARPS = 4;\n    const int WARP_ROW_TILES = 4;\n    const int WARP_COL_TILES = 2;\n    const int SKEW_UINT8 = 16;\n    const int BANK_VAL = 32;\n    const int NUM_BANK = (K_GLOBAL / BANK_VAL);\n    /////////// BLOCK_ROW_TILES <= N_TILES ////////////\n    // const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    // ////////// BLOCK_COL_TILES <= M_TILES ////////////\n    // const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    auto C_LAYOUT = wmma::mem_col_major;\n    const int CHUNK_K = 64;\n    const int CHUNK_K_SPARSE = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K_SPARSE = (int((CHUNK_K * K) * (1 - SPARSITY)));\n    const int WARP_COPY_BYTES = (WARP_SIZE * sizeof(int4));\n    const int CHUNK_LINE_BYTES_A (BLOCK_COL_TILES * M * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_A = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_A);\n    const int CHUNK_COPY_LINE_LANES_A = (CHUNK_LINE_BYTES_A / sizeof(int4));\n    const int SHARED_OFFSET_A = (BLOCK_COL_TILES * M + SKEW_UINT8);\n\n    const int CHUNK_LINE_BYTES_B = (BLOCK_SIZE_K_SPARSE * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_B = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_B);\n    const int CHUNK_COPY_LINE_LANES_B = (CHUNK_LINE_BYTES_B / sizeof(int4));\n    const int SHARED_OFFSET_B = (BLOCK_SIZE_K_SPARSE + SKEW_UINT8);\n    const int SHARED_TO_GLOBAL_BYTES_PER_LINE = ((WARP_COL_TILES * M) * sizeof(int));\n    const int SHARED_TO_GLOBAL_BYTES_PER_WARP = (WARP_SIZE * sizeof(int));\n    const int SHARED_TO_GLOBAL_LINES_PER_WARP = (SHARED_TO_GLOBAL_BYTES_PER_WARP / SHARED_TO_GLOBAL_BYTES_PER_LINE);\n    const int SHARED_TO_GLOBAL_LANES_PER_LINE = (WARP_SIZE / SHARED_TO_GLOBAL_LINES_PER_WARP);\n    const int SHARED_TO_GLOBAL_ITERS = ((WARP_ROW_TILES * N) / SHARED_TO_GLOBAL_LINES_PER_WARP);\n\n    const int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const int WARP_STRIDE = (WARP_COL_TILES * M);\n    const int WARPS_PER_BLOCK = (BLOCK_ROW_WARPS * BLOCK_COL_WARPS);\n    const int THREADS_PER_BLOCK = (WARP_SIZE * WARPS_PER_BLOCK);\n    const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    const int GLOBAL_MEM_STRIDE = M_GLOBAL;\n    const int SHMEM_STRIDE = (M * BLOCK_COL_TILES);\n    const int SHMEM_OFFSET = (M * WARP_COL_TILES);\n    const int BLOCK_SIZE_M = (M * BLOCK_COL_TILES);\n    const int BLOCK_SIZE_N = (N * BLOCK_ROW_TILES);\n\n   extern __shared__ uint8_t shmem[];\n\n\t// Warp and lane identification.\n\tconst unsigned int warpId = threadIdx.x / WARP_SIZE;\n\tconst unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n\n\t// Offset in shared memory from which the B matrix is stored.\n\t// const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\tconst size_t shmem_idx_b_off = BLOCK_SIZE_K_SPARSE * SHARED_OFFSET_A;\n\n\n\t// Each CTA slides along the 128 x 128 tiles from the top left corner of the\n\t// matrix to the right and down, and selects the next tile to compute. Once\n\t// there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n\tconst unsigned int block_tile_i =\n\t\t((block_pos * BLOCK_COL_TILES) / M_TILES) * (BLOCK_ROW_TILES);\n\tconst unsigned int block_tile_j = (block_pos * BLOCK_COL_TILES) % M_TILES;\n\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n\twmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_ROW_TILES]\n\t\t\t\t\t\t\t\t\t\t\t\t\t [WARP_COL_TILES];\n\n    // Load the C matrix tiles into fragments from shared memory.\n\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t\twmma::fill_fragment(c[i][j], 0);\n\t\t}\n\t}\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    //for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n    for(int tile_k_idx_sparse = 0, tile_k_idx = 0; tile_k_idx_sparse < K_GLOBAL_SPARSE; tile_k_idx_sparse += BLOCK_SIZE_K_SPARSE, tile_k_idx += BLOCK_SIZE_K){\n\n\t\tsize_t shmem_idx = \n\t\twarpId < (WARPS_PER_BLOCK / 2)\n\t\t\t? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A\n\t\t\t: (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B + shmem_idx_b_off;\n\n\t\tint4 *lane_ptr = NULL;\n\t\tint *lane_ptr_index = NULL;\n\t\tconst uint8_t *warp_ptr = NULL;\n\n\n\t\tif(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\t//warp_ptr = &A[block_tile_j * M] +\n\t\t\t//\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * M_GLOBAL;\n\t\t\twarp_ptr = &A[block_tile_j * M];\n\t\t\t\n\t\t\tconst int *warp_ptr_index = &B_index[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t\t\t\t\t\t((warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A);\n\n\t\t\tlane_ptr_index = (int *)(warp_ptr_index + tile_k_idx_sparse + (laneId / CHUNK_COPY_LINE_LANES_A));\n\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_A) * SHARED_OFFSET_A;\n\t\t}else{\n\t\t\twarp_ptr = &B[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * K_GLOBAL_SPARSE;\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_sparse +\n\t\t\t\t\t\t\t\t(laneId / CHUNK_COPY_LINE_LANES_B) * K_GLOBAL_SPARSE) +\n\t\t\t\t\t\t\t\t(laneId % CHUNK_COPY_LINE_LANES_B);\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_B) * SHARED_OFFSET_B;\n\t\t}\n\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      // shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n\t  int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n\t  \t? BLOCK_SIZE_K_SPARSE / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A)\n\t\t: BLOCK_SIZE_N / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\n\t  /*\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\t  */\n\n\t  /*\n      int tile_k_idx_A;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n          tile_k_idx_A = *(lane_ptr_index);\n      }\n\t  */\n\n\t  #pragma unroll\n\t  for(int i = 0; i < iter_index; i += 1){\n\t\t  if(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\tint tile_k_idx_A = *(lane_ptr_index);\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_A * M_GLOBAL) + (laneId % CHUNK_COPY_LINE_LANES_A);\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_A)) =\n\t\t\t\t*lane_ptr;\n\t\t\t//warp_ptr = (uint8_t *)((uint8_t *)warp_ptr + M_GLOBAL * (WARPS_PER_BLOCK / 2) *CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tlane_ptr_index = (int *)((int *)lane_ptr_index +  (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A;\n\t\t  }else{\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_B)) =\n\t\t\t\t*lane_ptr;\n\t\t\tlane_ptr = (int4 *)((uint8_t *)lane_ptr + K_GLOBAL_SPARSE * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B;\n\t\t  }\n\t  }\n\n      __syncthreads();\n\n\t#pragma unroll\n      for (int k_step = 0; k_step < CHUNK_K_SPARSE; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::col_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n\t#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i += 1) {\n\t\t\tsize_t shmem_idx_a = (warpId % BLOCK_COL_WARPS) * M * WARP_COL_TILES + (i * M);\n\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_a + k_step * K * SHARED_OFFSET_A;\n\n\t\t\twmma::load_matrix_sync(a[i], tile_ptr, SHARED_OFFSET_A);\n\t\t#pragma unroll\n\t\t\tfor(int j = 0; j < WARP_ROW_TILES; j += 1){\n\t\t\t\tif(i == 0){\n\t\t\t\t\tsize_t shmem_idx_b = shmem_idx_b_off +\n\t\t\t\t\t\t\t\t\t\t\t(warpId / BLOCK_COL_WARPS) * (WARP_ROW_TILES * N) * SHARED_OFFSET_B +\n\t\t\t\t\t\t\t\t\t\t\t(j * N) * SHARED_OFFSET_B;\n\t\t\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_b + k_step * K;\n\t\t\t\t\twmma::load_matrix_sync(b[j], tile_ptr, SHARED_OFFSET_B);\n\t\t\t\t}\n\t\t\t\twmma::mma_sync(c[j][i], a[i], b[j], c[j][i]);\n\t\t\t}\n\n        }\n      }\n\n      __syncthreads();\n    }\n\n    // This pointer is used to access the C and D matrix tiles this warp computes.\n\tint *shmem_warp_tile_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * N * WARP_ROW_TILES * SHMEM_STRIDE +\n\t(warpId % BLOCK_COL_WARPS) * SHMEM_OFFSET;\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t#pragma unroll\n\t\t\tfor(int t = 0; t < c[i][j].num_elements; t += 1){\n\t\t\t\tc[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n\t\t\t}\n\t\t\tint *tile_ptr = shmem_warp_tile_ptr + i * N * SHMEM_STRIDE + j * M;\n\t\t\twmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, C_LAYOUT);\n\t\t}\n\t}\n\n    __syncthreads();\n\n\tint *shmem_warp_stream_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N * SHMEM_STRIDE\n\t\t\t\t\t\t\t\t\t+ (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tconst size_t gmem_idx =\n\t\t(block_tile_i * N + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N) * GLOBAL_MEM_STRIDE +\n\t\tblock_tile_j * M + (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tuint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n\tint *shmem_lane_stream_ptr =\n\t\tshmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * SHMEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\t\n\tuint8_t *dst_gmem_lane_stream_ptr =\n\t\tdst_gmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * GLOBAL_MEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\n\tfor(int i = 0; i < WARP_ROW_TILES * N; i += SHARED_TO_GLOBAL_LINES_PER_WARP){\n\t\t*(dst_gmem_lane_stream_ptr + i * GLOBAL_MEM_STRIDE) = (uint8_t)(*(shmem_lane_stream_ptr + i * SHMEM_STRIDE));\n\t}\n\n\t__syncthreads();\n}\n", "launch_config": {"dimBlock": [1024, 1], "dimGrid": [256, 1]}}, "84": {"code": "\n__global__ void compute_gemm_imma(float *input0, float *input1, float *input2, float* input3, float* input4, float*input5, float* input6,\n                                  float *output0) {\n    // keys need to be replaced 4096, 1024, K_GLOBA_VALUEL, 0.75, 64\n\n\t//extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B = reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_index = reinterpret_cast<int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n\tconst int WARP_SIZE = 32;\n    const int M = 16;\n    const int K = 16;\n    const int N = 16;\n    const int WMMA_M = 16;\n    const int WMMA_K = 16;\n    const int WMMA_N = 16;\n    const int M_GLOBAL = 4096;\n    const int K_GLOBAL = 4096;\n    const int N_GLOBAL = 1024;\n    const float SPARSITY = 0.75;\n    const int K_GLOBAL_SPARSE = (int(K_GLOBAL * (1-SPARSITY)));\n    const int M_TILES = (M_GLOBAL / M);\n    const int N_TILES = (N_GLOBAL / N);\n    const int K_TILES = (K_GLOBAL / K);\n    const int BLOCK_ROW_WARPS = 2;\n    const int BLOCK_COL_WARPS = 4;\n    const int WARP_ROW_TILES = 4;\n    const int WARP_COL_TILES = 2;\n    const int SKEW_UINT8 = 16;\n    const int BANK_VAL = 32;\n    const int NUM_BANK = (K_GLOBAL / BANK_VAL);\n    /////////// BLOCK_ROW_TILES <= N_TILES ////////////\n    // const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    // ////////// BLOCK_COL_TILES <= M_TILES ////////////\n    // const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    auto C_LAYOUT = wmma::mem_col_major;\n    const int CHUNK_K = 64;\n    const int CHUNK_K_SPARSE = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K_SPARSE = (int((CHUNK_K * K) * (1 - SPARSITY)));\n    const int WARP_COPY_BYTES = (WARP_SIZE * sizeof(int4));\n    const int CHUNK_LINE_BYTES_A (BLOCK_COL_TILES * M * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_A = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_A);\n    const int CHUNK_COPY_LINE_LANES_A = (CHUNK_LINE_BYTES_A / sizeof(int4));\n    const int SHARED_OFFSET_A = (BLOCK_COL_TILES * M + SKEW_UINT8);\n\n    const int CHUNK_LINE_BYTES_B = (BLOCK_SIZE_K_SPARSE * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_B = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_B);\n    const int CHUNK_COPY_LINE_LANES_B = (CHUNK_LINE_BYTES_B / sizeof(int4));\n    const int SHARED_OFFSET_B = (BLOCK_SIZE_K_SPARSE + SKEW_UINT8);\n    const int SHARED_TO_GLOBAL_BYTES_PER_LINE = ((WARP_COL_TILES * M) * sizeof(int));\n    const int SHARED_TO_GLOBAL_BYTES_PER_WARP = (WARP_SIZE * sizeof(int));\n    const int SHARED_TO_GLOBAL_LINES_PER_WARP = (SHARED_TO_GLOBAL_BYTES_PER_WARP / SHARED_TO_GLOBAL_BYTES_PER_LINE);\n    const int SHARED_TO_GLOBAL_LANES_PER_LINE = (WARP_SIZE / SHARED_TO_GLOBAL_LINES_PER_WARP);\n    const int SHARED_TO_GLOBAL_ITERS = ((WARP_ROW_TILES * N) / SHARED_TO_GLOBAL_LINES_PER_WARP);\n\n    const int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const int WARP_STRIDE = (WARP_COL_TILES * M);\n    const int WARPS_PER_BLOCK = (BLOCK_ROW_WARPS * BLOCK_COL_WARPS);\n    const int THREADS_PER_BLOCK = (WARP_SIZE * WARPS_PER_BLOCK);\n    const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    const int GLOBAL_MEM_STRIDE = M_GLOBAL;\n    const int SHMEM_STRIDE = (M * BLOCK_COL_TILES);\n    const int SHMEM_OFFSET = (M * WARP_COL_TILES);\n    const int BLOCK_SIZE_M = (M * BLOCK_COL_TILES);\n    const int BLOCK_SIZE_N = (N * BLOCK_ROW_TILES);\n\n   extern __shared__ uint8_t shmem[];\n\n\t// Warp and lane identification.\n\tconst unsigned int warpId = threadIdx.x / WARP_SIZE;\n\tconst unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n\n\t// Offset in shared memory from which the B matrix is stored.\n\t// const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\tconst size_t shmem_idx_b_off = BLOCK_SIZE_K_SPARSE * SHARED_OFFSET_A;\n\n\n\t// Each CTA slides along the 128 x 128 tiles from the top left corner of the\n\t// matrix to the right and down, and selects the next tile to compute. Once\n\t// there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n\tconst unsigned int block_tile_i =\n\t\t((block_pos * BLOCK_COL_TILES) / M_TILES) * (BLOCK_ROW_TILES);\n\tconst unsigned int block_tile_j = (block_pos * BLOCK_COL_TILES) % M_TILES;\n\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n\twmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_ROW_TILES]\n\t\t\t\t\t\t\t\t\t\t\t\t\t [WARP_COL_TILES];\n\n    // Load the C matrix tiles into fragments from shared memory.\n\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t\twmma::fill_fragment(c[i][j], 0);\n\t\t}\n\t}\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    //for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n    for(int tile_k_idx_sparse = 0, tile_k_idx = 0; tile_k_idx_sparse < K_GLOBAL_SPARSE; tile_k_idx_sparse += BLOCK_SIZE_K_SPARSE, tile_k_idx += BLOCK_SIZE_K){\n\n\t\tsize_t shmem_idx = \n\t\twarpId < (WARPS_PER_BLOCK / 2)\n\t\t\t? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A\n\t\t\t: (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B + shmem_idx_b_off;\n\n\t\tint4 *lane_ptr = NULL;\n\t\tint *lane_ptr_index = NULL;\n\t\tconst uint8_t *warp_ptr = NULL;\n\n\n\t\tif(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\t//warp_ptr = &A[block_tile_j * M] +\n\t\t\t//\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * M_GLOBAL;\n\t\t\twarp_ptr = &A[block_tile_j * M];\n\t\t\t\n\t\t\tconst int *warp_ptr_index = &B_index[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t\t\t\t\t\t((warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A);\n\n\t\t\tlane_ptr_index = (int *)(warp_ptr_index + tile_k_idx_sparse + (laneId / CHUNK_COPY_LINE_LANES_A));\n\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_A) * SHARED_OFFSET_A;\n\t\t}else{\n\t\t\twarp_ptr = &B[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * K_GLOBAL_SPARSE;\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_sparse +\n\t\t\t\t\t\t\t\t(laneId / CHUNK_COPY_LINE_LANES_B) * K_GLOBAL_SPARSE) +\n\t\t\t\t\t\t\t\t(laneId % CHUNK_COPY_LINE_LANES_B);\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_B) * SHARED_OFFSET_B;\n\t\t}\n\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      // shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n\t  int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n\t  \t? BLOCK_SIZE_K_SPARSE / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A)\n\t\t: BLOCK_SIZE_N / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\n\t  /*\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\t  */\n\n\t  /*\n      int tile_k_idx_A;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n          tile_k_idx_A = *(lane_ptr_index);\n      }\n\t  */\n\n\t  #pragma unroll\n\t  for(int i = 0; i < iter_index; i += 1){\n\t\t  if(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\tint tile_k_idx_A = *(lane_ptr_index);\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_A * M_GLOBAL) + (laneId % CHUNK_COPY_LINE_LANES_A);\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_A)) =\n\t\t\t\t*lane_ptr;\n\t\t\t//warp_ptr = (uint8_t *)((uint8_t *)warp_ptr + M_GLOBAL * (WARPS_PER_BLOCK / 2) *CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tlane_ptr_index = (int *)((int *)lane_ptr_index +  (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A;\n\t\t  }else{\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_B)) =\n\t\t\t\t*lane_ptr;\n\t\t\tlane_ptr = (int4 *)((uint8_t *)lane_ptr + K_GLOBAL_SPARSE * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B;\n\t\t  }\n\t  }\n\n      __syncthreads();\n\n\t#pragma unroll\n      for (int k_step = 0; k_step < CHUNK_K_SPARSE; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::col_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n\t#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i += 1) {\n\t\t\tsize_t shmem_idx_a = (warpId % BLOCK_COL_WARPS) * M * WARP_COL_TILES + (i * M);\n\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_a + k_step * K * SHARED_OFFSET_A;\n\n\t\t\twmma::load_matrix_sync(a[i], tile_ptr, SHARED_OFFSET_A);\n\t\t#pragma unroll\n\t\t\tfor(int j = 0; j < WARP_ROW_TILES; j += 1){\n\t\t\t\tif(i == 0){\n\t\t\t\t\tsize_t shmem_idx_b = shmem_idx_b_off +\n\t\t\t\t\t\t\t\t\t\t\t(warpId / BLOCK_COL_WARPS) * (WARP_ROW_TILES * N) * SHARED_OFFSET_B +\n\t\t\t\t\t\t\t\t\t\t\t(j * N) * SHARED_OFFSET_B;\n\t\t\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_b + k_step * K;\n\t\t\t\t\twmma::load_matrix_sync(b[j], tile_ptr, SHARED_OFFSET_B);\n\t\t\t\t}\n\t\t\t\twmma::mma_sync(c[j][i], a[i], b[j], c[j][i]);\n\t\t\t}\n\n        }\n      }\n\n      __syncthreads();\n    }\n\n    // This pointer is used to access the C and D matrix tiles this warp computes.\n\tint *shmem_warp_tile_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * N * WARP_ROW_TILES * SHMEM_STRIDE +\n\t(warpId % BLOCK_COL_WARPS) * SHMEM_OFFSET;\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t#pragma unroll\n\t\t\tfor(int t = 0; t < c[i][j].num_elements; t += 1){\n\t\t\t\tc[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n\t\t\t}\n\t\t\tint *tile_ptr = shmem_warp_tile_ptr + i * N * SHMEM_STRIDE + j * M;\n\t\t\twmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, C_LAYOUT);\n\t\t}\n\t}\n\n    __syncthreads();\n\n\tint *shmem_warp_stream_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N * SHMEM_STRIDE\n\t\t\t\t\t\t\t\t\t+ (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tconst size_t gmem_idx =\n\t\t(block_tile_i * N + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N) * GLOBAL_MEM_STRIDE +\n\t\tblock_tile_j * M + (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tuint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n\tint *shmem_lane_stream_ptr =\n\t\tshmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * SHMEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\t\n\tuint8_t *dst_gmem_lane_stream_ptr =\n\t\tdst_gmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * GLOBAL_MEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\n\tfor(int i = 0; i < WARP_ROW_TILES * N; i += SHARED_TO_GLOBAL_LINES_PER_WARP){\n\t\t*(dst_gmem_lane_stream_ptr + i * GLOBAL_MEM_STRIDE) = (uint8_t)(*(shmem_lane_stream_ptr + i * SHMEM_STRIDE));\n\t}\n\n\t__syncthreads();\n}\n", "launch_config": {"dimBlock": [256, 1], "dimGrid": [256, 1]}}, "85": {"code": "\n__global__ void compute_gemm_imma(float *input0, float *input1, float *input2, float* input3, float* input4, float*input5, float* input6,\n                                  float *output0) {\n    // keys need to be replaced 4096, 1024, K_GLOBA_VALUEL, 0.75, 64\n\n\t//extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B = reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_index = reinterpret_cast<int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n\tconst int WARP_SIZE = 32;\n    const int M = 16;\n    const int K = 16;\n    const int N = 16;\n    const int WMMA_M = 16;\n    const int WMMA_K = 16;\n    const int WMMA_N = 16;\n    const int M_GLOBAL = 4096;\n    const int K_GLOBAL = 1024;\n    const int N_GLOBAL = 1024;\n    const float SPARSITY = 0.75;\n    const int K_GLOBAL_SPARSE = (int(K_GLOBAL * (1-SPARSITY)));\n    const int M_TILES = (M_GLOBAL / M);\n    const int N_TILES = (N_GLOBAL / N);\n    const int K_TILES = (K_GLOBAL / K);\n    const int BLOCK_ROW_WARPS = 2;\n    const int BLOCK_COL_WARPS = 4;\n    const int WARP_ROW_TILES = 4;\n    const int WARP_COL_TILES = 2;\n    const int SKEW_UINT8 = 16;\n    const int BANK_VAL = 32;\n    const int NUM_BANK = (K_GLOBAL / BANK_VAL);\n    /////////// BLOCK_ROW_TILES <= N_TILES ////////////\n    // const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    // ////////// BLOCK_COL_TILES <= M_TILES ////////////\n    // const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    auto C_LAYOUT = wmma::mem_col_major;\n    const int CHUNK_K = 64;\n    const int CHUNK_K_SPARSE = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K_SPARSE = (int((CHUNK_K * K) * (1 - SPARSITY)));\n    const int WARP_COPY_BYTES = (WARP_SIZE * sizeof(int4));\n    const int CHUNK_LINE_BYTES_A (BLOCK_COL_TILES * M * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_A = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_A);\n    const int CHUNK_COPY_LINE_LANES_A = (CHUNK_LINE_BYTES_A / sizeof(int4));\n    const int SHARED_OFFSET_A = (BLOCK_COL_TILES * M + SKEW_UINT8);\n\n    const int CHUNK_LINE_BYTES_B = (BLOCK_SIZE_K_SPARSE * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_B = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_B);\n    const int CHUNK_COPY_LINE_LANES_B = (CHUNK_LINE_BYTES_B / sizeof(int4));\n    const int SHARED_OFFSET_B = (BLOCK_SIZE_K_SPARSE + SKEW_UINT8);\n    const int SHARED_TO_GLOBAL_BYTES_PER_LINE = ((WARP_COL_TILES * M) * sizeof(int));\n    const int SHARED_TO_GLOBAL_BYTES_PER_WARP = (WARP_SIZE * sizeof(int));\n    const int SHARED_TO_GLOBAL_LINES_PER_WARP = (SHARED_TO_GLOBAL_BYTES_PER_WARP / SHARED_TO_GLOBAL_BYTES_PER_LINE);\n    const int SHARED_TO_GLOBAL_LANES_PER_LINE = (WARP_SIZE / SHARED_TO_GLOBAL_LINES_PER_WARP);\n    const int SHARED_TO_GLOBAL_ITERS = ((WARP_ROW_TILES * N) / SHARED_TO_GLOBAL_LINES_PER_WARP);\n\n    const int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const int WARP_STRIDE = (WARP_COL_TILES * M);\n    const int WARPS_PER_BLOCK = (BLOCK_ROW_WARPS * BLOCK_COL_WARPS);\n    const int THREADS_PER_BLOCK = (WARP_SIZE * WARPS_PER_BLOCK);\n    const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    const int GLOBAL_MEM_STRIDE = M_GLOBAL;\n    const int SHMEM_STRIDE = (M * BLOCK_COL_TILES);\n    const int SHMEM_OFFSET = (M * WARP_COL_TILES);\n    const int BLOCK_SIZE_M = (M * BLOCK_COL_TILES);\n    const int BLOCK_SIZE_N = (N * BLOCK_ROW_TILES);\n\n   extern __shared__ uint8_t shmem[];\n\n\t// Warp and lane identification.\n\tconst unsigned int warpId = threadIdx.x / WARP_SIZE;\n\tconst unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n\n\t// Offset in shared memory from which the B matrix is stored.\n\t// const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\tconst size_t shmem_idx_b_off = BLOCK_SIZE_K_SPARSE * SHARED_OFFSET_A;\n\n\n\t// Each CTA slides along the 128 x 128 tiles from the top left corner of the\n\t// matrix to the right and down, and selects the next tile to compute. Once\n\t// there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n\tconst unsigned int block_tile_i =\n\t\t((block_pos * BLOCK_COL_TILES) / M_TILES) * (BLOCK_ROW_TILES);\n\tconst unsigned int block_tile_j = (block_pos * BLOCK_COL_TILES) % M_TILES;\n\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n\twmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_ROW_TILES]\n\t\t\t\t\t\t\t\t\t\t\t\t\t [WARP_COL_TILES];\n\n    // Load the C matrix tiles into fragments from shared memory.\n\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t\twmma::fill_fragment(c[i][j], 0);\n\t\t}\n\t}\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    //for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n    for(int tile_k_idx_sparse = 0, tile_k_idx = 0; tile_k_idx_sparse < K_GLOBAL_SPARSE; tile_k_idx_sparse += BLOCK_SIZE_K_SPARSE, tile_k_idx += BLOCK_SIZE_K){\n\n\t\tsize_t shmem_idx = \n\t\twarpId < (WARPS_PER_BLOCK / 2)\n\t\t\t? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A\n\t\t\t: (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B + shmem_idx_b_off;\n\n\t\tint4 *lane_ptr = NULL;\n\t\tint *lane_ptr_index = NULL;\n\t\tconst uint8_t *warp_ptr = NULL;\n\n\n\t\tif(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\t//warp_ptr = &A[block_tile_j * M] +\n\t\t\t//\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * M_GLOBAL;\n\t\t\twarp_ptr = &A[block_tile_j * M];\n\t\t\t\n\t\t\tconst int *warp_ptr_index = &B_index[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t\t\t\t\t\t((warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A);\n\n\t\t\tlane_ptr_index = (int *)(warp_ptr_index + tile_k_idx_sparse + (laneId / CHUNK_COPY_LINE_LANES_A));\n\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_A) * SHARED_OFFSET_A;\n\t\t}else{\n\t\t\twarp_ptr = &B[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * K_GLOBAL_SPARSE;\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_sparse +\n\t\t\t\t\t\t\t\t(laneId / CHUNK_COPY_LINE_LANES_B) * K_GLOBAL_SPARSE) +\n\t\t\t\t\t\t\t\t(laneId % CHUNK_COPY_LINE_LANES_B);\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_B) * SHARED_OFFSET_B;\n\t\t}\n\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      // shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n\t  int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n\t  \t? BLOCK_SIZE_K_SPARSE / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A)\n\t\t: BLOCK_SIZE_N / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\n\t  /*\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\t  */\n\n\t  /*\n      int tile_k_idx_A;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n          tile_k_idx_A = *(lane_ptr_index);\n      }\n\t  */\n\n\t  #pragma unroll\n\t  for(int i = 0; i < iter_index; i += 1){\n\t\t  if(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\tint tile_k_idx_A = *(lane_ptr_index);\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_A * M_GLOBAL) + (laneId % CHUNK_COPY_LINE_LANES_A);\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_A)) =\n\t\t\t\t*lane_ptr;\n\t\t\t//warp_ptr = (uint8_t *)((uint8_t *)warp_ptr + M_GLOBAL * (WARPS_PER_BLOCK / 2) *CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tlane_ptr_index = (int *)((int *)lane_ptr_index +  (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A;\n\t\t  }else{\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_B)) =\n\t\t\t\t*lane_ptr;\n\t\t\tlane_ptr = (int4 *)((uint8_t *)lane_ptr + K_GLOBAL_SPARSE * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B;\n\t\t  }\n\t  }\n\n      __syncthreads();\n\n\t#pragma unroll\n      for (int k_step = 0; k_step < CHUNK_K_SPARSE; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::col_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n\t#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i += 1) {\n\t\t\tsize_t shmem_idx_a = (warpId % BLOCK_COL_WARPS) * M * WARP_COL_TILES + (i * M);\n\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_a + k_step * K * SHARED_OFFSET_A;\n\n\t\t\twmma::load_matrix_sync(a[i], tile_ptr, SHARED_OFFSET_A);\n\t\t#pragma unroll\n\t\t\tfor(int j = 0; j < WARP_ROW_TILES; j += 1){\n\t\t\t\tif(i == 0){\n\t\t\t\t\tsize_t shmem_idx_b = shmem_idx_b_off +\n\t\t\t\t\t\t\t\t\t\t\t(warpId / BLOCK_COL_WARPS) * (WARP_ROW_TILES * N) * SHARED_OFFSET_B +\n\t\t\t\t\t\t\t\t\t\t\t(j * N) * SHARED_OFFSET_B;\n\t\t\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_b + k_step * K;\n\t\t\t\t\twmma::load_matrix_sync(b[j], tile_ptr, SHARED_OFFSET_B);\n\t\t\t\t}\n\t\t\t\twmma::mma_sync(c[j][i], a[i], b[j], c[j][i]);\n\t\t\t}\n\n        }\n      }\n\n      __syncthreads();\n    }\n\n    // This pointer is used to access the C and D matrix tiles this warp computes.\n\tint *shmem_warp_tile_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * N * WARP_ROW_TILES * SHMEM_STRIDE +\n\t(warpId % BLOCK_COL_WARPS) * SHMEM_OFFSET;\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t#pragma unroll\n\t\t\tfor(int t = 0; t < c[i][j].num_elements; t += 1){\n\t\t\t\tc[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n\t\t\t}\n\t\t\tint *tile_ptr = shmem_warp_tile_ptr + i * N * SHMEM_STRIDE + j * M;\n\t\t\twmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, C_LAYOUT);\n\t\t}\n\t}\n\n    __syncthreads();\n\n\tint *shmem_warp_stream_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N * SHMEM_STRIDE\n\t\t\t\t\t\t\t\t\t+ (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tconst size_t gmem_idx =\n\t\t(block_tile_i * N + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N) * GLOBAL_MEM_STRIDE +\n\t\tblock_tile_j * M + (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tuint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n\tint *shmem_lane_stream_ptr =\n\t\tshmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * SHMEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\t\n\tuint8_t *dst_gmem_lane_stream_ptr =\n\t\tdst_gmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * GLOBAL_MEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\n\tfor(int i = 0; i < WARP_ROW_TILES * N; i += SHARED_TO_GLOBAL_LINES_PER_WARP){\n\t\t*(dst_gmem_lane_stream_ptr + i * GLOBAL_MEM_STRIDE) = (uint8_t)(*(shmem_lane_stream_ptr + i * SHMEM_STRIDE));\n\t}\n\n\t__syncthreads();\n}\n", "launch_config": {"dimBlock": [256, 1], "dimGrid": [256, 1]}}, "86": {"code": "\n__global__ void compute_gemm_imma(float *input0, float *input1, float *input2, float* input3, float* input4, float*input5, float* input6,\n                                  float *output0) {\n    // keys need to be replaced 4096, 1024, K_GLOBA_VALUEL, 0.75, 64\n\n\t//extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B = reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_index = reinterpret_cast<int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n\tconst int WARP_SIZE = 32;\n    const int M = 16;\n    const int K = 16;\n    const int N = 16;\n    const int WMMA_M = 16;\n    const int WMMA_K = 16;\n    const int WMMA_N = 16;\n    const int M_GLOBAL = 4096;\n    const int K_GLOBAL = 1024;\n    const int N_GLOBAL = 1024;\n    const float SPARSITY = 0.75;\n    const int K_GLOBAL_SPARSE = (int(K_GLOBAL * (1-SPARSITY)));\n    const int M_TILES = (M_GLOBAL / M);\n    const int N_TILES = (N_GLOBAL / N);\n    const int K_TILES = (K_GLOBAL / K);\n    const int BLOCK_ROW_WARPS = 2;\n    const int BLOCK_COL_WARPS = 4;\n    const int WARP_ROW_TILES = 4;\n    const int WARP_COL_TILES = 2;\n    const int SKEW_UINT8 = 16;\n    const int BANK_VAL = 32;\n    const int NUM_BANK = (K_GLOBAL / BANK_VAL);\n    /////////// BLOCK_ROW_TILES <= N_TILES ////////////\n    // const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    // ////////// BLOCK_COL_TILES <= M_TILES ////////////\n    // const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    auto C_LAYOUT = wmma::mem_col_major;\n    const int CHUNK_K = 64;\n    const int CHUNK_K_SPARSE = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K_SPARSE = (int((CHUNK_K * K) * (1 - SPARSITY)));\n    const int WARP_COPY_BYTES = (WARP_SIZE * sizeof(int4));\n    const int CHUNK_LINE_BYTES_A (BLOCK_COL_TILES * M * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_A = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_A);\n    const int CHUNK_COPY_LINE_LANES_A = (CHUNK_LINE_BYTES_A / sizeof(int4));\n    const int SHARED_OFFSET_A = (BLOCK_COL_TILES * M + SKEW_UINT8);\n\n    const int CHUNK_LINE_BYTES_B = (BLOCK_SIZE_K_SPARSE * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_B = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_B);\n    const int CHUNK_COPY_LINE_LANES_B = (CHUNK_LINE_BYTES_B / sizeof(int4));\n    const int SHARED_OFFSET_B = (BLOCK_SIZE_K_SPARSE + SKEW_UINT8);\n    const int SHARED_TO_GLOBAL_BYTES_PER_LINE = ((WARP_COL_TILES * M) * sizeof(int));\n    const int SHARED_TO_GLOBAL_BYTES_PER_WARP = (WARP_SIZE * sizeof(int));\n    const int SHARED_TO_GLOBAL_LINES_PER_WARP = (SHARED_TO_GLOBAL_BYTES_PER_WARP / SHARED_TO_GLOBAL_BYTES_PER_LINE);\n    const int SHARED_TO_GLOBAL_LANES_PER_LINE = (WARP_SIZE / SHARED_TO_GLOBAL_LINES_PER_WARP);\n    const int SHARED_TO_GLOBAL_ITERS = ((WARP_ROW_TILES * N) / SHARED_TO_GLOBAL_LINES_PER_WARP);\n\n    const int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const int WARP_STRIDE = (WARP_COL_TILES * M);\n    const int WARPS_PER_BLOCK = (BLOCK_ROW_WARPS * BLOCK_COL_WARPS);\n    const int THREADS_PER_BLOCK = (WARP_SIZE * WARPS_PER_BLOCK);\n    const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    const int GLOBAL_MEM_STRIDE = M_GLOBAL;\n    const int SHMEM_STRIDE = (M * BLOCK_COL_TILES);\n    const int SHMEM_OFFSET = (M * WARP_COL_TILES);\n    const int BLOCK_SIZE_M = (M * BLOCK_COL_TILES);\n    const int BLOCK_SIZE_N = (N * BLOCK_ROW_TILES);\n\n   extern __shared__ uint8_t shmem[];\n\n\t// Warp and lane identification.\n\tconst unsigned int warpId = threadIdx.x / WARP_SIZE;\n\tconst unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n\n\t// Offset in shared memory from which the B matrix is stored.\n\t// const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\tconst size_t shmem_idx_b_off = BLOCK_SIZE_K_SPARSE * SHARED_OFFSET_A;\n\n\n\t// Each CTA slides along the 128 x 128 tiles from the top left corner of the\n\t// matrix to the right and down, and selects the next tile to compute. Once\n\t// there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n\tconst unsigned int block_tile_i =\n\t\t((block_pos * BLOCK_COL_TILES) / M_TILES) * (BLOCK_ROW_TILES);\n\tconst unsigned int block_tile_j = (block_pos * BLOCK_COL_TILES) % M_TILES;\n\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n\twmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_ROW_TILES]\n\t\t\t\t\t\t\t\t\t\t\t\t\t [WARP_COL_TILES];\n\n    // Load the C matrix tiles into fragments from shared memory.\n\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t\twmma::fill_fragment(c[i][j], 0);\n\t\t}\n\t}\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    //for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n    for(int tile_k_idx_sparse = 0, tile_k_idx = 0; tile_k_idx_sparse < K_GLOBAL_SPARSE; tile_k_idx_sparse += BLOCK_SIZE_K_SPARSE, tile_k_idx += BLOCK_SIZE_K){\n\n\t\tsize_t shmem_idx = \n\t\twarpId < (WARPS_PER_BLOCK / 2)\n\t\t\t? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A\n\t\t\t: (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B + shmem_idx_b_off;\n\n\t\tint4 *lane_ptr = NULL;\n\t\tint *lane_ptr_index = NULL;\n\t\tconst uint8_t *warp_ptr = NULL;\n\n\n\t\tif(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\t//warp_ptr = &A[block_tile_j * M] +\n\t\t\t//\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * M_GLOBAL;\n\t\t\twarp_ptr = &A[block_tile_j * M];\n\t\t\t\n\t\t\tconst int *warp_ptr_index = &B_index[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t\t\t\t\t\t((warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A);\n\n\t\t\tlane_ptr_index = (int *)(warp_ptr_index + tile_k_idx_sparse + (laneId / CHUNK_COPY_LINE_LANES_A));\n\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_A) * SHARED_OFFSET_A;\n\t\t}else{\n\t\t\twarp_ptr = &B[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * K_GLOBAL_SPARSE;\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_sparse +\n\t\t\t\t\t\t\t\t(laneId / CHUNK_COPY_LINE_LANES_B) * K_GLOBAL_SPARSE) +\n\t\t\t\t\t\t\t\t(laneId % CHUNK_COPY_LINE_LANES_B);\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_B) * SHARED_OFFSET_B;\n\t\t}\n\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      // shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n\t  int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n\t  \t? BLOCK_SIZE_K_SPARSE / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A)\n\t\t: BLOCK_SIZE_N / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\n\t  /*\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\t  */\n\n\t  /*\n      int tile_k_idx_A;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n          tile_k_idx_A = *(lane_ptr_index);\n      }\n\t  */\n\n\t  #pragma unroll\n\t  for(int i = 0; i < iter_index; i += 1){\n\t\t  if(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\tint tile_k_idx_A = *(lane_ptr_index);\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_A * M_GLOBAL) + (laneId % CHUNK_COPY_LINE_LANES_A);\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_A)) =\n\t\t\t\t*lane_ptr;\n\t\t\t//warp_ptr = (uint8_t *)((uint8_t *)warp_ptr + M_GLOBAL * (WARPS_PER_BLOCK / 2) *CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tlane_ptr_index = (int *)((int *)lane_ptr_index +  (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A;\n\t\t  }else{\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_B)) =\n\t\t\t\t*lane_ptr;\n\t\t\tlane_ptr = (int4 *)((uint8_t *)lane_ptr + K_GLOBAL_SPARSE * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B;\n\t\t  }\n\t  }\n\n      __syncthreads();\n\n\t#pragma unroll\n      for (int k_step = 0; k_step < CHUNK_K_SPARSE; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::col_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n\t#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i += 1) {\n\t\t\tsize_t shmem_idx_a = (warpId % BLOCK_COL_WARPS) * M * WARP_COL_TILES + (i * M);\n\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_a + k_step * K * SHARED_OFFSET_A;\n\n\t\t\twmma::load_matrix_sync(a[i], tile_ptr, SHARED_OFFSET_A);\n\t\t#pragma unroll\n\t\t\tfor(int j = 0; j < WARP_ROW_TILES; j += 1){\n\t\t\t\tif(i == 0){\n\t\t\t\t\tsize_t shmem_idx_b = shmem_idx_b_off +\n\t\t\t\t\t\t\t\t\t\t\t(warpId / BLOCK_COL_WARPS) * (WARP_ROW_TILES * N) * SHARED_OFFSET_B +\n\t\t\t\t\t\t\t\t\t\t\t(j * N) * SHARED_OFFSET_B;\n\t\t\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_b + k_step * K;\n\t\t\t\t\twmma::load_matrix_sync(b[j], tile_ptr, SHARED_OFFSET_B);\n\t\t\t\t}\n\t\t\t\twmma::mma_sync(c[j][i], a[i], b[j], c[j][i]);\n\t\t\t}\n\n        }\n      }\n\n      __syncthreads();\n    }\n\n    // This pointer is used to access the C and D matrix tiles this warp computes.\n\tint *shmem_warp_tile_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * N * WARP_ROW_TILES * SHMEM_STRIDE +\n\t(warpId % BLOCK_COL_WARPS) * SHMEM_OFFSET;\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t#pragma unroll\n\t\t\tfor(int t = 0; t < c[i][j].num_elements; t += 1){\n\t\t\t\tc[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n\t\t\t}\n\t\t\tint *tile_ptr = shmem_warp_tile_ptr + i * N * SHMEM_STRIDE + j * M;\n\t\t\twmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, C_LAYOUT);\n\t\t}\n\t}\n\n    __syncthreads();\n\n\tint *shmem_warp_stream_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N * SHMEM_STRIDE\n\t\t\t\t\t\t\t\t\t+ (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tconst size_t gmem_idx =\n\t\t(block_tile_i * N + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N) * GLOBAL_MEM_STRIDE +\n\t\tblock_tile_j * M + (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tuint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n\tint *shmem_lane_stream_ptr =\n\t\tshmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * SHMEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\t\n\tuint8_t *dst_gmem_lane_stream_ptr =\n\t\tdst_gmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * GLOBAL_MEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\n\tfor(int i = 0; i < WARP_ROW_TILES * N; i += SHARED_TO_GLOBAL_LINES_PER_WARP){\n\t\t*(dst_gmem_lane_stream_ptr + i * GLOBAL_MEM_STRIDE) = (uint8_t)(*(shmem_lane_stream_ptr + i * SHMEM_STRIDE));\n\t}\n\n\t__syncthreads();\n}\n", "launch_config": {"dimBlock": [256, 1], "dimGrid": [256, 1]}}, "87": {"code": "\n__global__ void compute_gemm_imma(float *input0, float *input1, float *input2, float* input3, float* input4, float*input5, float* input6,\n                                  float *output0) {\n    // keys need to be replaced 4096, 1024, K_GLOBA_VALUEL, 0.75, 64\n\n\t//extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B = reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_index = reinterpret_cast<int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n\tconst int WARP_SIZE = 32;\n    const int M = 16;\n    const int K = 16;\n    const int N = 16;\n    const int WMMA_M = 16;\n    const int WMMA_K = 16;\n    const int WMMA_N = 16;\n    const int M_GLOBAL = 4096;\n    const int K_GLOBAL = 1024;\n    const int N_GLOBAL = 1024;\n    const float SPARSITY = 0.75;\n    const int K_GLOBAL_SPARSE = (int(K_GLOBAL * (1-SPARSITY)));\n    const int M_TILES = (M_GLOBAL / M);\n    const int N_TILES = (N_GLOBAL / N);\n    const int K_TILES = (K_GLOBAL / K);\n    const int BLOCK_ROW_WARPS = 2;\n    const int BLOCK_COL_WARPS = 4;\n    const int WARP_ROW_TILES = 4;\n    const int WARP_COL_TILES = 2;\n    const int SKEW_UINT8 = 16;\n    const int BANK_VAL = 32;\n    const int NUM_BANK = (K_GLOBAL / BANK_VAL);\n    /////////// BLOCK_ROW_TILES <= N_TILES ////////////\n    // const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    // ////////// BLOCK_COL_TILES <= M_TILES ////////////\n    // const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    auto C_LAYOUT = wmma::mem_col_major;\n    const int CHUNK_K = 64;\n    const int CHUNK_K_SPARSE = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K_SPARSE = (int((CHUNK_K * K) * (1 - SPARSITY)));\n    const int WARP_COPY_BYTES = (WARP_SIZE * sizeof(int4));\n    const int CHUNK_LINE_BYTES_A (BLOCK_COL_TILES * M * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_A = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_A);\n    const int CHUNK_COPY_LINE_LANES_A = (CHUNK_LINE_BYTES_A / sizeof(int4));\n    const int SHARED_OFFSET_A = (BLOCK_COL_TILES * M + SKEW_UINT8);\n\n    const int CHUNK_LINE_BYTES_B = (BLOCK_SIZE_K_SPARSE * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_B = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_B);\n    const int CHUNK_COPY_LINE_LANES_B = (CHUNK_LINE_BYTES_B / sizeof(int4));\n    const int SHARED_OFFSET_B = (BLOCK_SIZE_K_SPARSE + SKEW_UINT8);\n    const int SHARED_TO_GLOBAL_BYTES_PER_LINE = ((WARP_COL_TILES * M) * sizeof(int));\n    const int SHARED_TO_GLOBAL_BYTES_PER_WARP = (WARP_SIZE * sizeof(int));\n    const int SHARED_TO_GLOBAL_LINES_PER_WARP = (SHARED_TO_GLOBAL_BYTES_PER_WARP / SHARED_TO_GLOBAL_BYTES_PER_LINE);\n    const int SHARED_TO_GLOBAL_LANES_PER_LINE = (WARP_SIZE / SHARED_TO_GLOBAL_LINES_PER_WARP);\n    const int SHARED_TO_GLOBAL_ITERS = ((WARP_ROW_TILES * N) / SHARED_TO_GLOBAL_LINES_PER_WARP);\n\n    const int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const int WARP_STRIDE = (WARP_COL_TILES * M);\n    const int WARPS_PER_BLOCK = (BLOCK_ROW_WARPS * BLOCK_COL_WARPS);\n    const int THREADS_PER_BLOCK = (WARP_SIZE * WARPS_PER_BLOCK);\n    const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    const int GLOBAL_MEM_STRIDE = M_GLOBAL;\n    const int SHMEM_STRIDE = (M * BLOCK_COL_TILES);\n    const int SHMEM_OFFSET = (M * WARP_COL_TILES);\n    const int BLOCK_SIZE_M = (M * BLOCK_COL_TILES);\n    const int BLOCK_SIZE_N = (N * BLOCK_ROW_TILES);\n\n   extern __shared__ uint8_t shmem[];\n\n\t// Warp and lane identification.\n\tconst unsigned int warpId = threadIdx.x / WARP_SIZE;\n\tconst unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n\n\t// Offset in shared memory from which the B matrix is stored.\n\t// const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\tconst size_t shmem_idx_b_off = BLOCK_SIZE_K_SPARSE * SHARED_OFFSET_A;\n\n\n\t// Each CTA slides along the 128 x 128 tiles from the top left corner of the\n\t// matrix to the right and down, and selects the next tile to compute. Once\n\t// there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n\tconst unsigned int block_tile_i =\n\t\t((block_pos * BLOCK_COL_TILES) / M_TILES) * (BLOCK_ROW_TILES);\n\tconst unsigned int block_tile_j = (block_pos * BLOCK_COL_TILES) % M_TILES;\n\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n\twmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_ROW_TILES]\n\t\t\t\t\t\t\t\t\t\t\t\t\t [WARP_COL_TILES];\n\n    // Load the C matrix tiles into fragments from shared memory.\n\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t\twmma::fill_fragment(c[i][j], 0);\n\t\t}\n\t}\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    //for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n    for(int tile_k_idx_sparse = 0, tile_k_idx = 0; tile_k_idx_sparse < K_GLOBAL_SPARSE; tile_k_idx_sparse += BLOCK_SIZE_K_SPARSE, tile_k_idx += BLOCK_SIZE_K){\n\n\t\tsize_t shmem_idx = \n\t\twarpId < (WARPS_PER_BLOCK / 2)\n\t\t\t? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A\n\t\t\t: (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B + shmem_idx_b_off;\n\n\t\tint4 *lane_ptr = NULL;\n\t\tint *lane_ptr_index = NULL;\n\t\tconst uint8_t *warp_ptr = NULL;\n\n\n\t\tif(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\t//warp_ptr = &A[block_tile_j * M] +\n\t\t\t//\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * M_GLOBAL;\n\t\t\twarp_ptr = &A[block_tile_j * M];\n\t\t\t\n\t\t\tconst int *warp_ptr_index = &B_index[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t\t\t\t\t\t((warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A);\n\n\t\t\tlane_ptr_index = (int *)(warp_ptr_index + tile_k_idx_sparse + (laneId / CHUNK_COPY_LINE_LANES_A));\n\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_A) * SHARED_OFFSET_A;\n\t\t}else{\n\t\t\twarp_ptr = &B[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * K_GLOBAL_SPARSE;\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_sparse +\n\t\t\t\t\t\t\t\t(laneId / CHUNK_COPY_LINE_LANES_B) * K_GLOBAL_SPARSE) +\n\t\t\t\t\t\t\t\t(laneId % CHUNK_COPY_LINE_LANES_B);\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_B) * SHARED_OFFSET_B;\n\t\t}\n\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      // shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n\t  int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n\t  \t? BLOCK_SIZE_K_SPARSE / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A)\n\t\t: BLOCK_SIZE_N / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\n\t  /*\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\t  */\n\n\t  /*\n      int tile_k_idx_A;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n          tile_k_idx_A = *(lane_ptr_index);\n      }\n\t  */\n\n\t  #pragma unroll\n\t  for(int i = 0; i < iter_index; i += 1){\n\t\t  if(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\tint tile_k_idx_A = *(lane_ptr_index);\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_A * M_GLOBAL) + (laneId % CHUNK_COPY_LINE_LANES_A);\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_A)) =\n\t\t\t\t*lane_ptr;\n\t\t\t//warp_ptr = (uint8_t *)((uint8_t *)warp_ptr + M_GLOBAL * (WARPS_PER_BLOCK / 2) *CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tlane_ptr_index = (int *)((int *)lane_ptr_index +  (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A;\n\t\t  }else{\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_B)) =\n\t\t\t\t*lane_ptr;\n\t\t\tlane_ptr = (int4 *)((uint8_t *)lane_ptr + K_GLOBAL_SPARSE * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B;\n\t\t  }\n\t  }\n\n      __syncthreads();\n\n\t#pragma unroll\n      for (int k_step = 0; k_step < CHUNK_K_SPARSE; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::col_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n\t#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i += 1) {\n\t\t\tsize_t shmem_idx_a = (warpId % BLOCK_COL_WARPS) * M * WARP_COL_TILES + (i * M);\n\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_a + k_step * K * SHARED_OFFSET_A;\n\n\t\t\twmma::load_matrix_sync(a[i], tile_ptr, SHARED_OFFSET_A);\n\t\t#pragma unroll\n\t\t\tfor(int j = 0; j < WARP_ROW_TILES; j += 1){\n\t\t\t\tif(i == 0){\n\t\t\t\t\tsize_t shmem_idx_b = shmem_idx_b_off +\n\t\t\t\t\t\t\t\t\t\t\t(warpId / BLOCK_COL_WARPS) * (WARP_ROW_TILES * N) * SHARED_OFFSET_B +\n\t\t\t\t\t\t\t\t\t\t\t(j * N) * SHARED_OFFSET_B;\n\t\t\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_b + k_step * K;\n\t\t\t\t\twmma::load_matrix_sync(b[j], tile_ptr, SHARED_OFFSET_B);\n\t\t\t\t}\n\t\t\t\twmma::mma_sync(c[j][i], a[i], b[j], c[j][i]);\n\t\t\t}\n\n        }\n      }\n\n      __syncthreads();\n    }\n\n    // This pointer is used to access the C and D matrix tiles this warp computes.\n\tint *shmem_warp_tile_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * N * WARP_ROW_TILES * SHMEM_STRIDE +\n\t(warpId % BLOCK_COL_WARPS) * SHMEM_OFFSET;\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t#pragma unroll\n\t\t\tfor(int t = 0; t < c[i][j].num_elements; t += 1){\n\t\t\t\tc[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n\t\t\t}\n\t\t\tint *tile_ptr = shmem_warp_tile_ptr + i * N * SHMEM_STRIDE + j * M;\n\t\t\twmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, C_LAYOUT);\n\t\t}\n\t}\n\n    __syncthreads();\n\n\tint *shmem_warp_stream_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N * SHMEM_STRIDE\n\t\t\t\t\t\t\t\t\t+ (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tconst size_t gmem_idx =\n\t\t(block_tile_i * N + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N) * GLOBAL_MEM_STRIDE +\n\t\tblock_tile_j * M + (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tuint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n\tint *shmem_lane_stream_ptr =\n\t\tshmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * SHMEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\t\n\tuint8_t *dst_gmem_lane_stream_ptr =\n\t\tdst_gmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * GLOBAL_MEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\n\tfor(int i = 0; i < WARP_ROW_TILES * N; i += SHARED_TO_GLOBAL_LINES_PER_WARP){\n\t\t*(dst_gmem_lane_stream_ptr + i * GLOBAL_MEM_STRIDE) = (uint8_t)(*(shmem_lane_stream_ptr + i * SHMEM_STRIDE));\n\t}\n\n\t__syncthreads();\n}\n", "launch_config": {"dimBlock": [256, 1], "dimGrid": [256, 1]}}, "88": {"code": "\n__global__ void compute_gemm_imma(float *input0, float *input1, float *input2, float* input3, float* input4, float*input5, float* input6,\n                                  float *output0) {\n    // keys need to be replaced 4096, 1024, K_GLOBA_VALUEL, 0.75, 64\n\n\t//extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B = reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_index = reinterpret_cast<int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n\tconst int WARP_SIZE = 32;\n    const int M = 16;\n    const int K = 16;\n    const int N = 16;\n    const int WMMA_M = 16;\n    const int WMMA_K = 16;\n    const int WMMA_N = 16;\n    const int M_GLOBAL = 4096;\n    const int K_GLOBAL = 1024;\n    const int N_GLOBAL = 1024;\n    const float SPARSITY = 0.75;\n    const int K_GLOBAL_SPARSE = (int(K_GLOBAL * (1-SPARSITY)));\n    const int M_TILES = (M_GLOBAL / M);\n    const int N_TILES = (N_GLOBAL / N);\n    const int K_TILES = (K_GLOBAL / K);\n    const int BLOCK_ROW_WARPS = 2;\n    const int BLOCK_COL_WARPS = 4;\n    const int WARP_ROW_TILES = 4;\n    const int WARP_COL_TILES = 2;\n    const int SKEW_UINT8 = 16;\n    const int BANK_VAL = 32;\n    const int NUM_BANK = (K_GLOBAL / BANK_VAL);\n    /////////// BLOCK_ROW_TILES <= N_TILES ////////////\n    // const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    // ////////// BLOCK_COL_TILES <= M_TILES ////////////\n    // const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    auto C_LAYOUT = wmma::mem_col_major;\n    const int CHUNK_K = 64;\n    const int CHUNK_K_SPARSE = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K_SPARSE = (int((CHUNK_K * K) * (1 - SPARSITY)));\n    const int WARP_COPY_BYTES = (WARP_SIZE * sizeof(int4));\n    const int CHUNK_LINE_BYTES_A (BLOCK_COL_TILES * M * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_A = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_A);\n    const int CHUNK_COPY_LINE_LANES_A = (CHUNK_LINE_BYTES_A / sizeof(int4));\n    const int SHARED_OFFSET_A = (BLOCK_COL_TILES * M + SKEW_UINT8);\n\n    const int CHUNK_LINE_BYTES_B = (BLOCK_SIZE_K_SPARSE * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_B = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_B);\n    const int CHUNK_COPY_LINE_LANES_B = (CHUNK_LINE_BYTES_B / sizeof(int4));\n    const int SHARED_OFFSET_B = (BLOCK_SIZE_K_SPARSE + SKEW_UINT8);\n    const int SHARED_TO_GLOBAL_BYTES_PER_LINE = ((WARP_COL_TILES * M) * sizeof(int));\n    const int SHARED_TO_GLOBAL_BYTES_PER_WARP = (WARP_SIZE * sizeof(int));\n    const int SHARED_TO_GLOBAL_LINES_PER_WARP = (SHARED_TO_GLOBAL_BYTES_PER_WARP / SHARED_TO_GLOBAL_BYTES_PER_LINE);\n    const int SHARED_TO_GLOBAL_LANES_PER_LINE = (WARP_SIZE / SHARED_TO_GLOBAL_LINES_PER_WARP);\n    const int SHARED_TO_GLOBAL_ITERS = ((WARP_ROW_TILES * N) / SHARED_TO_GLOBAL_LINES_PER_WARP);\n\n    const int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const int WARP_STRIDE = (WARP_COL_TILES * M);\n    const int WARPS_PER_BLOCK = (BLOCK_ROW_WARPS * BLOCK_COL_WARPS);\n    const int THREADS_PER_BLOCK = (WARP_SIZE * WARPS_PER_BLOCK);\n    const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    const int GLOBAL_MEM_STRIDE = M_GLOBAL;\n    const int SHMEM_STRIDE = (M * BLOCK_COL_TILES);\n    const int SHMEM_OFFSET = (M * WARP_COL_TILES);\n    const int BLOCK_SIZE_M = (M * BLOCK_COL_TILES);\n    const int BLOCK_SIZE_N = (N * BLOCK_ROW_TILES);\n\n   extern __shared__ uint8_t shmem[];\n\n\t// Warp and lane identification.\n\tconst unsigned int warpId = threadIdx.x / WARP_SIZE;\n\tconst unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n\n\t// Offset in shared memory from which the B matrix is stored.\n\t// const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\tconst size_t shmem_idx_b_off = BLOCK_SIZE_K_SPARSE * SHARED_OFFSET_A;\n\n\n\t// Each CTA slides along the 128 x 128 tiles from the top left corner of the\n\t// matrix to the right and down, and selects the next tile to compute. Once\n\t// there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n\tconst unsigned int block_tile_i =\n\t\t((block_pos * BLOCK_COL_TILES) / M_TILES) * (BLOCK_ROW_TILES);\n\tconst unsigned int block_tile_j = (block_pos * BLOCK_COL_TILES) % M_TILES;\n\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n\twmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_ROW_TILES]\n\t\t\t\t\t\t\t\t\t\t\t\t\t [WARP_COL_TILES];\n\n    // Load the C matrix tiles into fragments from shared memory.\n\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t\twmma::fill_fragment(c[i][j], 0);\n\t\t}\n\t}\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    //for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n    for(int tile_k_idx_sparse = 0, tile_k_idx = 0; tile_k_idx_sparse < K_GLOBAL_SPARSE; tile_k_idx_sparse += BLOCK_SIZE_K_SPARSE, tile_k_idx += BLOCK_SIZE_K){\n\n\t\tsize_t shmem_idx = \n\t\twarpId < (WARPS_PER_BLOCK / 2)\n\t\t\t? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A\n\t\t\t: (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B + shmem_idx_b_off;\n\n\t\tint4 *lane_ptr = NULL;\n\t\tint *lane_ptr_index = NULL;\n\t\tconst uint8_t *warp_ptr = NULL;\n\n\n\t\tif(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\t//warp_ptr = &A[block_tile_j * M] +\n\t\t\t//\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * M_GLOBAL;\n\t\t\twarp_ptr = &A[block_tile_j * M];\n\t\t\t\n\t\t\tconst int *warp_ptr_index = &B_index[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t\t\t\t\t\t((warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A);\n\n\t\t\tlane_ptr_index = (int *)(warp_ptr_index + tile_k_idx_sparse + (laneId / CHUNK_COPY_LINE_LANES_A));\n\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_A) * SHARED_OFFSET_A;\n\t\t}else{\n\t\t\twarp_ptr = &B[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * K_GLOBAL_SPARSE;\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_sparse +\n\t\t\t\t\t\t\t\t(laneId / CHUNK_COPY_LINE_LANES_B) * K_GLOBAL_SPARSE) +\n\t\t\t\t\t\t\t\t(laneId % CHUNK_COPY_LINE_LANES_B);\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_B) * SHARED_OFFSET_B;\n\t\t}\n\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      // shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n\t  int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n\t  \t? BLOCK_SIZE_K_SPARSE / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A)\n\t\t: BLOCK_SIZE_N / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\n\t  /*\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\t  */\n\n\t  /*\n      int tile_k_idx_A;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n          tile_k_idx_A = *(lane_ptr_index);\n      }\n\t  */\n\n\t  #pragma unroll\n\t  for(int i = 0; i < iter_index; i += 1){\n\t\t  if(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\tint tile_k_idx_A = *(lane_ptr_index);\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_A * M_GLOBAL) + (laneId % CHUNK_COPY_LINE_LANES_A);\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_A)) =\n\t\t\t\t*lane_ptr;\n\t\t\t//warp_ptr = (uint8_t *)((uint8_t *)warp_ptr + M_GLOBAL * (WARPS_PER_BLOCK / 2) *CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tlane_ptr_index = (int *)((int *)lane_ptr_index +  (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A;\n\t\t  }else{\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_B)) =\n\t\t\t\t*lane_ptr;\n\t\t\tlane_ptr = (int4 *)((uint8_t *)lane_ptr + K_GLOBAL_SPARSE * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B;\n\t\t  }\n\t  }\n\n      __syncthreads();\n\n\t#pragma unroll\n      for (int k_step = 0; k_step < CHUNK_K_SPARSE; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::col_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n\t#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i += 1) {\n\t\t\tsize_t shmem_idx_a = (warpId % BLOCK_COL_WARPS) * M * WARP_COL_TILES + (i * M);\n\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_a + k_step * K * SHARED_OFFSET_A;\n\n\t\t\twmma::load_matrix_sync(a[i], tile_ptr, SHARED_OFFSET_A);\n\t\t#pragma unroll\n\t\t\tfor(int j = 0; j < WARP_ROW_TILES; j += 1){\n\t\t\t\tif(i == 0){\n\t\t\t\t\tsize_t shmem_idx_b = shmem_idx_b_off +\n\t\t\t\t\t\t\t\t\t\t\t(warpId / BLOCK_COL_WARPS) * (WARP_ROW_TILES * N) * SHARED_OFFSET_B +\n\t\t\t\t\t\t\t\t\t\t\t(j * N) * SHARED_OFFSET_B;\n\t\t\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_b + k_step * K;\n\t\t\t\t\twmma::load_matrix_sync(b[j], tile_ptr, SHARED_OFFSET_B);\n\t\t\t\t}\n\t\t\t\twmma::mma_sync(c[j][i], a[i], b[j], c[j][i]);\n\t\t\t}\n\n        }\n      }\n\n      __syncthreads();\n    }\n\n    // This pointer is used to access the C and D matrix tiles this warp computes.\n\tint *shmem_warp_tile_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * N * WARP_ROW_TILES * SHMEM_STRIDE +\n\t(warpId % BLOCK_COL_WARPS) * SHMEM_OFFSET;\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t#pragma unroll\n\t\t\tfor(int t = 0; t < c[i][j].num_elements; t += 1){\n\t\t\t\tc[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n\t\t\t}\n\t\t\tint *tile_ptr = shmem_warp_tile_ptr + i * N * SHMEM_STRIDE + j * M;\n\t\t\twmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, C_LAYOUT);\n\t\t}\n\t}\n\n    __syncthreads();\n\n\tint *shmem_warp_stream_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N * SHMEM_STRIDE\n\t\t\t\t\t\t\t\t\t+ (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tconst size_t gmem_idx =\n\t\t(block_tile_i * N + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N) * GLOBAL_MEM_STRIDE +\n\t\tblock_tile_j * M + (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tuint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n\tint *shmem_lane_stream_ptr =\n\t\tshmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * SHMEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\t\n\tuint8_t *dst_gmem_lane_stream_ptr =\n\t\tdst_gmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * GLOBAL_MEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\n\tfor(int i = 0; i < WARP_ROW_TILES * N; i += SHARED_TO_GLOBAL_LINES_PER_WARP){\n\t\t*(dst_gmem_lane_stream_ptr + i * GLOBAL_MEM_STRIDE) = (uint8_t)(*(shmem_lane_stream_ptr + i * SHMEM_STRIDE));\n\t}\n\n\t__syncthreads();\n}\n", "launch_config": {"dimBlock": [256, 1], "dimGrid": [256, 1]}}, "89": {"code": "\n__global__ void compute_gemm_imma(float *input0, float *input1, float *input2, float* input3, float* input4, float*input5, float* input6,\n                                  float *output0) {\n    // keys need to be replaced 4096, 4096, K_GLOBA_VALUEL, 0.75, 64\n\n\t//extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B = reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_index = reinterpret_cast<int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n\tconst int WARP_SIZE = 32;\n    const int M = 16;\n    const int K = 16;\n    const int N = 16;\n    const int WMMA_M = 16;\n    const int WMMA_K = 16;\n    const int WMMA_N = 16;\n    const int M_GLOBAL = 4096;\n    const int K_GLOBAL = 1024;\n    const int N_GLOBAL = 4096;\n    const float SPARSITY = 0.75;\n    const int K_GLOBAL_SPARSE = (int(K_GLOBAL * (1-SPARSITY)));\n    const int M_TILES = (M_GLOBAL / M);\n    const int N_TILES = (N_GLOBAL / N);\n    const int K_TILES = (K_GLOBAL / K);\n    const int BLOCK_ROW_WARPS = 2;\n    const int BLOCK_COL_WARPS = 4;\n    const int WARP_ROW_TILES = 4;\n    const int WARP_COL_TILES = 2;\n    const int SKEW_UINT8 = 16;\n    const int BANK_VAL = 32;\n    const int NUM_BANK = (K_GLOBAL / BANK_VAL);\n    /////////// BLOCK_ROW_TILES <= N_TILES ////////////\n    // const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    // ////////// BLOCK_COL_TILES <= M_TILES ////////////\n    // const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    auto C_LAYOUT = wmma::mem_col_major;\n    const int CHUNK_K = 64;\n    const int CHUNK_K_SPARSE = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K_SPARSE = (int((CHUNK_K * K) * (1 - SPARSITY)));\n    const int WARP_COPY_BYTES = (WARP_SIZE * sizeof(int4));\n    const int CHUNK_LINE_BYTES_A (BLOCK_COL_TILES * M * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_A = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_A);\n    const int CHUNK_COPY_LINE_LANES_A = (CHUNK_LINE_BYTES_A / sizeof(int4));\n    const int SHARED_OFFSET_A = (BLOCK_COL_TILES * M + SKEW_UINT8);\n\n    const int CHUNK_LINE_BYTES_B = (BLOCK_SIZE_K_SPARSE * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_B = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_B);\n    const int CHUNK_COPY_LINE_LANES_B = (CHUNK_LINE_BYTES_B / sizeof(int4));\n    const int SHARED_OFFSET_B = (BLOCK_SIZE_K_SPARSE + SKEW_UINT8);\n    const int SHARED_TO_GLOBAL_BYTES_PER_LINE = ((WARP_COL_TILES * M) * sizeof(int));\n    const int SHARED_TO_GLOBAL_BYTES_PER_WARP = (WARP_SIZE * sizeof(int));\n    const int SHARED_TO_GLOBAL_LINES_PER_WARP = (SHARED_TO_GLOBAL_BYTES_PER_WARP / SHARED_TO_GLOBAL_BYTES_PER_LINE);\n    const int SHARED_TO_GLOBAL_LANES_PER_LINE = (WARP_SIZE / SHARED_TO_GLOBAL_LINES_PER_WARP);\n    const int SHARED_TO_GLOBAL_ITERS = ((WARP_ROW_TILES * N) / SHARED_TO_GLOBAL_LINES_PER_WARP);\n\n    const int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const int WARP_STRIDE = (WARP_COL_TILES * M);\n    const int WARPS_PER_BLOCK = (BLOCK_ROW_WARPS * BLOCK_COL_WARPS);\n    const int THREADS_PER_BLOCK = (WARP_SIZE * WARPS_PER_BLOCK);\n    const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    const int GLOBAL_MEM_STRIDE = M_GLOBAL;\n    const int SHMEM_STRIDE = (M * BLOCK_COL_TILES);\n    const int SHMEM_OFFSET = (M * WARP_COL_TILES);\n    const int BLOCK_SIZE_M = (M * BLOCK_COL_TILES);\n    const int BLOCK_SIZE_N = (N * BLOCK_ROW_TILES);\n\n   extern __shared__ uint8_t shmem[];\n\n\t// Warp and lane identification.\n\tconst unsigned int warpId = threadIdx.x / WARP_SIZE;\n\tconst unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n\n\t// Offset in shared memory from which the B matrix is stored.\n\t// const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\tconst size_t shmem_idx_b_off = BLOCK_SIZE_K_SPARSE * SHARED_OFFSET_A;\n\n\n\t// Each CTA slides along the 128 x 128 tiles from the top left corner of the\n\t// matrix to the right and down, and selects the next tile to compute. Once\n\t// there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n\tconst unsigned int block_tile_i =\n\t\t((block_pos * BLOCK_COL_TILES) / M_TILES) * (BLOCK_ROW_TILES);\n\tconst unsigned int block_tile_j = (block_pos * BLOCK_COL_TILES) % M_TILES;\n\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n\twmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_ROW_TILES]\n\t\t\t\t\t\t\t\t\t\t\t\t\t [WARP_COL_TILES];\n\n    // Load the C matrix tiles into fragments from shared memory.\n\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t\twmma::fill_fragment(c[i][j], 0);\n\t\t}\n\t}\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    //for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n    for(int tile_k_idx_sparse = 0, tile_k_idx = 0; tile_k_idx_sparse < K_GLOBAL_SPARSE; tile_k_idx_sparse += BLOCK_SIZE_K_SPARSE, tile_k_idx += BLOCK_SIZE_K){\n\n\t\tsize_t shmem_idx = \n\t\twarpId < (WARPS_PER_BLOCK / 2)\n\t\t\t? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A\n\t\t\t: (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B + shmem_idx_b_off;\n\n\t\tint4 *lane_ptr = NULL;\n\t\tint *lane_ptr_index = NULL;\n\t\tconst uint8_t *warp_ptr = NULL;\n\n\n\t\tif(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\t//warp_ptr = &A[block_tile_j * M] +\n\t\t\t//\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * M_GLOBAL;\n\t\t\twarp_ptr = &A[block_tile_j * M];\n\t\t\t\n\t\t\tconst int *warp_ptr_index = &B_index[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t\t\t\t\t\t((warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A);\n\n\t\t\tlane_ptr_index = (int *)(warp_ptr_index + tile_k_idx_sparse + (laneId / CHUNK_COPY_LINE_LANES_A));\n\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_A) * SHARED_OFFSET_A;\n\t\t}else{\n\t\t\twarp_ptr = &B[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * K_GLOBAL_SPARSE;\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_sparse +\n\t\t\t\t\t\t\t\t(laneId / CHUNK_COPY_LINE_LANES_B) * K_GLOBAL_SPARSE) +\n\t\t\t\t\t\t\t\t(laneId % CHUNK_COPY_LINE_LANES_B);\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_B) * SHARED_OFFSET_B;\n\t\t}\n\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      // shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n\t  int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n\t  \t? BLOCK_SIZE_K_SPARSE / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A)\n\t\t: BLOCK_SIZE_N / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\n\t  /*\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\t  */\n\n\t  /*\n      int tile_k_idx_A;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n          tile_k_idx_A = *(lane_ptr_index);\n      }\n\t  */\n\n\t  #pragma unroll\n\t  for(int i = 0; i < iter_index; i += 1){\n\t\t  if(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\tint tile_k_idx_A = *(lane_ptr_index);\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_A * M_GLOBAL) + (laneId % CHUNK_COPY_LINE_LANES_A);\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_A)) =\n\t\t\t\t*lane_ptr;\n\t\t\t//warp_ptr = (uint8_t *)((uint8_t *)warp_ptr + M_GLOBAL * (WARPS_PER_BLOCK / 2) *CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tlane_ptr_index = (int *)((int *)lane_ptr_index +  (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A;\n\t\t  }else{\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_B)) =\n\t\t\t\t*lane_ptr;\n\t\t\tlane_ptr = (int4 *)((uint8_t *)lane_ptr + K_GLOBAL_SPARSE * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B;\n\t\t  }\n\t  }\n\n      __syncthreads();\n\n\t#pragma unroll\n      for (int k_step = 0; k_step < CHUNK_K_SPARSE; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::col_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n\t#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i += 1) {\n\t\t\tsize_t shmem_idx_a = (warpId % BLOCK_COL_WARPS) * M * WARP_COL_TILES + (i * M);\n\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_a + k_step * K * SHARED_OFFSET_A;\n\n\t\t\twmma::load_matrix_sync(a[i], tile_ptr, SHARED_OFFSET_A);\n\t\t#pragma unroll\n\t\t\tfor(int j = 0; j < WARP_ROW_TILES; j += 1){\n\t\t\t\tif(i == 0){\n\t\t\t\t\tsize_t shmem_idx_b = shmem_idx_b_off +\n\t\t\t\t\t\t\t\t\t\t\t(warpId / BLOCK_COL_WARPS) * (WARP_ROW_TILES * N) * SHARED_OFFSET_B +\n\t\t\t\t\t\t\t\t\t\t\t(j * N) * SHARED_OFFSET_B;\n\t\t\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_b + k_step * K;\n\t\t\t\t\twmma::load_matrix_sync(b[j], tile_ptr, SHARED_OFFSET_B);\n\t\t\t\t}\n\t\t\t\twmma::mma_sync(c[j][i], a[i], b[j], c[j][i]);\n\t\t\t}\n\n        }\n      }\n\n      __syncthreads();\n    }\n\n    // This pointer is used to access the C and D matrix tiles this warp computes.\n\tint *shmem_warp_tile_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * N * WARP_ROW_TILES * SHMEM_STRIDE +\n\t(warpId % BLOCK_COL_WARPS) * SHMEM_OFFSET;\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t#pragma unroll\n\t\t\tfor(int t = 0; t < c[i][j].num_elements; t += 1){\n\t\t\t\tc[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n\t\t\t}\n\t\t\tint *tile_ptr = shmem_warp_tile_ptr + i * N * SHMEM_STRIDE + j * M;\n\t\t\twmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, C_LAYOUT);\n\t\t}\n\t}\n\n    __syncthreads();\n\n\tint *shmem_warp_stream_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N * SHMEM_STRIDE\n\t\t\t\t\t\t\t\t\t+ (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tconst size_t gmem_idx =\n\t\t(block_tile_i * N + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N) * GLOBAL_MEM_STRIDE +\n\t\tblock_tile_j * M + (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tuint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n\tint *shmem_lane_stream_ptr =\n\t\tshmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * SHMEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\t\n\tuint8_t *dst_gmem_lane_stream_ptr =\n\t\tdst_gmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * GLOBAL_MEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\n\tfor(int i = 0; i < WARP_ROW_TILES * N; i += SHARED_TO_GLOBAL_LINES_PER_WARP){\n\t\t*(dst_gmem_lane_stream_ptr + i * GLOBAL_MEM_STRIDE) = (uint8_t)(*(shmem_lane_stream_ptr + i * SHMEM_STRIDE));\n\t}\n\n\t__syncthreads();\n}\n", "launch_config": {"dimBlock": [1024, 1], "dimGrid": [256, 1]}}, "90": {"code": "\n__global__ void compute_gemm_imma(float *input0, float *input1, float *input2, float* input3, float* input4, float*input5, float* input6,\n                                  float *output0) {\n    // keys need to be replaced 4096, 1024, K_GLOBA_VALUEL, 0.75, 64\n\n\t//extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B = reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_index = reinterpret_cast<int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n\tconst int WARP_SIZE = 32;\n    const int M = 16;\n    const int K = 16;\n    const int N = 16;\n    const int WMMA_M = 16;\n    const int WMMA_K = 16;\n    const int WMMA_N = 16;\n    const int M_GLOBAL = 4096;\n    const int K_GLOBAL = 4096;\n    const int N_GLOBAL = 1024;\n    const float SPARSITY = 0.75;\n    const int K_GLOBAL_SPARSE = (int(K_GLOBAL * (1-SPARSITY)));\n    const int M_TILES = (M_GLOBAL / M);\n    const int N_TILES = (N_GLOBAL / N);\n    const int K_TILES = (K_GLOBAL / K);\n    const int BLOCK_ROW_WARPS = 2;\n    const int BLOCK_COL_WARPS = 4;\n    const int WARP_ROW_TILES = 4;\n    const int WARP_COL_TILES = 2;\n    const int SKEW_UINT8 = 16;\n    const int BANK_VAL = 32;\n    const int NUM_BANK = (K_GLOBAL / BANK_VAL);\n    /////////// BLOCK_ROW_TILES <= N_TILES ////////////\n    // const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    // ////////// BLOCK_COL_TILES <= M_TILES ////////////\n    // const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    auto C_LAYOUT = wmma::mem_col_major;\n    const int CHUNK_K = 64;\n    const int CHUNK_K_SPARSE = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K_SPARSE = (int((CHUNK_K * K) * (1 - SPARSITY)));\n    const int WARP_COPY_BYTES = (WARP_SIZE * sizeof(int4));\n    const int CHUNK_LINE_BYTES_A (BLOCK_COL_TILES * M * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_A = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_A);\n    const int CHUNK_COPY_LINE_LANES_A = (CHUNK_LINE_BYTES_A / sizeof(int4));\n    const int SHARED_OFFSET_A = (BLOCK_COL_TILES * M + SKEW_UINT8);\n\n    const int CHUNK_LINE_BYTES_B = (BLOCK_SIZE_K_SPARSE * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_B = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_B);\n    const int CHUNK_COPY_LINE_LANES_B = (CHUNK_LINE_BYTES_B / sizeof(int4));\n    const int SHARED_OFFSET_B = (BLOCK_SIZE_K_SPARSE + SKEW_UINT8);\n    const int SHARED_TO_GLOBAL_BYTES_PER_LINE = ((WARP_COL_TILES * M) * sizeof(int));\n    const int SHARED_TO_GLOBAL_BYTES_PER_WARP = (WARP_SIZE * sizeof(int));\n    const int SHARED_TO_GLOBAL_LINES_PER_WARP = (SHARED_TO_GLOBAL_BYTES_PER_WARP / SHARED_TO_GLOBAL_BYTES_PER_LINE);\n    const int SHARED_TO_GLOBAL_LANES_PER_LINE = (WARP_SIZE / SHARED_TO_GLOBAL_LINES_PER_WARP);\n    const int SHARED_TO_GLOBAL_ITERS = ((WARP_ROW_TILES * N) / SHARED_TO_GLOBAL_LINES_PER_WARP);\n\n    const int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const int WARP_STRIDE = (WARP_COL_TILES * M);\n    const int WARPS_PER_BLOCK = (BLOCK_ROW_WARPS * BLOCK_COL_WARPS);\n    const int THREADS_PER_BLOCK = (WARP_SIZE * WARPS_PER_BLOCK);\n    const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    const int GLOBAL_MEM_STRIDE = M_GLOBAL;\n    const int SHMEM_STRIDE = (M * BLOCK_COL_TILES);\n    const int SHMEM_OFFSET = (M * WARP_COL_TILES);\n    const int BLOCK_SIZE_M = (M * BLOCK_COL_TILES);\n    const int BLOCK_SIZE_N = (N * BLOCK_ROW_TILES);\n\n   extern __shared__ uint8_t shmem[];\n\n\t// Warp and lane identification.\n\tconst unsigned int warpId = threadIdx.x / WARP_SIZE;\n\tconst unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n\n\t// Offset in shared memory from which the B matrix is stored.\n\t// const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\tconst size_t shmem_idx_b_off = BLOCK_SIZE_K_SPARSE * SHARED_OFFSET_A;\n\n\n\t// Each CTA slides along the 128 x 128 tiles from the top left corner of the\n\t// matrix to the right and down, and selects the next tile to compute. Once\n\t// there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n\tconst unsigned int block_tile_i =\n\t\t((block_pos * BLOCK_COL_TILES) / M_TILES) * (BLOCK_ROW_TILES);\n\tconst unsigned int block_tile_j = (block_pos * BLOCK_COL_TILES) % M_TILES;\n\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n\twmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_ROW_TILES]\n\t\t\t\t\t\t\t\t\t\t\t\t\t [WARP_COL_TILES];\n\n    // Load the C matrix tiles into fragments from shared memory.\n\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t\twmma::fill_fragment(c[i][j], 0);\n\t\t}\n\t}\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    //for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n    for(int tile_k_idx_sparse = 0, tile_k_idx = 0; tile_k_idx_sparse < K_GLOBAL_SPARSE; tile_k_idx_sparse += BLOCK_SIZE_K_SPARSE, tile_k_idx += BLOCK_SIZE_K){\n\n\t\tsize_t shmem_idx = \n\t\twarpId < (WARPS_PER_BLOCK / 2)\n\t\t\t? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A\n\t\t\t: (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B + shmem_idx_b_off;\n\n\t\tint4 *lane_ptr = NULL;\n\t\tint *lane_ptr_index = NULL;\n\t\tconst uint8_t *warp_ptr = NULL;\n\n\n\t\tif(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\t//warp_ptr = &A[block_tile_j * M] +\n\t\t\t//\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * M_GLOBAL;\n\t\t\twarp_ptr = &A[block_tile_j * M];\n\t\t\t\n\t\t\tconst int *warp_ptr_index = &B_index[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t\t\t\t\t\t((warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A);\n\n\t\t\tlane_ptr_index = (int *)(warp_ptr_index + tile_k_idx_sparse + (laneId / CHUNK_COPY_LINE_LANES_A));\n\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_A) * SHARED_OFFSET_A;\n\t\t}else{\n\t\t\twarp_ptr = &B[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * K_GLOBAL_SPARSE;\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_sparse +\n\t\t\t\t\t\t\t\t(laneId / CHUNK_COPY_LINE_LANES_B) * K_GLOBAL_SPARSE) +\n\t\t\t\t\t\t\t\t(laneId % CHUNK_COPY_LINE_LANES_B);\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_B) * SHARED_OFFSET_B;\n\t\t}\n\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      // shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n\t  int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n\t  \t? BLOCK_SIZE_K_SPARSE / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A)\n\t\t: BLOCK_SIZE_N / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\n\t  /*\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\t  */\n\n\t  /*\n      int tile_k_idx_A;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n          tile_k_idx_A = *(lane_ptr_index);\n      }\n\t  */\n\n\t  #pragma unroll\n\t  for(int i = 0; i < iter_index; i += 1){\n\t\t  if(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\tint tile_k_idx_A = *(lane_ptr_index);\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_A * M_GLOBAL) + (laneId % CHUNK_COPY_LINE_LANES_A);\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_A)) =\n\t\t\t\t*lane_ptr;\n\t\t\t//warp_ptr = (uint8_t *)((uint8_t *)warp_ptr + M_GLOBAL * (WARPS_PER_BLOCK / 2) *CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tlane_ptr_index = (int *)((int *)lane_ptr_index +  (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A;\n\t\t  }else{\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_B)) =\n\t\t\t\t*lane_ptr;\n\t\t\tlane_ptr = (int4 *)((uint8_t *)lane_ptr + K_GLOBAL_SPARSE * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B;\n\t\t  }\n\t  }\n\n      __syncthreads();\n\n\t#pragma unroll\n      for (int k_step = 0; k_step < CHUNK_K_SPARSE; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::col_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n\t#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i += 1) {\n\t\t\tsize_t shmem_idx_a = (warpId % BLOCK_COL_WARPS) * M * WARP_COL_TILES + (i * M);\n\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_a + k_step * K * SHARED_OFFSET_A;\n\n\t\t\twmma::load_matrix_sync(a[i], tile_ptr, SHARED_OFFSET_A);\n\t\t#pragma unroll\n\t\t\tfor(int j = 0; j < WARP_ROW_TILES; j += 1){\n\t\t\t\tif(i == 0){\n\t\t\t\t\tsize_t shmem_idx_b = shmem_idx_b_off +\n\t\t\t\t\t\t\t\t\t\t\t(warpId / BLOCK_COL_WARPS) * (WARP_ROW_TILES * N) * SHARED_OFFSET_B +\n\t\t\t\t\t\t\t\t\t\t\t(j * N) * SHARED_OFFSET_B;\n\t\t\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_b + k_step * K;\n\t\t\t\t\twmma::load_matrix_sync(b[j], tile_ptr, SHARED_OFFSET_B);\n\t\t\t\t}\n\t\t\t\twmma::mma_sync(c[j][i], a[i], b[j], c[j][i]);\n\t\t\t}\n\n        }\n      }\n\n      __syncthreads();\n    }\n\n    // This pointer is used to access the C and D matrix tiles this warp computes.\n\tint *shmem_warp_tile_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * N * WARP_ROW_TILES * SHMEM_STRIDE +\n\t(warpId % BLOCK_COL_WARPS) * SHMEM_OFFSET;\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t#pragma unroll\n\t\t\tfor(int t = 0; t < c[i][j].num_elements; t += 1){\n\t\t\t\tc[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n\t\t\t}\n\t\t\tint *tile_ptr = shmem_warp_tile_ptr + i * N * SHMEM_STRIDE + j * M;\n\t\t\twmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, C_LAYOUT);\n\t\t}\n\t}\n\n    __syncthreads();\n\n\tint *shmem_warp_stream_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N * SHMEM_STRIDE\n\t\t\t\t\t\t\t\t\t+ (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tconst size_t gmem_idx =\n\t\t(block_tile_i * N + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N) * GLOBAL_MEM_STRIDE +\n\t\tblock_tile_j * M + (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tuint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n\tint *shmem_lane_stream_ptr =\n\t\tshmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * SHMEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\t\n\tuint8_t *dst_gmem_lane_stream_ptr =\n\t\tdst_gmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * GLOBAL_MEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\n\tfor(int i = 0; i < WARP_ROW_TILES * N; i += SHARED_TO_GLOBAL_LINES_PER_WARP){\n\t\t*(dst_gmem_lane_stream_ptr + i * GLOBAL_MEM_STRIDE) = (uint8_t)(*(shmem_lane_stream_ptr + i * SHMEM_STRIDE));\n\t}\n\n\t__syncthreads();\n}\n", "launch_config": {"dimBlock": [256, 1], "dimGrid": [256, 1]}}, "91": {"code": "\n__global__ void compute_gemm_imma(float *input0, float *input1, float *input2, float* input3, float* input4, float*input5, float* input6,\n                                  float *output0) {\n    // keys need to be replaced 4096, 1024, K_GLOBA_VALUEL, 0.75, 64\n\n\t//extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B = reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_index = reinterpret_cast<int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n\tconst int WARP_SIZE = 32;\n    const int M = 16;\n    const int K = 16;\n    const int N = 16;\n    const int WMMA_M = 16;\n    const int WMMA_K = 16;\n    const int WMMA_N = 16;\n    const int M_GLOBAL = 4096;\n    const int K_GLOBAL = 1024;\n    const int N_GLOBAL = 1024;\n    const float SPARSITY = 0.75;\n    const int K_GLOBAL_SPARSE = (int(K_GLOBAL * (1-SPARSITY)));\n    const int M_TILES = (M_GLOBAL / M);\n    const int N_TILES = (N_GLOBAL / N);\n    const int K_TILES = (K_GLOBAL / K);\n    const int BLOCK_ROW_WARPS = 2;\n    const int BLOCK_COL_WARPS = 4;\n    const int WARP_ROW_TILES = 4;\n    const int WARP_COL_TILES = 2;\n    const int SKEW_UINT8 = 16;\n    const int BANK_VAL = 32;\n    const int NUM_BANK = (K_GLOBAL / BANK_VAL);\n    /////////// BLOCK_ROW_TILES <= N_TILES ////////////\n    // const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    // ////////// BLOCK_COL_TILES <= M_TILES ////////////\n    // const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    auto C_LAYOUT = wmma::mem_col_major;\n    const int CHUNK_K = 64;\n    const int CHUNK_K_SPARSE = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K_SPARSE = (int((CHUNK_K * K) * (1 - SPARSITY)));\n    const int WARP_COPY_BYTES = (WARP_SIZE * sizeof(int4));\n    const int CHUNK_LINE_BYTES_A (BLOCK_COL_TILES * M * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_A = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_A);\n    const int CHUNK_COPY_LINE_LANES_A = (CHUNK_LINE_BYTES_A / sizeof(int4));\n    const int SHARED_OFFSET_A = (BLOCK_COL_TILES * M + SKEW_UINT8);\n\n    const int CHUNK_LINE_BYTES_B = (BLOCK_SIZE_K_SPARSE * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_B = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_B);\n    const int CHUNK_COPY_LINE_LANES_B = (CHUNK_LINE_BYTES_B / sizeof(int4));\n    const int SHARED_OFFSET_B = (BLOCK_SIZE_K_SPARSE + SKEW_UINT8);\n    const int SHARED_TO_GLOBAL_BYTES_PER_LINE = ((WARP_COL_TILES * M) * sizeof(int));\n    const int SHARED_TO_GLOBAL_BYTES_PER_WARP = (WARP_SIZE * sizeof(int));\n    const int SHARED_TO_GLOBAL_LINES_PER_WARP = (SHARED_TO_GLOBAL_BYTES_PER_WARP / SHARED_TO_GLOBAL_BYTES_PER_LINE);\n    const int SHARED_TO_GLOBAL_LANES_PER_LINE = (WARP_SIZE / SHARED_TO_GLOBAL_LINES_PER_WARP);\n    const int SHARED_TO_GLOBAL_ITERS = ((WARP_ROW_TILES * N) / SHARED_TO_GLOBAL_LINES_PER_WARP);\n\n    const int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const int WARP_STRIDE = (WARP_COL_TILES * M);\n    const int WARPS_PER_BLOCK = (BLOCK_ROW_WARPS * BLOCK_COL_WARPS);\n    const int THREADS_PER_BLOCK = (WARP_SIZE * WARPS_PER_BLOCK);\n    const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    const int GLOBAL_MEM_STRIDE = M_GLOBAL;\n    const int SHMEM_STRIDE = (M * BLOCK_COL_TILES);\n    const int SHMEM_OFFSET = (M * WARP_COL_TILES);\n    const int BLOCK_SIZE_M = (M * BLOCK_COL_TILES);\n    const int BLOCK_SIZE_N = (N * BLOCK_ROW_TILES);\n\n   extern __shared__ uint8_t shmem[];\n\n\t// Warp and lane identification.\n\tconst unsigned int warpId = threadIdx.x / WARP_SIZE;\n\tconst unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n\n\t// Offset in shared memory from which the B matrix is stored.\n\t// const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\tconst size_t shmem_idx_b_off = BLOCK_SIZE_K_SPARSE * SHARED_OFFSET_A;\n\n\n\t// Each CTA slides along the 128 x 128 tiles from the top left corner of the\n\t// matrix to the right and down, and selects the next tile to compute. Once\n\t// there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n\tconst unsigned int block_tile_i =\n\t\t((block_pos * BLOCK_COL_TILES) / M_TILES) * (BLOCK_ROW_TILES);\n\tconst unsigned int block_tile_j = (block_pos * BLOCK_COL_TILES) % M_TILES;\n\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n\twmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_ROW_TILES]\n\t\t\t\t\t\t\t\t\t\t\t\t\t [WARP_COL_TILES];\n\n    // Load the C matrix tiles into fragments from shared memory.\n\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t\twmma::fill_fragment(c[i][j], 0);\n\t\t}\n\t}\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    //for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n    for(int tile_k_idx_sparse = 0, tile_k_idx = 0; tile_k_idx_sparse < K_GLOBAL_SPARSE; tile_k_idx_sparse += BLOCK_SIZE_K_SPARSE, tile_k_idx += BLOCK_SIZE_K){\n\n\t\tsize_t shmem_idx = \n\t\twarpId < (WARPS_PER_BLOCK / 2)\n\t\t\t? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A\n\t\t\t: (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B + shmem_idx_b_off;\n\n\t\tint4 *lane_ptr = NULL;\n\t\tint *lane_ptr_index = NULL;\n\t\tconst uint8_t *warp_ptr = NULL;\n\n\n\t\tif(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\t//warp_ptr = &A[block_tile_j * M] +\n\t\t\t//\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * M_GLOBAL;\n\t\t\twarp_ptr = &A[block_tile_j * M];\n\t\t\t\n\t\t\tconst int *warp_ptr_index = &B_index[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t\t\t\t\t\t((warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A);\n\n\t\t\tlane_ptr_index = (int *)(warp_ptr_index + tile_k_idx_sparse + (laneId / CHUNK_COPY_LINE_LANES_A));\n\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_A) * SHARED_OFFSET_A;\n\t\t}else{\n\t\t\twarp_ptr = &B[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * K_GLOBAL_SPARSE;\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_sparse +\n\t\t\t\t\t\t\t\t(laneId / CHUNK_COPY_LINE_LANES_B) * K_GLOBAL_SPARSE) +\n\t\t\t\t\t\t\t\t(laneId % CHUNK_COPY_LINE_LANES_B);\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_B) * SHARED_OFFSET_B;\n\t\t}\n\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      // shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n\t  int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n\t  \t? BLOCK_SIZE_K_SPARSE / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A)\n\t\t: BLOCK_SIZE_N / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\n\t  /*\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\t  */\n\n\t  /*\n      int tile_k_idx_A;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n          tile_k_idx_A = *(lane_ptr_index);\n      }\n\t  */\n\n\t  #pragma unroll\n\t  for(int i = 0; i < iter_index; i += 1){\n\t\t  if(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\tint tile_k_idx_A = *(lane_ptr_index);\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_A * M_GLOBAL) + (laneId % CHUNK_COPY_LINE_LANES_A);\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_A)) =\n\t\t\t\t*lane_ptr;\n\t\t\t//warp_ptr = (uint8_t *)((uint8_t *)warp_ptr + M_GLOBAL * (WARPS_PER_BLOCK / 2) *CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tlane_ptr_index = (int *)((int *)lane_ptr_index +  (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A;\n\t\t  }else{\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_B)) =\n\t\t\t\t*lane_ptr;\n\t\t\tlane_ptr = (int4 *)((uint8_t *)lane_ptr + K_GLOBAL_SPARSE * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B;\n\t\t  }\n\t  }\n\n      __syncthreads();\n\n\t#pragma unroll\n      for (int k_step = 0; k_step < CHUNK_K_SPARSE; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::col_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n\t#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i += 1) {\n\t\t\tsize_t shmem_idx_a = (warpId % BLOCK_COL_WARPS) * M * WARP_COL_TILES + (i * M);\n\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_a + k_step * K * SHARED_OFFSET_A;\n\n\t\t\twmma::load_matrix_sync(a[i], tile_ptr, SHARED_OFFSET_A);\n\t\t#pragma unroll\n\t\t\tfor(int j = 0; j < WARP_ROW_TILES; j += 1){\n\t\t\t\tif(i == 0){\n\t\t\t\t\tsize_t shmem_idx_b = shmem_idx_b_off +\n\t\t\t\t\t\t\t\t\t\t\t(warpId / BLOCK_COL_WARPS) * (WARP_ROW_TILES * N) * SHARED_OFFSET_B +\n\t\t\t\t\t\t\t\t\t\t\t(j * N) * SHARED_OFFSET_B;\n\t\t\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_b + k_step * K;\n\t\t\t\t\twmma::load_matrix_sync(b[j], tile_ptr, SHARED_OFFSET_B);\n\t\t\t\t}\n\t\t\t\twmma::mma_sync(c[j][i], a[i], b[j], c[j][i]);\n\t\t\t}\n\n        }\n      }\n\n      __syncthreads();\n    }\n\n    // This pointer is used to access the C and D matrix tiles this warp computes.\n\tint *shmem_warp_tile_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * N * WARP_ROW_TILES * SHMEM_STRIDE +\n\t(warpId % BLOCK_COL_WARPS) * SHMEM_OFFSET;\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t#pragma unroll\n\t\t\tfor(int t = 0; t < c[i][j].num_elements; t += 1){\n\t\t\t\tc[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n\t\t\t}\n\t\t\tint *tile_ptr = shmem_warp_tile_ptr + i * N * SHMEM_STRIDE + j * M;\n\t\t\twmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, C_LAYOUT);\n\t\t}\n\t}\n\n    __syncthreads();\n\n\tint *shmem_warp_stream_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N * SHMEM_STRIDE\n\t\t\t\t\t\t\t\t\t+ (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tconst size_t gmem_idx =\n\t\t(block_tile_i * N + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N) * GLOBAL_MEM_STRIDE +\n\t\tblock_tile_j * M + (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tuint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n\tint *shmem_lane_stream_ptr =\n\t\tshmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * SHMEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\t\n\tuint8_t *dst_gmem_lane_stream_ptr =\n\t\tdst_gmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * GLOBAL_MEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\n\tfor(int i = 0; i < WARP_ROW_TILES * N; i += SHARED_TO_GLOBAL_LINES_PER_WARP){\n\t\t*(dst_gmem_lane_stream_ptr + i * GLOBAL_MEM_STRIDE) = (uint8_t)(*(shmem_lane_stream_ptr + i * SHMEM_STRIDE));\n\t}\n\n\t__syncthreads();\n}\n", "launch_config": {"dimBlock": [256, 1], "dimGrid": [256, 1]}}, "92": {"code": "\n__global__ void compute_gemm_imma(float *input0, float *input1, float *input2, float* input3, float* input4, float*input5, float* input6,\n                                  float *output0) {\n    // keys need to be replaced 4096, 1024, K_GLOBA_VALUEL, 0.75, 64\n\n\t//extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B = reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_index = reinterpret_cast<int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n\tconst int WARP_SIZE = 32;\n    const int M = 16;\n    const int K = 16;\n    const int N = 16;\n    const int WMMA_M = 16;\n    const int WMMA_K = 16;\n    const int WMMA_N = 16;\n    const int M_GLOBAL = 4096;\n    const int K_GLOBAL = 1024;\n    const int N_GLOBAL = 1024;\n    const float SPARSITY = 0.75;\n    const int K_GLOBAL_SPARSE = (int(K_GLOBAL * (1-SPARSITY)));\n    const int M_TILES = (M_GLOBAL / M);\n    const int N_TILES = (N_GLOBAL / N);\n    const int K_TILES = (K_GLOBAL / K);\n    const int BLOCK_ROW_WARPS = 2;\n    const int BLOCK_COL_WARPS = 4;\n    const int WARP_ROW_TILES = 4;\n    const int WARP_COL_TILES = 2;\n    const int SKEW_UINT8 = 16;\n    const int BANK_VAL = 32;\n    const int NUM_BANK = (K_GLOBAL / BANK_VAL);\n    /////////// BLOCK_ROW_TILES <= N_TILES ////////////\n    // const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    // ////////// BLOCK_COL_TILES <= M_TILES ////////////\n    // const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    auto C_LAYOUT = wmma::mem_col_major;\n    const int CHUNK_K = 64;\n    const int CHUNK_K_SPARSE = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K_SPARSE = (int((CHUNK_K * K) * (1 - SPARSITY)));\n    const int WARP_COPY_BYTES = (WARP_SIZE * sizeof(int4));\n    const int CHUNK_LINE_BYTES_A (BLOCK_COL_TILES * M * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_A = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_A);\n    const int CHUNK_COPY_LINE_LANES_A = (CHUNK_LINE_BYTES_A / sizeof(int4));\n    const int SHARED_OFFSET_A = (BLOCK_COL_TILES * M + SKEW_UINT8);\n\n    const int CHUNK_LINE_BYTES_B = (BLOCK_SIZE_K_SPARSE * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_B = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_B);\n    const int CHUNK_COPY_LINE_LANES_B = (CHUNK_LINE_BYTES_B / sizeof(int4));\n    const int SHARED_OFFSET_B = (BLOCK_SIZE_K_SPARSE + SKEW_UINT8);\n    const int SHARED_TO_GLOBAL_BYTES_PER_LINE = ((WARP_COL_TILES * M) * sizeof(int));\n    const int SHARED_TO_GLOBAL_BYTES_PER_WARP = (WARP_SIZE * sizeof(int));\n    const int SHARED_TO_GLOBAL_LINES_PER_WARP = (SHARED_TO_GLOBAL_BYTES_PER_WARP / SHARED_TO_GLOBAL_BYTES_PER_LINE);\n    const int SHARED_TO_GLOBAL_LANES_PER_LINE = (WARP_SIZE / SHARED_TO_GLOBAL_LINES_PER_WARP);\n    const int SHARED_TO_GLOBAL_ITERS = ((WARP_ROW_TILES * N) / SHARED_TO_GLOBAL_LINES_PER_WARP);\n\n    const int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const int WARP_STRIDE = (WARP_COL_TILES * M);\n    const int WARPS_PER_BLOCK = (BLOCK_ROW_WARPS * BLOCK_COL_WARPS);\n    const int THREADS_PER_BLOCK = (WARP_SIZE * WARPS_PER_BLOCK);\n    const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    const int GLOBAL_MEM_STRIDE = M_GLOBAL;\n    const int SHMEM_STRIDE = (M * BLOCK_COL_TILES);\n    const int SHMEM_OFFSET = (M * WARP_COL_TILES);\n    const int BLOCK_SIZE_M = (M * BLOCK_COL_TILES);\n    const int BLOCK_SIZE_N = (N * BLOCK_ROW_TILES);\n\n   extern __shared__ uint8_t shmem[];\n\n\t// Warp and lane identification.\n\tconst unsigned int warpId = threadIdx.x / WARP_SIZE;\n\tconst unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n\n\t// Offset in shared memory from which the B matrix is stored.\n\t// const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\tconst size_t shmem_idx_b_off = BLOCK_SIZE_K_SPARSE * SHARED_OFFSET_A;\n\n\n\t// Each CTA slides along the 128 x 128 tiles from the top left corner of the\n\t// matrix to the right and down, and selects the next tile to compute. Once\n\t// there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n\tconst unsigned int block_tile_i =\n\t\t((block_pos * BLOCK_COL_TILES) / M_TILES) * (BLOCK_ROW_TILES);\n\tconst unsigned int block_tile_j = (block_pos * BLOCK_COL_TILES) % M_TILES;\n\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n\twmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_ROW_TILES]\n\t\t\t\t\t\t\t\t\t\t\t\t\t [WARP_COL_TILES];\n\n    // Load the C matrix tiles into fragments from shared memory.\n\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t\twmma::fill_fragment(c[i][j], 0);\n\t\t}\n\t}\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    //for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n    for(int tile_k_idx_sparse = 0, tile_k_idx = 0; tile_k_idx_sparse < K_GLOBAL_SPARSE; tile_k_idx_sparse += BLOCK_SIZE_K_SPARSE, tile_k_idx += BLOCK_SIZE_K){\n\n\t\tsize_t shmem_idx = \n\t\twarpId < (WARPS_PER_BLOCK / 2)\n\t\t\t? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A\n\t\t\t: (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B + shmem_idx_b_off;\n\n\t\tint4 *lane_ptr = NULL;\n\t\tint *lane_ptr_index = NULL;\n\t\tconst uint8_t *warp_ptr = NULL;\n\n\n\t\tif(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\t//warp_ptr = &A[block_tile_j * M] +\n\t\t\t//\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * M_GLOBAL;\n\t\t\twarp_ptr = &A[block_tile_j * M];\n\t\t\t\n\t\t\tconst int *warp_ptr_index = &B_index[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t\t\t\t\t\t((warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A);\n\n\t\t\tlane_ptr_index = (int *)(warp_ptr_index + tile_k_idx_sparse + (laneId / CHUNK_COPY_LINE_LANES_A));\n\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_A) * SHARED_OFFSET_A;\n\t\t}else{\n\t\t\twarp_ptr = &B[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * K_GLOBAL_SPARSE;\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_sparse +\n\t\t\t\t\t\t\t\t(laneId / CHUNK_COPY_LINE_LANES_B) * K_GLOBAL_SPARSE) +\n\t\t\t\t\t\t\t\t(laneId % CHUNK_COPY_LINE_LANES_B);\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_B) * SHARED_OFFSET_B;\n\t\t}\n\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      // shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n\t  int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n\t  \t? BLOCK_SIZE_K_SPARSE / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A)\n\t\t: BLOCK_SIZE_N / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\n\t  /*\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\t  */\n\n\t  /*\n      int tile_k_idx_A;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n          tile_k_idx_A = *(lane_ptr_index);\n      }\n\t  */\n\n\t  #pragma unroll\n\t  for(int i = 0; i < iter_index; i += 1){\n\t\t  if(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\tint tile_k_idx_A = *(lane_ptr_index);\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_A * M_GLOBAL) + (laneId % CHUNK_COPY_LINE_LANES_A);\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_A)) =\n\t\t\t\t*lane_ptr;\n\t\t\t//warp_ptr = (uint8_t *)((uint8_t *)warp_ptr + M_GLOBAL * (WARPS_PER_BLOCK / 2) *CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tlane_ptr_index = (int *)((int *)lane_ptr_index +  (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A;\n\t\t  }else{\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_B)) =\n\t\t\t\t*lane_ptr;\n\t\t\tlane_ptr = (int4 *)((uint8_t *)lane_ptr + K_GLOBAL_SPARSE * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B;\n\t\t  }\n\t  }\n\n      __syncthreads();\n\n\t#pragma unroll\n      for (int k_step = 0; k_step < CHUNK_K_SPARSE; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::col_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n\t#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i += 1) {\n\t\t\tsize_t shmem_idx_a = (warpId % BLOCK_COL_WARPS) * M * WARP_COL_TILES + (i * M);\n\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_a + k_step * K * SHARED_OFFSET_A;\n\n\t\t\twmma::load_matrix_sync(a[i], tile_ptr, SHARED_OFFSET_A);\n\t\t#pragma unroll\n\t\t\tfor(int j = 0; j < WARP_ROW_TILES; j += 1){\n\t\t\t\tif(i == 0){\n\t\t\t\t\tsize_t shmem_idx_b = shmem_idx_b_off +\n\t\t\t\t\t\t\t\t\t\t\t(warpId / BLOCK_COL_WARPS) * (WARP_ROW_TILES * N) * SHARED_OFFSET_B +\n\t\t\t\t\t\t\t\t\t\t\t(j * N) * SHARED_OFFSET_B;\n\t\t\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_b + k_step * K;\n\t\t\t\t\twmma::load_matrix_sync(b[j], tile_ptr, SHARED_OFFSET_B);\n\t\t\t\t}\n\t\t\t\twmma::mma_sync(c[j][i], a[i], b[j], c[j][i]);\n\t\t\t}\n\n        }\n      }\n\n      __syncthreads();\n    }\n\n    // This pointer is used to access the C and D matrix tiles this warp computes.\n\tint *shmem_warp_tile_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * N * WARP_ROW_TILES * SHMEM_STRIDE +\n\t(warpId % BLOCK_COL_WARPS) * SHMEM_OFFSET;\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t#pragma unroll\n\t\t\tfor(int t = 0; t < c[i][j].num_elements; t += 1){\n\t\t\t\tc[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n\t\t\t}\n\t\t\tint *tile_ptr = shmem_warp_tile_ptr + i * N * SHMEM_STRIDE + j * M;\n\t\t\twmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, C_LAYOUT);\n\t\t}\n\t}\n\n    __syncthreads();\n\n\tint *shmem_warp_stream_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N * SHMEM_STRIDE\n\t\t\t\t\t\t\t\t\t+ (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tconst size_t gmem_idx =\n\t\t(block_tile_i * N + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N) * GLOBAL_MEM_STRIDE +\n\t\tblock_tile_j * M + (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tuint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n\tint *shmem_lane_stream_ptr =\n\t\tshmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * SHMEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\t\n\tuint8_t *dst_gmem_lane_stream_ptr =\n\t\tdst_gmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * GLOBAL_MEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\n\tfor(int i = 0; i < WARP_ROW_TILES * N; i += SHARED_TO_GLOBAL_LINES_PER_WARP){\n\t\t*(dst_gmem_lane_stream_ptr + i * GLOBAL_MEM_STRIDE) = (uint8_t)(*(shmem_lane_stream_ptr + i * SHMEM_STRIDE));\n\t}\n\n\t__syncthreads();\n}\n", "launch_config": {"dimBlock": [256, 1], "dimGrid": [256, 1]}}, "93": {"code": "\n__global__ void compute_gemm_imma(float *input0, float *input1, float *input2, float* input3, float* input4, float*input5, float* input6,\n                                  float *output0) {\n    // keys need to be replaced 4096, 1024, K_GLOBA_VALUEL, 0.75, 64\n\n\t//extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B = reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_index = reinterpret_cast<int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n\tconst int WARP_SIZE = 32;\n    const int M = 16;\n    const int K = 16;\n    const int N = 16;\n    const int WMMA_M = 16;\n    const int WMMA_K = 16;\n    const int WMMA_N = 16;\n    const int M_GLOBAL = 4096;\n    const int K_GLOBAL = 1024;\n    const int N_GLOBAL = 1024;\n    const float SPARSITY = 0.75;\n    const int K_GLOBAL_SPARSE = (int(K_GLOBAL * (1-SPARSITY)));\n    const int M_TILES = (M_GLOBAL / M);\n    const int N_TILES = (N_GLOBAL / N);\n    const int K_TILES = (K_GLOBAL / K);\n    const int BLOCK_ROW_WARPS = 2;\n    const int BLOCK_COL_WARPS = 4;\n    const int WARP_ROW_TILES = 4;\n    const int WARP_COL_TILES = 2;\n    const int SKEW_UINT8 = 16;\n    const int BANK_VAL = 32;\n    const int NUM_BANK = (K_GLOBAL / BANK_VAL);\n    /////////// BLOCK_ROW_TILES <= N_TILES ////////////\n    // const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    // ////////// BLOCK_COL_TILES <= M_TILES ////////////\n    // const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    auto C_LAYOUT = wmma::mem_col_major;\n    const int CHUNK_K = 64;\n    const int CHUNK_K_SPARSE = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K_SPARSE = (int((CHUNK_K * K) * (1 - SPARSITY)));\n    const int WARP_COPY_BYTES = (WARP_SIZE * sizeof(int4));\n    const int CHUNK_LINE_BYTES_A (BLOCK_COL_TILES * M * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_A = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_A);\n    const int CHUNK_COPY_LINE_LANES_A = (CHUNK_LINE_BYTES_A / sizeof(int4));\n    const int SHARED_OFFSET_A = (BLOCK_COL_TILES * M + SKEW_UINT8);\n\n    const int CHUNK_LINE_BYTES_B = (BLOCK_SIZE_K_SPARSE * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_B = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_B);\n    const int CHUNK_COPY_LINE_LANES_B = (CHUNK_LINE_BYTES_B / sizeof(int4));\n    const int SHARED_OFFSET_B = (BLOCK_SIZE_K_SPARSE + SKEW_UINT8);\n    const int SHARED_TO_GLOBAL_BYTES_PER_LINE = ((WARP_COL_TILES * M) * sizeof(int));\n    const int SHARED_TO_GLOBAL_BYTES_PER_WARP = (WARP_SIZE * sizeof(int));\n    const int SHARED_TO_GLOBAL_LINES_PER_WARP = (SHARED_TO_GLOBAL_BYTES_PER_WARP / SHARED_TO_GLOBAL_BYTES_PER_LINE);\n    const int SHARED_TO_GLOBAL_LANES_PER_LINE = (WARP_SIZE / SHARED_TO_GLOBAL_LINES_PER_WARP);\n    const int SHARED_TO_GLOBAL_ITERS = ((WARP_ROW_TILES * N) / SHARED_TO_GLOBAL_LINES_PER_WARP);\n\n    const int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const int WARP_STRIDE = (WARP_COL_TILES * M);\n    const int WARPS_PER_BLOCK = (BLOCK_ROW_WARPS * BLOCK_COL_WARPS);\n    const int THREADS_PER_BLOCK = (WARP_SIZE * WARPS_PER_BLOCK);\n    const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    const int GLOBAL_MEM_STRIDE = M_GLOBAL;\n    const int SHMEM_STRIDE = (M * BLOCK_COL_TILES);\n    const int SHMEM_OFFSET = (M * WARP_COL_TILES);\n    const int BLOCK_SIZE_M = (M * BLOCK_COL_TILES);\n    const int BLOCK_SIZE_N = (N * BLOCK_ROW_TILES);\n\n   extern __shared__ uint8_t shmem[];\n\n\t// Warp and lane identification.\n\tconst unsigned int warpId = threadIdx.x / WARP_SIZE;\n\tconst unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n\n\t// Offset in shared memory from which the B matrix is stored.\n\t// const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\tconst size_t shmem_idx_b_off = BLOCK_SIZE_K_SPARSE * SHARED_OFFSET_A;\n\n\n\t// Each CTA slides along the 128 x 128 tiles from the top left corner of the\n\t// matrix to the right and down, and selects the next tile to compute. Once\n\t// there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n\tconst unsigned int block_tile_i =\n\t\t((block_pos * BLOCK_COL_TILES) / M_TILES) * (BLOCK_ROW_TILES);\n\tconst unsigned int block_tile_j = (block_pos * BLOCK_COL_TILES) % M_TILES;\n\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n\twmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_ROW_TILES]\n\t\t\t\t\t\t\t\t\t\t\t\t\t [WARP_COL_TILES];\n\n    // Load the C matrix tiles into fragments from shared memory.\n\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t\twmma::fill_fragment(c[i][j], 0);\n\t\t}\n\t}\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    //for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n    for(int tile_k_idx_sparse = 0, tile_k_idx = 0; tile_k_idx_sparse < K_GLOBAL_SPARSE; tile_k_idx_sparse += BLOCK_SIZE_K_SPARSE, tile_k_idx += BLOCK_SIZE_K){\n\n\t\tsize_t shmem_idx = \n\t\twarpId < (WARPS_PER_BLOCK / 2)\n\t\t\t? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A\n\t\t\t: (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B + shmem_idx_b_off;\n\n\t\tint4 *lane_ptr = NULL;\n\t\tint *lane_ptr_index = NULL;\n\t\tconst uint8_t *warp_ptr = NULL;\n\n\n\t\tif(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\t//warp_ptr = &A[block_tile_j * M] +\n\t\t\t//\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * M_GLOBAL;\n\t\t\twarp_ptr = &A[block_tile_j * M];\n\t\t\t\n\t\t\tconst int *warp_ptr_index = &B_index[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t\t\t\t\t\t((warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A);\n\n\t\t\tlane_ptr_index = (int *)(warp_ptr_index + tile_k_idx_sparse + (laneId / CHUNK_COPY_LINE_LANES_A));\n\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_A) * SHARED_OFFSET_A;\n\t\t}else{\n\t\t\twarp_ptr = &B[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * K_GLOBAL_SPARSE;\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_sparse +\n\t\t\t\t\t\t\t\t(laneId / CHUNK_COPY_LINE_LANES_B) * K_GLOBAL_SPARSE) +\n\t\t\t\t\t\t\t\t(laneId % CHUNK_COPY_LINE_LANES_B);\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_B) * SHARED_OFFSET_B;\n\t\t}\n\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      // shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n\t  int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n\t  \t? BLOCK_SIZE_K_SPARSE / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A)\n\t\t: BLOCK_SIZE_N / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\n\t  /*\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\t  */\n\n\t  /*\n      int tile_k_idx_A;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n          tile_k_idx_A = *(lane_ptr_index);\n      }\n\t  */\n\n\t  #pragma unroll\n\t  for(int i = 0; i < iter_index; i += 1){\n\t\t  if(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\tint tile_k_idx_A = *(lane_ptr_index);\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_A * M_GLOBAL) + (laneId % CHUNK_COPY_LINE_LANES_A);\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_A)) =\n\t\t\t\t*lane_ptr;\n\t\t\t//warp_ptr = (uint8_t *)((uint8_t *)warp_ptr + M_GLOBAL * (WARPS_PER_BLOCK / 2) *CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tlane_ptr_index = (int *)((int *)lane_ptr_index +  (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A;\n\t\t  }else{\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_B)) =\n\t\t\t\t*lane_ptr;\n\t\t\tlane_ptr = (int4 *)((uint8_t *)lane_ptr + K_GLOBAL_SPARSE * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B;\n\t\t  }\n\t  }\n\n      __syncthreads();\n\n\t#pragma unroll\n      for (int k_step = 0; k_step < CHUNK_K_SPARSE; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::col_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n\t#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i += 1) {\n\t\t\tsize_t shmem_idx_a = (warpId % BLOCK_COL_WARPS) * M * WARP_COL_TILES + (i * M);\n\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_a + k_step * K * SHARED_OFFSET_A;\n\n\t\t\twmma::load_matrix_sync(a[i], tile_ptr, SHARED_OFFSET_A);\n\t\t#pragma unroll\n\t\t\tfor(int j = 0; j < WARP_ROW_TILES; j += 1){\n\t\t\t\tif(i == 0){\n\t\t\t\t\tsize_t shmem_idx_b = shmem_idx_b_off +\n\t\t\t\t\t\t\t\t\t\t\t(warpId / BLOCK_COL_WARPS) * (WARP_ROW_TILES * N) * SHARED_OFFSET_B +\n\t\t\t\t\t\t\t\t\t\t\t(j * N) * SHARED_OFFSET_B;\n\t\t\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_b + k_step * K;\n\t\t\t\t\twmma::load_matrix_sync(b[j], tile_ptr, SHARED_OFFSET_B);\n\t\t\t\t}\n\t\t\t\twmma::mma_sync(c[j][i], a[i], b[j], c[j][i]);\n\t\t\t}\n\n        }\n      }\n\n      __syncthreads();\n    }\n\n    // This pointer is used to access the C and D matrix tiles this warp computes.\n\tint *shmem_warp_tile_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * N * WARP_ROW_TILES * SHMEM_STRIDE +\n\t(warpId % BLOCK_COL_WARPS) * SHMEM_OFFSET;\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t#pragma unroll\n\t\t\tfor(int t = 0; t < c[i][j].num_elements; t += 1){\n\t\t\t\tc[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n\t\t\t}\n\t\t\tint *tile_ptr = shmem_warp_tile_ptr + i * N * SHMEM_STRIDE + j * M;\n\t\t\twmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, C_LAYOUT);\n\t\t}\n\t}\n\n    __syncthreads();\n\n\tint *shmem_warp_stream_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N * SHMEM_STRIDE\n\t\t\t\t\t\t\t\t\t+ (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tconst size_t gmem_idx =\n\t\t(block_tile_i * N + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N) * GLOBAL_MEM_STRIDE +\n\t\tblock_tile_j * M + (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tuint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n\tint *shmem_lane_stream_ptr =\n\t\tshmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * SHMEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\t\n\tuint8_t *dst_gmem_lane_stream_ptr =\n\t\tdst_gmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * GLOBAL_MEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\n\tfor(int i = 0; i < WARP_ROW_TILES * N; i += SHARED_TO_GLOBAL_LINES_PER_WARP){\n\t\t*(dst_gmem_lane_stream_ptr + i * GLOBAL_MEM_STRIDE) = (uint8_t)(*(shmem_lane_stream_ptr + i * SHMEM_STRIDE));\n\t}\n\n\t__syncthreads();\n}\n", "launch_config": {"dimBlock": [256, 1], "dimGrid": [256, 1]}}, "94": {"code": "\n__global__ void compute_gemm_imma(float *input0, float *input1, float *input2, float* input3, float* input4, float*input5, float* input6,\n                                  float *output0) {\n    // keys need to be replaced 4096, 1024, K_GLOBA_VALUEL, 0.75, 64\n\n\t//extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B = reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_index = reinterpret_cast<int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n\tconst int WARP_SIZE = 32;\n    const int M = 16;\n    const int K = 16;\n    const int N = 16;\n    const int WMMA_M = 16;\n    const int WMMA_K = 16;\n    const int WMMA_N = 16;\n    const int M_GLOBAL = 4096;\n    const int K_GLOBAL = 1024;\n    const int N_GLOBAL = 1024;\n    const float SPARSITY = 0.75;\n    const int K_GLOBAL_SPARSE = (int(K_GLOBAL * (1-SPARSITY)));\n    const int M_TILES = (M_GLOBAL / M);\n    const int N_TILES = (N_GLOBAL / N);\n    const int K_TILES = (K_GLOBAL / K);\n    const int BLOCK_ROW_WARPS = 2;\n    const int BLOCK_COL_WARPS = 4;\n    const int WARP_ROW_TILES = 4;\n    const int WARP_COL_TILES = 2;\n    const int SKEW_UINT8 = 16;\n    const int BANK_VAL = 32;\n    const int NUM_BANK = (K_GLOBAL / BANK_VAL);\n    /////////// BLOCK_ROW_TILES <= N_TILES ////////////\n    // const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    // ////////// BLOCK_COL_TILES <= M_TILES ////////////\n    // const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    auto C_LAYOUT = wmma::mem_col_major;\n    const int CHUNK_K = 64;\n    const int CHUNK_K_SPARSE = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K_SPARSE = (int((CHUNK_K * K) * (1 - SPARSITY)));\n    const int WARP_COPY_BYTES = (WARP_SIZE * sizeof(int4));\n    const int CHUNK_LINE_BYTES_A (BLOCK_COL_TILES * M * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_A = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_A);\n    const int CHUNK_COPY_LINE_LANES_A = (CHUNK_LINE_BYTES_A / sizeof(int4));\n    const int SHARED_OFFSET_A = (BLOCK_COL_TILES * M + SKEW_UINT8);\n\n    const int CHUNK_LINE_BYTES_B = (BLOCK_SIZE_K_SPARSE * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_B = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_B);\n    const int CHUNK_COPY_LINE_LANES_B = (CHUNK_LINE_BYTES_B / sizeof(int4));\n    const int SHARED_OFFSET_B = (BLOCK_SIZE_K_SPARSE + SKEW_UINT8);\n    const int SHARED_TO_GLOBAL_BYTES_PER_LINE = ((WARP_COL_TILES * M) * sizeof(int));\n    const int SHARED_TO_GLOBAL_BYTES_PER_WARP = (WARP_SIZE * sizeof(int));\n    const int SHARED_TO_GLOBAL_LINES_PER_WARP = (SHARED_TO_GLOBAL_BYTES_PER_WARP / SHARED_TO_GLOBAL_BYTES_PER_LINE);\n    const int SHARED_TO_GLOBAL_LANES_PER_LINE = (WARP_SIZE / SHARED_TO_GLOBAL_LINES_PER_WARP);\n    const int SHARED_TO_GLOBAL_ITERS = ((WARP_ROW_TILES * N) / SHARED_TO_GLOBAL_LINES_PER_WARP);\n\n    const int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const int WARP_STRIDE = (WARP_COL_TILES * M);\n    const int WARPS_PER_BLOCK = (BLOCK_ROW_WARPS * BLOCK_COL_WARPS);\n    const int THREADS_PER_BLOCK = (WARP_SIZE * WARPS_PER_BLOCK);\n    const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    const int GLOBAL_MEM_STRIDE = M_GLOBAL;\n    const int SHMEM_STRIDE = (M * BLOCK_COL_TILES);\n    const int SHMEM_OFFSET = (M * WARP_COL_TILES);\n    const int BLOCK_SIZE_M = (M * BLOCK_COL_TILES);\n    const int BLOCK_SIZE_N = (N * BLOCK_ROW_TILES);\n\n   extern __shared__ uint8_t shmem[];\n\n\t// Warp and lane identification.\n\tconst unsigned int warpId = threadIdx.x / WARP_SIZE;\n\tconst unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n\n\t// Offset in shared memory from which the B matrix is stored.\n\t// const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\tconst size_t shmem_idx_b_off = BLOCK_SIZE_K_SPARSE * SHARED_OFFSET_A;\n\n\n\t// Each CTA slides along the 128 x 128 tiles from the top left corner of the\n\t// matrix to the right and down, and selects the next tile to compute. Once\n\t// there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n\tconst unsigned int block_tile_i =\n\t\t((block_pos * BLOCK_COL_TILES) / M_TILES) * (BLOCK_ROW_TILES);\n\tconst unsigned int block_tile_j = (block_pos * BLOCK_COL_TILES) % M_TILES;\n\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n\twmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_ROW_TILES]\n\t\t\t\t\t\t\t\t\t\t\t\t\t [WARP_COL_TILES];\n\n    // Load the C matrix tiles into fragments from shared memory.\n\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t\twmma::fill_fragment(c[i][j], 0);\n\t\t}\n\t}\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    //for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n    for(int tile_k_idx_sparse = 0, tile_k_idx = 0; tile_k_idx_sparse < K_GLOBAL_SPARSE; tile_k_idx_sparse += BLOCK_SIZE_K_SPARSE, tile_k_idx += BLOCK_SIZE_K){\n\n\t\tsize_t shmem_idx = \n\t\twarpId < (WARPS_PER_BLOCK / 2)\n\t\t\t? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A\n\t\t\t: (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B + shmem_idx_b_off;\n\n\t\tint4 *lane_ptr = NULL;\n\t\tint *lane_ptr_index = NULL;\n\t\tconst uint8_t *warp_ptr = NULL;\n\n\n\t\tif(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\t//warp_ptr = &A[block_tile_j * M] +\n\t\t\t//\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * M_GLOBAL;\n\t\t\twarp_ptr = &A[block_tile_j * M];\n\t\t\t\n\t\t\tconst int *warp_ptr_index = &B_index[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t\t\t\t\t\t((warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A);\n\n\t\t\tlane_ptr_index = (int *)(warp_ptr_index + tile_k_idx_sparse + (laneId / CHUNK_COPY_LINE_LANES_A));\n\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_A) * SHARED_OFFSET_A;\n\t\t}else{\n\t\t\twarp_ptr = &B[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * K_GLOBAL_SPARSE;\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_sparse +\n\t\t\t\t\t\t\t\t(laneId / CHUNK_COPY_LINE_LANES_B) * K_GLOBAL_SPARSE) +\n\t\t\t\t\t\t\t\t(laneId % CHUNK_COPY_LINE_LANES_B);\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_B) * SHARED_OFFSET_B;\n\t\t}\n\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      // shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n\t  int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n\t  \t? BLOCK_SIZE_K_SPARSE / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A)\n\t\t: BLOCK_SIZE_N / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\n\t  /*\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\t  */\n\n\t  /*\n      int tile_k_idx_A;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n          tile_k_idx_A = *(lane_ptr_index);\n      }\n\t  */\n\n\t  #pragma unroll\n\t  for(int i = 0; i < iter_index; i += 1){\n\t\t  if(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\tint tile_k_idx_A = *(lane_ptr_index);\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_A * M_GLOBAL) + (laneId % CHUNK_COPY_LINE_LANES_A);\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_A)) =\n\t\t\t\t*lane_ptr;\n\t\t\t//warp_ptr = (uint8_t *)((uint8_t *)warp_ptr + M_GLOBAL * (WARPS_PER_BLOCK / 2) *CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tlane_ptr_index = (int *)((int *)lane_ptr_index +  (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A;\n\t\t  }else{\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_B)) =\n\t\t\t\t*lane_ptr;\n\t\t\tlane_ptr = (int4 *)((uint8_t *)lane_ptr + K_GLOBAL_SPARSE * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B;\n\t\t  }\n\t  }\n\n      __syncthreads();\n\n\t#pragma unroll\n      for (int k_step = 0; k_step < CHUNK_K_SPARSE; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::col_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n\t#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i += 1) {\n\t\t\tsize_t shmem_idx_a = (warpId % BLOCK_COL_WARPS) * M * WARP_COL_TILES + (i * M);\n\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_a + k_step * K * SHARED_OFFSET_A;\n\n\t\t\twmma::load_matrix_sync(a[i], tile_ptr, SHARED_OFFSET_A);\n\t\t#pragma unroll\n\t\t\tfor(int j = 0; j < WARP_ROW_TILES; j += 1){\n\t\t\t\tif(i == 0){\n\t\t\t\t\tsize_t shmem_idx_b = shmem_idx_b_off +\n\t\t\t\t\t\t\t\t\t\t\t(warpId / BLOCK_COL_WARPS) * (WARP_ROW_TILES * N) * SHARED_OFFSET_B +\n\t\t\t\t\t\t\t\t\t\t\t(j * N) * SHARED_OFFSET_B;\n\t\t\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_b + k_step * K;\n\t\t\t\t\twmma::load_matrix_sync(b[j], tile_ptr, SHARED_OFFSET_B);\n\t\t\t\t}\n\t\t\t\twmma::mma_sync(c[j][i], a[i], b[j], c[j][i]);\n\t\t\t}\n\n        }\n      }\n\n      __syncthreads();\n    }\n\n    // This pointer is used to access the C and D matrix tiles this warp computes.\n\tint *shmem_warp_tile_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * N * WARP_ROW_TILES * SHMEM_STRIDE +\n\t(warpId % BLOCK_COL_WARPS) * SHMEM_OFFSET;\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t#pragma unroll\n\t\t\tfor(int t = 0; t < c[i][j].num_elements; t += 1){\n\t\t\t\tc[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n\t\t\t}\n\t\t\tint *tile_ptr = shmem_warp_tile_ptr + i * N * SHMEM_STRIDE + j * M;\n\t\t\twmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, C_LAYOUT);\n\t\t}\n\t}\n\n    __syncthreads();\n\n\tint *shmem_warp_stream_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N * SHMEM_STRIDE\n\t\t\t\t\t\t\t\t\t+ (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tconst size_t gmem_idx =\n\t\t(block_tile_i * N + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N) * GLOBAL_MEM_STRIDE +\n\t\tblock_tile_j * M + (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tuint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n\tint *shmem_lane_stream_ptr =\n\t\tshmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * SHMEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\t\n\tuint8_t *dst_gmem_lane_stream_ptr =\n\t\tdst_gmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * GLOBAL_MEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\n\tfor(int i = 0; i < WARP_ROW_TILES * N; i += SHARED_TO_GLOBAL_LINES_PER_WARP){\n\t\t*(dst_gmem_lane_stream_ptr + i * GLOBAL_MEM_STRIDE) = (uint8_t)(*(shmem_lane_stream_ptr + i * SHMEM_STRIDE));\n\t}\n\n\t__syncthreads();\n}\n", "launch_config": {"dimBlock": [256, 1], "dimGrid": [256, 1]}}, "95": {"code": "\n__global__ void compute_gemm_imma(float *input0, float *input1, float *input2, float* input3, float* input4, float*input5, float* input6,\n                                  float *output0) {\n    // keys need to be replaced 4096, 4096, K_GLOBA_VALUEL, 0.75, 64\n\n\t//extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B = reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_index = reinterpret_cast<int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n\tconst int WARP_SIZE = 32;\n    const int M = 16;\n    const int K = 16;\n    const int N = 16;\n    const int WMMA_M = 16;\n    const int WMMA_K = 16;\n    const int WMMA_N = 16;\n    const int M_GLOBAL = 4096;\n    const int K_GLOBAL = 1024;\n    const int N_GLOBAL = 4096;\n    const float SPARSITY = 0.75;\n    const int K_GLOBAL_SPARSE = (int(K_GLOBAL * (1-SPARSITY)));\n    const int M_TILES = (M_GLOBAL / M);\n    const int N_TILES = (N_GLOBAL / N);\n    const int K_TILES = (K_GLOBAL / K);\n    const int BLOCK_ROW_WARPS = 2;\n    const int BLOCK_COL_WARPS = 4;\n    const int WARP_ROW_TILES = 4;\n    const int WARP_COL_TILES = 2;\n    const int SKEW_UINT8 = 16;\n    const int BANK_VAL = 32;\n    const int NUM_BANK = (K_GLOBAL / BANK_VAL);\n    /////////// BLOCK_ROW_TILES <= N_TILES ////////////\n    // const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    // ////////// BLOCK_COL_TILES <= M_TILES ////////////\n    // const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    auto C_LAYOUT = wmma::mem_col_major;\n    const int CHUNK_K = 64;\n    const int CHUNK_K_SPARSE = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K_SPARSE = (int((CHUNK_K * K) * (1 - SPARSITY)));\n    const int WARP_COPY_BYTES = (WARP_SIZE * sizeof(int4));\n    const int CHUNK_LINE_BYTES_A (BLOCK_COL_TILES * M * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_A = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_A);\n    const int CHUNK_COPY_LINE_LANES_A = (CHUNK_LINE_BYTES_A / sizeof(int4));\n    const int SHARED_OFFSET_A = (BLOCK_COL_TILES * M + SKEW_UINT8);\n\n    const int CHUNK_LINE_BYTES_B = (BLOCK_SIZE_K_SPARSE * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_B = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_B);\n    const int CHUNK_COPY_LINE_LANES_B = (CHUNK_LINE_BYTES_B / sizeof(int4));\n    const int SHARED_OFFSET_B = (BLOCK_SIZE_K_SPARSE + SKEW_UINT8);\n    const int SHARED_TO_GLOBAL_BYTES_PER_LINE = ((WARP_COL_TILES * M) * sizeof(int));\n    const int SHARED_TO_GLOBAL_BYTES_PER_WARP = (WARP_SIZE * sizeof(int));\n    const int SHARED_TO_GLOBAL_LINES_PER_WARP = (SHARED_TO_GLOBAL_BYTES_PER_WARP / SHARED_TO_GLOBAL_BYTES_PER_LINE);\n    const int SHARED_TO_GLOBAL_LANES_PER_LINE = (WARP_SIZE / SHARED_TO_GLOBAL_LINES_PER_WARP);\n    const int SHARED_TO_GLOBAL_ITERS = ((WARP_ROW_TILES * N) / SHARED_TO_GLOBAL_LINES_PER_WARP);\n\n    const int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const int WARP_STRIDE = (WARP_COL_TILES * M);\n    const int WARPS_PER_BLOCK = (BLOCK_ROW_WARPS * BLOCK_COL_WARPS);\n    const int THREADS_PER_BLOCK = (WARP_SIZE * WARPS_PER_BLOCK);\n    const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    const int GLOBAL_MEM_STRIDE = M_GLOBAL;\n    const int SHMEM_STRIDE = (M * BLOCK_COL_TILES);\n    const int SHMEM_OFFSET = (M * WARP_COL_TILES);\n    const int BLOCK_SIZE_M = (M * BLOCK_COL_TILES);\n    const int BLOCK_SIZE_N = (N * BLOCK_ROW_TILES);\n\n   extern __shared__ uint8_t shmem[];\n\n\t// Warp and lane identification.\n\tconst unsigned int warpId = threadIdx.x / WARP_SIZE;\n\tconst unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n\n\t// Offset in shared memory from which the B matrix is stored.\n\t// const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\tconst size_t shmem_idx_b_off = BLOCK_SIZE_K_SPARSE * SHARED_OFFSET_A;\n\n\n\t// Each CTA slides along the 128 x 128 tiles from the top left corner of the\n\t// matrix to the right and down, and selects the next tile to compute. Once\n\t// there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n\tconst unsigned int block_tile_i =\n\t\t((block_pos * BLOCK_COL_TILES) / M_TILES) * (BLOCK_ROW_TILES);\n\tconst unsigned int block_tile_j = (block_pos * BLOCK_COL_TILES) % M_TILES;\n\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n\twmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_ROW_TILES]\n\t\t\t\t\t\t\t\t\t\t\t\t\t [WARP_COL_TILES];\n\n    // Load the C matrix tiles into fragments from shared memory.\n\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t\twmma::fill_fragment(c[i][j], 0);\n\t\t}\n\t}\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    //for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n    for(int tile_k_idx_sparse = 0, tile_k_idx = 0; tile_k_idx_sparse < K_GLOBAL_SPARSE; tile_k_idx_sparse += BLOCK_SIZE_K_SPARSE, tile_k_idx += BLOCK_SIZE_K){\n\n\t\tsize_t shmem_idx = \n\t\twarpId < (WARPS_PER_BLOCK / 2)\n\t\t\t? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A\n\t\t\t: (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B + shmem_idx_b_off;\n\n\t\tint4 *lane_ptr = NULL;\n\t\tint *lane_ptr_index = NULL;\n\t\tconst uint8_t *warp_ptr = NULL;\n\n\n\t\tif(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\t//warp_ptr = &A[block_tile_j * M] +\n\t\t\t//\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * M_GLOBAL;\n\t\t\twarp_ptr = &A[block_tile_j * M];\n\t\t\t\n\t\t\tconst int *warp_ptr_index = &B_index[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t\t\t\t\t\t((warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A);\n\n\t\t\tlane_ptr_index = (int *)(warp_ptr_index + tile_k_idx_sparse + (laneId / CHUNK_COPY_LINE_LANES_A));\n\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_A) * SHARED_OFFSET_A;\n\t\t}else{\n\t\t\twarp_ptr = &B[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * K_GLOBAL_SPARSE;\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_sparse +\n\t\t\t\t\t\t\t\t(laneId / CHUNK_COPY_LINE_LANES_B) * K_GLOBAL_SPARSE) +\n\t\t\t\t\t\t\t\t(laneId % CHUNK_COPY_LINE_LANES_B);\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_B) * SHARED_OFFSET_B;\n\t\t}\n\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      // shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n\t  int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n\t  \t? BLOCK_SIZE_K_SPARSE / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A)\n\t\t: BLOCK_SIZE_N / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\n\t  /*\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\t  */\n\n\t  /*\n      int tile_k_idx_A;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n          tile_k_idx_A = *(lane_ptr_index);\n      }\n\t  */\n\n\t  #pragma unroll\n\t  for(int i = 0; i < iter_index; i += 1){\n\t\t  if(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\tint tile_k_idx_A = *(lane_ptr_index);\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_A * M_GLOBAL) + (laneId % CHUNK_COPY_LINE_LANES_A);\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_A)) =\n\t\t\t\t*lane_ptr;\n\t\t\t//warp_ptr = (uint8_t *)((uint8_t *)warp_ptr + M_GLOBAL * (WARPS_PER_BLOCK / 2) *CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tlane_ptr_index = (int *)((int *)lane_ptr_index +  (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A;\n\t\t  }else{\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_B)) =\n\t\t\t\t*lane_ptr;\n\t\t\tlane_ptr = (int4 *)((uint8_t *)lane_ptr + K_GLOBAL_SPARSE * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B;\n\t\t  }\n\t  }\n\n      __syncthreads();\n\n\t#pragma unroll\n      for (int k_step = 0; k_step < CHUNK_K_SPARSE; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::col_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n\t#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i += 1) {\n\t\t\tsize_t shmem_idx_a = (warpId % BLOCK_COL_WARPS) * M * WARP_COL_TILES + (i * M);\n\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_a + k_step * K * SHARED_OFFSET_A;\n\n\t\t\twmma::load_matrix_sync(a[i], tile_ptr, SHARED_OFFSET_A);\n\t\t#pragma unroll\n\t\t\tfor(int j = 0; j < WARP_ROW_TILES; j += 1){\n\t\t\t\tif(i == 0){\n\t\t\t\t\tsize_t shmem_idx_b = shmem_idx_b_off +\n\t\t\t\t\t\t\t\t\t\t\t(warpId / BLOCK_COL_WARPS) * (WARP_ROW_TILES * N) * SHARED_OFFSET_B +\n\t\t\t\t\t\t\t\t\t\t\t(j * N) * SHARED_OFFSET_B;\n\t\t\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_b + k_step * K;\n\t\t\t\t\twmma::load_matrix_sync(b[j], tile_ptr, SHARED_OFFSET_B);\n\t\t\t\t}\n\t\t\t\twmma::mma_sync(c[j][i], a[i], b[j], c[j][i]);\n\t\t\t}\n\n        }\n      }\n\n      __syncthreads();\n    }\n\n    // This pointer is used to access the C and D matrix tiles this warp computes.\n\tint *shmem_warp_tile_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * N * WARP_ROW_TILES * SHMEM_STRIDE +\n\t(warpId % BLOCK_COL_WARPS) * SHMEM_OFFSET;\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t#pragma unroll\n\t\t\tfor(int t = 0; t < c[i][j].num_elements; t += 1){\n\t\t\t\tc[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n\t\t\t}\n\t\t\tint *tile_ptr = shmem_warp_tile_ptr + i * N * SHMEM_STRIDE + j * M;\n\t\t\twmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, C_LAYOUT);\n\t\t}\n\t}\n\n    __syncthreads();\n\n\tint *shmem_warp_stream_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N * SHMEM_STRIDE\n\t\t\t\t\t\t\t\t\t+ (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tconst size_t gmem_idx =\n\t\t(block_tile_i * N + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N) * GLOBAL_MEM_STRIDE +\n\t\tblock_tile_j * M + (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tuint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n\tint *shmem_lane_stream_ptr =\n\t\tshmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * SHMEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\t\n\tuint8_t *dst_gmem_lane_stream_ptr =\n\t\tdst_gmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * GLOBAL_MEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\n\tfor(int i = 0; i < WARP_ROW_TILES * N; i += SHARED_TO_GLOBAL_LINES_PER_WARP){\n\t\t*(dst_gmem_lane_stream_ptr + i * GLOBAL_MEM_STRIDE) = (uint8_t)(*(shmem_lane_stream_ptr + i * SHMEM_STRIDE));\n\t}\n\n\t__syncthreads();\n}\n", "launch_config": {"dimBlock": [1024, 1], "dimGrid": [256, 1]}}, "96": {"code": "\n__global__ void compute_gemm_imma(float *input0, float *input1, float *input2, float* input3, float* input4, float*input5, float* input6,\n                                  float *output0) {\n    // keys need to be replaced 4096, 1024, K_GLOBA_VALUEL, 0.75, 64\n\n\t//extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B = reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_index = reinterpret_cast<int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n\tconst int WARP_SIZE = 32;\n    const int M = 16;\n    const int K = 16;\n    const int N = 16;\n    const int WMMA_M = 16;\n    const int WMMA_K = 16;\n    const int WMMA_N = 16;\n    const int M_GLOBAL = 4096;\n    const int K_GLOBAL = 4096;\n    const int N_GLOBAL = 1024;\n    const float SPARSITY = 0.75;\n    const int K_GLOBAL_SPARSE = (int(K_GLOBAL * (1-SPARSITY)));\n    const int M_TILES = (M_GLOBAL / M);\n    const int N_TILES = (N_GLOBAL / N);\n    const int K_TILES = (K_GLOBAL / K);\n    const int BLOCK_ROW_WARPS = 2;\n    const int BLOCK_COL_WARPS = 4;\n    const int WARP_ROW_TILES = 4;\n    const int WARP_COL_TILES = 2;\n    const int SKEW_UINT8 = 16;\n    const int BANK_VAL = 32;\n    const int NUM_BANK = (K_GLOBAL / BANK_VAL);\n    /////////// BLOCK_ROW_TILES <= N_TILES ////////////\n    // const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    // ////////// BLOCK_COL_TILES <= M_TILES ////////////\n    // const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    auto C_LAYOUT = wmma::mem_col_major;\n    const int CHUNK_K = 64;\n    const int CHUNK_K_SPARSE = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K_SPARSE = (int((CHUNK_K * K) * (1 - SPARSITY)));\n    const int WARP_COPY_BYTES = (WARP_SIZE * sizeof(int4));\n    const int CHUNK_LINE_BYTES_A (BLOCK_COL_TILES * M * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_A = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_A);\n    const int CHUNK_COPY_LINE_LANES_A = (CHUNK_LINE_BYTES_A / sizeof(int4));\n    const int SHARED_OFFSET_A = (BLOCK_COL_TILES * M + SKEW_UINT8);\n\n    const int CHUNK_LINE_BYTES_B = (BLOCK_SIZE_K_SPARSE * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_B = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_B);\n    const int CHUNK_COPY_LINE_LANES_B = (CHUNK_LINE_BYTES_B / sizeof(int4));\n    const int SHARED_OFFSET_B = (BLOCK_SIZE_K_SPARSE + SKEW_UINT8);\n    const int SHARED_TO_GLOBAL_BYTES_PER_LINE = ((WARP_COL_TILES * M) * sizeof(int));\n    const int SHARED_TO_GLOBAL_BYTES_PER_WARP = (WARP_SIZE * sizeof(int));\n    const int SHARED_TO_GLOBAL_LINES_PER_WARP = (SHARED_TO_GLOBAL_BYTES_PER_WARP / SHARED_TO_GLOBAL_BYTES_PER_LINE);\n    const int SHARED_TO_GLOBAL_LANES_PER_LINE = (WARP_SIZE / SHARED_TO_GLOBAL_LINES_PER_WARP);\n    const int SHARED_TO_GLOBAL_ITERS = ((WARP_ROW_TILES * N) / SHARED_TO_GLOBAL_LINES_PER_WARP);\n\n    const int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const int WARP_STRIDE = (WARP_COL_TILES * M);\n    const int WARPS_PER_BLOCK = (BLOCK_ROW_WARPS * BLOCK_COL_WARPS);\n    const int THREADS_PER_BLOCK = (WARP_SIZE * WARPS_PER_BLOCK);\n    const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    const int GLOBAL_MEM_STRIDE = M_GLOBAL;\n    const int SHMEM_STRIDE = (M * BLOCK_COL_TILES);\n    const int SHMEM_OFFSET = (M * WARP_COL_TILES);\n    const int BLOCK_SIZE_M = (M * BLOCK_COL_TILES);\n    const int BLOCK_SIZE_N = (N * BLOCK_ROW_TILES);\n\n   extern __shared__ uint8_t shmem[];\n\n\t// Warp and lane identification.\n\tconst unsigned int warpId = threadIdx.x / WARP_SIZE;\n\tconst unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n\n\t// Offset in shared memory from which the B matrix is stored.\n\t// const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\tconst size_t shmem_idx_b_off = BLOCK_SIZE_K_SPARSE * SHARED_OFFSET_A;\n\n\n\t// Each CTA slides along the 128 x 128 tiles from the top left corner of the\n\t// matrix to the right and down, and selects the next tile to compute. Once\n\t// there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n\tconst unsigned int block_tile_i =\n\t\t((block_pos * BLOCK_COL_TILES) / M_TILES) * (BLOCK_ROW_TILES);\n\tconst unsigned int block_tile_j = (block_pos * BLOCK_COL_TILES) % M_TILES;\n\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n\twmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_ROW_TILES]\n\t\t\t\t\t\t\t\t\t\t\t\t\t [WARP_COL_TILES];\n\n    // Load the C matrix tiles into fragments from shared memory.\n\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t\twmma::fill_fragment(c[i][j], 0);\n\t\t}\n\t}\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    //for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n    for(int tile_k_idx_sparse = 0, tile_k_idx = 0; tile_k_idx_sparse < K_GLOBAL_SPARSE; tile_k_idx_sparse += BLOCK_SIZE_K_SPARSE, tile_k_idx += BLOCK_SIZE_K){\n\n\t\tsize_t shmem_idx = \n\t\twarpId < (WARPS_PER_BLOCK / 2)\n\t\t\t? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A\n\t\t\t: (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B + shmem_idx_b_off;\n\n\t\tint4 *lane_ptr = NULL;\n\t\tint *lane_ptr_index = NULL;\n\t\tconst uint8_t *warp_ptr = NULL;\n\n\n\t\tif(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\t//warp_ptr = &A[block_tile_j * M] +\n\t\t\t//\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * M_GLOBAL;\n\t\t\twarp_ptr = &A[block_tile_j * M];\n\t\t\t\n\t\t\tconst int *warp_ptr_index = &B_index[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t\t\t\t\t\t((warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A);\n\n\t\t\tlane_ptr_index = (int *)(warp_ptr_index + tile_k_idx_sparse + (laneId / CHUNK_COPY_LINE_LANES_A));\n\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_A) * SHARED_OFFSET_A;\n\t\t}else{\n\t\t\twarp_ptr = &B[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * K_GLOBAL_SPARSE;\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_sparse +\n\t\t\t\t\t\t\t\t(laneId / CHUNK_COPY_LINE_LANES_B) * K_GLOBAL_SPARSE) +\n\t\t\t\t\t\t\t\t(laneId % CHUNK_COPY_LINE_LANES_B);\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_B) * SHARED_OFFSET_B;\n\t\t}\n\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      // shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n\t  int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n\t  \t? BLOCK_SIZE_K_SPARSE / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A)\n\t\t: BLOCK_SIZE_N / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\n\t  /*\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\t  */\n\n\t  /*\n      int tile_k_idx_A;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n          tile_k_idx_A = *(lane_ptr_index);\n      }\n\t  */\n\n\t  #pragma unroll\n\t  for(int i = 0; i < iter_index; i += 1){\n\t\t  if(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\tint tile_k_idx_A = *(lane_ptr_index);\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_A * M_GLOBAL) + (laneId % CHUNK_COPY_LINE_LANES_A);\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_A)) =\n\t\t\t\t*lane_ptr;\n\t\t\t//warp_ptr = (uint8_t *)((uint8_t *)warp_ptr + M_GLOBAL * (WARPS_PER_BLOCK / 2) *CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tlane_ptr_index = (int *)((int *)lane_ptr_index +  (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A;\n\t\t  }else{\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_B)) =\n\t\t\t\t*lane_ptr;\n\t\t\tlane_ptr = (int4 *)((uint8_t *)lane_ptr + K_GLOBAL_SPARSE * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B;\n\t\t  }\n\t  }\n\n      __syncthreads();\n\n\t#pragma unroll\n      for (int k_step = 0; k_step < CHUNK_K_SPARSE; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::col_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n\t#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i += 1) {\n\t\t\tsize_t shmem_idx_a = (warpId % BLOCK_COL_WARPS) * M * WARP_COL_TILES + (i * M);\n\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_a + k_step * K * SHARED_OFFSET_A;\n\n\t\t\twmma::load_matrix_sync(a[i], tile_ptr, SHARED_OFFSET_A);\n\t\t#pragma unroll\n\t\t\tfor(int j = 0; j < WARP_ROW_TILES; j += 1){\n\t\t\t\tif(i == 0){\n\t\t\t\t\tsize_t shmem_idx_b = shmem_idx_b_off +\n\t\t\t\t\t\t\t\t\t\t\t(warpId / BLOCK_COL_WARPS) * (WARP_ROW_TILES * N) * SHARED_OFFSET_B +\n\t\t\t\t\t\t\t\t\t\t\t(j * N) * SHARED_OFFSET_B;\n\t\t\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_b + k_step * K;\n\t\t\t\t\twmma::load_matrix_sync(b[j], tile_ptr, SHARED_OFFSET_B);\n\t\t\t\t}\n\t\t\t\twmma::mma_sync(c[j][i], a[i], b[j], c[j][i]);\n\t\t\t}\n\n        }\n      }\n\n      __syncthreads();\n    }\n\n    // This pointer is used to access the C and D matrix tiles this warp computes.\n\tint *shmem_warp_tile_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * N * WARP_ROW_TILES * SHMEM_STRIDE +\n\t(warpId % BLOCK_COL_WARPS) * SHMEM_OFFSET;\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t#pragma unroll\n\t\t\tfor(int t = 0; t < c[i][j].num_elements; t += 1){\n\t\t\t\tc[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n\t\t\t}\n\t\t\tint *tile_ptr = shmem_warp_tile_ptr + i * N * SHMEM_STRIDE + j * M;\n\t\t\twmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, C_LAYOUT);\n\t\t}\n\t}\n\n    __syncthreads();\n\n\tint *shmem_warp_stream_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N * SHMEM_STRIDE\n\t\t\t\t\t\t\t\t\t+ (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tconst size_t gmem_idx =\n\t\t(block_tile_i * N + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N) * GLOBAL_MEM_STRIDE +\n\t\tblock_tile_j * M + (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tuint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n\tint *shmem_lane_stream_ptr =\n\t\tshmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * SHMEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\t\n\tuint8_t *dst_gmem_lane_stream_ptr =\n\t\tdst_gmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * GLOBAL_MEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\n\tfor(int i = 0; i < WARP_ROW_TILES * N; i += SHARED_TO_GLOBAL_LINES_PER_WARP){\n\t\t*(dst_gmem_lane_stream_ptr + i * GLOBAL_MEM_STRIDE) = (uint8_t)(*(shmem_lane_stream_ptr + i * SHMEM_STRIDE));\n\t}\n\n\t__syncthreads();\n}\n", "launch_config": {"dimBlock": [256, 1], "dimGrid": [256, 1]}}, "97": {"code": "\n__global__ void compute_gemm_imma(float *input0, float *input1, float *input2, float* input3, float* input4, float*input5, float* input6,\n                                  float *output0) {\n    // keys need to be replaced 4096, 1024, K_GLOBA_VALUEL, 0.75, 64\n\n\t//extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B = reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_index = reinterpret_cast<int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n\tconst int WARP_SIZE = 32;\n    const int M = 16;\n    const int K = 16;\n    const int N = 16;\n    const int WMMA_M = 16;\n    const int WMMA_K = 16;\n    const int WMMA_N = 16;\n    const int M_GLOBAL = 4096;\n    const int K_GLOBAL = 1024;\n    const int N_GLOBAL = 1024;\n    const float SPARSITY = 0.75;\n    const int K_GLOBAL_SPARSE = (int(K_GLOBAL * (1-SPARSITY)));\n    const int M_TILES = (M_GLOBAL / M);\n    const int N_TILES = (N_GLOBAL / N);\n    const int K_TILES = (K_GLOBAL / K);\n    const int BLOCK_ROW_WARPS = 2;\n    const int BLOCK_COL_WARPS = 4;\n    const int WARP_ROW_TILES = 4;\n    const int WARP_COL_TILES = 2;\n    const int SKEW_UINT8 = 16;\n    const int BANK_VAL = 32;\n    const int NUM_BANK = (K_GLOBAL / BANK_VAL);\n    /////////// BLOCK_ROW_TILES <= N_TILES ////////////\n    // const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    // ////////// BLOCK_COL_TILES <= M_TILES ////////////\n    // const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    auto C_LAYOUT = wmma::mem_col_major;\n    const int CHUNK_K = 64;\n    const int CHUNK_K_SPARSE = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K_SPARSE = (int((CHUNK_K * K) * (1 - SPARSITY)));\n    const int WARP_COPY_BYTES = (WARP_SIZE * sizeof(int4));\n    const int CHUNK_LINE_BYTES_A (BLOCK_COL_TILES * M * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_A = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_A);\n    const int CHUNK_COPY_LINE_LANES_A = (CHUNK_LINE_BYTES_A / sizeof(int4));\n    const int SHARED_OFFSET_A = (BLOCK_COL_TILES * M + SKEW_UINT8);\n\n    const int CHUNK_LINE_BYTES_B = (BLOCK_SIZE_K_SPARSE * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_B = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_B);\n    const int CHUNK_COPY_LINE_LANES_B = (CHUNK_LINE_BYTES_B / sizeof(int4));\n    const int SHARED_OFFSET_B = (BLOCK_SIZE_K_SPARSE + SKEW_UINT8);\n    const int SHARED_TO_GLOBAL_BYTES_PER_LINE = ((WARP_COL_TILES * M) * sizeof(int));\n    const int SHARED_TO_GLOBAL_BYTES_PER_WARP = (WARP_SIZE * sizeof(int));\n    const int SHARED_TO_GLOBAL_LINES_PER_WARP = (SHARED_TO_GLOBAL_BYTES_PER_WARP / SHARED_TO_GLOBAL_BYTES_PER_LINE);\n    const int SHARED_TO_GLOBAL_LANES_PER_LINE = (WARP_SIZE / SHARED_TO_GLOBAL_LINES_PER_WARP);\n    const int SHARED_TO_GLOBAL_ITERS = ((WARP_ROW_TILES * N) / SHARED_TO_GLOBAL_LINES_PER_WARP);\n\n    const int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const int WARP_STRIDE = (WARP_COL_TILES * M);\n    const int WARPS_PER_BLOCK = (BLOCK_ROW_WARPS * BLOCK_COL_WARPS);\n    const int THREADS_PER_BLOCK = (WARP_SIZE * WARPS_PER_BLOCK);\n    const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    const int GLOBAL_MEM_STRIDE = M_GLOBAL;\n    const int SHMEM_STRIDE = (M * BLOCK_COL_TILES);\n    const int SHMEM_OFFSET = (M * WARP_COL_TILES);\n    const int BLOCK_SIZE_M = (M * BLOCK_COL_TILES);\n    const int BLOCK_SIZE_N = (N * BLOCK_ROW_TILES);\n\n   extern __shared__ uint8_t shmem[];\n\n\t// Warp and lane identification.\n\tconst unsigned int warpId = threadIdx.x / WARP_SIZE;\n\tconst unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n\n\t// Offset in shared memory from which the B matrix is stored.\n\t// const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\tconst size_t shmem_idx_b_off = BLOCK_SIZE_K_SPARSE * SHARED_OFFSET_A;\n\n\n\t// Each CTA slides along the 128 x 128 tiles from the top left corner of the\n\t// matrix to the right and down, and selects the next tile to compute. Once\n\t// there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n\tconst unsigned int block_tile_i =\n\t\t((block_pos * BLOCK_COL_TILES) / M_TILES) * (BLOCK_ROW_TILES);\n\tconst unsigned int block_tile_j = (block_pos * BLOCK_COL_TILES) % M_TILES;\n\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n\twmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_ROW_TILES]\n\t\t\t\t\t\t\t\t\t\t\t\t\t [WARP_COL_TILES];\n\n    // Load the C matrix tiles into fragments from shared memory.\n\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t\twmma::fill_fragment(c[i][j], 0);\n\t\t}\n\t}\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    //for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n    for(int tile_k_idx_sparse = 0, tile_k_idx = 0; tile_k_idx_sparse < K_GLOBAL_SPARSE; tile_k_idx_sparse += BLOCK_SIZE_K_SPARSE, tile_k_idx += BLOCK_SIZE_K){\n\n\t\tsize_t shmem_idx = \n\t\twarpId < (WARPS_PER_BLOCK / 2)\n\t\t\t? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A\n\t\t\t: (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B + shmem_idx_b_off;\n\n\t\tint4 *lane_ptr = NULL;\n\t\tint *lane_ptr_index = NULL;\n\t\tconst uint8_t *warp_ptr = NULL;\n\n\n\t\tif(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\t//warp_ptr = &A[block_tile_j * M] +\n\t\t\t//\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * M_GLOBAL;\n\t\t\twarp_ptr = &A[block_tile_j * M];\n\t\t\t\n\t\t\tconst int *warp_ptr_index = &B_index[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t\t\t\t\t\t((warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A);\n\n\t\t\tlane_ptr_index = (int *)(warp_ptr_index + tile_k_idx_sparse + (laneId / CHUNK_COPY_LINE_LANES_A));\n\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_A) * SHARED_OFFSET_A;\n\t\t}else{\n\t\t\twarp_ptr = &B[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * K_GLOBAL_SPARSE;\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_sparse +\n\t\t\t\t\t\t\t\t(laneId / CHUNK_COPY_LINE_LANES_B) * K_GLOBAL_SPARSE) +\n\t\t\t\t\t\t\t\t(laneId % CHUNK_COPY_LINE_LANES_B);\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_B) * SHARED_OFFSET_B;\n\t\t}\n\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      // shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n\t  int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n\t  \t? BLOCK_SIZE_K_SPARSE / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A)\n\t\t: BLOCK_SIZE_N / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\n\t  /*\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\t  */\n\n\t  /*\n      int tile_k_idx_A;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n          tile_k_idx_A = *(lane_ptr_index);\n      }\n\t  */\n\n\t  #pragma unroll\n\t  for(int i = 0; i < iter_index; i += 1){\n\t\t  if(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\tint tile_k_idx_A = *(lane_ptr_index);\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_A * M_GLOBAL) + (laneId % CHUNK_COPY_LINE_LANES_A);\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_A)) =\n\t\t\t\t*lane_ptr;\n\t\t\t//warp_ptr = (uint8_t *)((uint8_t *)warp_ptr + M_GLOBAL * (WARPS_PER_BLOCK / 2) *CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tlane_ptr_index = (int *)((int *)lane_ptr_index +  (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A;\n\t\t  }else{\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_B)) =\n\t\t\t\t*lane_ptr;\n\t\t\tlane_ptr = (int4 *)((uint8_t *)lane_ptr + K_GLOBAL_SPARSE * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B;\n\t\t  }\n\t  }\n\n      __syncthreads();\n\n\t#pragma unroll\n      for (int k_step = 0; k_step < CHUNK_K_SPARSE; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::col_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n\t#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i += 1) {\n\t\t\tsize_t shmem_idx_a = (warpId % BLOCK_COL_WARPS) * M * WARP_COL_TILES + (i * M);\n\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_a + k_step * K * SHARED_OFFSET_A;\n\n\t\t\twmma::load_matrix_sync(a[i], tile_ptr, SHARED_OFFSET_A);\n\t\t#pragma unroll\n\t\t\tfor(int j = 0; j < WARP_ROW_TILES; j += 1){\n\t\t\t\tif(i == 0){\n\t\t\t\t\tsize_t shmem_idx_b = shmem_idx_b_off +\n\t\t\t\t\t\t\t\t\t\t\t(warpId / BLOCK_COL_WARPS) * (WARP_ROW_TILES * N) * SHARED_OFFSET_B +\n\t\t\t\t\t\t\t\t\t\t\t(j * N) * SHARED_OFFSET_B;\n\t\t\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_b + k_step * K;\n\t\t\t\t\twmma::load_matrix_sync(b[j], tile_ptr, SHARED_OFFSET_B);\n\t\t\t\t}\n\t\t\t\twmma::mma_sync(c[j][i], a[i], b[j], c[j][i]);\n\t\t\t}\n\n        }\n      }\n\n      __syncthreads();\n    }\n\n    // This pointer is used to access the C and D matrix tiles this warp computes.\n\tint *shmem_warp_tile_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * N * WARP_ROW_TILES * SHMEM_STRIDE +\n\t(warpId % BLOCK_COL_WARPS) * SHMEM_OFFSET;\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t#pragma unroll\n\t\t\tfor(int t = 0; t < c[i][j].num_elements; t += 1){\n\t\t\t\tc[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n\t\t\t}\n\t\t\tint *tile_ptr = shmem_warp_tile_ptr + i * N * SHMEM_STRIDE + j * M;\n\t\t\twmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, C_LAYOUT);\n\t\t}\n\t}\n\n    __syncthreads();\n\n\tint *shmem_warp_stream_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N * SHMEM_STRIDE\n\t\t\t\t\t\t\t\t\t+ (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tconst size_t gmem_idx =\n\t\t(block_tile_i * N + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N) * GLOBAL_MEM_STRIDE +\n\t\tblock_tile_j * M + (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tuint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n\tint *shmem_lane_stream_ptr =\n\t\tshmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * SHMEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\t\n\tuint8_t *dst_gmem_lane_stream_ptr =\n\t\tdst_gmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * GLOBAL_MEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\n\tfor(int i = 0; i < WARP_ROW_TILES * N; i += SHARED_TO_GLOBAL_LINES_PER_WARP){\n\t\t*(dst_gmem_lane_stream_ptr + i * GLOBAL_MEM_STRIDE) = (uint8_t)(*(shmem_lane_stream_ptr + i * SHMEM_STRIDE));\n\t}\n\n\t__syncthreads();\n}\n", "launch_config": {"dimBlock": [256, 1], "dimGrid": [256, 1]}}, "98": {"code": "\n__global__ void compute_gemm_imma(float *input0, float *input1, float *input2, float* input3, float* input4, float*input5, float* input6,\n                                  float *output0) {\n    // keys need to be replaced 4096, 1024, K_GLOBA_VALUEL, 0.75, 64\n\n\t//extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B = reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_index = reinterpret_cast<int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n\tconst int WARP_SIZE = 32;\n    const int M = 16;\n    const int K = 16;\n    const int N = 16;\n    const int WMMA_M = 16;\n    const int WMMA_K = 16;\n    const int WMMA_N = 16;\n    const int M_GLOBAL = 4096;\n    const int K_GLOBAL = 1024;\n    const int N_GLOBAL = 1024;\n    const float SPARSITY = 0.75;\n    const int K_GLOBAL_SPARSE = (int(K_GLOBAL * (1-SPARSITY)));\n    const int M_TILES = (M_GLOBAL / M);\n    const int N_TILES = (N_GLOBAL / N);\n    const int K_TILES = (K_GLOBAL / K);\n    const int BLOCK_ROW_WARPS = 2;\n    const int BLOCK_COL_WARPS = 4;\n    const int WARP_ROW_TILES = 4;\n    const int WARP_COL_TILES = 2;\n    const int SKEW_UINT8 = 16;\n    const int BANK_VAL = 32;\n    const int NUM_BANK = (K_GLOBAL / BANK_VAL);\n    /////////// BLOCK_ROW_TILES <= N_TILES ////////////\n    // const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    // ////////// BLOCK_COL_TILES <= M_TILES ////////////\n    // const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    auto C_LAYOUT = wmma::mem_col_major;\n    const int CHUNK_K = 64;\n    const int CHUNK_K_SPARSE = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K_SPARSE = (int((CHUNK_K * K) * (1 - SPARSITY)));\n    const int WARP_COPY_BYTES = (WARP_SIZE * sizeof(int4));\n    const int CHUNK_LINE_BYTES_A (BLOCK_COL_TILES * M * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_A = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_A);\n    const int CHUNK_COPY_LINE_LANES_A = (CHUNK_LINE_BYTES_A / sizeof(int4));\n    const int SHARED_OFFSET_A = (BLOCK_COL_TILES * M + SKEW_UINT8);\n\n    const int CHUNK_LINE_BYTES_B = (BLOCK_SIZE_K_SPARSE * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_B = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_B);\n    const int CHUNK_COPY_LINE_LANES_B = (CHUNK_LINE_BYTES_B / sizeof(int4));\n    const int SHARED_OFFSET_B = (BLOCK_SIZE_K_SPARSE + SKEW_UINT8);\n    const int SHARED_TO_GLOBAL_BYTES_PER_LINE = ((WARP_COL_TILES * M) * sizeof(int));\n    const int SHARED_TO_GLOBAL_BYTES_PER_WARP = (WARP_SIZE * sizeof(int));\n    const int SHARED_TO_GLOBAL_LINES_PER_WARP = (SHARED_TO_GLOBAL_BYTES_PER_WARP / SHARED_TO_GLOBAL_BYTES_PER_LINE);\n    const int SHARED_TO_GLOBAL_LANES_PER_LINE = (WARP_SIZE / SHARED_TO_GLOBAL_LINES_PER_WARP);\n    const int SHARED_TO_GLOBAL_ITERS = ((WARP_ROW_TILES * N) / SHARED_TO_GLOBAL_LINES_PER_WARP);\n\n    const int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const int WARP_STRIDE = (WARP_COL_TILES * M);\n    const int WARPS_PER_BLOCK = (BLOCK_ROW_WARPS * BLOCK_COL_WARPS);\n    const int THREADS_PER_BLOCK = (WARP_SIZE * WARPS_PER_BLOCK);\n    const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    const int GLOBAL_MEM_STRIDE = M_GLOBAL;\n    const int SHMEM_STRIDE = (M * BLOCK_COL_TILES);\n    const int SHMEM_OFFSET = (M * WARP_COL_TILES);\n    const int BLOCK_SIZE_M = (M * BLOCK_COL_TILES);\n    const int BLOCK_SIZE_N = (N * BLOCK_ROW_TILES);\n\n   extern __shared__ uint8_t shmem[];\n\n\t// Warp and lane identification.\n\tconst unsigned int warpId = threadIdx.x / WARP_SIZE;\n\tconst unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n\n\t// Offset in shared memory from which the B matrix is stored.\n\t// const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\tconst size_t shmem_idx_b_off = BLOCK_SIZE_K_SPARSE * SHARED_OFFSET_A;\n\n\n\t// Each CTA slides along the 128 x 128 tiles from the top left corner of the\n\t// matrix to the right and down, and selects the next tile to compute. Once\n\t// there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n\tconst unsigned int block_tile_i =\n\t\t((block_pos * BLOCK_COL_TILES) / M_TILES) * (BLOCK_ROW_TILES);\n\tconst unsigned int block_tile_j = (block_pos * BLOCK_COL_TILES) % M_TILES;\n\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n\twmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_ROW_TILES]\n\t\t\t\t\t\t\t\t\t\t\t\t\t [WARP_COL_TILES];\n\n    // Load the C matrix tiles into fragments from shared memory.\n\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t\twmma::fill_fragment(c[i][j], 0);\n\t\t}\n\t}\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    //for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n    for(int tile_k_idx_sparse = 0, tile_k_idx = 0; tile_k_idx_sparse < K_GLOBAL_SPARSE; tile_k_idx_sparse += BLOCK_SIZE_K_SPARSE, tile_k_idx += BLOCK_SIZE_K){\n\n\t\tsize_t shmem_idx = \n\t\twarpId < (WARPS_PER_BLOCK / 2)\n\t\t\t? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A\n\t\t\t: (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B + shmem_idx_b_off;\n\n\t\tint4 *lane_ptr = NULL;\n\t\tint *lane_ptr_index = NULL;\n\t\tconst uint8_t *warp_ptr = NULL;\n\n\n\t\tif(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\t//warp_ptr = &A[block_tile_j * M] +\n\t\t\t//\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * M_GLOBAL;\n\t\t\twarp_ptr = &A[block_tile_j * M];\n\t\t\t\n\t\t\tconst int *warp_ptr_index = &B_index[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t\t\t\t\t\t((warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A);\n\n\t\t\tlane_ptr_index = (int *)(warp_ptr_index + tile_k_idx_sparse + (laneId / CHUNK_COPY_LINE_LANES_A));\n\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_A) * SHARED_OFFSET_A;\n\t\t}else{\n\t\t\twarp_ptr = &B[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * K_GLOBAL_SPARSE;\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_sparse +\n\t\t\t\t\t\t\t\t(laneId / CHUNK_COPY_LINE_LANES_B) * K_GLOBAL_SPARSE) +\n\t\t\t\t\t\t\t\t(laneId % CHUNK_COPY_LINE_LANES_B);\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_B) * SHARED_OFFSET_B;\n\t\t}\n\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      // shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n\t  int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n\t  \t? BLOCK_SIZE_K_SPARSE / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A)\n\t\t: BLOCK_SIZE_N / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\n\t  /*\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\t  */\n\n\t  /*\n      int tile_k_idx_A;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n          tile_k_idx_A = *(lane_ptr_index);\n      }\n\t  */\n\n\t  #pragma unroll\n\t  for(int i = 0; i < iter_index; i += 1){\n\t\t  if(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\tint tile_k_idx_A = *(lane_ptr_index);\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_A * M_GLOBAL) + (laneId % CHUNK_COPY_LINE_LANES_A);\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_A)) =\n\t\t\t\t*lane_ptr;\n\t\t\t//warp_ptr = (uint8_t *)((uint8_t *)warp_ptr + M_GLOBAL * (WARPS_PER_BLOCK / 2) *CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tlane_ptr_index = (int *)((int *)lane_ptr_index +  (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A;\n\t\t  }else{\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_B)) =\n\t\t\t\t*lane_ptr;\n\t\t\tlane_ptr = (int4 *)((uint8_t *)lane_ptr + K_GLOBAL_SPARSE * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B;\n\t\t  }\n\t  }\n\n      __syncthreads();\n\n\t#pragma unroll\n      for (int k_step = 0; k_step < CHUNK_K_SPARSE; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::col_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n\t#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i += 1) {\n\t\t\tsize_t shmem_idx_a = (warpId % BLOCK_COL_WARPS) * M * WARP_COL_TILES + (i * M);\n\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_a + k_step * K * SHARED_OFFSET_A;\n\n\t\t\twmma::load_matrix_sync(a[i], tile_ptr, SHARED_OFFSET_A);\n\t\t#pragma unroll\n\t\t\tfor(int j = 0; j < WARP_ROW_TILES; j += 1){\n\t\t\t\tif(i == 0){\n\t\t\t\t\tsize_t shmem_idx_b = shmem_idx_b_off +\n\t\t\t\t\t\t\t\t\t\t\t(warpId / BLOCK_COL_WARPS) * (WARP_ROW_TILES * N) * SHARED_OFFSET_B +\n\t\t\t\t\t\t\t\t\t\t\t(j * N) * SHARED_OFFSET_B;\n\t\t\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_b + k_step * K;\n\t\t\t\t\twmma::load_matrix_sync(b[j], tile_ptr, SHARED_OFFSET_B);\n\t\t\t\t}\n\t\t\t\twmma::mma_sync(c[j][i], a[i], b[j], c[j][i]);\n\t\t\t}\n\n        }\n      }\n\n      __syncthreads();\n    }\n\n    // This pointer is used to access the C and D matrix tiles this warp computes.\n\tint *shmem_warp_tile_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * N * WARP_ROW_TILES * SHMEM_STRIDE +\n\t(warpId % BLOCK_COL_WARPS) * SHMEM_OFFSET;\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t#pragma unroll\n\t\t\tfor(int t = 0; t < c[i][j].num_elements; t += 1){\n\t\t\t\tc[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n\t\t\t}\n\t\t\tint *tile_ptr = shmem_warp_tile_ptr + i * N * SHMEM_STRIDE + j * M;\n\t\t\twmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, C_LAYOUT);\n\t\t}\n\t}\n\n    __syncthreads();\n\n\tint *shmem_warp_stream_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N * SHMEM_STRIDE\n\t\t\t\t\t\t\t\t\t+ (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tconst size_t gmem_idx =\n\t\t(block_tile_i * N + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N) * GLOBAL_MEM_STRIDE +\n\t\tblock_tile_j * M + (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tuint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n\tint *shmem_lane_stream_ptr =\n\t\tshmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * SHMEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\t\n\tuint8_t *dst_gmem_lane_stream_ptr =\n\t\tdst_gmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * GLOBAL_MEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\n\tfor(int i = 0; i < WARP_ROW_TILES * N; i += SHARED_TO_GLOBAL_LINES_PER_WARP){\n\t\t*(dst_gmem_lane_stream_ptr + i * GLOBAL_MEM_STRIDE) = (uint8_t)(*(shmem_lane_stream_ptr + i * SHMEM_STRIDE));\n\t}\n\n\t__syncthreads();\n}\n", "launch_config": {"dimBlock": [256, 1], "dimGrid": [256, 1]}}, "99": {"code": "\n__global__ void compute_gemm_imma(float *input0, float *input1, float *input2, float* input3, float* input4, float*input5, float* input6,\n                                  float *output0) {\n    // keys need to be replaced 4096, 1024, K_GLOBA_VALUEL, 0.75, 64\n\n\t//extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B = reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_index = reinterpret_cast<int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n\tconst int WARP_SIZE = 32;\n    const int M = 16;\n    const int K = 16;\n    const int N = 16;\n    const int WMMA_M = 16;\n    const int WMMA_K = 16;\n    const int WMMA_N = 16;\n    const int M_GLOBAL = 4096;\n    const int K_GLOBAL = 1024;\n    const int N_GLOBAL = 1024;\n    const float SPARSITY = 0.75;\n    const int K_GLOBAL_SPARSE = (int(K_GLOBAL * (1-SPARSITY)));\n    const int M_TILES = (M_GLOBAL / M);\n    const int N_TILES = (N_GLOBAL / N);\n    const int K_TILES = (K_GLOBAL / K);\n    const int BLOCK_ROW_WARPS = 2;\n    const int BLOCK_COL_WARPS = 4;\n    const int WARP_ROW_TILES = 4;\n    const int WARP_COL_TILES = 2;\n    const int SKEW_UINT8 = 16;\n    const int BANK_VAL = 32;\n    const int NUM_BANK = (K_GLOBAL / BANK_VAL);\n    /////////// BLOCK_ROW_TILES <= N_TILES ////////////\n    // const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    // ////////// BLOCK_COL_TILES <= M_TILES ////////////\n    // const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    auto C_LAYOUT = wmma::mem_col_major;\n    const int CHUNK_K = 64;\n    const int CHUNK_K_SPARSE = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K_SPARSE = (int((CHUNK_K * K) * (1 - SPARSITY)));\n    const int WARP_COPY_BYTES = (WARP_SIZE * sizeof(int4));\n    const int CHUNK_LINE_BYTES_A (BLOCK_COL_TILES * M * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_A = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_A);\n    const int CHUNK_COPY_LINE_LANES_A = (CHUNK_LINE_BYTES_A / sizeof(int4));\n    const int SHARED_OFFSET_A = (BLOCK_COL_TILES * M + SKEW_UINT8);\n\n    const int CHUNK_LINE_BYTES_B = (BLOCK_SIZE_K_SPARSE * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_B = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_B);\n    const int CHUNK_COPY_LINE_LANES_B = (CHUNK_LINE_BYTES_B / sizeof(int4));\n    const int SHARED_OFFSET_B = (BLOCK_SIZE_K_SPARSE + SKEW_UINT8);\n    const int SHARED_TO_GLOBAL_BYTES_PER_LINE = ((WARP_COL_TILES * M) * sizeof(int));\n    const int SHARED_TO_GLOBAL_BYTES_PER_WARP = (WARP_SIZE * sizeof(int));\n    const int SHARED_TO_GLOBAL_LINES_PER_WARP = (SHARED_TO_GLOBAL_BYTES_PER_WARP / SHARED_TO_GLOBAL_BYTES_PER_LINE);\n    const int SHARED_TO_GLOBAL_LANES_PER_LINE = (WARP_SIZE / SHARED_TO_GLOBAL_LINES_PER_WARP);\n    const int SHARED_TO_GLOBAL_ITERS = ((WARP_ROW_TILES * N) / SHARED_TO_GLOBAL_LINES_PER_WARP);\n\n    const int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const int WARP_STRIDE = (WARP_COL_TILES * M);\n    const int WARPS_PER_BLOCK = (BLOCK_ROW_WARPS * BLOCK_COL_WARPS);\n    const int THREADS_PER_BLOCK = (WARP_SIZE * WARPS_PER_BLOCK);\n    const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    const int GLOBAL_MEM_STRIDE = M_GLOBAL;\n    const int SHMEM_STRIDE = (M * BLOCK_COL_TILES);\n    const int SHMEM_OFFSET = (M * WARP_COL_TILES);\n    const int BLOCK_SIZE_M = (M * BLOCK_COL_TILES);\n    const int BLOCK_SIZE_N = (N * BLOCK_ROW_TILES);\n\n   extern __shared__ uint8_t shmem[];\n\n\t// Warp and lane identification.\n\tconst unsigned int warpId = threadIdx.x / WARP_SIZE;\n\tconst unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n\n\t// Offset in shared memory from which the B matrix is stored.\n\t// const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\tconst size_t shmem_idx_b_off = BLOCK_SIZE_K_SPARSE * SHARED_OFFSET_A;\n\n\n\t// Each CTA slides along the 128 x 128 tiles from the top left corner of the\n\t// matrix to the right and down, and selects the next tile to compute. Once\n\t// there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n\tconst unsigned int block_tile_i =\n\t\t((block_pos * BLOCK_COL_TILES) / M_TILES) * (BLOCK_ROW_TILES);\n\tconst unsigned int block_tile_j = (block_pos * BLOCK_COL_TILES) % M_TILES;\n\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n\twmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_ROW_TILES]\n\t\t\t\t\t\t\t\t\t\t\t\t\t [WARP_COL_TILES];\n\n    // Load the C matrix tiles into fragments from shared memory.\n\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t\twmma::fill_fragment(c[i][j], 0);\n\t\t}\n\t}\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    //for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n    for(int tile_k_idx_sparse = 0, tile_k_idx = 0; tile_k_idx_sparse < K_GLOBAL_SPARSE; tile_k_idx_sparse += BLOCK_SIZE_K_SPARSE, tile_k_idx += BLOCK_SIZE_K){\n\n\t\tsize_t shmem_idx = \n\t\twarpId < (WARPS_PER_BLOCK / 2)\n\t\t\t? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A\n\t\t\t: (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B + shmem_idx_b_off;\n\n\t\tint4 *lane_ptr = NULL;\n\t\tint *lane_ptr_index = NULL;\n\t\tconst uint8_t *warp_ptr = NULL;\n\n\n\t\tif(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\t//warp_ptr = &A[block_tile_j * M] +\n\t\t\t//\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * M_GLOBAL;\n\t\t\twarp_ptr = &A[block_tile_j * M];\n\t\t\t\n\t\t\tconst int *warp_ptr_index = &B_index[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t\t\t\t\t\t((warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A);\n\n\t\t\tlane_ptr_index = (int *)(warp_ptr_index + tile_k_idx_sparse + (laneId / CHUNK_COPY_LINE_LANES_A));\n\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_A) * SHARED_OFFSET_A;\n\t\t}else{\n\t\t\twarp_ptr = &B[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * K_GLOBAL_SPARSE;\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_sparse +\n\t\t\t\t\t\t\t\t(laneId / CHUNK_COPY_LINE_LANES_B) * K_GLOBAL_SPARSE) +\n\t\t\t\t\t\t\t\t(laneId % CHUNK_COPY_LINE_LANES_B);\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_B) * SHARED_OFFSET_B;\n\t\t}\n\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      // shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n\t  int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n\t  \t? BLOCK_SIZE_K_SPARSE / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A)\n\t\t: BLOCK_SIZE_N / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\n\t  /*\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\t  */\n\n\t  /*\n      int tile_k_idx_A;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n          tile_k_idx_A = *(lane_ptr_index);\n      }\n\t  */\n\n\t  #pragma unroll\n\t  for(int i = 0; i < iter_index; i += 1){\n\t\t  if(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\tint tile_k_idx_A = *(lane_ptr_index);\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_A * M_GLOBAL) + (laneId % CHUNK_COPY_LINE_LANES_A);\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_A)) =\n\t\t\t\t*lane_ptr;\n\t\t\t//warp_ptr = (uint8_t *)((uint8_t *)warp_ptr + M_GLOBAL * (WARPS_PER_BLOCK / 2) *CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tlane_ptr_index = (int *)((int *)lane_ptr_index +  (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A;\n\t\t  }else{\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_B)) =\n\t\t\t\t*lane_ptr;\n\t\t\tlane_ptr = (int4 *)((uint8_t *)lane_ptr + K_GLOBAL_SPARSE * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B;\n\t\t  }\n\t  }\n\n      __syncthreads();\n\n\t#pragma unroll\n      for (int k_step = 0; k_step < CHUNK_K_SPARSE; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::col_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n\t#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i += 1) {\n\t\t\tsize_t shmem_idx_a = (warpId % BLOCK_COL_WARPS) * M * WARP_COL_TILES + (i * M);\n\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_a + k_step * K * SHARED_OFFSET_A;\n\n\t\t\twmma::load_matrix_sync(a[i], tile_ptr, SHARED_OFFSET_A);\n\t\t#pragma unroll\n\t\t\tfor(int j = 0; j < WARP_ROW_TILES; j += 1){\n\t\t\t\tif(i == 0){\n\t\t\t\t\tsize_t shmem_idx_b = shmem_idx_b_off +\n\t\t\t\t\t\t\t\t\t\t\t(warpId / BLOCK_COL_WARPS) * (WARP_ROW_TILES * N) * SHARED_OFFSET_B +\n\t\t\t\t\t\t\t\t\t\t\t(j * N) * SHARED_OFFSET_B;\n\t\t\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_b + k_step * K;\n\t\t\t\t\twmma::load_matrix_sync(b[j], tile_ptr, SHARED_OFFSET_B);\n\t\t\t\t}\n\t\t\t\twmma::mma_sync(c[j][i], a[i], b[j], c[j][i]);\n\t\t\t}\n\n        }\n      }\n\n      __syncthreads();\n    }\n\n    // This pointer is used to access the C and D matrix tiles this warp computes.\n\tint *shmem_warp_tile_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * N * WARP_ROW_TILES * SHMEM_STRIDE +\n\t(warpId % BLOCK_COL_WARPS) * SHMEM_OFFSET;\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t#pragma unroll\n\t\t\tfor(int t = 0; t < c[i][j].num_elements; t += 1){\n\t\t\t\tc[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n\t\t\t}\n\t\t\tint *tile_ptr = shmem_warp_tile_ptr + i * N * SHMEM_STRIDE + j * M;\n\t\t\twmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, C_LAYOUT);\n\t\t}\n\t}\n\n    __syncthreads();\n\n\tint *shmem_warp_stream_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N * SHMEM_STRIDE\n\t\t\t\t\t\t\t\t\t+ (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tconst size_t gmem_idx =\n\t\t(block_tile_i * N + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N) * GLOBAL_MEM_STRIDE +\n\t\tblock_tile_j * M + (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tuint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n\tint *shmem_lane_stream_ptr =\n\t\tshmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * SHMEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\t\n\tuint8_t *dst_gmem_lane_stream_ptr =\n\t\tdst_gmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * GLOBAL_MEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\n\tfor(int i = 0; i < WARP_ROW_TILES * N; i += SHARED_TO_GLOBAL_LINES_PER_WARP){\n\t\t*(dst_gmem_lane_stream_ptr + i * GLOBAL_MEM_STRIDE) = (uint8_t)(*(shmem_lane_stream_ptr + i * SHMEM_STRIDE));\n\t}\n\n\t__syncthreads();\n}\n", "launch_config": {"dimBlock": [256, 1], "dimGrid": [256, 1]}}, "100": {"code": "\n__global__ void compute_gemm_imma(float *input0, float *input1, float *input2, float* input3, float* input4, float*input5, float* input6,\n                                  float *output0) {\n    // keys need to be replaced 4096, 1024, K_GLOBA_VALUEL, 0.75, 64\n\n\t//extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B = reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_index = reinterpret_cast<int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n\tconst int WARP_SIZE = 32;\n    const int M = 16;\n    const int K = 16;\n    const int N = 16;\n    const int WMMA_M = 16;\n    const int WMMA_K = 16;\n    const int WMMA_N = 16;\n    const int M_GLOBAL = 4096;\n    const int K_GLOBAL = 1024;\n    const int N_GLOBAL = 1024;\n    const float SPARSITY = 0.75;\n    const int K_GLOBAL_SPARSE = (int(K_GLOBAL * (1-SPARSITY)));\n    const int M_TILES = (M_GLOBAL / M);\n    const int N_TILES = (N_GLOBAL / N);\n    const int K_TILES = (K_GLOBAL / K);\n    const int BLOCK_ROW_WARPS = 2;\n    const int BLOCK_COL_WARPS = 4;\n    const int WARP_ROW_TILES = 4;\n    const int WARP_COL_TILES = 2;\n    const int SKEW_UINT8 = 16;\n    const int BANK_VAL = 32;\n    const int NUM_BANK = (K_GLOBAL / BANK_VAL);\n    /////////// BLOCK_ROW_TILES <= N_TILES ////////////\n    // const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    // ////////// BLOCK_COL_TILES <= M_TILES ////////////\n    // const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    auto C_LAYOUT = wmma::mem_col_major;\n    const int CHUNK_K = 64;\n    const int CHUNK_K_SPARSE = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K_SPARSE = (int((CHUNK_K * K) * (1 - SPARSITY)));\n    const int WARP_COPY_BYTES = (WARP_SIZE * sizeof(int4));\n    const int CHUNK_LINE_BYTES_A (BLOCK_COL_TILES * M * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_A = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_A);\n    const int CHUNK_COPY_LINE_LANES_A = (CHUNK_LINE_BYTES_A / sizeof(int4));\n    const int SHARED_OFFSET_A = (BLOCK_COL_TILES * M + SKEW_UINT8);\n\n    const int CHUNK_LINE_BYTES_B = (BLOCK_SIZE_K_SPARSE * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_B = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_B);\n    const int CHUNK_COPY_LINE_LANES_B = (CHUNK_LINE_BYTES_B / sizeof(int4));\n    const int SHARED_OFFSET_B = (BLOCK_SIZE_K_SPARSE + SKEW_UINT8);\n    const int SHARED_TO_GLOBAL_BYTES_PER_LINE = ((WARP_COL_TILES * M) * sizeof(int));\n    const int SHARED_TO_GLOBAL_BYTES_PER_WARP = (WARP_SIZE * sizeof(int));\n    const int SHARED_TO_GLOBAL_LINES_PER_WARP = (SHARED_TO_GLOBAL_BYTES_PER_WARP / SHARED_TO_GLOBAL_BYTES_PER_LINE);\n    const int SHARED_TO_GLOBAL_LANES_PER_LINE = (WARP_SIZE / SHARED_TO_GLOBAL_LINES_PER_WARP);\n    const int SHARED_TO_GLOBAL_ITERS = ((WARP_ROW_TILES * N) / SHARED_TO_GLOBAL_LINES_PER_WARP);\n\n    const int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const int WARP_STRIDE = (WARP_COL_TILES * M);\n    const int WARPS_PER_BLOCK = (BLOCK_ROW_WARPS * BLOCK_COL_WARPS);\n    const int THREADS_PER_BLOCK = (WARP_SIZE * WARPS_PER_BLOCK);\n    const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    const int GLOBAL_MEM_STRIDE = M_GLOBAL;\n    const int SHMEM_STRIDE = (M * BLOCK_COL_TILES);\n    const int SHMEM_OFFSET = (M * WARP_COL_TILES);\n    const int BLOCK_SIZE_M = (M * BLOCK_COL_TILES);\n    const int BLOCK_SIZE_N = (N * BLOCK_ROW_TILES);\n\n   extern __shared__ uint8_t shmem[];\n\n\t// Warp and lane identification.\n\tconst unsigned int warpId = threadIdx.x / WARP_SIZE;\n\tconst unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n\n\t// Offset in shared memory from which the B matrix is stored.\n\t// const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\tconst size_t shmem_idx_b_off = BLOCK_SIZE_K_SPARSE * SHARED_OFFSET_A;\n\n\n\t// Each CTA slides along the 128 x 128 tiles from the top left corner of the\n\t// matrix to the right and down, and selects the next tile to compute. Once\n\t// there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n\tconst unsigned int block_tile_i =\n\t\t((block_pos * BLOCK_COL_TILES) / M_TILES) * (BLOCK_ROW_TILES);\n\tconst unsigned int block_tile_j = (block_pos * BLOCK_COL_TILES) % M_TILES;\n\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n\twmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_ROW_TILES]\n\t\t\t\t\t\t\t\t\t\t\t\t\t [WARP_COL_TILES];\n\n    // Load the C matrix tiles into fragments from shared memory.\n\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t\twmma::fill_fragment(c[i][j], 0);\n\t\t}\n\t}\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    //for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n    for(int tile_k_idx_sparse = 0, tile_k_idx = 0; tile_k_idx_sparse < K_GLOBAL_SPARSE; tile_k_idx_sparse += BLOCK_SIZE_K_SPARSE, tile_k_idx += BLOCK_SIZE_K){\n\n\t\tsize_t shmem_idx = \n\t\twarpId < (WARPS_PER_BLOCK / 2)\n\t\t\t? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A\n\t\t\t: (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B + shmem_idx_b_off;\n\n\t\tint4 *lane_ptr = NULL;\n\t\tint *lane_ptr_index = NULL;\n\t\tconst uint8_t *warp_ptr = NULL;\n\n\n\t\tif(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\t//warp_ptr = &A[block_tile_j * M] +\n\t\t\t//\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * M_GLOBAL;\n\t\t\twarp_ptr = &A[block_tile_j * M];\n\t\t\t\n\t\t\tconst int *warp_ptr_index = &B_index[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t\t\t\t\t\t((warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A);\n\n\t\t\tlane_ptr_index = (int *)(warp_ptr_index + tile_k_idx_sparse + (laneId / CHUNK_COPY_LINE_LANES_A));\n\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_A) * SHARED_OFFSET_A;\n\t\t}else{\n\t\t\twarp_ptr = &B[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * K_GLOBAL_SPARSE;\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_sparse +\n\t\t\t\t\t\t\t\t(laneId / CHUNK_COPY_LINE_LANES_B) * K_GLOBAL_SPARSE) +\n\t\t\t\t\t\t\t\t(laneId % CHUNK_COPY_LINE_LANES_B);\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_B) * SHARED_OFFSET_B;\n\t\t}\n\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      // shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n\t  int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n\t  \t? BLOCK_SIZE_K_SPARSE / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A)\n\t\t: BLOCK_SIZE_N / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\n\t  /*\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\t  */\n\n\t  /*\n      int tile_k_idx_A;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n          tile_k_idx_A = *(lane_ptr_index);\n      }\n\t  */\n\n\t  #pragma unroll\n\t  for(int i = 0; i < iter_index; i += 1){\n\t\t  if(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\tint tile_k_idx_A = *(lane_ptr_index);\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_A * M_GLOBAL) + (laneId % CHUNK_COPY_LINE_LANES_A);\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_A)) =\n\t\t\t\t*lane_ptr;\n\t\t\t//warp_ptr = (uint8_t *)((uint8_t *)warp_ptr + M_GLOBAL * (WARPS_PER_BLOCK / 2) *CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tlane_ptr_index = (int *)((int *)lane_ptr_index +  (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A;\n\t\t  }else{\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_B)) =\n\t\t\t\t*lane_ptr;\n\t\t\tlane_ptr = (int4 *)((uint8_t *)lane_ptr + K_GLOBAL_SPARSE * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B;\n\t\t  }\n\t  }\n\n      __syncthreads();\n\n\t#pragma unroll\n      for (int k_step = 0; k_step < CHUNK_K_SPARSE; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::col_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n\t#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i += 1) {\n\t\t\tsize_t shmem_idx_a = (warpId % BLOCK_COL_WARPS) * M * WARP_COL_TILES + (i * M);\n\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_a + k_step * K * SHARED_OFFSET_A;\n\n\t\t\twmma::load_matrix_sync(a[i], tile_ptr, SHARED_OFFSET_A);\n\t\t#pragma unroll\n\t\t\tfor(int j = 0; j < WARP_ROW_TILES; j += 1){\n\t\t\t\tif(i == 0){\n\t\t\t\t\tsize_t shmem_idx_b = shmem_idx_b_off +\n\t\t\t\t\t\t\t\t\t\t\t(warpId / BLOCK_COL_WARPS) * (WARP_ROW_TILES * N) * SHARED_OFFSET_B +\n\t\t\t\t\t\t\t\t\t\t\t(j * N) * SHARED_OFFSET_B;\n\t\t\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_b + k_step * K;\n\t\t\t\t\twmma::load_matrix_sync(b[j], tile_ptr, SHARED_OFFSET_B);\n\t\t\t\t}\n\t\t\t\twmma::mma_sync(c[j][i], a[i], b[j], c[j][i]);\n\t\t\t}\n\n        }\n      }\n\n      __syncthreads();\n    }\n\n    // This pointer is used to access the C and D matrix tiles this warp computes.\n\tint *shmem_warp_tile_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * N * WARP_ROW_TILES * SHMEM_STRIDE +\n\t(warpId % BLOCK_COL_WARPS) * SHMEM_OFFSET;\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t#pragma unroll\n\t\t\tfor(int t = 0; t < c[i][j].num_elements; t += 1){\n\t\t\t\tc[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n\t\t\t}\n\t\t\tint *tile_ptr = shmem_warp_tile_ptr + i * N * SHMEM_STRIDE + j * M;\n\t\t\twmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, C_LAYOUT);\n\t\t}\n\t}\n\n    __syncthreads();\n\n\tint *shmem_warp_stream_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N * SHMEM_STRIDE\n\t\t\t\t\t\t\t\t\t+ (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tconst size_t gmem_idx =\n\t\t(block_tile_i * N + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N) * GLOBAL_MEM_STRIDE +\n\t\tblock_tile_j * M + (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tuint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n\tint *shmem_lane_stream_ptr =\n\t\tshmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * SHMEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\t\n\tuint8_t *dst_gmem_lane_stream_ptr =\n\t\tdst_gmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * GLOBAL_MEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\n\tfor(int i = 0; i < WARP_ROW_TILES * N; i += SHARED_TO_GLOBAL_LINES_PER_WARP){\n\t\t*(dst_gmem_lane_stream_ptr + i * GLOBAL_MEM_STRIDE) = (uint8_t)(*(shmem_lane_stream_ptr + i * SHMEM_STRIDE));\n\t}\n\n\t__syncthreads();\n}\n", "launch_config": {"dimBlock": [256, 1], "dimGrid": [256, 1]}}, "101": {"code": "\n__global__ void compute_gemm_imma(float *input0, float *input1, float *input2, float* input3, float* input4, float*input5, float* input6,\n                                  float *output0) {\n    // keys need to be replaced 4096, 4096, K_GLOBA_VALUEL, 0.75, 64\n\n\t//extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B = reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_index = reinterpret_cast<int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n\tconst int WARP_SIZE = 32;\n    const int M = 16;\n    const int K = 16;\n    const int N = 16;\n    const int WMMA_M = 16;\n    const int WMMA_K = 16;\n    const int WMMA_N = 16;\n    const int M_GLOBAL = 4096;\n    const int K_GLOBAL = 1024;\n    const int N_GLOBAL = 4096;\n    const float SPARSITY = 0.75;\n    const int K_GLOBAL_SPARSE = (int(K_GLOBAL * (1-SPARSITY)));\n    const int M_TILES = (M_GLOBAL / M);\n    const int N_TILES = (N_GLOBAL / N);\n    const int K_TILES = (K_GLOBAL / K);\n    const int BLOCK_ROW_WARPS = 2;\n    const int BLOCK_COL_WARPS = 4;\n    const int WARP_ROW_TILES = 4;\n    const int WARP_COL_TILES = 2;\n    const int SKEW_UINT8 = 16;\n    const int BANK_VAL = 32;\n    const int NUM_BANK = (K_GLOBAL / BANK_VAL);\n    /////////// BLOCK_ROW_TILES <= N_TILES ////////////\n    // const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    // ////////// BLOCK_COL_TILES <= M_TILES ////////////\n    // const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    auto C_LAYOUT = wmma::mem_col_major;\n    const int CHUNK_K = 64;\n    const int CHUNK_K_SPARSE = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K_SPARSE = (int((CHUNK_K * K) * (1 - SPARSITY)));\n    const int WARP_COPY_BYTES = (WARP_SIZE * sizeof(int4));\n    const int CHUNK_LINE_BYTES_A (BLOCK_COL_TILES * M * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_A = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_A);\n    const int CHUNK_COPY_LINE_LANES_A = (CHUNK_LINE_BYTES_A / sizeof(int4));\n    const int SHARED_OFFSET_A = (BLOCK_COL_TILES * M + SKEW_UINT8);\n\n    const int CHUNK_LINE_BYTES_B = (BLOCK_SIZE_K_SPARSE * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_B = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_B);\n    const int CHUNK_COPY_LINE_LANES_B = (CHUNK_LINE_BYTES_B / sizeof(int4));\n    const int SHARED_OFFSET_B = (BLOCK_SIZE_K_SPARSE + SKEW_UINT8);\n    const int SHARED_TO_GLOBAL_BYTES_PER_LINE = ((WARP_COL_TILES * M) * sizeof(int));\n    const int SHARED_TO_GLOBAL_BYTES_PER_WARP = (WARP_SIZE * sizeof(int));\n    const int SHARED_TO_GLOBAL_LINES_PER_WARP = (SHARED_TO_GLOBAL_BYTES_PER_WARP / SHARED_TO_GLOBAL_BYTES_PER_LINE);\n    const int SHARED_TO_GLOBAL_LANES_PER_LINE = (WARP_SIZE / SHARED_TO_GLOBAL_LINES_PER_WARP);\n    const int SHARED_TO_GLOBAL_ITERS = ((WARP_ROW_TILES * N) / SHARED_TO_GLOBAL_LINES_PER_WARP);\n\n    const int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const int WARP_STRIDE = (WARP_COL_TILES * M);\n    const int WARPS_PER_BLOCK = (BLOCK_ROW_WARPS * BLOCK_COL_WARPS);\n    const int THREADS_PER_BLOCK = (WARP_SIZE * WARPS_PER_BLOCK);\n    const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    const int GLOBAL_MEM_STRIDE = M_GLOBAL;\n    const int SHMEM_STRIDE = (M * BLOCK_COL_TILES);\n    const int SHMEM_OFFSET = (M * WARP_COL_TILES);\n    const int BLOCK_SIZE_M = (M * BLOCK_COL_TILES);\n    const int BLOCK_SIZE_N = (N * BLOCK_ROW_TILES);\n\n   extern __shared__ uint8_t shmem[];\n\n\t// Warp and lane identification.\n\tconst unsigned int warpId = threadIdx.x / WARP_SIZE;\n\tconst unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n\n\t// Offset in shared memory from which the B matrix is stored.\n\t// const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\tconst size_t shmem_idx_b_off = BLOCK_SIZE_K_SPARSE * SHARED_OFFSET_A;\n\n\n\t// Each CTA slides along the 128 x 128 tiles from the top left corner of the\n\t// matrix to the right and down, and selects the next tile to compute. Once\n\t// there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n\tconst unsigned int block_tile_i =\n\t\t((block_pos * BLOCK_COL_TILES) / M_TILES) * (BLOCK_ROW_TILES);\n\tconst unsigned int block_tile_j = (block_pos * BLOCK_COL_TILES) % M_TILES;\n\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n\twmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_ROW_TILES]\n\t\t\t\t\t\t\t\t\t\t\t\t\t [WARP_COL_TILES];\n\n    // Load the C matrix tiles into fragments from shared memory.\n\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t\twmma::fill_fragment(c[i][j], 0);\n\t\t}\n\t}\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    //for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n    for(int tile_k_idx_sparse = 0, tile_k_idx = 0; tile_k_idx_sparse < K_GLOBAL_SPARSE; tile_k_idx_sparse += BLOCK_SIZE_K_SPARSE, tile_k_idx += BLOCK_SIZE_K){\n\n\t\tsize_t shmem_idx = \n\t\twarpId < (WARPS_PER_BLOCK / 2)\n\t\t\t? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A\n\t\t\t: (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B + shmem_idx_b_off;\n\n\t\tint4 *lane_ptr = NULL;\n\t\tint *lane_ptr_index = NULL;\n\t\tconst uint8_t *warp_ptr = NULL;\n\n\n\t\tif(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\t//warp_ptr = &A[block_tile_j * M] +\n\t\t\t//\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * M_GLOBAL;\n\t\t\twarp_ptr = &A[block_tile_j * M];\n\t\t\t\n\t\t\tconst int *warp_ptr_index = &B_index[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t\t\t\t\t\t((warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A);\n\n\t\t\tlane_ptr_index = (int *)(warp_ptr_index + tile_k_idx_sparse + (laneId / CHUNK_COPY_LINE_LANES_A));\n\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_A) * SHARED_OFFSET_A;\n\t\t}else{\n\t\t\twarp_ptr = &B[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * K_GLOBAL_SPARSE;\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_sparse +\n\t\t\t\t\t\t\t\t(laneId / CHUNK_COPY_LINE_LANES_B) * K_GLOBAL_SPARSE) +\n\t\t\t\t\t\t\t\t(laneId % CHUNK_COPY_LINE_LANES_B);\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_B) * SHARED_OFFSET_B;\n\t\t}\n\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      // shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n\t  int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n\t  \t? BLOCK_SIZE_K_SPARSE / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A)\n\t\t: BLOCK_SIZE_N / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\n\t  /*\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\t  */\n\n\t  /*\n      int tile_k_idx_A;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n          tile_k_idx_A = *(lane_ptr_index);\n      }\n\t  */\n\n\t  #pragma unroll\n\t  for(int i = 0; i < iter_index; i += 1){\n\t\t  if(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\tint tile_k_idx_A = *(lane_ptr_index);\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_A * M_GLOBAL) + (laneId % CHUNK_COPY_LINE_LANES_A);\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_A)) =\n\t\t\t\t*lane_ptr;\n\t\t\t//warp_ptr = (uint8_t *)((uint8_t *)warp_ptr + M_GLOBAL * (WARPS_PER_BLOCK / 2) *CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tlane_ptr_index = (int *)((int *)lane_ptr_index +  (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A;\n\t\t  }else{\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_B)) =\n\t\t\t\t*lane_ptr;\n\t\t\tlane_ptr = (int4 *)((uint8_t *)lane_ptr + K_GLOBAL_SPARSE * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B;\n\t\t  }\n\t  }\n\n      __syncthreads();\n\n\t#pragma unroll\n      for (int k_step = 0; k_step < CHUNK_K_SPARSE; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::col_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n\t#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i += 1) {\n\t\t\tsize_t shmem_idx_a = (warpId % BLOCK_COL_WARPS) * M * WARP_COL_TILES + (i * M);\n\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_a + k_step * K * SHARED_OFFSET_A;\n\n\t\t\twmma::load_matrix_sync(a[i], tile_ptr, SHARED_OFFSET_A);\n\t\t#pragma unroll\n\t\t\tfor(int j = 0; j < WARP_ROW_TILES; j += 1){\n\t\t\t\tif(i == 0){\n\t\t\t\t\tsize_t shmem_idx_b = shmem_idx_b_off +\n\t\t\t\t\t\t\t\t\t\t\t(warpId / BLOCK_COL_WARPS) * (WARP_ROW_TILES * N) * SHARED_OFFSET_B +\n\t\t\t\t\t\t\t\t\t\t\t(j * N) * SHARED_OFFSET_B;\n\t\t\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_b + k_step * K;\n\t\t\t\t\twmma::load_matrix_sync(b[j], tile_ptr, SHARED_OFFSET_B);\n\t\t\t\t}\n\t\t\t\twmma::mma_sync(c[j][i], a[i], b[j], c[j][i]);\n\t\t\t}\n\n        }\n      }\n\n      __syncthreads();\n    }\n\n    // This pointer is used to access the C and D matrix tiles this warp computes.\n\tint *shmem_warp_tile_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * N * WARP_ROW_TILES * SHMEM_STRIDE +\n\t(warpId % BLOCK_COL_WARPS) * SHMEM_OFFSET;\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t#pragma unroll\n\t\t\tfor(int t = 0; t < c[i][j].num_elements; t += 1){\n\t\t\t\tc[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n\t\t\t}\n\t\t\tint *tile_ptr = shmem_warp_tile_ptr + i * N * SHMEM_STRIDE + j * M;\n\t\t\twmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, C_LAYOUT);\n\t\t}\n\t}\n\n    __syncthreads();\n\n\tint *shmem_warp_stream_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N * SHMEM_STRIDE\n\t\t\t\t\t\t\t\t\t+ (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tconst size_t gmem_idx =\n\t\t(block_tile_i * N + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N) * GLOBAL_MEM_STRIDE +\n\t\tblock_tile_j * M + (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tuint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n\tint *shmem_lane_stream_ptr =\n\t\tshmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * SHMEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\t\n\tuint8_t *dst_gmem_lane_stream_ptr =\n\t\tdst_gmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * GLOBAL_MEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\n\tfor(int i = 0; i < WARP_ROW_TILES * N; i += SHARED_TO_GLOBAL_LINES_PER_WARP){\n\t\t*(dst_gmem_lane_stream_ptr + i * GLOBAL_MEM_STRIDE) = (uint8_t)(*(shmem_lane_stream_ptr + i * SHMEM_STRIDE));\n\t}\n\n\t__syncthreads();\n}\n", "launch_config": {"dimBlock": [1024, 1], "dimGrid": [256, 1]}}, "102": {"code": "\n__global__ void compute_gemm_imma(float *input0, float *input1, float *input2, float* input3, float* input4, float*input5, float* input6,\n                                  float *output0) {\n    // keys need to be replaced 4096, 1024, K_GLOBA_VALUEL, 0.75, 64\n\n\t//extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B = reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_index = reinterpret_cast<int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n\tconst int WARP_SIZE = 32;\n    const int M = 16;\n    const int K = 16;\n    const int N = 16;\n    const int WMMA_M = 16;\n    const int WMMA_K = 16;\n    const int WMMA_N = 16;\n    const int M_GLOBAL = 4096;\n    const int K_GLOBAL = 4096;\n    const int N_GLOBAL = 1024;\n    const float SPARSITY = 0.75;\n    const int K_GLOBAL_SPARSE = (int(K_GLOBAL * (1-SPARSITY)));\n    const int M_TILES = (M_GLOBAL / M);\n    const int N_TILES = (N_GLOBAL / N);\n    const int K_TILES = (K_GLOBAL / K);\n    const int BLOCK_ROW_WARPS = 2;\n    const int BLOCK_COL_WARPS = 4;\n    const int WARP_ROW_TILES = 4;\n    const int WARP_COL_TILES = 2;\n    const int SKEW_UINT8 = 16;\n    const int BANK_VAL = 32;\n    const int NUM_BANK = (K_GLOBAL / BANK_VAL);\n    /////////// BLOCK_ROW_TILES <= N_TILES ////////////\n    // const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    // ////////// BLOCK_COL_TILES <= M_TILES ////////////\n    // const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    auto C_LAYOUT = wmma::mem_col_major;\n    const int CHUNK_K = 64;\n    const int CHUNK_K_SPARSE = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K_SPARSE = (int((CHUNK_K * K) * (1 - SPARSITY)));\n    const int WARP_COPY_BYTES = (WARP_SIZE * sizeof(int4));\n    const int CHUNK_LINE_BYTES_A (BLOCK_COL_TILES * M * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_A = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_A);\n    const int CHUNK_COPY_LINE_LANES_A = (CHUNK_LINE_BYTES_A / sizeof(int4));\n    const int SHARED_OFFSET_A = (BLOCK_COL_TILES * M + SKEW_UINT8);\n\n    const int CHUNK_LINE_BYTES_B = (BLOCK_SIZE_K_SPARSE * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_B = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_B);\n    const int CHUNK_COPY_LINE_LANES_B = (CHUNK_LINE_BYTES_B / sizeof(int4));\n    const int SHARED_OFFSET_B = (BLOCK_SIZE_K_SPARSE + SKEW_UINT8);\n    const int SHARED_TO_GLOBAL_BYTES_PER_LINE = ((WARP_COL_TILES * M) * sizeof(int));\n    const int SHARED_TO_GLOBAL_BYTES_PER_WARP = (WARP_SIZE * sizeof(int));\n    const int SHARED_TO_GLOBAL_LINES_PER_WARP = (SHARED_TO_GLOBAL_BYTES_PER_WARP / SHARED_TO_GLOBAL_BYTES_PER_LINE);\n    const int SHARED_TO_GLOBAL_LANES_PER_LINE = (WARP_SIZE / SHARED_TO_GLOBAL_LINES_PER_WARP);\n    const int SHARED_TO_GLOBAL_ITERS = ((WARP_ROW_TILES * N) / SHARED_TO_GLOBAL_LINES_PER_WARP);\n\n    const int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const int WARP_STRIDE = (WARP_COL_TILES * M);\n    const int WARPS_PER_BLOCK = (BLOCK_ROW_WARPS * BLOCK_COL_WARPS);\n    const int THREADS_PER_BLOCK = (WARP_SIZE * WARPS_PER_BLOCK);\n    const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    const int GLOBAL_MEM_STRIDE = M_GLOBAL;\n    const int SHMEM_STRIDE = (M * BLOCK_COL_TILES);\n    const int SHMEM_OFFSET = (M * WARP_COL_TILES);\n    const int BLOCK_SIZE_M = (M * BLOCK_COL_TILES);\n    const int BLOCK_SIZE_N = (N * BLOCK_ROW_TILES);\n\n   extern __shared__ uint8_t shmem[];\n\n\t// Warp and lane identification.\n\tconst unsigned int warpId = threadIdx.x / WARP_SIZE;\n\tconst unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n\n\t// Offset in shared memory from which the B matrix is stored.\n\t// const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\tconst size_t shmem_idx_b_off = BLOCK_SIZE_K_SPARSE * SHARED_OFFSET_A;\n\n\n\t// Each CTA slides along the 128 x 128 tiles from the top left corner of the\n\t// matrix to the right and down, and selects the next tile to compute. Once\n\t// there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n\tconst unsigned int block_tile_i =\n\t\t((block_pos * BLOCK_COL_TILES) / M_TILES) * (BLOCK_ROW_TILES);\n\tconst unsigned int block_tile_j = (block_pos * BLOCK_COL_TILES) % M_TILES;\n\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n\twmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_ROW_TILES]\n\t\t\t\t\t\t\t\t\t\t\t\t\t [WARP_COL_TILES];\n\n    // Load the C matrix tiles into fragments from shared memory.\n\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t\twmma::fill_fragment(c[i][j], 0);\n\t\t}\n\t}\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    //for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n    for(int tile_k_idx_sparse = 0, tile_k_idx = 0; tile_k_idx_sparse < K_GLOBAL_SPARSE; tile_k_idx_sparse += BLOCK_SIZE_K_SPARSE, tile_k_idx += BLOCK_SIZE_K){\n\n\t\tsize_t shmem_idx = \n\t\twarpId < (WARPS_PER_BLOCK / 2)\n\t\t\t? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A\n\t\t\t: (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B + shmem_idx_b_off;\n\n\t\tint4 *lane_ptr = NULL;\n\t\tint *lane_ptr_index = NULL;\n\t\tconst uint8_t *warp_ptr = NULL;\n\n\n\t\tif(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\t//warp_ptr = &A[block_tile_j * M] +\n\t\t\t//\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * M_GLOBAL;\n\t\t\twarp_ptr = &A[block_tile_j * M];\n\t\t\t\n\t\t\tconst int *warp_ptr_index = &B_index[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t\t\t\t\t\t((warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A);\n\n\t\t\tlane_ptr_index = (int *)(warp_ptr_index + tile_k_idx_sparse + (laneId / CHUNK_COPY_LINE_LANES_A));\n\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_A) * SHARED_OFFSET_A;\n\t\t}else{\n\t\t\twarp_ptr = &B[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * K_GLOBAL_SPARSE;\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_sparse +\n\t\t\t\t\t\t\t\t(laneId / CHUNK_COPY_LINE_LANES_B) * K_GLOBAL_SPARSE) +\n\t\t\t\t\t\t\t\t(laneId % CHUNK_COPY_LINE_LANES_B);\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_B) * SHARED_OFFSET_B;\n\t\t}\n\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      // shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n\t  int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n\t  \t? BLOCK_SIZE_K_SPARSE / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A)\n\t\t: BLOCK_SIZE_N / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\n\t  /*\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\t  */\n\n\t  /*\n      int tile_k_idx_A;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n          tile_k_idx_A = *(lane_ptr_index);\n      }\n\t  */\n\n\t  #pragma unroll\n\t  for(int i = 0; i < iter_index; i += 1){\n\t\t  if(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\tint tile_k_idx_A = *(lane_ptr_index);\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_A * M_GLOBAL) + (laneId % CHUNK_COPY_LINE_LANES_A);\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_A)) =\n\t\t\t\t*lane_ptr;\n\t\t\t//warp_ptr = (uint8_t *)((uint8_t *)warp_ptr + M_GLOBAL * (WARPS_PER_BLOCK / 2) *CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tlane_ptr_index = (int *)((int *)lane_ptr_index +  (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A;\n\t\t  }else{\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_B)) =\n\t\t\t\t*lane_ptr;\n\t\t\tlane_ptr = (int4 *)((uint8_t *)lane_ptr + K_GLOBAL_SPARSE * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B;\n\t\t  }\n\t  }\n\n      __syncthreads();\n\n\t#pragma unroll\n      for (int k_step = 0; k_step < CHUNK_K_SPARSE; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::col_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n\t#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i += 1) {\n\t\t\tsize_t shmem_idx_a = (warpId % BLOCK_COL_WARPS) * M * WARP_COL_TILES + (i * M);\n\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_a + k_step * K * SHARED_OFFSET_A;\n\n\t\t\twmma::load_matrix_sync(a[i], tile_ptr, SHARED_OFFSET_A);\n\t\t#pragma unroll\n\t\t\tfor(int j = 0; j < WARP_ROW_TILES; j += 1){\n\t\t\t\tif(i == 0){\n\t\t\t\t\tsize_t shmem_idx_b = shmem_idx_b_off +\n\t\t\t\t\t\t\t\t\t\t\t(warpId / BLOCK_COL_WARPS) * (WARP_ROW_TILES * N) * SHARED_OFFSET_B +\n\t\t\t\t\t\t\t\t\t\t\t(j * N) * SHARED_OFFSET_B;\n\t\t\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_b + k_step * K;\n\t\t\t\t\twmma::load_matrix_sync(b[j], tile_ptr, SHARED_OFFSET_B);\n\t\t\t\t}\n\t\t\t\twmma::mma_sync(c[j][i], a[i], b[j], c[j][i]);\n\t\t\t}\n\n        }\n      }\n\n      __syncthreads();\n    }\n\n    // This pointer is used to access the C and D matrix tiles this warp computes.\n\tint *shmem_warp_tile_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * N * WARP_ROW_TILES * SHMEM_STRIDE +\n\t(warpId % BLOCK_COL_WARPS) * SHMEM_OFFSET;\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t#pragma unroll\n\t\t\tfor(int t = 0; t < c[i][j].num_elements; t += 1){\n\t\t\t\tc[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n\t\t\t}\n\t\t\tint *tile_ptr = shmem_warp_tile_ptr + i * N * SHMEM_STRIDE + j * M;\n\t\t\twmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, C_LAYOUT);\n\t\t}\n\t}\n\n    __syncthreads();\n\n\tint *shmem_warp_stream_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N * SHMEM_STRIDE\n\t\t\t\t\t\t\t\t\t+ (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tconst size_t gmem_idx =\n\t\t(block_tile_i * N + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N) * GLOBAL_MEM_STRIDE +\n\t\tblock_tile_j * M + (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tuint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n\tint *shmem_lane_stream_ptr =\n\t\tshmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * SHMEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\t\n\tuint8_t *dst_gmem_lane_stream_ptr =\n\t\tdst_gmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * GLOBAL_MEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\n\tfor(int i = 0; i < WARP_ROW_TILES * N; i += SHARED_TO_GLOBAL_LINES_PER_WARP){\n\t\t*(dst_gmem_lane_stream_ptr + i * GLOBAL_MEM_STRIDE) = (uint8_t)(*(shmem_lane_stream_ptr + i * SHMEM_STRIDE));\n\t}\n\n\t__syncthreads();\n}\n", "launch_config": {"dimBlock": [256, 1], "dimGrid": [256, 1]}}, "103": {"code": "\n__global__ void compute_gemm_imma(float *input0, float *input1, float *input2, float* input3, float* input4, float*input5, float* input6,\n                                  float *output0) {\n    // keys need to be replaced 4096, 1024, K_GLOBA_VALUEL, 0.75, 64\n\n\t//extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B = reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_index = reinterpret_cast<int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n\tconst int WARP_SIZE = 32;\n    const int M = 16;\n    const int K = 16;\n    const int N = 16;\n    const int WMMA_M = 16;\n    const int WMMA_K = 16;\n    const int WMMA_N = 16;\n    const int M_GLOBAL = 4096;\n    const int K_GLOBAL = 1024;\n    const int N_GLOBAL = 1024;\n    const float SPARSITY = 0.75;\n    const int K_GLOBAL_SPARSE = (int(K_GLOBAL * (1-SPARSITY)));\n    const int M_TILES = (M_GLOBAL / M);\n    const int N_TILES = (N_GLOBAL / N);\n    const int K_TILES = (K_GLOBAL / K);\n    const int BLOCK_ROW_WARPS = 2;\n    const int BLOCK_COL_WARPS = 4;\n    const int WARP_ROW_TILES = 4;\n    const int WARP_COL_TILES = 2;\n    const int SKEW_UINT8 = 16;\n    const int BANK_VAL = 32;\n    const int NUM_BANK = (K_GLOBAL / BANK_VAL);\n    /////////// BLOCK_ROW_TILES <= N_TILES ////////////\n    // const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    // ////////// BLOCK_COL_TILES <= M_TILES ////////////\n    // const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    auto C_LAYOUT = wmma::mem_col_major;\n    const int CHUNK_K = 64;\n    const int CHUNK_K_SPARSE = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K_SPARSE = (int((CHUNK_K * K) * (1 - SPARSITY)));\n    const int WARP_COPY_BYTES = (WARP_SIZE * sizeof(int4));\n    const int CHUNK_LINE_BYTES_A (BLOCK_COL_TILES * M * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_A = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_A);\n    const int CHUNK_COPY_LINE_LANES_A = (CHUNK_LINE_BYTES_A / sizeof(int4));\n    const int SHARED_OFFSET_A = (BLOCK_COL_TILES * M + SKEW_UINT8);\n\n    const int CHUNK_LINE_BYTES_B = (BLOCK_SIZE_K_SPARSE * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_B = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_B);\n    const int CHUNK_COPY_LINE_LANES_B = (CHUNK_LINE_BYTES_B / sizeof(int4));\n    const int SHARED_OFFSET_B = (BLOCK_SIZE_K_SPARSE + SKEW_UINT8);\n    const int SHARED_TO_GLOBAL_BYTES_PER_LINE = ((WARP_COL_TILES * M) * sizeof(int));\n    const int SHARED_TO_GLOBAL_BYTES_PER_WARP = (WARP_SIZE * sizeof(int));\n    const int SHARED_TO_GLOBAL_LINES_PER_WARP = (SHARED_TO_GLOBAL_BYTES_PER_WARP / SHARED_TO_GLOBAL_BYTES_PER_LINE);\n    const int SHARED_TO_GLOBAL_LANES_PER_LINE = (WARP_SIZE / SHARED_TO_GLOBAL_LINES_PER_WARP);\n    const int SHARED_TO_GLOBAL_ITERS = ((WARP_ROW_TILES * N) / SHARED_TO_GLOBAL_LINES_PER_WARP);\n\n    const int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const int WARP_STRIDE = (WARP_COL_TILES * M);\n    const int WARPS_PER_BLOCK = (BLOCK_ROW_WARPS * BLOCK_COL_WARPS);\n    const int THREADS_PER_BLOCK = (WARP_SIZE * WARPS_PER_BLOCK);\n    const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    const int GLOBAL_MEM_STRIDE = M_GLOBAL;\n    const int SHMEM_STRIDE = (M * BLOCK_COL_TILES);\n    const int SHMEM_OFFSET = (M * WARP_COL_TILES);\n    const int BLOCK_SIZE_M = (M * BLOCK_COL_TILES);\n    const int BLOCK_SIZE_N = (N * BLOCK_ROW_TILES);\n\n   extern __shared__ uint8_t shmem[];\n\n\t// Warp and lane identification.\n\tconst unsigned int warpId = threadIdx.x / WARP_SIZE;\n\tconst unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n\n\t// Offset in shared memory from which the B matrix is stored.\n\t// const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\tconst size_t shmem_idx_b_off = BLOCK_SIZE_K_SPARSE * SHARED_OFFSET_A;\n\n\n\t// Each CTA slides along the 128 x 128 tiles from the top left corner of the\n\t// matrix to the right and down, and selects the next tile to compute. Once\n\t// there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n\tconst unsigned int block_tile_i =\n\t\t((block_pos * BLOCK_COL_TILES) / M_TILES) * (BLOCK_ROW_TILES);\n\tconst unsigned int block_tile_j = (block_pos * BLOCK_COL_TILES) % M_TILES;\n\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n\twmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_ROW_TILES]\n\t\t\t\t\t\t\t\t\t\t\t\t\t [WARP_COL_TILES];\n\n    // Load the C matrix tiles into fragments from shared memory.\n\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t\twmma::fill_fragment(c[i][j], 0);\n\t\t}\n\t}\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    //for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n    for(int tile_k_idx_sparse = 0, tile_k_idx = 0; tile_k_idx_sparse < K_GLOBAL_SPARSE; tile_k_idx_sparse += BLOCK_SIZE_K_SPARSE, tile_k_idx += BLOCK_SIZE_K){\n\n\t\tsize_t shmem_idx = \n\t\twarpId < (WARPS_PER_BLOCK / 2)\n\t\t\t? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A\n\t\t\t: (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B + shmem_idx_b_off;\n\n\t\tint4 *lane_ptr = NULL;\n\t\tint *lane_ptr_index = NULL;\n\t\tconst uint8_t *warp_ptr = NULL;\n\n\n\t\tif(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\t//warp_ptr = &A[block_tile_j * M] +\n\t\t\t//\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * M_GLOBAL;\n\t\t\twarp_ptr = &A[block_tile_j * M];\n\t\t\t\n\t\t\tconst int *warp_ptr_index = &B_index[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t\t\t\t\t\t((warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A);\n\n\t\t\tlane_ptr_index = (int *)(warp_ptr_index + tile_k_idx_sparse + (laneId / CHUNK_COPY_LINE_LANES_A));\n\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_A) * SHARED_OFFSET_A;\n\t\t}else{\n\t\t\twarp_ptr = &B[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * K_GLOBAL_SPARSE;\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_sparse +\n\t\t\t\t\t\t\t\t(laneId / CHUNK_COPY_LINE_LANES_B) * K_GLOBAL_SPARSE) +\n\t\t\t\t\t\t\t\t(laneId % CHUNK_COPY_LINE_LANES_B);\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_B) * SHARED_OFFSET_B;\n\t\t}\n\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      // shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n\t  int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n\t  \t? BLOCK_SIZE_K_SPARSE / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A)\n\t\t: BLOCK_SIZE_N / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\n\t  /*\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\t  */\n\n\t  /*\n      int tile_k_idx_A;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n          tile_k_idx_A = *(lane_ptr_index);\n      }\n\t  */\n\n\t  #pragma unroll\n\t  for(int i = 0; i < iter_index; i += 1){\n\t\t  if(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\tint tile_k_idx_A = *(lane_ptr_index);\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_A * M_GLOBAL) + (laneId % CHUNK_COPY_LINE_LANES_A);\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_A)) =\n\t\t\t\t*lane_ptr;\n\t\t\t//warp_ptr = (uint8_t *)((uint8_t *)warp_ptr + M_GLOBAL * (WARPS_PER_BLOCK / 2) *CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tlane_ptr_index = (int *)((int *)lane_ptr_index +  (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A;\n\t\t  }else{\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_B)) =\n\t\t\t\t*lane_ptr;\n\t\t\tlane_ptr = (int4 *)((uint8_t *)lane_ptr + K_GLOBAL_SPARSE * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B;\n\t\t  }\n\t  }\n\n      __syncthreads();\n\n\t#pragma unroll\n      for (int k_step = 0; k_step < CHUNK_K_SPARSE; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::col_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n\t#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i += 1) {\n\t\t\tsize_t shmem_idx_a = (warpId % BLOCK_COL_WARPS) * M * WARP_COL_TILES + (i * M);\n\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_a + k_step * K * SHARED_OFFSET_A;\n\n\t\t\twmma::load_matrix_sync(a[i], tile_ptr, SHARED_OFFSET_A);\n\t\t#pragma unroll\n\t\t\tfor(int j = 0; j < WARP_ROW_TILES; j += 1){\n\t\t\t\tif(i == 0){\n\t\t\t\t\tsize_t shmem_idx_b = shmem_idx_b_off +\n\t\t\t\t\t\t\t\t\t\t\t(warpId / BLOCK_COL_WARPS) * (WARP_ROW_TILES * N) * SHARED_OFFSET_B +\n\t\t\t\t\t\t\t\t\t\t\t(j * N) * SHARED_OFFSET_B;\n\t\t\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_b + k_step * K;\n\t\t\t\t\twmma::load_matrix_sync(b[j], tile_ptr, SHARED_OFFSET_B);\n\t\t\t\t}\n\t\t\t\twmma::mma_sync(c[j][i], a[i], b[j], c[j][i]);\n\t\t\t}\n\n        }\n      }\n\n      __syncthreads();\n    }\n\n    // This pointer is used to access the C and D matrix tiles this warp computes.\n\tint *shmem_warp_tile_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * N * WARP_ROW_TILES * SHMEM_STRIDE +\n\t(warpId % BLOCK_COL_WARPS) * SHMEM_OFFSET;\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t#pragma unroll\n\t\t\tfor(int t = 0; t < c[i][j].num_elements; t += 1){\n\t\t\t\tc[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n\t\t\t}\n\t\t\tint *tile_ptr = shmem_warp_tile_ptr + i * N * SHMEM_STRIDE + j * M;\n\t\t\twmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, C_LAYOUT);\n\t\t}\n\t}\n\n    __syncthreads();\n\n\tint *shmem_warp_stream_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N * SHMEM_STRIDE\n\t\t\t\t\t\t\t\t\t+ (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tconst size_t gmem_idx =\n\t\t(block_tile_i * N + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N) * GLOBAL_MEM_STRIDE +\n\t\tblock_tile_j * M + (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tuint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n\tint *shmem_lane_stream_ptr =\n\t\tshmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * SHMEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\t\n\tuint8_t *dst_gmem_lane_stream_ptr =\n\t\tdst_gmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * GLOBAL_MEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\n\tfor(int i = 0; i < WARP_ROW_TILES * N; i += SHARED_TO_GLOBAL_LINES_PER_WARP){\n\t\t*(dst_gmem_lane_stream_ptr + i * GLOBAL_MEM_STRIDE) = (uint8_t)(*(shmem_lane_stream_ptr + i * SHMEM_STRIDE));\n\t}\n\n\t__syncthreads();\n}\n", "launch_config": {"dimBlock": [256, 1], "dimGrid": [256, 1]}}, "104": {"code": "\n__global__ void compute_gemm_imma(float *input0, float *input1, float *input2, float* input3, float* input4, float*input5, float* input6,\n                                  float *output0) {\n    // keys need to be replaced 4096, 1024, K_GLOBA_VALUEL, 0.75, 64\n\n\t//extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B = reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_index = reinterpret_cast<int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n\tconst int WARP_SIZE = 32;\n    const int M = 16;\n    const int K = 16;\n    const int N = 16;\n    const int WMMA_M = 16;\n    const int WMMA_K = 16;\n    const int WMMA_N = 16;\n    const int M_GLOBAL = 4096;\n    const int K_GLOBAL = 1024;\n    const int N_GLOBAL = 1024;\n    const float SPARSITY = 0.75;\n    const int K_GLOBAL_SPARSE = (int(K_GLOBAL * (1-SPARSITY)));\n    const int M_TILES = (M_GLOBAL / M);\n    const int N_TILES = (N_GLOBAL / N);\n    const int K_TILES = (K_GLOBAL / K);\n    const int BLOCK_ROW_WARPS = 2;\n    const int BLOCK_COL_WARPS = 4;\n    const int WARP_ROW_TILES = 4;\n    const int WARP_COL_TILES = 2;\n    const int SKEW_UINT8 = 16;\n    const int BANK_VAL = 32;\n    const int NUM_BANK = (K_GLOBAL / BANK_VAL);\n    /////////// BLOCK_ROW_TILES <= N_TILES ////////////\n    // const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    // ////////// BLOCK_COL_TILES <= M_TILES ////////////\n    // const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    auto C_LAYOUT = wmma::mem_col_major;\n    const int CHUNK_K = 64;\n    const int CHUNK_K_SPARSE = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K_SPARSE = (int((CHUNK_K * K) * (1 - SPARSITY)));\n    const int WARP_COPY_BYTES = (WARP_SIZE * sizeof(int4));\n    const int CHUNK_LINE_BYTES_A (BLOCK_COL_TILES * M * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_A = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_A);\n    const int CHUNK_COPY_LINE_LANES_A = (CHUNK_LINE_BYTES_A / sizeof(int4));\n    const int SHARED_OFFSET_A = (BLOCK_COL_TILES * M + SKEW_UINT8);\n\n    const int CHUNK_LINE_BYTES_B = (BLOCK_SIZE_K_SPARSE * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_B = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_B);\n    const int CHUNK_COPY_LINE_LANES_B = (CHUNK_LINE_BYTES_B / sizeof(int4));\n    const int SHARED_OFFSET_B = (BLOCK_SIZE_K_SPARSE + SKEW_UINT8);\n    const int SHARED_TO_GLOBAL_BYTES_PER_LINE = ((WARP_COL_TILES * M) * sizeof(int));\n    const int SHARED_TO_GLOBAL_BYTES_PER_WARP = (WARP_SIZE * sizeof(int));\n    const int SHARED_TO_GLOBAL_LINES_PER_WARP = (SHARED_TO_GLOBAL_BYTES_PER_WARP / SHARED_TO_GLOBAL_BYTES_PER_LINE);\n    const int SHARED_TO_GLOBAL_LANES_PER_LINE = (WARP_SIZE / SHARED_TO_GLOBAL_LINES_PER_WARP);\n    const int SHARED_TO_GLOBAL_ITERS = ((WARP_ROW_TILES * N) / SHARED_TO_GLOBAL_LINES_PER_WARP);\n\n    const int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const int WARP_STRIDE = (WARP_COL_TILES * M);\n    const int WARPS_PER_BLOCK = (BLOCK_ROW_WARPS * BLOCK_COL_WARPS);\n    const int THREADS_PER_BLOCK = (WARP_SIZE * WARPS_PER_BLOCK);\n    const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    const int GLOBAL_MEM_STRIDE = M_GLOBAL;\n    const int SHMEM_STRIDE = (M * BLOCK_COL_TILES);\n    const int SHMEM_OFFSET = (M * WARP_COL_TILES);\n    const int BLOCK_SIZE_M = (M * BLOCK_COL_TILES);\n    const int BLOCK_SIZE_N = (N * BLOCK_ROW_TILES);\n\n   extern __shared__ uint8_t shmem[];\n\n\t// Warp and lane identification.\n\tconst unsigned int warpId = threadIdx.x / WARP_SIZE;\n\tconst unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n\n\t// Offset in shared memory from which the B matrix is stored.\n\t// const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\tconst size_t shmem_idx_b_off = BLOCK_SIZE_K_SPARSE * SHARED_OFFSET_A;\n\n\n\t// Each CTA slides along the 128 x 128 tiles from the top left corner of the\n\t// matrix to the right and down, and selects the next tile to compute. Once\n\t// there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n\tconst unsigned int block_tile_i =\n\t\t((block_pos * BLOCK_COL_TILES) / M_TILES) * (BLOCK_ROW_TILES);\n\tconst unsigned int block_tile_j = (block_pos * BLOCK_COL_TILES) % M_TILES;\n\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n\twmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_ROW_TILES]\n\t\t\t\t\t\t\t\t\t\t\t\t\t [WARP_COL_TILES];\n\n    // Load the C matrix tiles into fragments from shared memory.\n\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t\twmma::fill_fragment(c[i][j], 0);\n\t\t}\n\t}\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    //for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n    for(int tile_k_idx_sparse = 0, tile_k_idx = 0; tile_k_idx_sparse < K_GLOBAL_SPARSE; tile_k_idx_sparse += BLOCK_SIZE_K_SPARSE, tile_k_idx += BLOCK_SIZE_K){\n\n\t\tsize_t shmem_idx = \n\t\twarpId < (WARPS_PER_BLOCK / 2)\n\t\t\t? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A\n\t\t\t: (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B + shmem_idx_b_off;\n\n\t\tint4 *lane_ptr = NULL;\n\t\tint *lane_ptr_index = NULL;\n\t\tconst uint8_t *warp_ptr = NULL;\n\n\n\t\tif(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\t//warp_ptr = &A[block_tile_j * M] +\n\t\t\t//\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * M_GLOBAL;\n\t\t\twarp_ptr = &A[block_tile_j * M];\n\t\t\t\n\t\t\tconst int *warp_ptr_index = &B_index[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t\t\t\t\t\t((warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A);\n\n\t\t\tlane_ptr_index = (int *)(warp_ptr_index + tile_k_idx_sparse + (laneId / CHUNK_COPY_LINE_LANES_A));\n\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_A) * SHARED_OFFSET_A;\n\t\t}else{\n\t\t\twarp_ptr = &B[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * K_GLOBAL_SPARSE;\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_sparse +\n\t\t\t\t\t\t\t\t(laneId / CHUNK_COPY_LINE_LANES_B) * K_GLOBAL_SPARSE) +\n\t\t\t\t\t\t\t\t(laneId % CHUNK_COPY_LINE_LANES_B);\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_B) * SHARED_OFFSET_B;\n\t\t}\n\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      // shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n\t  int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n\t  \t? BLOCK_SIZE_K_SPARSE / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A)\n\t\t: BLOCK_SIZE_N / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\n\t  /*\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\t  */\n\n\t  /*\n      int tile_k_idx_A;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n          tile_k_idx_A = *(lane_ptr_index);\n      }\n\t  */\n\n\t  #pragma unroll\n\t  for(int i = 0; i < iter_index; i += 1){\n\t\t  if(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\tint tile_k_idx_A = *(lane_ptr_index);\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_A * M_GLOBAL) + (laneId % CHUNK_COPY_LINE_LANES_A);\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_A)) =\n\t\t\t\t*lane_ptr;\n\t\t\t//warp_ptr = (uint8_t *)((uint8_t *)warp_ptr + M_GLOBAL * (WARPS_PER_BLOCK / 2) *CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tlane_ptr_index = (int *)((int *)lane_ptr_index +  (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A;\n\t\t  }else{\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_B)) =\n\t\t\t\t*lane_ptr;\n\t\t\tlane_ptr = (int4 *)((uint8_t *)lane_ptr + K_GLOBAL_SPARSE * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B;\n\t\t  }\n\t  }\n\n      __syncthreads();\n\n\t#pragma unroll\n      for (int k_step = 0; k_step < CHUNK_K_SPARSE; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::col_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n\t#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i += 1) {\n\t\t\tsize_t shmem_idx_a = (warpId % BLOCK_COL_WARPS) * M * WARP_COL_TILES + (i * M);\n\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_a + k_step * K * SHARED_OFFSET_A;\n\n\t\t\twmma::load_matrix_sync(a[i], tile_ptr, SHARED_OFFSET_A);\n\t\t#pragma unroll\n\t\t\tfor(int j = 0; j < WARP_ROW_TILES; j += 1){\n\t\t\t\tif(i == 0){\n\t\t\t\t\tsize_t shmem_idx_b = shmem_idx_b_off +\n\t\t\t\t\t\t\t\t\t\t\t(warpId / BLOCK_COL_WARPS) * (WARP_ROW_TILES * N) * SHARED_OFFSET_B +\n\t\t\t\t\t\t\t\t\t\t\t(j * N) * SHARED_OFFSET_B;\n\t\t\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_b + k_step * K;\n\t\t\t\t\twmma::load_matrix_sync(b[j], tile_ptr, SHARED_OFFSET_B);\n\t\t\t\t}\n\t\t\t\twmma::mma_sync(c[j][i], a[i], b[j], c[j][i]);\n\t\t\t}\n\n        }\n      }\n\n      __syncthreads();\n    }\n\n    // This pointer is used to access the C and D matrix tiles this warp computes.\n\tint *shmem_warp_tile_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * N * WARP_ROW_TILES * SHMEM_STRIDE +\n\t(warpId % BLOCK_COL_WARPS) * SHMEM_OFFSET;\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t#pragma unroll\n\t\t\tfor(int t = 0; t < c[i][j].num_elements; t += 1){\n\t\t\t\tc[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n\t\t\t}\n\t\t\tint *tile_ptr = shmem_warp_tile_ptr + i * N * SHMEM_STRIDE + j * M;\n\t\t\twmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, C_LAYOUT);\n\t\t}\n\t}\n\n    __syncthreads();\n\n\tint *shmem_warp_stream_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N * SHMEM_STRIDE\n\t\t\t\t\t\t\t\t\t+ (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tconst size_t gmem_idx =\n\t\t(block_tile_i * N + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N) * GLOBAL_MEM_STRIDE +\n\t\tblock_tile_j * M + (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tuint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n\tint *shmem_lane_stream_ptr =\n\t\tshmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * SHMEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\t\n\tuint8_t *dst_gmem_lane_stream_ptr =\n\t\tdst_gmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * GLOBAL_MEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\n\tfor(int i = 0; i < WARP_ROW_TILES * N; i += SHARED_TO_GLOBAL_LINES_PER_WARP){\n\t\t*(dst_gmem_lane_stream_ptr + i * GLOBAL_MEM_STRIDE) = (uint8_t)(*(shmem_lane_stream_ptr + i * SHMEM_STRIDE));\n\t}\n\n\t__syncthreads();\n}\n", "launch_config": {"dimBlock": [256, 1], "dimGrid": [256, 1]}}, "105": {"code": "\n__global__ void compute_gemm_imma(float *input0, float *input1, float *input2, float* input3, float* input4, float*input5, float* input6,\n                                  float *output0) {\n    // keys need to be replaced 4096, 1024, K_GLOBA_VALUEL, 0.75, 64\n\n\t//extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B = reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_index = reinterpret_cast<int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n\tconst int WARP_SIZE = 32;\n    const int M = 16;\n    const int K = 16;\n    const int N = 16;\n    const int WMMA_M = 16;\n    const int WMMA_K = 16;\n    const int WMMA_N = 16;\n    const int M_GLOBAL = 4096;\n    const int K_GLOBAL = 1024;\n    const int N_GLOBAL = 1024;\n    const float SPARSITY = 0.75;\n    const int K_GLOBAL_SPARSE = (int(K_GLOBAL * (1-SPARSITY)));\n    const int M_TILES = (M_GLOBAL / M);\n    const int N_TILES = (N_GLOBAL / N);\n    const int K_TILES = (K_GLOBAL / K);\n    const int BLOCK_ROW_WARPS = 2;\n    const int BLOCK_COL_WARPS = 4;\n    const int WARP_ROW_TILES = 4;\n    const int WARP_COL_TILES = 2;\n    const int SKEW_UINT8 = 16;\n    const int BANK_VAL = 32;\n    const int NUM_BANK = (K_GLOBAL / BANK_VAL);\n    /////////// BLOCK_ROW_TILES <= N_TILES ////////////\n    // const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    // ////////// BLOCK_COL_TILES <= M_TILES ////////////\n    // const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    auto C_LAYOUT = wmma::mem_col_major;\n    const int CHUNK_K = 64;\n    const int CHUNK_K_SPARSE = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K_SPARSE = (int((CHUNK_K * K) * (1 - SPARSITY)));\n    const int WARP_COPY_BYTES = (WARP_SIZE * sizeof(int4));\n    const int CHUNK_LINE_BYTES_A (BLOCK_COL_TILES * M * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_A = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_A);\n    const int CHUNK_COPY_LINE_LANES_A = (CHUNK_LINE_BYTES_A / sizeof(int4));\n    const int SHARED_OFFSET_A = (BLOCK_COL_TILES * M + SKEW_UINT8);\n\n    const int CHUNK_LINE_BYTES_B = (BLOCK_SIZE_K_SPARSE * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_B = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_B);\n    const int CHUNK_COPY_LINE_LANES_B = (CHUNK_LINE_BYTES_B / sizeof(int4));\n    const int SHARED_OFFSET_B = (BLOCK_SIZE_K_SPARSE + SKEW_UINT8);\n    const int SHARED_TO_GLOBAL_BYTES_PER_LINE = ((WARP_COL_TILES * M) * sizeof(int));\n    const int SHARED_TO_GLOBAL_BYTES_PER_WARP = (WARP_SIZE * sizeof(int));\n    const int SHARED_TO_GLOBAL_LINES_PER_WARP = (SHARED_TO_GLOBAL_BYTES_PER_WARP / SHARED_TO_GLOBAL_BYTES_PER_LINE);\n    const int SHARED_TO_GLOBAL_LANES_PER_LINE = (WARP_SIZE / SHARED_TO_GLOBAL_LINES_PER_WARP);\n    const int SHARED_TO_GLOBAL_ITERS = ((WARP_ROW_TILES * N) / SHARED_TO_GLOBAL_LINES_PER_WARP);\n\n    const int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const int WARP_STRIDE = (WARP_COL_TILES * M);\n    const int WARPS_PER_BLOCK = (BLOCK_ROW_WARPS * BLOCK_COL_WARPS);\n    const int THREADS_PER_BLOCK = (WARP_SIZE * WARPS_PER_BLOCK);\n    const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    const int GLOBAL_MEM_STRIDE = M_GLOBAL;\n    const int SHMEM_STRIDE = (M * BLOCK_COL_TILES);\n    const int SHMEM_OFFSET = (M * WARP_COL_TILES);\n    const int BLOCK_SIZE_M = (M * BLOCK_COL_TILES);\n    const int BLOCK_SIZE_N = (N * BLOCK_ROW_TILES);\n\n   extern __shared__ uint8_t shmem[];\n\n\t// Warp and lane identification.\n\tconst unsigned int warpId = threadIdx.x / WARP_SIZE;\n\tconst unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n\n\t// Offset in shared memory from which the B matrix is stored.\n\t// const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\tconst size_t shmem_idx_b_off = BLOCK_SIZE_K_SPARSE * SHARED_OFFSET_A;\n\n\n\t// Each CTA slides along the 128 x 128 tiles from the top left corner of the\n\t// matrix to the right and down, and selects the next tile to compute. Once\n\t// there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n\tconst unsigned int block_tile_i =\n\t\t((block_pos * BLOCK_COL_TILES) / M_TILES) * (BLOCK_ROW_TILES);\n\tconst unsigned int block_tile_j = (block_pos * BLOCK_COL_TILES) % M_TILES;\n\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n\twmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_ROW_TILES]\n\t\t\t\t\t\t\t\t\t\t\t\t\t [WARP_COL_TILES];\n\n    // Load the C matrix tiles into fragments from shared memory.\n\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t\twmma::fill_fragment(c[i][j], 0);\n\t\t}\n\t}\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    //for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n    for(int tile_k_idx_sparse = 0, tile_k_idx = 0; tile_k_idx_sparse < K_GLOBAL_SPARSE; tile_k_idx_sparse += BLOCK_SIZE_K_SPARSE, tile_k_idx += BLOCK_SIZE_K){\n\n\t\tsize_t shmem_idx = \n\t\twarpId < (WARPS_PER_BLOCK / 2)\n\t\t\t? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A\n\t\t\t: (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B + shmem_idx_b_off;\n\n\t\tint4 *lane_ptr = NULL;\n\t\tint *lane_ptr_index = NULL;\n\t\tconst uint8_t *warp_ptr = NULL;\n\n\n\t\tif(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\t//warp_ptr = &A[block_tile_j * M] +\n\t\t\t//\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * M_GLOBAL;\n\t\t\twarp_ptr = &A[block_tile_j * M];\n\t\t\t\n\t\t\tconst int *warp_ptr_index = &B_index[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t\t\t\t\t\t((warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A);\n\n\t\t\tlane_ptr_index = (int *)(warp_ptr_index + tile_k_idx_sparse + (laneId / CHUNK_COPY_LINE_LANES_A));\n\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_A) * SHARED_OFFSET_A;\n\t\t}else{\n\t\t\twarp_ptr = &B[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * K_GLOBAL_SPARSE;\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_sparse +\n\t\t\t\t\t\t\t\t(laneId / CHUNK_COPY_LINE_LANES_B) * K_GLOBAL_SPARSE) +\n\t\t\t\t\t\t\t\t(laneId % CHUNK_COPY_LINE_LANES_B);\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_B) * SHARED_OFFSET_B;\n\t\t}\n\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      // shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n\t  int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n\t  \t? BLOCK_SIZE_K_SPARSE / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A)\n\t\t: BLOCK_SIZE_N / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\n\t  /*\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\t  */\n\n\t  /*\n      int tile_k_idx_A;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n          tile_k_idx_A = *(lane_ptr_index);\n      }\n\t  */\n\n\t  #pragma unroll\n\t  for(int i = 0; i < iter_index; i += 1){\n\t\t  if(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\tint tile_k_idx_A = *(lane_ptr_index);\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_A * M_GLOBAL) + (laneId % CHUNK_COPY_LINE_LANES_A);\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_A)) =\n\t\t\t\t*lane_ptr;\n\t\t\t//warp_ptr = (uint8_t *)((uint8_t *)warp_ptr + M_GLOBAL * (WARPS_PER_BLOCK / 2) *CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tlane_ptr_index = (int *)((int *)lane_ptr_index +  (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A;\n\t\t  }else{\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_B)) =\n\t\t\t\t*lane_ptr;\n\t\t\tlane_ptr = (int4 *)((uint8_t *)lane_ptr + K_GLOBAL_SPARSE * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B;\n\t\t  }\n\t  }\n\n      __syncthreads();\n\n\t#pragma unroll\n      for (int k_step = 0; k_step < CHUNK_K_SPARSE; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::col_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n\t#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i += 1) {\n\t\t\tsize_t shmem_idx_a = (warpId % BLOCK_COL_WARPS) * M * WARP_COL_TILES + (i * M);\n\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_a + k_step * K * SHARED_OFFSET_A;\n\n\t\t\twmma::load_matrix_sync(a[i], tile_ptr, SHARED_OFFSET_A);\n\t\t#pragma unroll\n\t\t\tfor(int j = 0; j < WARP_ROW_TILES; j += 1){\n\t\t\t\tif(i == 0){\n\t\t\t\t\tsize_t shmem_idx_b = shmem_idx_b_off +\n\t\t\t\t\t\t\t\t\t\t\t(warpId / BLOCK_COL_WARPS) * (WARP_ROW_TILES * N) * SHARED_OFFSET_B +\n\t\t\t\t\t\t\t\t\t\t\t(j * N) * SHARED_OFFSET_B;\n\t\t\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_b + k_step * K;\n\t\t\t\t\twmma::load_matrix_sync(b[j], tile_ptr, SHARED_OFFSET_B);\n\t\t\t\t}\n\t\t\t\twmma::mma_sync(c[j][i], a[i], b[j], c[j][i]);\n\t\t\t}\n\n        }\n      }\n\n      __syncthreads();\n    }\n\n    // This pointer is used to access the C and D matrix tiles this warp computes.\n\tint *shmem_warp_tile_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * N * WARP_ROW_TILES * SHMEM_STRIDE +\n\t(warpId % BLOCK_COL_WARPS) * SHMEM_OFFSET;\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t#pragma unroll\n\t\t\tfor(int t = 0; t < c[i][j].num_elements; t += 1){\n\t\t\t\tc[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n\t\t\t}\n\t\t\tint *tile_ptr = shmem_warp_tile_ptr + i * N * SHMEM_STRIDE + j * M;\n\t\t\twmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, C_LAYOUT);\n\t\t}\n\t}\n\n    __syncthreads();\n\n\tint *shmem_warp_stream_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N * SHMEM_STRIDE\n\t\t\t\t\t\t\t\t\t+ (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tconst size_t gmem_idx =\n\t\t(block_tile_i * N + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N) * GLOBAL_MEM_STRIDE +\n\t\tblock_tile_j * M + (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tuint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n\tint *shmem_lane_stream_ptr =\n\t\tshmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * SHMEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\t\n\tuint8_t *dst_gmem_lane_stream_ptr =\n\t\tdst_gmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * GLOBAL_MEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\n\tfor(int i = 0; i < WARP_ROW_TILES * N; i += SHARED_TO_GLOBAL_LINES_PER_WARP){\n\t\t*(dst_gmem_lane_stream_ptr + i * GLOBAL_MEM_STRIDE) = (uint8_t)(*(shmem_lane_stream_ptr + i * SHMEM_STRIDE));\n\t}\n\n\t__syncthreads();\n}\n", "launch_config": {"dimBlock": [256, 1], "dimGrid": [256, 1]}}, "106": {"code": "\n__global__ void compute_gemm_imma(float *input0, float *input1, float *input2, float* input3, float* input4, float*input5, float* input6,\n                                  float *output0) {\n    // keys need to be replaced 4096, 1024, K_GLOBA_VALUEL, 0.75, 64\n\n\t//extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B = reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_index = reinterpret_cast<int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n\tconst int WARP_SIZE = 32;\n    const int M = 16;\n    const int K = 16;\n    const int N = 16;\n    const int WMMA_M = 16;\n    const int WMMA_K = 16;\n    const int WMMA_N = 16;\n    const int M_GLOBAL = 4096;\n    const int K_GLOBAL = 1024;\n    const int N_GLOBAL = 1024;\n    const float SPARSITY = 0.75;\n    const int K_GLOBAL_SPARSE = (int(K_GLOBAL * (1-SPARSITY)));\n    const int M_TILES = (M_GLOBAL / M);\n    const int N_TILES = (N_GLOBAL / N);\n    const int K_TILES = (K_GLOBAL / K);\n    const int BLOCK_ROW_WARPS = 2;\n    const int BLOCK_COL_WARPS = 4;\n    const int WARP_ROW_TILES = 4;\n    const int WARP_COL_TILES = 2;\n    const int SKEW_UINT8 = 16;\n    const int BANK_VAL = 32;\n    const int NUM_BANK = (K_GLOBAL / BANK_VAL);\n    /////////// BLOCK_ROW_TILES <= N_TILES ////////////\n    // const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    // ////////// BLOCK_COL_TILES <= M_TILES ////////////\n    // const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    auto C_LAYOUT = wmma::mem_col_major;\n    const int CHUNK_K = 64;\n    const int CHUNK_K_SPARSE = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K_SPARSE = (int((CHUNK_K * K) * (1 - SPARSITY)));\n    const int WARP_COPY_BYTES = (WARP_SIZE * sizeof(int4));\n    const int CHUNK_LINE_BYTES_A (BLOCK_COL_TILES * M * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_A = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_A);\n    const int CHUNK_COPY_LINE_LANES_A = (CHUNK_LINE_BYTES_A / sizeof(int4));\n    const int SHARED_OFFSET_A = (BLOCK_COL_TILES * M + SKEW_UINT8);\n\n    const int CHUNK_LINE_BYTES_B = (BLOCK_SIZE_K_SPARSE * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_B = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_B);\n    const int CHUNK_COPY_LINE_LANES_B = (CHUNK_LINE_BYTES_B / sizeof(int4));\n    const int SHARED_OFFSET_B = (BLOCK_SIZE_K_SPARSE + SKEW_UINT8);\n    const int SHARED_TO_GLOBAL_BYTES_PER_LINE = ((WARP_COL_TILES * M) * sizeof(int));\n    const int SHARED_TO_GLOBAL_BYTES_PER_WARP = (WARP_SIZE * sizeof(int));\n    const int SHARED_TO_GLOBAL_LINES_PER_WARP = (SHARED_TO_GLOBAL_BYTES_PER_WARP / SHARED_TO_GLOBAL_BYTES_PER_LINE);\n    const int SHARED_TO_GLOBAL_LANES_PER_LINE = (WARP_SIZE / SHARED_TO_GLOBAL_LINES_PER_WARP);\n    const int SHARED_TO_GLOBAL_ITERS = ((WARP_ROW_TILES * N) / SHARED_TO_GLOBAL_LINES_PER_WARP);\n\n    const int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const int WARP_STRIDE = (WARP_COL_TILES * M);\n    const int WARPS_PER_BLOCK = (BLOCK_ROW_WARPS * BLOCK_COL_WARPS);\n    const int THREADS_PER_BLOCK = (WARP_SIZE * WARPS_PER_BLOCK);\n    const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    const int GLOBAL_MEM_STRIDE = M_GLOBAL;\n    const int SHMEM_STRIDE = (M * BLOCK_COL_TILES);\n    const int SHMEM_OFFSET = (M * WARP_COL_TILES);\n    const int BLOCK_SIZE_M = (M * BLOCK_COL_TILES);\n    const int BLOCK_SIZE_N = (N * BLOCK_ROW_TILES);\n\n   extern __shared__ uint8_t shmem[];\n\n\t// Warp and lane identification.\n\tconst unsigned int warpId = threadIdx.x / WARP_SIZE;\n\tconst unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n\n\t// Offset in shared memory from which the B matrix is stored.\n\t// const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\tconst size_t shmem_idx_b_off = BLOCK_SIZE_K_SPARSE * SHARED_OFFSET_A;\n\n\n\t// Each CTA slides along the 128 x 128 tiles from the top left corner of the\n\t// matrix to the right and down, and selects the next tile to compute. Once\n\t// there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n\tconst unsigned int block_tile_i =\n\t\t((block_pos * BLOCK_COL_TILES) / M_TILES) * (BLOCK_ROW_TILES);\n\tconst unsigned int block_tile_j = (block_pos * BLOCK_COL_TILES) % M_TILES;\n\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n\twmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_ROW_TILES]\n\t\t\t\t\t\t\t\t\t\t\t\t\t [WARP_COL_TILES];\n\n    // Load the C matrix tiles into fragments from shared memory.\n\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t\twmma::fill_fragment(c[i][j], 0);\n\t\t}\n\t}\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    //for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n    for(int tile_k_idx_sparse = 0, tile_k_idx = 0; tile_k_idx_sparse < K_GLOBAL_SPARSE; tile_k_idx_sparse += BLOCK_SIZE_K_SPARSE, tile_k_idx += BLOCK_SIZE_K){\n\n\t\tsize_t shmem_idx = \n\t\twarpId < (WARPS_PER_BLOCK / 2)\n\t\t\t? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A\n\t\t\t: (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B + shmem_idx_b_off;\n\n\t\tint4 *lane_ptr = NULL;\n\t\tint *lane_ptr_index = NULL;\n\t\tconst uint8_t *warp_ptr = NULL;\n\n\n\t\tif(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\t//warp_ptr = &A[block_tile_j * M] +\n\t\t\t//\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * M_GLOBAL;\n\t\t\twarp_ptr = &A[block_tile_j * M];\n\t\t\t\n\t\t\tconst int *warp_ptr_index = &B_index[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t\t\t\t\t\t((warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A);\n\n\t\t\tlane_ptr_index = (int *)(warp_ptr_index + tile_k_idx_sparse + (laneId / CHUNK_COPY_LINE_LANES_A));\n\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_A) * SHARED_OFFSET_A;\n\t\t}else{\n\t\t\twarp_ptr = &B[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * K_GLOBAL_SPARSE;\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_sparse +\n\t\t\t\t\t\t\t\t(laneId / CHUNK_COPY_LINE_LANES_B) * K_GLOBAL_SPARSE) +\n\t\t\t\t\t\t\t\t(laneId % CHUNK_COPY_LINE_LANES_B);\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_B) * SHARED_OFFSET_B;\n\t\t}\n\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      // shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n\t  int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n\t  \t? BLOCK_SIZE_K_SPARSE / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A)\n\t\t: BLOCK_SIZE_N / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\n\t  /*\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\t  */\n\n\t  /*\n      int tile_k_idx_A;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n          tile_k_idx_A = *(lane_ptr_index);\n      }\n\t  */\n\n\t  #pragma unroll\n\t  for(int i = 0; i < iter_index; i += 1){\n\t\t  if(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\tint tile_k_idx_A = *(lane_ptr_index);\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_A * M_GLOBAL) + (laneId % CHUNK_COPY_LINE_LANES_A);\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_A)) =\n\t\t\t\t*lane_ptr;\n\t\t\t//warp_ptr = (uint8_t *)((uint8_t *)warp_ptr + M_GLOBAL * (WARPS_PER_BLOCK / 2) *CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tlane_ptr_index = (int *)((int *)lane_ptr_index +  (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A;\n\t\t  }else{\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_B)) =\n\t\t\t\t*lane_ptr;\n\t\t\tlane_ptr = (int4 *)((uint8_t *)lane_ptr + K_GLOBAL_SPARSE * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B;\n\t\t  }\n\t  }\n\n      __syncthreads();\n\n\t#pragma unroll\n      for (int k_step = 0; k_step < CHUNK_K_SPARSE; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::col_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n\t#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i += 1) {\n\t\t\tsize_t shmem_idx_a = (warpId % BLOCK_COL_WARPS) * M * WARP_COL_TILES + (i * M);\n\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_a + k_step * K * SHARED_OFFSET_A;\n\n\t\t\twmma::load_matrix_sync(a[i], tile_ptr, SHARED_OFFSET_A);\n\t\t#pragma unroll\n\t\t\tfor(int j = 0; j < WARP_ROW_TILES; j += 1){\n\t\t\t\tif(i == 0){\n\t\t\t\t\tsize_t shmem_idx_b = shmem_idx_b_off +\n\t\t\t\t\t\t\t\t\t\t\t(warpId / BLOCK_COL_WARPS) * (WARP_ROW_TILES * N) * SHARED_OFFSET_B +\n\t\t\t\t\t\t\t\t\t\t\t(j * N) * SHARED_OFFSET_B;\n\t\t\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_b + k_step * K;\n\t\t\t\t\twmma::load_matrix_sync(b[j], tile_ptr, SHARED_OFFSET_B);\n\t\t\t\t}\n\t\t\t\twmma::mma_sync(c[j][i], a[i], b[j], c[j][i]);\n\t\t\t}\n\n        }\n      }\n\n      __syncthreads();\n    }\n\n    // This pointer is used to access the C and D matrix tiles this warp computes.\n\tint *shmem_warp_tile_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * N * WARP_ROW_TILES * SHMEM_STRIDE +\n\t(warpId % BLOCK_COL_WARPS) * SHMEM_OFFSET;\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t#pragma unroll\n\t\t\tfor(int t = 0; t < c[i][j].num_elements; t += 1){\n\t\t\t\tc[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n\t\t\t}\n\t\t\tint *tile_ptr = shmem_warp_tile_ptr + i * N * SHMEM_STRIDE + j * M;\n\t\t\twmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, C_LAYOUT);\n\t\t}\n\t}\n\n    __syncthreads();\n\n\tint *shmem_warp_stream_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N * SHMEM_STRIDE\n\t\t\t\t\t\t\t\t\t+ (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tconst size_t gmem_idx =\n\t\t(block_tile_i * N + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N) * GLOBAL_MEM_STRIDE +\n\t\tblock_tile_j * M + (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tuint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n\tint *shmem_lane_stream_ptr =\n\t\tshmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * SHMEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\t\n\tuint8_t *dst_gmem_lane_stream_ptr =\n\t\tdst_gmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * GLOBAL_MEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\n\tfor(int i = 0; i < WARP_ROW_TILES * N; i += SHARED_TO_GLOBAL_LINES_PER_WARP){\n\t\t*(dst_gmem_lane_stream_ptr + i * GLOBAL_MEM_STRIDE) = (uint8_t)(*(shmem_lane_stream_ptr + i * SHMEM_STRIDE));\n\t}\n\n\t__syncthreads();\n}\n", "launch_config": {"dimBlock": [256, 1], "dimGrid": [256, 1]}}, "107": {"code": "\n__global__ void compute_gemm_imma(float *input0, float *input1, float *input2, float* input3, float* input4, float*input5, float* input6,\n                                  float *output0) {\n    // keys need to be replaced 4096, 4096, K_GLOBA_VALUEL, 0.75, 64\n\n\t//extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B = reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_index = reinterpret_cast<int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n\tconst int WARP_SIZE = 32;\n    const int M = 16;\n    const int K = 16;\n    const int N = 16;\n    const int WMMA_M = 16;\n    const int WMMA_K = 16;\n    const int WMMA_N = 16;\n    const int M_GLOBAL = 4096;\n    const int K_GLOBAL = 1024;\n    const int N_GLOBAL = 4096;\n    const float SPARSITY = 0.75;\n    const int K_GLOBAL_SPARSE = (int(K_GLOBAL * (1-SPARSITY)));\n    const int M_TILES = (M_GLOBAL / M);\n    const int N_TILES = (N_GLOBAL / N);\n    const int K_TILES = (K_GLOBAL / K);\n    const int BLOCK_ROW_WARPS = 2;\n    const int BLOCK_COL_WARPS = 4;\n    const int WARP_ROW_TILES = 4;\n    const int WARP_COL_TILES = 2;\n    const int SKEW_UINT8 = 16;\n    const int BANK_VAL = 32;\n    const int NUM_BANK = (K_GLOBAL / BANK_VAL);\n    /////////// BLOCK_ROW_TILES <= N_TILES ////////////\n    // const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    // ////////// BLOCK_COL_TILES <= M_TILES ////////////\n    // const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    auto C_LAYOUT = wmma::mem_col_major;\n    const int CHUNK_K = 64;\n    const int CHUNK_K_SPARSE = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K_SPARSE = (int((CHUNK_K * K) * (1 - SPARSITY)));\n    const int WARP_COPY_BYTES = (WARP_SIZE * sizeof(int4));\n    const int CHUNK_LINE_BYTES_A (BLOCK_COL_TILES * M * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_A = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_A);\n    const int CHUNK_COPY_LINE_LANES_A = (CHUNK_LINE_BYTES_A / sizeof(int4));\n    const int SHARED_OFFSET_A = (BLOCK_COL_TILES * M + SKEW_UINT8);\n\n    const int CHUNK_LINE_BYTES_B = (BLOCK_SIZE_K_SPARSE * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_B = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_B);\n    const int CHUNK_COPY_LINE_LANES_B = (CHUNK_LINE_BYTES_B / sizeof(int4));\n    const int SHARED_OFFSET_B = (BLOCK_SIZE_K_SPARSE + SKEW_UINT8);\n    const int SHARED_TO_GLOBAL_BYTES_PER_LINE = ((WARP_COL_TILES * M) * sizeof(int));\n    const int SHARED_TO_GLOBAL_BYTES_PER_WARP = (WARP_SIZE * sizeof(int));\n    const int SHARED_TO_GLOBAL_LINES_PER_WARP = (SHARED_TO_GLOBAL_BYTES_PER_WARP / SHARED_TO_GLOBAL_BYTES_PER_LINE);\n    const int SHARED_TO_GLOBAL_LANES_PER_LINE = (WARP_SIZE / SHARED_TO_GLOBAL_LINES_PER_WARP);\n    const int SHARED_TO_GLOBAL_ITERS = ((WARP_ROW_TILES * N) / SHARED_TO_GLOBAL_LINES_PER_WARP);\n\n    const int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const int WARP_STRIDE = (WARP_COL_TILES * M);\n    const int WARPS_PER_BLOCK = (BLOCK_ROW_WARPS * BLOCK_COL_WARPS);\n    const int THREADS_PER_BLOCK = (WARP_SIZE * WARPS_PER_BLOCK);\n    const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    const int GLOBAL_MEM_STRIDE = M_GLOBAL;\n    const int SHMEM_STRIDE = (M * BLOCK_COL_TILES);\n    const int SHMEM_OFFSET = (M * WARP_COL_TILES);\n    const int BLOCK_SIZE_M = (M * BLOCK_COL_TILES);\n    const int BLOCK_SIZE_N = (N * BLOCK_ROW_TILES);\n\n   extern __shared__ uint8_t shmem[];\n\n\t// Warp and lane identification.\n\tconst unsigned int warpId = threadIdx.x / WARP_SIZE;\n\tconst unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n\n\t// Offset in shared memory from which the B matrix is stored.\n\t// const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\tconst size_t shmem_idx_b_off = BLOCK_SIZE_K_SPARSE * SHARED_OFFSET_A;\n\n\n\t// Each CTA slides along the 128 x 128 tiles from the top left corner of the\n\t// matrix to the right and down, and selects the next tile to compute. Once\n\t// there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n\tconst unsigned int block_tile_i =\n\t\t((block_pos * BLOCK_COL_TILES) / M_TILES) * (BLOCK_ROW_TILES);\n\tconst unsigned int block_tile_j = (block_pos * BLOCK_COL_TILES) % M_TILES;\n\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n\twmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_ROW_TILES]\n\t\t\t\t\t\t\t\t\t\t\t\t\t [WARP_COL_TILES];\n\n    // Load the C matrix tiles into fragments from shared memory.\n\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t\twmma::fill_fragment(c[i][j], 0);\n\t\t}\n\t}\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    //for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n    for(int tile_k_idx_sparse = 0, tile_k_idx = 0; tile_k_idx_sparse < K_GLOBAL_SPARSE; tile_k_idx_sparse += BLOCK_SIZE_K_SPARSE, tile_k_idx += BLOCK_SIZE_K){\n\n\t\tsize_t shmem_idx = \n\t\twarpId < (WARPS_PER_BLOCK / 2)\n\t\t\t? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A\n\t\t\t: (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B + shmem_idx_b_off;\n\n\t\tint4 *lane_ptr = NULL;\n\t\tint *lane_ptr_index = NULL;\n\t\tconst uint8_t *warp_ptr = NULL;\n\n\n\t\tif(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\t//warp_ptr = &A[block_tile_j * M] +\n\t\t\t//\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * M_GLOBAL;\n\t\t\twarp_ptr = &A[block_tile_j * M];\n\t\t\t\n\t\t\tconst int *warp_ptr_index = &B_index[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t\t\t\t\t\t((warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A);\n\n\t\t\tlane_ptr_index = (int *)(warp_ptr_index + tile_k_idx_sparse + (laneId / CHUNK_COPY_LINE_LANES_A));\n\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_A) * SHARED_OFFSET_A;\n\t\t}else{\n\t\t\twarp_ptr = &B[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * K_GLOBAL_SPARSE;\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_sparse +\n\t\t\t\t\t\t\t\t(laneId / CHUNK_COPY_LINE_LANES_B) * K_GLOBAL_SPARSE) +\n\t\t\t\t\t\t\t\t(laneId % CHUNK_COPY_LINE_LANES_B);\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_B) * SHARED_OFFSET_B;\n\t\t}\n\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      // shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n\t  int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n\t  \t? BLOCK_SIZE_K_SPARSE / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A)\n\t\t: BLOCK_SIZE_N / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\n\t  /*\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\t  */\n\n\t  /*\n      int tile_k_idx_A;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n          tile_k_idx_A = *(lane_ptr_index);\n      }\n\t  */\n\n\t  #pragma unroll\n\t  for(int i = 0; i < iter_index; i += 1){\n\t\t  if(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\tint tile_k_idx_A = *(lane_ptr_index);\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_A * M_GLOBAL) + (laneId % CHUNK_COPY_LINE_LANES_A);\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_A)) =\n\t\t\t\t*lane_ptr;\n\t\t\t//warp_ptr = (uint8_t *)((uint8_t *)warp_ptr + M_GLOBAL * (WARPS_PER_BLOCK / 2) *CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tlane_ptr_index = (int *)((int *)lane_ptr_index +  (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A;\n\t\t  }else{\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_B)) =\n\t\t\t\t*lane_ptr;\n\t\t\tlane_ptr = (int4 *)((uint8_t *)lane_ptr + K_GLOBAL_SPARSE * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B;\n\t\t  }\n\t  }\n\n      __syncthreads();\n\n\t#pragma unroll\n      for (int k_step = 0; k_step < CHUNK_K_SPARSE; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::col_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n\t#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i += 1) {\n\t\t\tsize_t shmem_idx_a = (warpId % BLOCK_COL_WARPS) * M * WARP_COL_TILES + (i * M);\n\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_a + k_step * K * SHARED_OFFSET_A;\n\n\t\t\twmma::load_matrix_sync(a[i], tile_ptr, SHARED_OFFSET_A);\n\t\t#pragma unroll\n\t\t\tfor(int j = 0; j < WARP_ROW_TILES; j += 1){\n\t\t\t\tif(i == 0){\n\t\t\t\t\tsize_t shmem_idx_b = shmem_idx_b_off +\n\t\t\t\t\t\t\t\t\t\t\t(warpId / BLOCK_COL_WARPS) * (WARP_ROW_TILES * N) * SHARED_OFFSET_B +\n\t\t\t\t\t\t\t\t\t\t\t(j * N) * SHARED_OFFSET_B;\n\t\t\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_b + k_step * K;\n\t\t\t\t\twmma::load_matrix_sync(b[j], tile_ptr, SHARED_OFFSET_B);\n\t\t\t\t}\n\t\t\t\twmma::mma_sync(c[j][i], a[i], b[j], c[j][i]);\n\t\t\t}\n\n        }\n      }\n\n      __syncthreads();\n    }\n\n    // This pointer is used to access the C and D matrix tiles this warp computes.\n\tint *shmem_warp_tile_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * N * WARP_ROW_TILES * SHMEM_STRIDE +\n\t(warpId % BLOCK_COL_WARPS) * SHMEM_OFFSET;\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t#pragma unroll\n\t\t\tfor(int t = 0; t < c[i][j].num_elements; t += 1){\n\t\t\t\tc[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n\t\t\t}\n\t\t\tint *tile_ptr = shmem_warp_tile_ptr + i * N * SHMEM_STRIDE + j * M;\n\t\t\twmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, C_LAYOUT);\n\t\t}\n\t}\n\n    __syncthreads();\n\n\tint *shmem_warp_stream_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N * SHMEM_STRIDE\n\t\t\t\t\t\t\t\t\t+ (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tconst size_t gmem_idx =\n\t\t(block_tile_i * N + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N) * GLOBAL_MEM_STRIDE +\n\t\tblock_tile_j * M + (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tuint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n\tint *shmem_lane_stream_ptr =\n\t\tshmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * SHMEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\t\n\tuint8_t *dst_gmem_lane_stream_ptr =\n\t\tdst_gmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * GLOBAL_MEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\n\tfor(int i = 0; i < WARP_ROW_TILES * N; i += SHARED_TO_GLOBAL_LINES_PER_WARP){\n\t\t*(dst_gmem_lane_stream_ptr + i * GLOBAL_MEM_STRIDE) = (uint8_t)(*(shmem_lane_stream_ptr + i * SHMEM_STRIDE));\n\t}\n\n\t__syncthreads();\n}\n", "launch_config": {"dimBlock": [1024, 1], "dimGrid": [256, 1]}}, "108": {"code": "\n__global__ void compute_gemm_imma(float *input0, float *input1, float *input2, float* input3, float* input4, float*input5, float* input6,\n                                  float *output0) {\n    // keys need to be replaced 4096, 1024, K_GLOBA_VALUEL, 0.75, 64\n\n\t//extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B = reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_index = reinterpret_cast<int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n\tconst int WARP_SIZE = 32;\n    const int M = 16;\n    const int K = 16;\n    const int N = 16;\n    const int WMMA_M = 16;\n    const int WMMA_K = 16;\n    const int WMMA_N = 16;\n    const int M_GLOBAL = 4096;\n    const int K_GLOBAL = 4096;\n    const int N_GLOBAL = 1024;\n    const float SPARSITY = 0.75;\n    const int K_GLOBAL_SPARSE = (int(K_GLOBAL * (1-SPARSITY)));\n    const int M_TILES = (M_GLOBAL / M);\n    const int N_TILES = (N_GLOBAL / N);\n    const int K_TILES = (K_GLOBAL / K);\n    const int BLOCK_ROW_WARPS = 2;\n    const int BLOCK_COL_WARPS = 4;\n    const int WARP_ROW_TILES = 4;\n    const int WARP_COL_TILES = 2;\n    const int SKEW_UINT8 = 16;\n    const int BANK_VAL = 32;\n    const int NUM_BANK = (K_GLOBAL / BANK_VAL);\n    /////////// BLOCK_ROW_TILES <= N_TILES ////////////\n    // const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    // ////////// BLOCK_COL_TILES <= M_TILES ////////////\n    // const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    auto C_LAYOUT = wmma::mem_col_major;\n    const int CHUNK_K = 64;\n    const int CHUNK_K_SPARSE = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K_SPARSE = (int((CHUNK_K * K) * (1 - SPARSITY)));\n    const int WARP_COPY_BYTES = (WARP_SIZE * sizeof(int4));\n    const int CHUNK_LINE_BYTES_A (BLOCK_COL_TILES * M * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_A = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_A);\n    const int CHUNK_COPY_LINE_LANES_A = (CHUNK_LINE_BYTES_A / sizeof(int4));\n    const int SHARED_OFFSET_A = (BLOCK_COL_TILES * M + SKEW_UINT8);\n\n    const int CHUNK_LINE_BYTES_B = (BLOCK_SIZE_K_SPARSE * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_B = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_B);\n    const int CHUNK_COPY_LINE_LANES_B = (CHUNK_LINE_BYTES_B / sizeof(int4));\n    const int SHARED_OFFSET_B = (BLOCK_SIZE_K_SPARSE + SKEW_UINT8);\n    const int SHARED_TO_GLOBAL_BYTES_PER_LINE = ((WARP_COL_TILES * M) * sizeof(int));\n    const int SHARED_TO_GLOBAL_BYTES_PER_WARP = (WARP_SIZE * sizeof(int));\n    const int SHARED_TO_GLOBAL_LINES_PER_WARP = (SHARED_TO_GLOBAL_BYTES_PER_WARP / SHARED_TO_GLOBAL_BYTES_PER_LINE);\n    const int SHARED_TO_GLOBAL_LANES_PER_LINE = (WARP_SIZE / SHARED_TO_GLOBAL_LINES_PER_WARP);\n    const int SHARED_TO_GLOBAL_ITERS = ((WARP_ROW_TILES * N) / SHARED_TO_GLOBAL_LINES_PER_WARP);\n\n    const int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const int WARP_STRIDE = (WARP_COL_TILES * M);\n    const int WARPS_PER_BLOCK = (BLOCK_ROW_WARPS * BLOCK_COL_WARPS);\n    const int THREADS_PER_BLOCK = (WARP_SIZE * WARPS_PER_BLOCK);\n    const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    const int GLOBAL_MEM_STRIDE = M_GLOBAL;\n    const int SHMEM_STRIDE = (M * BLOCK_COL_TILES);\n    const int SHMEM_OFFSET = (M * WARP_COL_TILES);\n    const int BLOCK_SIZE_M = (M * BLOCK_COL_TILES);\n    const int BLOCK_SIZE_N = (N * BLOCK_ROW_TILES);\n\n   extern __shared__ uint8_t shmem[];\n\n\t// Warp and lane identification.\n\tconst unsigned int warpId = threadIdx.x / WARP_SIZE;\n\tconst unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n\n\t// Offset in shared memory from which the B matrix is stored.\n\t// const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\tconst size_t shmem_idx_b_off = BLOCK_SIZE_K_SPARSE * SHARED_OFFSET_A;\n\n\n\t// Each CTA slides along the 128 x 128 tiles from the top left corner of the\n\t// matrix to the right and down, and selects the next tile to compute. Once\n\t// there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n\tconst unsigned int block_tile_i =\n\t\t((block_pos * BLOCK_COL_TILES) / M_TILES) * (BLOCK_ROW_TILES);\n\tconst unsigned int block_tile_j = (block_pos * BLOCK_COL_TILES) % M_TILES;\n\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n\twmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_ROW_TILES]\n\t\t\t\t\t\t\t\t\t\t\t\t\t [WARP_COL_TILES];\n\n    // Load the C matrix tiles into fragments from shared memory.\n\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t\twmma::fill_fragment(c[i][j], 0);\n\t\t}\n\t}\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    //for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n    for(int tile_k_idx_sparse = 0, tile_k_idx = 0; tile_k_idx_sparse < K_GLOBAL_SPARSE; tile_k_idx_sparse += BLOCK_SIZE_K_SPARSE, tile_k_idx += BLOCK_SIZE_K){\n\n\t\tsize_t shmem_idx = \n\t\twarpId < (WARPS_PER_BLOCK / 2)\n\t\t\t? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A\n\t\t\t: (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B + shmem_idx_b_off;\n\n\t\tint4 *lane_ptr = NULL;\n\t\tint *lane_ptr_index = NULL;\n\t\tconst uint8_t *warp_ptr = NULL;\n\n\n\t\tif(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\t//warp_ptr = &A[block_tile_j * M] +\n\t\t\t//\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * M_GLOBAL;\n\t\t\twarp_ptr = &A[block_tile_j * M];\n\t\t\t\n\t\t\tconst int *warp_ptr_index = &B_index[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t\t\t\t\t\t((warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A);\n\n\t\t\tlane_ptr_index = (int *)(warp_ptr_index + tile_k_idx_sparse + (laneId / CHUNK_COPY_LINE_LANES_A));\n\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_A) * SHARED_OFFSET_A;\n\t\t}else{\n\t\t\twarp_ptr = &B[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * K_GLOBAL_SPARSE;\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_sparse +\n\t\t\t\t\t\t\t\t(laneId / CHUNK_COPY_LINE_LANES_B) * K_GLOBAL_SPARSE) +\n\t\t\t\t\t\t\t\t(laneId % CHUNK_COPY_LINE_LANES_B);\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_B) * SHARED_OFFSET_B;\n\t\t}\n\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      // shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n\t  int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n\t  \t? BLOCK_SIZE_K_SPARSE / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A)\n\t\t: BLOCK_SIZE_N / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\n\t  /*\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\t  */\n\n\t  /*\n      int tile_k_idx_A;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n          tile_k_idx_A = *(lane_ptr_index);\n      }\n\t  */\n\n\t  #pragma unroll\n\t  for(int i = 0; i < iter_index; i += 1){\n\t\t  if(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\tint tile_k_idx_A = *(lane_ptr_index);\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_A * M_GLOBAL) + (laneId % CHUNK_COPY_LINE_LANES_A);\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_A)) =\n\t\t\t\t*lane_ptr;\n\t\t\t//warp_ptr = (uint8_t *)((uint8_t *)warp_ptr + M_GLOBAL * (WARPS_PER_BLOCK / 2) *CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tlane_ptr_index = (int *)((int *)lane_ptr_index +  (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A;\n\t\t  }else{\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_B)) =\n\t\t\t\t*lane_ptr;\n\t\t\tlane_ptr = (int4 *)((uint8_t *)lane_ptr + K_GLOBAL_SPARSE * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B;\n\t\t  }\n\t  }\n\n      __syncthreads();\n\n\t#pragma unroll\n      for (int k_step = 0; k_step < CHUNK_K_SPARSE; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::col_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n\t#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i += 1) {\n\t\t\tsize_t shmem_idx_a = (warpId % BLOCK_COL_WARPS) * M * WARP_COL_TILES + (i * M);\n\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_a + k_step * K * SHARED_OFFSET_A;\n\n\t\t\twmma::load_matrix_sync(a[i], tile_ptr, SHARED_OFFSET_A);\n\t\t#pragma unroll\n\t\t\tfor(int j = 0; j < WARP_ROW_TILES; j += 1){\n\t\t\t\tif(i == 0){\n\t\t\t\t\tsize_t shmem_idx_b = shmem_idx_b_off +\n\t\t\t\t\t\t\t\t\t\t\t(warpId / BLOCK_COL_WARPS) * (WARP_ROW_TILES * N) * SHARED_OFFSET_B +\n\t\t\t\t\t\t\t\t\t\t\t(j * N) * SHARED_OFFSET_B;\n\t\t\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_b + k_step * K;\n\t\t\t\t\twmma::load_matrix_sync(b[j], tile_ptr, SHARED_OFFSET_B);\n\t\t\t\t}\n\t\t\t\twmma::mma_sync(c[j][i], a[i], b[j], c[j][i]);\n\t\t\t}\n\n        }\n      }\n\n      __syncthreads();\n    }\n\n    // This pointer is used to access the C and D matrix tiles this warp computes.\n\tint *shmem_warp_tile_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * N * WARP_ROW_TILES * SHMEM_STRIDE +\n\t(warpId % BLOCK_COL_WARPS) * SHMEM_OFFSET;\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t#pragma unroll\n\t\t\tfor(int t = 0; t < c[i][j].num_elements; t += 1){\n\t\t\t\tc[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n\t\t\t}\n\t\t\tint *tile_ptr = shmem_warp_tile_ptr + i * N * SHMEM_STRIDE + j * M;\n\t\t\twmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, C_LAYOUT);\n\t\t}\n\t}\n\n    __syncthreads();\n\n\tint *shmem_warp_stream_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N * SHMEM_STRIDE\n\t\t\t\t\t\t\t\t\t+ (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tconst size_t gmem_idx =\n\t\t(block_tile_i * N + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N) * GLOBAL_MEM_STRIDE +\n\t\tblock_tile_j * M + (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tuint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n\tint *shmem_lane_stream_ptr =\n\t\tshmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * SHMEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\t\n\tuint8_t *dst_gmem_lane_stream_ptr =\n\t\tdst_gmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * GLOBAL_MEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\n\tfor(int i = 0; i < WARP_ROW_TILES * N; i += SHARED_TO_GLOBAL_LINES_PER_WARP){\n\t\t*(dst_gmem_lane_stream_ptr + i * GLOBAL_MEM_STRIDE) = (uint8_t)(*(shmem_lane_stream_ptr + i * SHMEM_STRIDE));\n\t}\n\n\t__syncthreads();\n}\n", "launch_config": {"dimBlock": [256, 1], "dimGrid": [256, 1]}}, "109": {"code": "\n__global__ void compute_gemm_imma(float *input0, float *input1, float *input2, float* input3, float* input4, float*input5, float* input6,\n                                  float *output0) {\n    // keys need to be replaced 4096, 1024, K_GLOBA_VALUEL, 0.75, 64\n\n\t//extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B = reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_index = reinterpret_cast<int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n\tconst int WARP_SIZE = 32;\n    const int M = 16;\n    const int K = 16;\n    const int N = 16;\n    const int WMMA_M = 16;\n    const int WMMA_K = 16;\n    const int WMMA_N = 16;\n    const int M_GLOBAL = 4096;\n    const int K_GLOBAL = 1024;\n    const int N_GLOBAL = 1024;\n    const float SPARSITY = 0.75;\n    const int K_GLOBAL_SPARSE = (int(K_GLOBAL * (1-SPARSITY)));\n    const int M_TILES = (M_GLOBAL / M);\n    const int N_TILES = (N_GLOBAL / N);\n    const int K_TILES = (K_GLOBAL / K);\n    const int BLOCK_ROW_WARPS = 2;\n    const int BLOCK_COL_WARPS = 4;\n    const int WARP_ROW_TILES = 4;\n    const int WARP_COL_TILES = 2;\n    const int SKEW_UINT8 = 16;\n    const int BANK_VAL = 32;\n    const int NUM_BANK = (K_GLOBAL / BANK_VAL);\n    /////////// BLOCK_ROW_TILES <= N_TILES ////////////\n    // const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    // ////////// BLOCK_COL_TILES <= M_TILES ////////////\n    // const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    auto C_LAYOUT = wmma::mem_col_major;\n    const int CHUNK_K = 64;\n    const int CHUNK_K_SPARSE = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K_SPARSE = (int((CHUNK_K * K) * (1 - SPARSITY)));\n    const int WARP_COPY_BYTES = (WARP_SIZE * sizeof(int4));\n    const int CHUNK_LINE_BYTES_A (BLOCK_COL_TILES * M * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_A = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_A);\n    const int CHUNK_COPY_LINE_LANES_A = (CHUNK_LINE_BYTES_A / sizeof(int4));\n    const int SHARED_OFFSET_A = (BLOCK_COL_TILES * M + SKEW_UINT8);\n\n    const int CHUNK_LINE_BYTES_B = (BLOCK_SIZE_K_SPARSE * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_B = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_B);\n    const int CHUNK_COPY_LINE_LANES_B = (CHUNK_LINE_BYTES_B / sizeof(int4));\n    const int SHARED_OFFSET_B = (BLOCK_SIZE_K_SPARSE + SKEW_UINT8);\n    const int SHARED_TO_GLOBAL_BYTES_PER_LINE = ((WARP_COL_TILES * M) * sizeof(int));\n    const int SHARED_TO_GLOBAL_BYTES_PER_WARP = (WARP_SIZE * sizeof(int));\n    const int SHARED_TO_GLOBAL_LINES_PER_WARP = (SHARED_TO_GLOBAL_BYTES_PER_WARP / SHARED_TO_GLOBAL_BYTES_PER_LINE);\n    const int SHARED_TO_GLOBAL_LANES_PER_LINE = (WARP_SIZE / SHARED_TO_GLOBAL_LINES_PER_WARP);\n    const int SHARED_TO_GLOBAL_ITERS = ((WARP_ROW_TILES * N) / SHARED_TO_GLOBAL_LINES_PER_WARP);\n\n    const int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const int WARP_STRIDE = (WARP_COL_TILES * M);\n    const int WARPS_PER_BLOCK = (BLOCK_ROW_WARPS * BLOCK_COL_WARPS);\n    const int THREADS_PER_BLOCK = (WARP_SIZE * WARPS_PER_BLOCK);\n    const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    const int GLOBAL_MEM_STRIDE = M_GLOBAL;\n    const int SHMEM_STRIDE = (M * BLOCK_COL_TILES);\n    const int SHMEM_OFFSET = (M * WARP_COL_TILES);\n    const int BLOCK_SIZE_M = (M * BLOCK_COL_TILES);\n    const int BLOCK_SIZE_N = (N * BLOCK_ROW_TILES);\n\n   extern __shared__ uint8_t shmem[];\n\n\t// Warp and lane identification.\n\tconst unsigned int warpId = threadIdx.x / WARP_SIZE;\n\tconst unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n\n\t// Offset in shared memory from which the B matrix is stored.\n\t// const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\tconst size_t shmem_idx_b_off = BLOCK_SIZE_K_SPARSE * SHARED_OFFSET_A;\n\n\n\t// Each CTA slides along the 128 x 128 tiles from the top left corner of the\n\t// matrix to the right and down, and selects the next tile to compute. Once\n\t// there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n\tconst unsigned int block_tile_i =\n\t\t((block_pos * BLOCK_COL_TILES) / M_TILES) * (BLOCK_ROW_TILES);\n\tconst unsigned int block_tile_j = (block_pos * BLOCK_COL_TILES) % M_TILES;\n\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n\twmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_ROW_TILES]\n\t\t\t\t\t\t\t\t\t\t\t\t\t [WARP_COL_TILES];\n\n    // Load the C matrix tiles into fragments from shared memory.\n\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t\twmma::fill_fragment(c[i][j], 0);\n\t\t}\n\t}\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    //for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n    for(int tile_k_idx_sparse = 0, tile_k_idx = 0; tile_k_idx_sparse < K_GLOBAL_SPARSE; tile_k_idx_sparse += BLOCK_SIZE_K_SPARSE, tile_k_idx += BLOCK_SIZE_K){\n\n\t\tsize_t shmem_idx = \n\t\twarpId < (WARPS_PER_BLOCK / 2)\n\t\t\t? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A\n\t\t\t: (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B + shmem_idx_b_off;\n\n\t\tint4 *lane_ptr = NULL;\n\t\tint *lane_ptr_index = NULL;\n\t\tconst uint8_t *warp_ptr = NULL;\n\n\n\t\tif(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\t//warp_ptr = &A[block_tile_j * M] +\n\t\t\t//\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * M_GLOBAL;\n\t\t\twarp_ptr = &A[block_tile_j * M];\n\t\t\t\n\t\t\tconst int *warp_ptr_index = &B_index[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t\t\t\t\t\t((warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A);\n\n\t\t\tlane_ptr_index = (int *)(warp_ptr_index + tile_k_idx_sparse + (laneId / CHUNK_COPY_LINE_LANES_A));\n\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_A) * SHARED_OFFSET_A;\n\t\t}else{\n\t\t\twarp_ptr = &B[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * K_GLOBAL_SPARSE;\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_sparse +\n\t\t\t\t\t\t\t\t(laneId / CHUNK_COPY_LINE_LANES_B) * K_GLOBAL_SPARSE) +\n\t\t\t\t\t\t\t\t(laneId % CHUNK_COPY_LINE_LANES_B);\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_B) * SHARED_OFFSET_B;\n\t\t}\n\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      // shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n\t  int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n\t  \t? BLOCK_SIZE_K_SPARSE / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A)\n\t\t: BLOCK_SIZE_N / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\n\t  /*\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\t  */\n\n\t  /*\n      int tile_k_idx_A;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n          tile_k_idx_A = *(lane_ptr_index);\n      }\n\t  */\n\n\t  #pragma unroll\n\t  for(int i = 0; i < iter_index; i += 1){\n\t\t  if(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\tint tile_k_idx_A = *(lane_ptr_index);\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_A * M_GLOBAL) + (laneId % CHUNK_COPY_LINE_LANES_A);\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_A)) =\n\t\t\t\t*lane_ptr;\n\t\t\t//warp_ptr = (uint8_t *)((uint8_t *)warp_ptr + M_GLOBAL * (WARPS_PER_BLOCK / 2) *CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tlane_ptr_index = (int *)((int *)lane_ptr_index +  (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A;\n\t\t  }else{\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_B)) =\n\t\t\t\t*lane_ptr;\n\t\t\tlane_ptr = (int4 *)((uint8_t *)lane_ptr + K_GLOBAL_SPARSE * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B;\n\t\t  }\n\t  }\n\n      __syncthreads();\n\n\t#pragma unroll\n      for (int k_step = 0; k_step < CHUNK_K_SPARSE; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::col_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n\t#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i += 1) {\n\t\t\tsize_t shmem_idx_a = (warpId % BLOCK_COL_WARPS) * M * WARP_COL_TILES + (i * M);\n\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_a + k_step * K * SHARED_OFFSET_A;\n\n\t\t\twmma::load_matrix_sync(a[i], tile_ptr, SHARED_OFFSET_A);\n\t\t#pragma unroll\n\t\t\tfor(int j = 0; j < WARP_ROW_TILES; j += 1){\n\t\t\t\tif(i == 0){\n\t\t\t\t\tsize_t shmem_idx_b = shmem_idx_b_off +\n\t\t\t\t\t\t\t\t\t\t\t(warpId / BLOCK_COL_WARPS) * (WARP_ROW_TILES * N) * SHARED_OFFSET_B +\n\t\t\t\t\t\t\t\t\t\t\t(j * N) * SHARED_OFFSET_B;\n\t\t\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_b + k_step * K;\n\t\t\t\t\twmma::load_matrix_sync(b[j], tile_ptr, SHARED_OFFSET_B);\n\t\t\t\t}\n\t\t\t\twmma::mma_sync(c[j][i], a[i], b[j], c[j][i]);\n\t\t\t}\n\n        }\n      }\n\n      __syncthreads();\n    }\n\n    // This pointer is used to access the C and D matrix tiles this warp computes.\n\tint *shmem_warp_tile_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * N * WARP_ROW_TILES * SHMEM_STRIDE +\n\t(warpId % BLOCK_COL_WARPS) * SHMEM_OFFSET;\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t#pragma unroll\n\t\t\tfor(int t = 0; t < c[i][j].num_elements; t += 1){\n\t\t\t\tc[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n\t\t\t}\n\t\t\tint *tile_ptr = shmem_warp_tile_ptr + i * N * SHMEM_STRIDE + j * M;\n\t\t\twmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, C_LAYOUT);\n\t\t}\n\t}\n\n    __syncthreads();\n\n\tint *shmem_warp_stream_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N * SHMEM_STRIDE\n\t\t\t\t\t\t\t\t\t+ (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tconst size_t gmem_idx =\n\t\t(block_tile_i * N + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N) * GLOBAL_MEM_STRIDE +\n\t\tblock_tile_j * M + (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tuint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n\tint *shmem_lane_stream_ptr =\n\t\tshmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * SHMEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\t\n\tuint8_t *dst_gmem_lane_stream_ptr =\n\t\tdst_gmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * GLOBAL_MEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\n\tfor(int i = 0; i < WARP_ROW_TILES * N; i += SHARED_TO_GLOBAL_LINES_PER_WARP){\n\t\t*(dst_gmem_lane_stream_ptr + i * GLOBAL_MEM_STRIDE) = (uint8_t)(*(shmem_lane_stream_ptr + i * SHMEM_STRIDE));\n\t}\n\n\t__syncthreads();\n}\n", "launch_config": {"dimBlock": [256, 1], "dimGrid": [256, 1]}}, "110": {"code": "\n__global__ void compute_gemm_imma(float *input0, float *input1, float *input2, float* input3, float* input4, float*input5, float* input6,\n                                  float *output0) {\n    // keys need to be replaced 4096, 1024, K_GLOBA_VALUEL, 0.75, 64\n\n\t//extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B = reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_index = reinterpret_cast<int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n\tconst int WARP_SIZE = 32;\n    const int M = 16;\n    const int K = 16;\n    const int N = 16;\n    const int WMMA_M = 16;\n    const int WMMA_K = 16;\n    const int WMMA_N = 16;\n    const int M_GLOBAL = 4096;\n    const int K_GLOBAL = 1024;\n    const int N_GLOBAL = 1024;\n    const float SPARSITY = 0.75;\n    const int K_GLOBAL_SPARSE = (int(K_GLOBAL * (1-SPARSITY)));\n    const int M_TILES = (M_GLOBAL / M);\n    const int N_TILES = (N_GLOBAL / N);\n    const int K_TILES = (K_GLOBAL / K);\n    const int BLOCK_ROW_WARPS = 2;\n    const int BLOCK_COL_WARPS = 4;\n    const int WARP_ROW_TILES = 4;\n    const int WARP_COL_TILES = 2;\n    const int SKEW_UINT8 = 16;\n    const int BANK_VAL = 32;\n    const int NUM_BANK = (K_GLOBAL / BANK_VAL);\n    /////////// BLOCK_ROW_TILES <= N_TILES ////////////\n    // const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    // ////////// BLOCK_COL_TILES <= M_TILES ////////////\n    // const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    auto C_LAYOUT = wmma::mem_col_major;\n    const int CHUNK_K = 64;\n    const int CHUNK_K_SPARSE = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K_SPARSE = (int((CHUNK_K * K) * (1 - SPARSITY)));\n    const int WARP_COPY_BYTES = (WARP_SIZE * sizeof(int4));\n    const int CHUNK_LINE_BYTES_A (BLOCK_COL_TILES * M * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_A = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_A);\n    const int CHUNK_COPY_LINE_LANES_A = (CHUNK_LINE_BYTES_A / sizeof(int4));\n    const int SHARED_OFFSET_A = (BLOCK_COL_TILES * M + SKEW_UINT8);\n\n    const int CHUNK_LINE_BYTES_B = (BLOCK_SIZE_K_SPARSE * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_B = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_B);\n    const int CHUNK_COPY_LINE_LANES_B = (CHUNK_LINE_BYTES_B / sizeof(int4));\n    const int SHARED_OFFSET_B = (BLOCK_SIZE_K_SPARSE + SKEW_UINT8);\n    const int SHARED_TO_GLOBAL_BYTES_PER_LINE = ((WARP_COL_TILES * M) * sizeof(int));\n    const int SHARED_TO_GLOBAL_BYTES_PER_WARP = (WARP_SIZE * sizeof(int));\n    const int SHARED_TO_GLOBAL_LINES_PER_WARP = (SHARED_TO_GLOBAL_BYTES_PER_WARP / SHARED_TO_GLOBAL_BYTES_PER_LINE);\n    const int SHARED_TO_GLOBAL_LANES_PER_LINE = (WARP_SIZE / SHARED_TO_GLOBAL_LINES_PER_WARP);\n    const int SHARED_TO_GLOBAL_ITERS = ((WARP_ROW_TILES * N) / SHARED_TO_GLOBAL_LINES_PER_WARP);\n\n    const int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const int WARP_STRIDE = (WARP_COL_TILES * M);\n    const int WARPS_PER_BLOCK = (BLOCK_ROW_WARPS * BLOCK_COL_WARPS);\n    const int THREADS_PER_BLOCK = (WARP_SIZE * WARPS_PER_BLOCK);\n    const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    const int GLOBAL_MEM_STRIDE = M_GLOBAL;\n    const int SHMEM_STRIDE = (M * BLOCK_COL_TILES);\n    const int SHMEM_OFFSET = (M * WARP_COL_TILES);\n    const int BLOCK_SIZE_M = (M * BLOCK_COL_TILES);\n    const int BLOCK_SIZE_N = (N * BLOCK_ROW_TILES);\n\n   extern __shared__ uint8_t shmem[];\n\n\t// Warp and lane identification.\n\tconst unsigned int warpId = threadIdx.x / WARP_SIZE;\n\tconst unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n\n\t// Offset in shared memory from which the B matrix is stored.\n\t// const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\tconst size_t shmem_idx_b_off = BLOCK_SIZE_K_SPARSE * SHARED_OFFSET_A;\n\n\n\t// Each CTA slides along the 128 x 128 tiles from the top left corner of the\n\t// matrix to the right and down, and selects the next tile to compute. Once\n\t// there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n\tconst unsigned int block_tile_i =\n\t\t((block_pos * BLOCK_COL_TILES) / M_TILES) * (BLOCK_ROW_TILES);\n\tconst unsigned int block_tile_j = (block_pos * BLOCK_COL_TILES) % M_TILES;\n\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n\twmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_ROW_TILES]\n\t\t\t\t\t\t\t\t\t\t\t\t\t [WARP_COL_TILES];\n\n    // Load the C matrix tiles into fragments from shared memory.\n\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t\twmma::fill_fragment(c[i][j], 0);\n\t\t}\n\t}\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    //for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n    for(int tile_k_idx_sparse = 0, tile_k_idx = 0; tile_k_idx_sparse < K_GLOBAL_SPARSE; tile_k_idx_sparse += BLOCK_SIZE_K_SPARSE, tile_k_idx += BLOCK_SIZE_K){\n\n\t\tsize_t shmem_idx = \n\t\twarpId < (WARPS_PER_BLOCK / 2)\n\t\t\t? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A\n\t\t\t: (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B + shmem_idx_b_off;\n\n\t\tint4 *lane_ptr = NULL;\n\t\tint *lane_ptr_index = NULL;\n\t\tconst uint8_t *warp_ptr = NULL;\n\n\n\t\tif(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\t//warp_ptr = &A[block_tile_j * M] +\n\t\t\t//\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * M_GLOBAL;\n\t\t\twarp_ptr = &A[block_tile_j * M];\n\t\t\t\n\t\t\tconst int *warp_ptr_index = &B_index[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t\t\t\t\t\t((warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A);\n\n\t\t\tlane_ptr_index = (int *)(warp_ptr_index + tile_k_idx_sparse + (laneId / CHUNK_COPY_LINE_LANES_A));\n\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_A) * SHARED_OFFSET_A;\n\t\t}else{\n\t\t\twarp_ptr = &B[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * K_GLOBAL_SPARSE;\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_sparse +\n\t\t\t\t\t\t\t\t(laneId / CHUNK_COPY_LINE_LANES_B) * K_GLOBAL_SPARSE) +\n\t\t\t\t\t\t\t\t(laneId % CHUNK_COPY_LINE_LANES_B);\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_B) * SHARED_OFFSET_B;\n\t\t}\n\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      // shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n\t  int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n\t  \t? BLOCK_SIZE_K_SPARSE / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A)\n\t\t: BLOCK_SIZE_N / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\n\t  /*\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\t  */\n\n\t  /*\n      int tile_k_idx_A;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n          tile_k_idx_A = *(lane_ptr_index);\n      }\n\t  */\n\n\t  #pragma unroll\n\t  for(int i = 0; i < iter_index; i += 1){\n\t\t  if(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\tint tile_k_idx_A = *(lane_ptr_index);\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_A * M_GLOBAL) + (laneId % CHUNK_COPY_LINE_LANES_A);\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_A)) =\n\t\t\t\t*lane_ptr;\n\t\t\t//warp_ptr = (uint8_t *)((uint8_t *)warp_ptr + M_GLOBAL * (WARPS_PER_BLOCK / 2) *CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tlane_ptr_index = (int *)((int *)lane_ptr_index +  (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A;\n\t\t  }else{\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_B)) =\n\t\t\t\t*lane_ptr;\n\t\t\tlane_ptr = (int4 *)((uint8_t *)lane_ptr + K_GLOBAL_SPARSE * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B;\n\t\t  }\n\t  }\n\n      __syncthreads();\n\n\t#pragma unroll\n      for (int k_step = 0; k_step < CHUNK_K_SPARSE; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::col_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n\t#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i += 1) {\n\t\t\tsize_t shmem_idx_a = (warpId % BLOCK_COL_WARPS) * M * WARP_COL_TILES + (i * M);\n\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_a + k_step * K * SHARED_OFFSET_A;\n\n\t\t\twmma::load_matrix_sync(a[i], tile_ptr, SHARED_OFFSET_A);\n\t\t#pragma unroll\n\t\t\tfor(int j = 0; j < WARP_ROW_TILES; j += 1){\n\t\t\t\tif(i == 0){\n\t\t\t\t\tsize_t shmem_idx_b = shmem_idx_b_off +\n\t\t\t\t\t\t\t\t\t\t\t(warpId / BLOCK_COL_WARPS) * (WARP_ROW_TILES * N) * SHARED_OFFSET_B +\n\t\t\t\t\t\t\t\t\t\t\t(j * N) * SHARED_OFFSET_B;\n\t\t\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_b + k_step * K;\n\t\t\t\t\twmma::load_matrix_sync(b[j], tile_ptr, SHARED_OFFSET_B);\n\t\t\t\t}\n\t\t\t\twmma::mma_sync(c[j][i], a[i], b[j], c[j][i]);\n\t\t\t}\n\n        }\n      }\n\n      __syncthreads();\n    }\n\n    // This pointer is used to access the C and D matrix tiles this warp computes.\n\tint *shmem_warp_tile_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * N * WARP_ROW_TILES * SHMEM_STRIDE +\n\t(warpId % BLOCK_COL_WARPS) * SHMEM_OFFSET;\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t#pragma unroll\n\t\t\tfor(int t = 0; t < c[i][j].num_elements; t += 1){\n\t\t\t\tc[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n\t\t\t}\n\t\t\tint *tile_ptr = shmem_warp_tile_ptr + i * N * SHMEM_STRIDE + j * M;\n\t\t\twmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, C_LAYOUT);\n\t\t}\n\t}\n\n    __syncthreads();\n\n\tint *shmem_warp_stream_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N * SHMEM_STRIDE\n\t\t\t\t\t\t\t\t\t+ (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tconst size_t gmem_idx =\n\t\t(block_tile_i * N + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N) * GLOBAL_MEM_STRIDE +\n\t\tblock_tile_j * M + (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tuint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n\tint *shmem_lane_stream_ptr =\n\t\tshmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * SHMEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\t\n\tuint8_t *dst_gmem_lane_stream_ptr =\n\t\tdst_gmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * GLOBAL_MEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\n\tfor(int i = 0; i < WARP_ROW_TILES * N; i += SHARED_TO_GLOBAL_LINES_PER_WARP){\n\t\t*(dst_gmem_lane_stream_ptr + i * GLOBAL_MEM_STRIDE) = (uint8_t)(*(shmem_lane_stream_ptr + i * SHMEM_STRIDE));\n\t}\n\n\t__syncthreads();\n}\n", "launch_config": {"dimBlock": [256, 1], "dimGrid": [256, 1]}}, "111": {"code": "\n__global__ void compute_gemm_imma(float *input0, float *input1, float *input2, float* input3, float* input4, float*input5, float* input6,\n                                  float *output0) {\n    // keys need to be replaced 4096, 1024, K_GLOBA_VALUEL, 0.75, 64\n\n\t//extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B = reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_index = reinterpret_cast<int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n\tconst int WARP_SIZE = 32;\n    const int M = 16;\n    const int K = 16;\n    const int N = 16;\n    const int WMMA_M = 16;\n    const int WMMA_K = 16;\n    const int WMMA_N = 16;\n    const int M_GLOBAL = 4096;\n    const int K_GLOBAL = 1024;\n    const int N_GLOBAL = 1024;\n    const float SPARSITY = 0.75;\n    const int K_GLOBAL_SPARSE = (int(K_GLOBAL * (1-SPARSITY)));\n    const int M_TILES = (M_GLOBAL / M);\n    const int N_TILES = (N_GLOBAL / N);\n    const int K_TILES = (K_GLOBAL / K);\n    const int BLOCK_ROW_WARPS = 2;\n    const int BLOCK_COL_WARPS = 4;\n    const int WARP_ROW_TILES = 4;\n    const int WARP_COL_TILES = 2;\n    const int SKEW_UINT8 = 16;\n    const int BANK_VAL = 32;\n    const int NUM_BANK = (K_GLOBAL / BANK_VAL);\n    /////////// BLOCK_ROW_TILES <= N_TILES ////////////\n    // const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    // ////////// BLOCK_COL_TILES <= M_TILES ////////////\n    // const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    auto C_LAYOUT = wmma::mem_col_major;\n    const int CHUNK_K = 64;\n    const int CHUNK_K_SPARSE = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K_SPARSE = (int((CHUNK_K * K) * (1 - SPARSITY)));\n    const int WARP_COPY_BYTES = (WARP_SIZE * sizeof(int4));\n    const int CHUNK_LINE_BYTES_A (BLOCK_COL_TILES * M * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_A = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_A);\n    const int CHUNK_COPY_LINE_LANES_A = (CHUNK_LINE_BYTES_A / sizeof(int4));\n    const int SHARED_OFFSET_A = (BLOCK_COL_TILES * M + SKEW_UINT8);\n\n    const int CHUNK_LINE_BYTES_B = (BLOCK_SIZE_K_SPARSE * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_B = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_B);\n    const int CHUNK_COPY_LINE_LANES_B = (CHUNK_LINE_BYTES_B / sizeof(int4));\n    const int SHARED_OFFSET_B = (BLOCK_SIZE_K_SPARSE + SKEW_UINT8);\n    const int SHARED_TO_GLOBAL_BYTES_PER_LINE = ((WARP_COL_TILES * M) * sizeof(int));\n    const int SHARED_TO_GLOBAL_BYTES_PER_WARP = (WARP_SIZE * sizeof(int));\n    const int SHARED_TO_GLOBAL_LINES_PER_WARP = (SHARED_TO_GLOBAL_BYTES_PER_WARP / SHARED_TO_GLOBAL_BYTES_PER_LINE);\n    const int SHARED_TO_GLOBAL_LANES_PER_LINE = (WARP_SIZE / SHARED_TO_GLOBAL_LINES_PER_WARP);\n    const int SHARED_TO_GLOBAL_ITERS = ((WARP_ROW_TILES * N) / SHARED_TO_GLOBAL_LINES_PER_WARP);\n\n    const int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const int WARP_STRIDE = (WARP_COL_TILES * M);\n    const int WARPS_PER_BLOCK = (BLOCK_ROW_WARPS * BLOCK_COL_WARPS);\n    const int THREADS_PER_BLOCK = (WARP_SIZE * WARPS_PER_BLOCK);\n    const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    const int GLOBAL_MEM_STRIDE = M_GLOBAL;\n    const int SHMEM_STRIDE = (M * BLOCK_COL_TILES);\n    const int SHMEM_OFFSET = (M * WARP_COL_TILES);\n    const int BLOCK_SIZE_M = (M * BLOCK_COL_TILES);\n    const int BLOCK_SIZE_N = (N * BLOCK_ROW_TILES);\n\n   extern __shared__ uint8_t shmem[];\n\n\t// Warp and lane identification.\n\tconst unsigned int warpId = threadIdx.x / WARP_SIZE;\n\tconst unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n\n\t// Offset in shared memory from which the B matrix is stored.\n\t// const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\tconst size_t shmem_idx_b_off = BLOCK_SIZE_K_SPARSE * SHARED_OFFSET_A;\n\n\n\t// Each CTA slides along the 128 x 128 tiles from the top left corner of the\n\t// matrix to the right and down, and selects the next tile to compute. Once\n\t// there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n\tconst unsigned int block_tile_i =\n\t\t((block_pos * BLOCK_COL_TILES) / M_TILES) * (BLOCK_ROW_TILES);\n\tconst unsigned int block_tile_j = (block_pos * BLOCK_COL_TILES) % M_TILES;\n\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n\twmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_ROW_TILES]\n\t\t\t\t\t\t\t\t\t\t\t\t\t [WARP_COL_TILES];\n\n    // Load the C matrix tiles into fragments from shared memory.\n\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t\twmma::fill_fragment(c[i][j], 0);\n\t\t}\n\t}\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    //for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n    for(int tile_k_idx_sparse = 0, tile_k_idx = 0; tile_k_idx_sparse < K_GLOBAL_SPARSE; tile_k_idx_sparse += BLOCK_SIZE_K_SPARSE, tile_k_idx += BLOCK_SIZE_K){\n\n\t\tsize_t shmem_idx = \n\t\twarpId < (WARPS_PER_BLOCK / 2)\n\t\t\t? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A\n\t\t\t: (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B + shmem_idx_b_off;\n\n\t\tint4 *lane_ptr = NULL;\n\t\tint *lane_ptr_index = NULL;\n\t\tconst uint8_t *warp_ptr = NULL;\n\n\n\t\tif(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\t//warp_ptr = &A[block_tile_j * M] +\n\t\t\t//\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * M_GLOBAL;\n\t\t\twarp_ptr = &A[block_tile_j * M];\n\t\t\t\n\t\t\tconst int *warp_ptr_index = &B_index[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t\t\t\t\t\t((warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A);\n\n\t\t\tlane_ptr_index = (int *)(warp_ptr_index + tile_k_idx_sparse + (laneId / CHUNK_COPY_LINE_LANES_A));\n\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_A) * SHARED_OFFSET_A;\n\t\t}else{\n\t\t\twarp_ptr = &B[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * K_GLOBAL_SPARSE;\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_sparse +\n\t\t\t\t\t\t\t\t(laneId / CHUNK_COPY_LINE_LANES_B) * K_GLOBAL_SPARSE) +\n\t\t\t\t\t\t\t\t(laneId % CHUNK_COPY_LINE_LANES_B);\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_B) * SHARED_OFFSET_B;\n\t\t}\n\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      // shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n\t  int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n\t  \t? BLOCK_SIZE_K_SPARSE / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A)\n\t\t: BLOCK_SIZE_N / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\n\t  /*\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\t  */\n\n\t  /*\n      int tile_k_idx_A;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n          tile_k_idx_A = *(lane_ptr_index);\n      }\n\t  */\n\n\t  #pragma unroll\n\t  for(int i = 0; i < iter_index; i += 1){\n\t\t  if(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\tint tile_k_idx_A = *(lane_ptr_index);\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_A * M_GLOBAL) + (laneId % CHUNK_COPY_LINE_LANES_A);\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_A)) =\n\t\t\t\t*lane_ptr;\n\t\t\t//warp_ptr = (uint8_t *)((uint8_t *)warp_ptr + M_GLOBAL * (WARPS_PER_BLOCK / 2) *CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tlane_ptr_index = (int *)((int *)lane_ptr_index +  (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A;\n\t\t  }else{\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_B)) =\n\t\t\t\t*lane_ptr;\n\t\t\tlane_ptr = (int4 *)((uint8_t *)lane_ptr + K_GLOBAL_SPARSE * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B;\n\t\t  }\n\t  }\n\n      __syncthreads();\n\n\t#pragma unroll\n      for (int k_step = 0; k_step < CHUNK_K_SPARSE; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::col_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n\t#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i += 1) {\n\t\t\tsize_t shmem_idx_a = (warpId % BLOCK_COL_WARPS) * M * WARP_COL_TILES + (i * M);\n\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_a + k_step * K * SHARED_OFFSET_A;\n\n\t\t\twmma::load_matrix_sync(a[i], tile_ptr, SHARED_OFFSET_A);\n\t\t#pragma unroll\n\t\t\tfor(int j = 0; j < WARP_ROW_TILES; j += 1){\n\t\t\t\tif(i == 0){\n\t\t\t\t\tsize_t shmem_idx_b = shmem_idx_b_off +\n\t\t\t\t\t\t\t\t\t\t\t(warpId / BLOCK_COL_WARPS) * (WARP_ROW_TILES * N) * SHARED_OFFSET_B +\n\t\t\t\t\t\t\t\t\t\t\t(j * N) * SHARED_OFFSET_B;\n\t\t\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_b + k_step * K;\n\t\t\t\t\twmma::load_matrix_sync(b[j], tile_ptr, SHARED_OFFSET_B);\n\t\t\t\t}\n\t\t\t\twmma::mma_sync(c[j][i], a[i], b[j], c[j][i]);\n\t\t\t}\n\n        }\n      }\n\n      __syncthreads();\n    }\n\n    // This pointer is used to access the C and D matrix tiles this warp computes.\n\tint *shmem_warp_tile_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * N * WARP_ROW_TILES * SHMEM_STRIDE +\n\t(warpId % BLOCK_COL_WARPS) * SHMEM_OFFSET;\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t#pragma unroll\n\t\t\tfor(int t = 0; t < c[i][j].num_elements; t += 1){\n\t\t\t\tc[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n\t\t\t}\n\t\t\tint *tile_ptr = shmem_warp_tile_ptr + i * N * SHMEM_STRIDE + j * M;\n\t\t\twmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, C_LAYOUT);\n\t\t}\n\t}\n\n    __syncthreads();\n\n\tint *shmem_warp_stream_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N * SHMEM_STRIDE\n\t\t\t\t\t\t\t\t\t+ (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tconst size_t gmem_idx =\n\t\t(block_tile_i * N + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N) * GLOBAL_MEM_STRIDE +\n\t\tblock_tile_j * M + (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tuint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n\tint *shmem_lane_stream_ptr =\n\t\tshmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * SHMEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\t\n\tuint8_t *dst_gmem_lane_stream_ptr =\n\t\tdst_gmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * GLOBAL_MEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\n\tfor(int i = 0; i < WARP_ROW_TILES * N; i += SHARED_TO_GLOBAL_LINES_PER_WARP){\n\t\t*(dst_gmem_lane_stream_ptr + i * GLOBAL_MEM_STRIDE) = (uint8_t)(*(shmem_lane_stream_ptr + i * SHMEM_STRIDE));\n\t}\n\n\t__syncthreads();\n}\n", "launch_config": {"dimBlock": [256, 1], "dimGrid": [256, 1]}}, "112": {"code": "\n__global__ void compute_gemm_imma(float *input0, float *input1, float *input2, float* input3, float* input4, float*input5, float* input6,\n                                  float *output0) {\n    // keys need to be replaced 4096, 1024, K_GLOBA_VALUEL, 0.75, 64\n\n\t//extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B = reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_index = reinterpret_cast<int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n\tconst int WARP_SIZE = 32;\n    const int M = 16;\n    const int K = 16;\n    const int N = 16;\n    const int WMMA_M = 16;\n    const int WMMA_K = 16;\n    const int WMMA_N = 16;\n    const int M_GLOBAL = 4096;\n    const int K_GLOBAL = 1024;\n    const int N_GLOBAL = 1024;\n    const float SPARSITY = 0.75;\n    const int K_GLOBAL_SPARSE = (int(K_GLOBAL * (1-SPARSITY)));\n    const int M_TILES = (M_GLOBAL / M);\n    const int N_TILES = (N_GLOBAL / N);\n    const int K_TILES = (K_GLOBAL / K);\n    const int BLOCK_ROW_WARPS = 2;\n    const int BLOCK_COL_WARPS = 4;\n    const int WARP_ROW_TILES = 4;\n    const int WARP_COL_TILES = 2;\n    const int SKEW_UINT8 = 16;\n    const int BANK_VAL = 32;\n    const int NUM_BANK = (K_GLOBAL / BANK_VAL);\n    /////////// BLOCK_ROW_TILES <= N_TILES ////////////\n    // const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    // ////////// BLOCK_COL_TILES <= M_TILES ////////////\n    // const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    auto C_LAYOUT = wmma::mem_col_major;\n    const int CHUNK_K = 64;\n    const int CHUNK_K_SPARSE = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K_SPARSE = (int((CHUNK_K * K) * (1 - SPARSITY)));\n    const int WARP_COPY_BYTES = (WARP_SIZE * sizeof(int4));\n    const int CHUNK_LINE_BYTES_A (BLOCK_COL_TILES * M * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_A = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_A);\n    const int CHUNK_COPY_LINE_LANES_A = (CHUNK_LINE_BYTES_A / sizeof(int4));\n    const int SHARED_OFFSET_A = (BLOCK_COL_TILES * M + SKEW_UINT8);\n\n    const int CHUNK_LINE_BYTES_B = (BLOCK_SIZE_K_SPARSE * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_B = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_B);\n    const int CHUNK_COPY_LINE_LANES_B = (CHUNK_LINE_BYTES_B / sizeof(int4));\n    const int SHARED_OFFSET_B = (BLOCK_SIZE_K_SPARSE + SKEW_UINT8);\n    const int SHARED_TO_GLOBAL_BYTES_PER_LINE = ((WARP_COL_TILES * M) * sizeof(int));\n    const int SHARED_TO_GLOBAL_BYTES_PER_WARP = (WARP_SIZE * sizeof(int));\n    const int SHARED_TO_GLOBAL_LINES_PER_WARP = (SHARED_TO_GLOBAL_BYTES_PER_WARP / SHARED_TO_GLOBAL_BYTES_PER_LINE);\n    const int SHARED_TO_GLOBAL_LANES_PER_LINE = (WARP_SIZE / SHARED_TO_GLOBAL_LINES_PER_WARP);\n    const int SHARED_TO_GLOBAL_ITERS = ((WARP_ROW_TILES * N) / SHARED_TO_GLOBAL_LINES_PER_WARP);\n\n    const int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const int WARP_STRIDE = (WARP_COL_TILES * M);\n    const int WARPS_PER_BLOCK = (BLOCK_ROW_WARPS * BLOCK_COL_WARPS);\n    const int THREADS_PER_BLOCK = (WARP_SIZE * WARPS_PER_BLOCK);\n    const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    const int GLOBAL_MEM_STRIDE = M_GLOBAL;\n    const int SHMEM_STRIDE = (M * BLOCK_COL_TILES);\n    const int SHMEM_OFFSET = (M * WARP_COL_TILES);\n    const int BLOCK_SIZE_M = (M * BLOCK_COL_TILES);\n    const int BLOCK_SIZE_N = (N * BLOCK_ROW_TILES);\n\n   extern __shared__ uint8_t shmem[];\n\n\t// Warp and lane identification.\n\tconst unsigned int warpId = threadIdx.x / WARP_SIZE;\n\tconst unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n\n\t// Offset in shared memory from which the B matrix is stored.\n\t// const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\tconst size_t shmem_idx_b_off = BLOCK_SIZE_K_SPARSE * SHARED_OFFSET_A;\n\n\n\t// Each CTA slides along the 128 x 128 tiles from the top left corner of the\n\t// matrix to the right and down, and selects the next tile to compute. Once\n\t// there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n\tconst unsigned int block_tile_i =\n\t\t((block_pos * BLOCK_COL_TILES) / M_TILES) * (BLOCK_ROW_TILES);\n\tconst unsigned int block_tile_j = (block_pos * BLOCK_COL_TILES) % M_TILES;\n\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n\twmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_ROW_TILES]\n\t\t\t\t\t\t\t\t\t\t\t\t\t [WARP_COL_TILES];\n\n    // Load the C matrix tiles into fragments from shared memory.\n\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t\twmma::fill_fragment(c[i][j], 0);\n\t\t}\n\t}\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    //for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n    for(int tile_k_idx_sparse = 0, tile_k_idx = 0; tile_k_idx_sparse < K_GLOBAL_SPARSE; tile_k_idx_sparse += BLOCK_SIZE_K_SPARSE, tile_k_idx += BLOCK_SIZE_K){\n\n\t\tsize_t shmem_idx = \n\t\twarpId < (WARPS_PER_BLOCK / 2)\n\t\t\t? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A\n\t\t\t: (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B + shmem_idx_b_off;\n\n\t\tint4 *lane_ptr = NULL;\n\t\tint *lane_ptr_index = NULL;\n\t\tconst uint8_t *warp_ptr = NULL;\n\n\n\t\tif(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\t//warp_ptr = &A[block_tile_j * M] +\n\t\t\t//\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * M_GLOBAL;\n\t\t\twarp_ptr = &A[block_tile_j * M];\n\t\t\t\n\t\t\tconst int *warp_ptr_index = &B_index[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t\t\t\t\t\t((warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A);\n\n\t\t\tlane_ptr_index = (int *)(warp_ptr_index + tile_k_idx_sparse + (laneId / CHUNK_COPY_LINE_LANES_A));\n\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_A) * SHARED_OFFSET_A;\n\t\t}else{\n\t\t\twarp_ptr = &B[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * K_GLOBAL_SPARSE;\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_sparse +\n\t\t\t\t\t\t\t\t(laneId / CHUNK_COPY_LINE_LANES_B) * K_GLOBAL_SPARSE) +\n\t\t\t\t\t\t\t\t(laneId % CHUNK_COPY_LINE_LANES_B);\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_B) * SHARED_OFFSET_B;\n\t\t}\n\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      // shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n\t  int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n\t  \t? BLOCK_SIZE_K_SPARSE / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A)\n\t\t: BLOCK_SIZE_N / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\n\t  /*\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\t  */\n\n\t  /*\n      int tile_k_idx_A;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n          tile_k_idx_A = *(lane_ptr_index);\n      }\n\t  */\n\n\t  #pragma unroll\n\t  for(int i = 0; i < iter_index; i += 1){\n\t\t  if(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\tint tile_k_idx_A = *(lane_ptr_index);\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_A * M_GLOBAL) + (laneId % CHUNK_COPY_LINE_LANES_A);\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_A)) =\n\t\t\t\t*lane_ptr;\n\t\t\t//warp_ptr = (uint8_t *)((uint8_t *)warp_ptr + M_GLOBAL * (WARPS_PER_BLOCK / 2) *CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tlane_ptr_index = (int *)((int *)lane_ptr_index +  (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A;\n\t\t  }else{\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_B)) =\n\t\t\t\t*lane_ptr;\n\t\t\tlane_ptr = (int4 *)((uint8_t *)lane_ptr + K_GLOBAL_SPARSE * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B;\n\t\t  }\n\t  }\n\n      __syncthreads();\n\n\t#pragma unroll\n      for (int k_step = 0; k_step < CHUNK_K_SPARSE; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::col_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n\t#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i += 1) {\n\t\t\tsize_t shmem_idx_a = (warpId % BLOCK_COL_WARPS) * M * WARP_COL_TILES + (i * M);\n\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_a + k_step * K * SHARED_OFFSET_A;\n\n\t\t\twmma::load_matrix_sync(a[i], tile_ptr, SHARED_OFFSET_A);\n\t\t#pragma unroll\n\t\t\tfor(int j = 0; j < WARP_ROW_TILES; j += 1){\n\t\t\t\tif(i == 0){\n\t\t\t\t\tsize_t shmem_idx_b = shmem_idx_b_off +\n\t\t\t\t\t\t\t\t\t\t\t(warpId / BLOCK_COL_WARPS) * (WARP_ROW_TILES * N) * SHARED_OFFSET_B +\n\t\t\t\t\t\t\t\t\t\t\t(j * N) * SHARED_OFFSET_B;\n\t\t\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_b + k_step * K;\n\t\t\t\t\twmma::load_matrix_sync(b[j], tile_ptr, SHARED_OFFSET_B);\n\t\t\t\t}\n\t\t\t\twmma::mma_sync(c[j][i], a[i], b[j], c[j][i]);\n\t\t\t}\n\n        }\n      }\n\n      __syncthreads();\n    }\n\n    // This pointer is used to access the C and D matrix tiles this warp computes.\n\tint *shmem_warp_tile_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * N * WARP_ROW_TILES * SHMEM_STRIDE +\n\t(warpId % BLOCK_COL_WARPS) * SHMEM_OFFSET;\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t#pragma unroll\n\t\t\tfor(int t = 0; t < c[i][j].num_elements; t += 1){\n\t\t\t\tc[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n\t\t\t}\n\t\t\tint *tile_ptr = shmem_warp_tile_ptr + i * N * SHMEM_STRIDE + j * M;\n\t\t\twmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, C_LAYOUT);\n\t\t}\n\t}\n\n    __syncthreads();\n\n\tint *shmem_warp_stream_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N * SHMEM_STRIDE\n\t\t\t\t\t\t\t\t\t+ (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tconst size_t gmem_idx =\n\t\t(block_tile_i * N + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N) * GLOBAL_MEM_STRIDE +\n\t\tblock_tile_j * M + (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tuint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n\tint *shmem_lane_stream_ptr =\n\t\tshmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * SHMEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\t\n\tuint8_t *dst_gmem_lane_stream_ptr =\n\t\tdst_gmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * GLOBAL_MEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\n\tfor(int i = 0; i < WARP_ROW_TILES * N; i += SHARED_TO_GLOBAL_LINES_PER_WARP){\n\t\t*(dst_gmem_lane_stream_ptr + i * GLOBAL_MEM_STRIDE) = (uint8_t)(*(shmem_lane_stream_ptr + i * SHMEM_STRIDE));\n\t}\n\n\t__syncthreads();\n}\n", "launch_config": {"dimBlock": [256, 1], "dimGrid": [256, 1]}}, "113": {"code": "\n__global__ void compute_gemm_imma(float *input0, float *input1, float *input2, float* input3, float* input4, float*input5, float* input6,\n                                  float *output0) {\n    // keys need to be replaced 4096, 4096, K_GLOBA_VALUEL, 0.75, 64\n\n\t//extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B = reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_index = reinterpret_cast<int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n\tconst int WARP_SIZE = 32;\n    const int M = 16;\n    const int K = 16;\n    const int N = 16;\n    const int WMMA_M = 16;\n    const int WMMA_K = 16;\n    const int WMMA_N = 16;\n    const int M_GLOBAL = 4096;\n    const int K_GLOBAL = 1024;\n    const int N_GLOBAL = 4096;\n    const float SPARSITY = 0.75;\n    const int K_GLOBAL_SPARSE = (int(K_GLOBAL * (1-SPARSITY)));\n    const int M_TILES = (M_GLOBAL / M);\n    const int N_TILES = (N_GLOBAL / N);\n    const int K_TILES = (K_GLOBAL / K);\n    const int BLOCK_ROW_WARPS = 2;\n    const int BLOCK_COL_WARPS = 4;\n    const int WARP_ROW_TILES = 4;\n    const int WARP_COL_TILES = 2;\n    const int SKEW_UINT8 = 16;\n    const int BANK_VAL = 32;\n    const int NUM_BANK = (K_GLOBAL / BANK_VAL);\n    /////////// BLOCK_ROW_TILES <= N_TILES ////////////\n    // const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    // ////////// BLOCK_COL_TILES <= M_TILES ////////////\n    // const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    auto C_LAYOUT = wmma::mem_col_major;\n    const int CHUNK_K = 64;\n    const int CHUNK_K_SPARSE = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K_SPARSE = (int((CHUNK_K * K) * (1 - SPARSITY)));\n    const int WARP_COPY_BYTES = (WARP_SIZE * sizeof(int4));\n    const int CHUNK_LINE_BYTES_A (BLOCK_COL_TILES * M * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_A = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_A);\n    const int CHUNK_COPY_LINE_LANES_A = (CHUNK_LINE_BYTES_A / sizeof(int4));\n    const int SHARED_OFFSET_A = (BLOCK_COL_TILES * M + SKEW_UINT8);\n\n    const int CHUNK_LINE_BYTES_B = (BLOCK_SIZE_K_SPARSE * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_B = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_B);\n    const int CHUNK_COPY_LINE_LANES_B = (CHUNK_LINE_BYTES_B / sizeof(int4));\n    const int SHARED_OFFSET_B = (BLOCK_SIZE_K_SPARSE + SKEW_UINT8);\n    const int SHARED_TO_GLOBAL_BYTES_PER_LINE = ((WARP_COL_TILES * M) * sizeof(int));\n    const int SHARED_TO_GLOBAL_BYTES_PER_WARP = (WARP_SIZE * sizeof(int));\n    const int SHARED_TO_GLOBAL_LINES_PER_WARP = (SHARED_TO_GLOBAL_BYTES_PER_WARP / SHARED_TO_GLOBAL_BYTES_PER_LINE);\n    const int SHARED_TO_GLOBAL_LANES_PER_LINE = (WARP_SIZE / SHARED_TO_GLOBAL_LINES_PER_WARP);\n    const int SHARED_TO_GLOBAL_ITERS = ((WARP_ROW_TILES * N) / SHARED_TO_GLOBAL_LINES_PER_WARP);\n\n    const int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const int WARP_STRIDE = (WARP_COL_TILES * M);\n    const int WARPS_PER_BLOCK = (BLOCK_ROW_WARPS * BLOCK_COL_WARPS);\n    const int THREADS_PER_BLOCK = (WARP_SIZE * WARPS_PER_BLOCK);\n    const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    const int GLOBAL_MEM_STRIDE = M_GLOBAL;\n    const int SHMEM_STRIDE = (M * BLOCK_COL_TILES);\n    const int SHMEM_OFFSET = (M * WARP_COL_TILES);\n    const int BLOCK_SIZE_M = (M * BLOCK_COL_TILES);\n    const int BLOCK_SIZE_N = (N * BLOCK_ROW_TILES);\n\n   extern __shared__ uint8_t shmem[];\n\n\t// Warp and lane identification.\n\tconst unsigned int warpId = threadIdx.x / WARP_SIZE;\n\tconst unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n\n\t// Offset in shared memory from which the B matrix is stored.\n\t// const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\tconst size_t shmem_idx_b_off = BLOCK_SIZE_K_SPARSE * SHARED_OFFSET_A;\n\n\n\t// Each CTA slides along the 128 x 128 tiles from the top left corner of the\n\t// matrix to the right and down, and selects the next tile to compute. Once\n\t// there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n\tconst unsigned int block_tile_i =\n\t\t((block_pos * BLOCK_COL_TILES) / M_TILES) * (BLOCK_ROW_TILES);\n\tconst unsigned int block_tile_j = (block_pos * BLOCK_COL_TILES) % M_TILES;\n\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n\twmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_ROW_TILES]\n\t\t\t\t\t\t\t\t\t\t\t\t\t [WARP_COL_TILES];\n\n    // Load the C matrix tiles into fragments from shared memory.\n\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t\twmma::fill_fragment(c[i][j], 0);\n\t\t}\n\t}\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    //for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n    for(int tile_k_idx_sparse = 0, tile_k_idx = 0; tile_k_idx_sparse < K_GLOBAL_SPARSE; tile_k_idx_sparse += BLOCK_SIZE_K_SPARSE, tile_k_idx += BLOCK_SIZE_K){\n\n\t\tsize_t shmem_idx = \n\t\twarpId < (WARPS_PER_BLOCK / 2)\n\t\t\t? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A\n\t\t\t: (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B + shmem_idx_b_off;\n\n\t\tint4 *lane_ptr = NULL;\n\t\tint *lane_ptr_index = NULL;\n\t\tconst uint8_t *warp_ptr = NULL;\n\n\n\t\tif(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\t//warp_ptr = &A[block_tile_j * M] +\n\t\t\t//\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * M_GLOBAL;\n\t\t\twarp_ptr = &A[block_tile_j * M];\n\t\t\t\n\t\t\tconst int *warp_ptr_index = &B_index[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t\t\t\t\t\t((warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A);\n\n\t\t\tlane_ptr_index = (int *)(warp_ptr_index + tile_k_idx_sparse + (laneId / CHUNK_COPY_LINE_LANES_A));\n\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_A) * SHARED_OFFSET_A;\n\t\t}else{\n\t\t\twarp_ptr = &B[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * K_GLOBAL_SPARSE;\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_sparse +\n\t\t\t\t\t\t\t\t(laneId / CHUNK_COPY_LINE_LANES_B) * K_GLOBAL_SPARSE) +\n\t\t\t\t\t\t\t\t(laneId % CHUNK_COPY_LINE_LANES_B);\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_B) * SHARED_OFFSET_B;\n\t\t}\n\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      // shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n\t  int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n\t  \t? BLOCK_SIZE_K_SPARSE / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A)\n\t\t: BLOCK_SIZE_N / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\n\t  /*\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\t  */\n\n\t  /*\n      int tile_k_idx_A;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n          tile_k_idx_A = *(lane_ptr_index);\n      }\n\t  */\n\n\t  #pragma unroll\n\t  for(int i = 0; i < iter_index; i += 1){\n\t\t  if(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\tint tile_k_idx_A = *(lane_ptr_index);\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_A * M_GLOBAL) + (laneId % CHUNK_COPY_LINE_LANES_A);\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_A)) =\n\t\t\t\t*lane_ptr;\n\t\t\t//warp_ptr = (uint8_t *)((uint8_t *)warp_ptr + M_GLOBAL * (WARPS_PER_BLOCK / 2) *CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tlane_ptr_index = (int *)((int *)lane_ptr_index +  (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A;\n\t\t  }else{\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_B)) =\n\t\t\t\t*lane_ptr;\n\t\t\tlane_ptr = (int4 *)((uint8_t *)lane_ptr + K_GLOBAL_SPARSE * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B;\n\t\t  }\n\t  }\n\n      __syncthreads();\n\n\t#pragma unroll\n      for (int k_step = 0; k_step < CHUNK_K_SPARSE; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::col_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n\t#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i += 1) {\n\t\t\tsize_t shmem_idx_a = (warpId % BLOCK_COL_WARPS) * M * WARP_COL_TILES + (i * M);\n\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_a + k_step * K * SHARED_OFFSET_A;\n\n\t\t\twmma::load_matrix_sync(a[i], tile_ptr, SHARED_OFFSET_A);\n\t\t#pragma unroll\n\t\t\tfor(int j = 0; j < WARP_ROW_TILES; j += 1){\n\t\t\t\tif(i == 0){\n\t\t\t\t\tsize_t shmem_idx_b = shmem_idx_b_off +\n\t\t\t\t\t\t\t\t\t\t\t(warpId / BLOCK_COL_WARPS) * (WARP_ROW_TILES * N) * SHARED_OFFSET_B +\n\t\t\t\t\t\t\t\t\t\t\t(j * N) * SHARED_OFFSET_B;\n\t\t\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_b + k_step * K;\n\t\t\t\t\twmma::load_matrix_sync(b[j], tile_ptr, SHARED_OFFSET_B);\n\t\t\t\t}\n\t\t\t\twmma::mma_sync(c[j][i], a[i], b[j], c[j][i]);\n\t\t\t}\n\n        }\n      }\n\n      __syncthreads();\n    }\n\n    // This pointer is used to access the C and D matrix tiles this warp computes.\n\tint *shmem_warp_tile_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * N * WARP_ROW_TILES * SHMEM_STRIDE +\n\t(warpId % BLOCK_COL_WARPS) * SHMEM_OFFSET;\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t#pragma unroll\n\t\t\tfor(int t = 0; t < c[i][j].num_elements; t += 1){\n\t\t\t\tc[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n\t\t\t}\n\t\t\tint *tile_ptr = shmem_warp_tile_ptr + i * N * SHMEM_STRIDE + j * M;\n\t\t\twmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, C_LAYOUT);\n\t\t}\n\t}\n\n    __syncthreads();\n\n\tint *shmem_warp_stream_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N * SHMEM_STRIDE\n\t\t\t\t\t\t\t\t\t+ (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tconst size_t gmem_idx =\n\t\t(block_tile_i * N + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N) * GLOBAL_MEM_STRIDE +\n\t\tblock_tile_j * M + (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tuint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n\tint *shmem_lane_stream_ptr =\n\t\tshmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * SHMEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\t\n\tuint8_t *dst_gmem_lane_stream_ptr =\n\t\tdst_gmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * GLOBAL_MEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\n\tfor(int i = 0; i < WARP_ROW_TILES * N; i += SHARED_TO_GLOBAL_LINES_PER_WARP){\n\t\t*(dst_gmem_lane_stream_ptr + i * GLOBAL_MEM_STRIDE) = (uint8_t)(*(shmem_lane_stream_ptr + i * SHMEM_STRIDE));\n\t}\n\n\t__syncthreads();\n}\n", "launch_config": {"dimBlock": [1024, 1], "dimGrid": [256, 1]}}, "114": {"code": "\n__global__ void compute_gemm_imma(float *input0, float *input1, float *input2, float* input3, float* input4, float*input5, float* input6,\n                                  float *output0) {\n    // keys need to be replaced 4096, 1024, K_GLOBA_VALUEL, 0.75, 64\n\n\t//extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B = reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_index = reinterpret_cast<int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n\tconst int WARP_SIZE = 32;\n    const int M = 16;\n    const int K = 16;\n    const int N = 16;\n    const int WMMA_M = 16;\n    const int WMMA_K = 16;\n    const int WMMA_N = 16;\n    const int M_GLOBAL = 4096;\n    const int K_GLOBAL = 4096;\n    const int N_GLOBAL = 1024;\n    const float SPARSITY = 0.75;\n    const int K_GLOBAL_SPARSE = (int(K_GLOBAL * (1-SPARSITY)));\n    const int M_TILES = (M_GLOBAL / M);\n    const int N_TILES = (N_GLOBAL / N);\n    const int K_TILES = (K_GLOBAL / K);\n    const int BLOCK_ROW_WARPS = 2;\n    const int BLOCK_COL_WARPS = 4;\n    const int WARP_ROW_TILES = 4;\n    const int WARP_COL_TILES = 2;\n    const int SKEW_UINT8 = 16;\n    const int BANK_VAL = 32;\n    const int NUM_BANK = (K_GLOBAL / BANK_VAL);\n    /////////// BLOCK_ROW_TILES <= N_TILES ////////////\n    // const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    // ////////// BLOCK_COL_TILES <= M_TILES ////////////\n    // const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    auto C_LAYOUT = wmma::mem_col_major;\n    const int CHUNK_K = 64;\n    const int CHUNK_K_SPARSE = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K_SPARSE = (int((CHUNK_K * K) * (1 - SPARSITY)));\n    const int WARP_COPY_BYTES = (WARP_SIZE * sizeof(int4));\n    const int CHUNK_LINE_BYTES_A (BLOCK_COL_TILES * M * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_A = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_A);\n    const int CHUNK_COPY_LINE_LANES_A = (CHUNK_LINE_BYTES_A / sizeof(int4));\n    const int SHARED_OFFSET_A = (BLOCK_COL_TILES * M + SKEW_UINT8);\n\n    const int CHUNK_LINE_BYTES_B = (BLOCK_SIZE_K_SPARSE * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_B = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_B);\n    const int CHUNK_COPY_LINE_LANES_B = (CHUNK_LINE_BYTES_B / sizeof(int4));\n    const int SHARED_OFFSET_B = (BLOCK_SIZE_K_SPARSE + SKEW_UINT8);\n    const int SHARED_TO_GLOBAL_BYTES_PER_LINE = ((WARP_COL_TILES * M) * sizeof(int));\n    const int SHARED_TO_GLOBAL_BYTES_PER_WARP = (WARP_SIZE * sizeof(int));\n    const int SHARED_TO_GLOBAL_LINES_PER_WARP = (SHARED_TO_GLOBAL_BYTES_PER_WARP / SHARED_TO_GLOBAL_BYTES_PER_LINE);\n    const int SHARED_TO_GLOBAL_LANES_PER_LINE = (WARP_SIZE / SHARED_TO_GLOBAL_LINES_PER_WARP);\n    const int SHARED_TO_GLOBAL_ITERS = ((WARP_ROW_TILES * N) / SHARED_TO_GLOBAL_LINES_PER_WARP);\n\n    const int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const int WARP_STRIDE = (WARP_COL_TILES * M);\n    const int WARPS_PER_BLOCK = (BLOCK_ROW_WARPS * BLOCK_COL_WARPS);\n    const int THREADS_PER_BLOCK = (WARP_SIZE * WARPS_PER_BLOCK);\n    const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    const int GLOBAL_MEM_STRIDE = M_GLOBAL;\n    const int SHMEM_STRIDE = (M * BLOCK_COL_TILES);\n    const int SHMEM_OFFSET = (M * WARP_COL_TILES);\n    const int BLOCK_SIZE_M = (M * BLOCK_COL_TILES);\n    const int BLOCK_SIZE_N = (N * BLOCK_ROW_TILES);\n\n   extern __shared__ uint8_t shmem[];\n\n\t// Warp and lane identification.\n\tconst unsigned int warpId = threadIdx.x / WARP_SIZE;\n\tconst unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n\n\t// Offset in shared memory from which the B matrix is stored.\n\t// const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\tconst size_t shmem_idx_b_off = BLOCK_SIZE_K_SPARSE * SHARED_OFFSET_A;\n\n\n\t// Each CTA slides along the 128 x 128 tiles from the top left corner of the\n\t// matrix to the right and down, and selects the next tile to compute. Once\n\t// there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n\tconst unsigned int block_tile_i =\n\t\t((block_pos * BLOCK_COL_TILES) / M_TILES) * (BLOCK_ROW_TILES);\n\tconst unsigned int block_tile_j = (block_pos * BLOCK_COL_TILES) % M_TILES;\n\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n\twmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_ROW_TILES]\n\t\t\t\t\t\t\t\t\t\t\t\t\t [WARP_COL_TILES];\n\n    // Load the C matrix tiles into fragments from shared memory.\n\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t\twmma::fill_fragment(c[i][j], 0);\n\t\t}\n\t}\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    //for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n    for(int tile_k_idx_sparse = 0, tile_k_idx = 0; tile_k_idx_sparse < K_GLOBAL_SPARSE; tile_k_idx_sparse += BLOCK_SIZE_K_SPARSE, tile_k_idx += BLOCK_SIZE_K){\n\n\t\tsize_t shmem_idx = \n\t\twarpId < (WARPS_PER_BLOCK / 2)\n\t\t\t? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A\n\t\t\t: (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B + shmem_idx_b_off;\n\n\t\tint4 *lane_ptr = NULL;\n\t\tint *lane_ptr_index = NULL;\n\t\tconst uint8_t *warp_ptr = NULL;\n\n\n\t\tif(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\t//warp_ptr = &A[block_tile_j * M] +\n\t\t\t//\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * M_GLOBAL;\n\t\t\twarp_ptr = &A[block_tile_j * M];\n\t\t\t\n\t\t\tconst int *warp_ptr_index = &B_index[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t\t\t\t\t\t((warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A);\n\n\t\t\tlane_ptr_index = (int *)(warp_ptr_index + tile_k_idx_sparse + (laneId / CHUNK_COPY_LINE_LANES_A));\n\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_A) * SHARED_OFFSET_A;\n\t\t}else{\n\t\t\twarp_ptr = &B[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * K_GLOBAL_SPARSE;\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_sparse +\n\t\t\t\t\t\t\t\t(laneId / CHUNK_COPY_LINE_LANES_B) * K_GLOBAL_SPARSE) +\n\t\t\t\t\t\t\t\t(laneId % CHUNK_COPY_LINE_LANES_B);\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_B) * SHARED_OFFSET_B;\n\t\t}\n\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      // shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n\t  int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n\t  \t? BLOCK_SIZE_K_SPARSE / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A)\n\t\t: BLOCK_SIZE_N / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\n\t  /*\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\t  */\n\n\t  /*\n      int tile_k_idx_A;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n          tile_k_idx_A = *(lane_ptr_index);\n      }\n\t  */\n\n\t  #pragma unroll\n\t  for(int i = 0; i < iter_index; i += 1){\n\t\t  if(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\tint tile_k_idx_A = *(lane_ptr_index);\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_A * M_GLOBAL) + (laneId % CHUNK_COPY_LINE_LANES_A);\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_A)) =\n\t\t\t\t*lane_ptr;\n\t\t\t//warp_ptr = (uint8_t *)((uint8_t *)warp_ptr + M_GLOBAL * (WARPS_PER_BLOCK / 2) *CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tlane_ptr_index = (int *)((int *)lane_ptr_index +  (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A;\n\t\t  }else{\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_B)) =\n\t\t\t\t*lane_ptr;\n\t\t\tlane_ptr = (int4 *)((uint8_t *)lane_ptr + K_GLOBAL_SPARSE * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B;\n\t\t  }\n\t  }\n\n      __syncthreads();\n\n\t#pragma unroll\n      for (int k_step = 0; k_step < CHUNK_K_SPARSE; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::col_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n\t#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i += 1) {\n\t\t\tsize_t shmem_idx_a = (warpId % BLOCK_COL_WARPS) * M * WARP_COL_TILES + (i * M);\n\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_a + k_step * K * SHARED_OFFSET_A;\n\n\t\t\twmma::load_matrix_sync(a[i], tile_ptr, SHARED_OFFSET_A);\n\t\t#pragma unroll\n\t\t\tfor(int j = 0; j < WARP_ROW_TILES; j += 1){\n\t\t\t\tif(i == 0){\n\t\t\t\t\tsize_t shmem_idx_b = shmem_idx_b_off +\n\t\t\t\t\t\t\t\t\t\t\t(warpId / BLOCK_COL_WARPS) * (WARP_ROW_TILES * N) * SHARED_OFFSET_B +\n\t\t\t\t\t\t\t\t\t\t\t(j * N) * SHARED_OFFSET_B;\n\t\t\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_b + k_step * K;\n\t\t\t\t\twmma::load_matrix_sync(b[j], tile_ptr, SHARED_OFFSET_B);\n\t\t\t\t}\n\t\t\t\twmma::mma_sync(c[j][i], a[i], b[j], c[j][i]);\n\t\t\t}\n\n        }\n      }\n\n      __syncthreads();\n    }\n\n    // This pointer is used to access the C and D matrix tiles this warp computes.\n\tint *shmem_warp_tile_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * N * WARP_ROW_TILES * SHMEM_STRIDE +\n\t(warpId % BLOCK_COL_WARPS) * SHMEM_OFFSET;\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t#pragma unroll\n\t\t\tfor(int t = 0; t < c[i][j].num_elements; t += 1){\n\t\t\t\tc[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n\t\t\t}\n\t\t\tint *tile_ptr = shmem_warp_tile_ptr + i * N * SHMEM_STRIDE + j * M;\n\t\t\twmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, C_LAYOUT);\n\t\t}\n\t}\n\n    __syncthreads();\n\n\tint *shmem_warp_stream_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N * SHMEM_STRIDE\n\t\t\t\t\t\t\t\t\t+ (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tconst size_t gmem_idx =\n\t\t(block_tile_i * N + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N) * GLOBAL_MEM_STRIDE +\n\t\tblock_tile_j * M + (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tuint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n\tint *shmem_lane_stream_ptr =\n\t\tshmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * SHMEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\t\n\tuint8_t *dst_gmem_lane_stream_ptr =\n\t\tdst_gmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * GLOBAL_MEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\n\tfor(int i = 0; i < WARP_ROW_TILES * N; i += SHARED_TO_GLOBAL_LINES_PER_WARP){\n\t\t*(dst_gmem_lane_stream_ptr + i * GLOBAL_MEM_STRIDE) = (uint8_t)(*(shmem_lane_stream_ptr + i * SHMEM_STRIDE));\n\t}\n\n\t__syncthreads();\n}\n", "launch_config": {"dimBlock": [256, 1], "dimGrid": [256, 1]}}, "115": {"code": "\n__global__ void compute_gemm_imma(float *input0, float *input1, float *input2, float* input3, float* input4, float*input5, float* input6,\n                                  float *output0) {\n    // keys need to be replaced 4096, 1024, K_GLOBA_VALUEL, 0.75, 64\n\n\t//extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B = reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_index = reinterpret_cast<int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n\tconst int WARP_SIZE = 32;\n    const int M = 16;\n    const int K = 16;\n    const int N = 16;\n    const int WMMA_M = 16;\n    const int WMMA_K = 16;\n    const int WMMA_N = 16;\n    const int M_GLOBAL = 4096;\n    const int K_GLOBAL = 1024;\n    const int N_GLOBAL = 1024;\n    const float SPARSITY = 0.75;\n    const int K_GLOBAL_SPARSE = (int(K_GLOBAL * (1-SPARSITY)));\n    const int M_TILES = (M_GLOBAL / M);\n    const int N_TILES = (N_GLOBAL / N);\n    const int K_TILES = (K_GLOBAL / K);\n    const int BLOCK_ROW_WARPS = 2;\n    const int BLOCK_COL_WARPS = 4;\n    const int WARP_ROW_TILES = 4;\n    const int WARP_COL_TILES = 2;\n    const int SKEW_UINT8 = 16;\n    const int BANK_VAL = 32;\n    const int NUM_BANK = (K_GLOBAL / BANK_VAL);\n    /////////// BLOCK_ROW_TILES <= N_TILES ////////////\n    // const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    // ////////// BLOCK_COL_TILES <= M_TILES ////////////\n    // const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    auto C_LAYOUT = wmma::mem_col_major;\n    const int CHUNK_K = 64;\n    const int CHUNK_K_SPARSE = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K_SPARSE = (int((CHUNK_K * K) * (1 - SPARSITY)));\n    const int WARP_COPY_BYTES = (WARP_SIZE * sizeof(int4));\n    const int CHUNK_LINE_BYTES_A (BLOCK_COL_TILES * M * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_A = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_A);\n    const int CHUNK_COPY_LINE_LANES_A = (CHUNK_LINE_BYTES_A / sizeof(int4));\n    const int SHARED_OFFSET_A = (BLOCK_COL_TILES * M + SKEW_UINT8);\n\n    const int CHUNK_LINE_BYTES_B = (BLOCK_SIZE_K_SPARSE * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_B = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_B);\n    const int CHUNK_COPY_LINE_LANES_B = (CHUNK_LINE_BYTES_B / sizeof(int4));\n    const int SHARED_OFFSET_B = (BLOCK_SIZE_K_SPARSE + SKEW_UINT8);\n    const int SHARED_TO_GLOBAL_BYTES_PER_LINE = ((WARP_COL_TILES * M) * sizeof(int));\n    const int SHARED_TO_GLOBAL_BYTES_PER_WARP = (WARP_SIZE * sizeof(int));\n    const int SHARED_TO_GLOBAL_LINES_PER_WARP = (SHARED_TO_GLOBAL_BYTES_PER_WARP / SHARED_TO_GLOBAL_BYTES_PER_LINE);\n    const int SHARED_TO_GLOBAL_LANES_PER_LINE = (WARP_SIZE / SHARED_TO_GLOBAL_LINES_PER_WARP);\n    const int SHARED_TO_GLOBAL_ITERS = ((WARP_ROW_TILES * N) / SHARED_TO_GLOBAL_LINES_PER_WARP);\n\n    const int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const int WARP_STRIDE = (WARP_COL_TILES * M);\n    const int WARPS_PER_BLOCK = (BLOCK_ROW_WARPS * BLOCK_COL_WARPS);\n    const int THREADS_PER_BLOCK = (WARP_SIZE * WARPS_PER_BLOCK);\n    const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    const int GLOBAL_MEM_STRIDE = M_GLOBAL;\n    const int SHMEM_STRIDE = (M * BLOCK_COL_TILES);\n    const int SHMEM_OFFSET = (M * WARP_COL_TILES);\n    const int BLOCK_SIZE_M = (M * BLOCK_COL_TILES);\n    const int BLOCK_SIZE_N = (N * BLOCK_ROW_TILES);\n\n   extern __shared__ uint8_t shmem[];\n\n\t// Warp and lane identification.\n\tconst unsigned int warpId = threadIdx.x / WARP_SIZE;\n\tconst unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n\n\t// Offset in shared memory from which the B matrix is stored.\n\t// const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\tconst size_t shmem_idx_b_off = BLOCK_SIZE_K_SPARSE * SHARED_OFFSET_A;\n\n\n\t// Each CTA slides along the 128 x 128 tiles from the top left corner of the\n\t// matrix to the right and down, and selects the next tile to compute. Once\n\t// there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n\tconst unsigned int block_tile_i =\n\t\t((block_pos * BLOCK_COL_TILES) / M_TILES) * (BLOCK_ROW_TILES);\n\tconst unsigned int block_tile_j = (block_pos * BLOCK_COL_TILES) % M_TILES;\n\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n\twmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_ROW_TILES]\n\t\t\t\t\t\t\t\t\t\t\t\t\t [WARP_COL_TILES];\n\n    // Load the C matrix tiles into fragments from shared memory.\n\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t\twmma::fill_fragment(c[i][j], 0);\n\t\t}\n\t}\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    //for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n    for(int tile_k_idx_sparse = 0, tile_k_idx = 0; tile_k_idx_sparse < K_GLOBAL_SPARSE; tile_k_idx_sparse += BLOCK_SIZE_K_SPARSE, tile_k_idx += BLOCK_SIZE_K){\n\n\t\tsize_t shmem_idx = \n\t\twarpId < (WARPS_PER_BLOCK / 2)\n\t\t\t? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A\n\t\t\t: (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B + shmem_idx_b_off;\n\n\t\tint4 *lane_ptr = NULL;\n\t\tint *lane_ptr_index = NULL;\n\t\tconst uint8_t *warp_ptr = NULL;\n\n\n\t\tif(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\t//warp_ptr = &A[block_tile_j * M] +\n\t\t\t//\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * M_GLOBAL;\n\t\t\twarp_ptr = &A[block_tile_j * M];\n\t\t\t\n\t\t\tconst int *warp_ptr_index = &B_index[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t\t\t\t\t\t((warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A);\n\n\t\t\tlane_ptr_index = (int *)(warp_ptr_index + tile_k_idx_sparse + (laneId / CHUNK_COPY_LINE_LANES_A));\n\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_A) * SHARED_OFFSET_A;\n\t\t}else{\n\t\t\twarp_ptr = &B[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * K_GLOBAL_SPARSE;\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_sparse +\n\t\t\t\t\t\t\t\t(laneId / CHUNK_COPY_LINE_LANES_B) * K_GLOBAL_SPARSE) +\n\t\t\t\t\t\t\t\t(laneId % CHUNK_COPY_LINE_LANES_B);\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_B) * SHARED_OFFSET_B;\n\t\t}\n\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      // shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n\t  int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n\t  \t? BLOCK_SIZE_K_SPARSE / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A)\n\t\t: BLOCK_SIZE_N / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\n\t  /*\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\t  */\n\n\t  /*\n      int tile_k_idx_A;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n          tile_k_idx_A = *(lane_ptr_index);\n      }\n\t  */\n\n\t  #pragma unroll\n\t  for(int i = 0; i < iter_index; i += 1){\n\t\t  if(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\tint tile_k_idx_A = *(lane_ptr_index);\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_A * M_GLOBAL) + (laneId % CHUNK_COPY_LINE_LANES_A);\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_A)) =\n\t\t\t\t*lane_ptr;\n\t\t\t//warp_ptr = (uint8_t *)((uint8_t *)warp_ptr + M_GLOBAL * (WARPS_PER_BLOCK / 2) *CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tlane_ptr_index = (int *)((int *)lane_ptr_index +  (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A;\n\t\t  }else{\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_B)) =\n\t\t\t\t*lane_ptr;\n\t\t\tlane_ptr = (int4 *)((uint8_t *)lane_ptr + K_GLOBAL_SPARSE * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B;\n\t\t  }\n\t  }\n\n      __syncthreads();\n\n\t#pragma unroll\n      for (int k_step = 0; k_step < CHUNK_K_SPARSE; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::col_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n\t#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i += 1) {\n\t\t\tsize_t shmem_idx_a = (warpId % BLOCK_COL_WARPS) * M * WARP_COL_TILES + (i * M);\n\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_a + k_step * K * SHARED_OFFSET_A;\n\n\t\t\twmma::load_matrix_sync(a[i], tile_ptr, SHARED_OFFSET_A);\n\t\t#pragma unroll\n\t\t\tfor(int j = 0; j < WARP_ROW_TILES; j += 1){\n\t\t\t\tif(i == 0){\n\t\t\t\t\tsize_t shmem_idx_b = shmem_idx_b_off +\n\t\t\t\t\t\t\t\t\t\t\t(warpId / BLOCK_COL_WARPS) * (WARP_ROW_TILES * N) * SHARED_OFFSET_B +\n\t\t\t\t\t\t\t\t\t\t\t(j * N) * SHARED_OFFSET_B;\n\t\t\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_b + k_step * K;\n\t\t\t\t\twmma::load_matrix_sync(b[j], tile_ptr, SHARED_OFFSET_B);\n\t\t\t\t}\n\t\t\t\twmma::mma_sync(c[j][i], a[i], b[j], c[j][i]);\n\t\t\t}\n\n        }\n      }\n\n      __syncthreads();\n    }\n\n    // This pointer is used to access the C and D matrix tiles this warp computes.\n\tint *shmem_warp_tile_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * N * WARP_ROW_TILES * SHMEM_STRIDE +\n\t(warpId % BLOCK_COL_WARPS) * SHMEM_OFFSET;\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t#pragma unroll\n\t\t\tfor(int t = 0; t < c[i][j].num_elements; t += 1){\n\t\t\t\tc[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n\t\t\t}\n\t\t\tint *tile_ptr = shmem_warp_tile_ptr + i * N * SHMEM_STRIDE + j * M;\n\t\t\twmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, C_LAYOUT);\n\t\t}\n\t}\n\n    __syncthreads();\n\n\tint *shmem_warp_stream_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N * SHMEM_STRIDE\n\t\t\t\t\t\t\t\t\t+ (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tconst size_t gmem_idx =\n\t\t(block_tile_i * N + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N) * GLOBAL_MEM_STRIDE +\n\t\tblock_tile_j * M + (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tuint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n\tint *shmem_lane_stream_ptr =\n\t\tshmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * SHMEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\t\n\tuint8_t *dst_gmem_lane_stream_ptr =\n\t\tdst_gmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * GLOBAL_MEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\n\tfor(int i = 0; i < WARP_ROW_TILES * N; i += SHARED_TO_GLOBAL_LINES_PER_WARP){\n\t\t*(dst_gmem_lane_stream_ptr + i * GLOBAL_MEM_STRIDE) = (uint8_t)(*(shmem_lane_stream_ptr + i * SHMEM_STRIDE));\n\t}\n\n\t__syncthreads();\n}\n", "launch_config": {"dimBlock": [256, 1], "dimGrid": [256, 1]}}, "116": {"code": "\n__global__ void compute_gemm_imma(float *input0, float *input1, float *input2, float* input3, float* input4, float*input5, float* input6,\n                                  float *output0) {\n    // keys need to be replaced 4096, 1024, K_GLOBA_VALUEL, 0.75, 64\n\n\t//extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B = reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_index = reinterpret_cast<int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n\tconst int WARP_SIZE = 32;\n    const int M = 16;\n    const int K = 16;\n    const int N = 16;\n    const int WMMA_M = 16;\n    const int WMMA_K = 16;\n    const int WMMA_N = 16;\n    const int M_GLOBAL = 4096;\n    const int K_GLOBAL = 1024;\n    const int N_GLOBAL = 1024;\n    const float SPARSITY = 0.75;\n    const int K_GLOBAL_SPARSE = (int(K_GLOBAL * (1-SPARSITY)));\n    const int M_TILES = (M_GLOBAL / M);\n    const int N_TILES = (N_GLOBAL / N);\n    const int K_TILES = (K_GLOBAL / K);\n    const int BLOCK_ROW_WARPS = 2;\n    const int BLOCK_COL_WARPS = 4;\n    const int WARP_ROW_TILES = 4;\n    const int WARP_COL_TILES = 2;\n    const int SKEW_UINT8 = 16;\n    const int BANK_VAL = 32;\n    const int NUM_BANK = (K_GLOBAL / BANK_VAL);\n    /////////// BLOCK_ROW_TILES <= N_TILES ////////////\n    // const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    // ////////// BLOCK_COL_TILES <= M_TILES ////////////\n    // const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    auto C_LAYOUT = wmma::mem_col_major;\n    const int CHUNK_K = 64;\n    const int CHUNK_K_SPARSE = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K_SPARSE = (int((CHUNK_K * K) * (1 - SPARSITY)));\n    const int WARP_COPY_BYTES = (WARP_SIZE * sizeof(int4));\n    const int CHUNK_LINE_BYTES_A (BLOCK_COL_TILES * M * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_A = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_A);\n    const int CHUNK_COPY_LINE_LANES_A = (CHUNK_LINE_BYTES_A / sizeof(int4));\n    const int SHARED_OFFSET_A = (BLOCK_COL_TILES * M + SKEW_UINT8);\n\n    const int CHUNK_LINE_BYTES_B = (BLOCK_SIZE_K_SPARSE * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_B = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_B);\n    const int CHUNK_COPY_LINE_LANES_B = (CHUNK_LINE_BYTES_B / sizeof(int4));\n    const int SHARED_OFFSET_B = (BLOCK_SIZE_K_SPARSE + SKEW_UINT8);\n    const int SHARED_TO_GLOBAL_BYTES_PER_LINE = ((WARP_COL_TILES * M) * sizeof(int));\n    const int SHARED_TO_GLOBAL_BYTES_PER_WARP = (WARP_SIZE * sizeof(int));\n    const int SHARED_TO_GLOBAL_LINES_PER_WARP = (SHARED_TO_GLOBAL_BYTES_PER_WARP / SHARED_TO_GLOBAL_BYTES_PER_LINE);\n    const int SHARED_TO_GLOBAL_LANES_PER_LINE = (WARP_SIZE / SHARED_TO_GLOBAL_LINES_PER_WARP);\n    const int SHARED_TO_GLOBAL_ITERS = ((WARP_ROW_TILES * N) / SHARED_TO_GLOBAL_LINES_PER_WARP);\n\n    const int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const int WARP_STRIDE = (WARP_COL_TILES * M);\n    const int WARPS_PER_BLOCK = (BLOCK_ROW_WARPS * BLOCK_COL_WARPS);\n    const int THREADS_PER_BLOCK = (WARP_SIZE * WARPS_PER_BLOCK);\n    const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    const int GLOBAL_MEM_STRIDE = M_GLOBAL;\n    const int SHMEM_STRIDE = (M * BLOCK_COL_TILES);\n    const int SHMEM_OFFSET = (M * WARP_COL_TILES);\n    const int BLOCK_SIZE_M = (M * BLOCK_COL_TILES);\n    const int BLOCK_SIZE_N = (N * BLOCK_ROW_TILES);\n\n   extern __shared__ uint8_t shmem[];\n\n\t// Warp and lane identification.\n\tconst unsigned int warpId = threadIdx.x / WARP_SIZE;\n\tconst unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n\n\t// Offset in shared memory from which the B matrix is stored.\n\t// const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\tconst size_t shmem_idx_b_off = BLOCK_SIZE_K_SPARSE * SHARED_OFFSET_A;\n\n\n\t// Each CTA slides along the 128 x 128 tiles from the top left corner of the\n\t// matrix to the right and down, and selects the next tile to compute. Once\n\t// there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n\tconst unsigned int block_tile_i =\n\t\t((block_pos * BLOCK_COL_TILES) / M_TILES) * (BLOCK_ROW_TILES);\n\tconst unsigned int block_tile_j = (block_pos * BLOCK_COL_TILES) % M_TILES;\n\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n\twmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_ROW_TILES]\n\t\t\t\t\t\t\t\t\t\t\t\t\t [WARP_COL_TILES];\n\n    // Load the C matrix tiles into fragments from shared memory.\n\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t\twmma::fill_fragment(c[i][j], 0);\n\t\t}\n\t}\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    //for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n    for(int tile_k_idx_sparse = 0, tile_k_idx = 0; tile_k_idx_sparse < K_GLOBAL_SPARSE; tile_k_idx_sparse += BLOCK_SIZE_K_SPARSE, tile_k_idx += BLOCK_SIZE_K){\n\n\t\tsize_t shmem_idx = \n\t\twarpId < (WARPS_PER_BLOCK / 2)\n\t\t\t? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A\n\t\t\t: (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B + shmem_idx_b_off;\n\n\t\tint4 *lane_ptr = NULL;\n\t\tint *lane_ptr_index = NULL;\n\t\tconst uint8_t *warp_ptr = NULL;\n\n\n\t\tif(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\t//warp_ptr = &A[block_tile_j * M] +\n\t\t\t//\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * M_GLOBAL;\n\t\t\twarp_ptr = &A[block_tile_j * M];\n\t\t\t\n\t\t\tconst int *warp_ptr_index = &B_index[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t\t\t\t\t\t((warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A);\n\n\t\t\tlane_ptr_index = (int *)(warp_ptr_index + tile_k_idx_sparse + (laneId / CHUNK_COPY_LINE_LANES_A));\n\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_A) * SHARED_OFFSET_A;\n\t\t}else{\n\t\t\twarp_ptr = &B[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * K_GLOBAL_SPARSE;\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_sparse +\n\t\t\t\t\t\t\t\t(laneId / CHUNK_COPY_LINE_LANES_B) * K_GLOBAL_SPARSE) +\n\t\t\t\t\t\t\t\t(laneId % CHUNK_COPY_LINE_LANES_B);\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_B) * SHARED_OFFSET_B;\n\t\t}\n\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      // shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n\t  int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n\t  \t? BLOCK_SIZE_K_SPARSE / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A)\n\t\t: BLOCK_SIZE_N / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\n\t  /*\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\t  */\n\n\t  /*\n      int tile_k_idx_A;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n          tile_k_idx_A = *(lane_ptr_index);\n      }\n\t  */\n\n\t  #pragma unroll\n\t  for(int i = 0; i < iter_index; i += 1){\n\t\t  if(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\tint tile_k_idx_A = *(lane_ptr_index);\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_A * M_GLOBAL) + (laneId % CHUNK_COPY_LINE_LANES_A);\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_A)) =\n\t\t\t\t*lane_ptr;\n\t\t\t//warp_ptr = (uint8_t *)((uint8_t *)warp_ptr + M_GLOBAL * (WARPS_PER_BLOCK / 2) *CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tlane_ptr_index = (int *)((int *)lane_ptr_index +  (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A;\n\t\t  }else{\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_B)) =\n\t\t\t\t*lane_ptr;\n\t\t\tlane_ptr = (int4 *)((uint8_t *)lane_ptr + K_GLOBAL_SPARSE * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B;\n\t\t  }\n\t  }\n\n      __syncthreads();\n\n\t#pragma unroll\n      for (int k_step = 0; k_step < CHUNK_K_SPARSE; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::col_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n\t#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i += 1) {\n\t\t\tsize_t shmem_idx_a = (warpId % BLOCK_COL_WARPS) * M * WARP_COL_TILES + (i * M);\n\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_a + k_step * K * SHARED_OFFSET_A;\n\n\t\t\twmma::load_matrix_sync(a[i], tile_ptr, SHARED_OFFSET_A);\n\t\t#pragma unroll\n\t\t\tfor(int j = 0; j < WARP_ROW_TILES; j += 1){\n\t\t\t\tif(i == 0){\n\t\t\t\t\tsize_t shmem_idx_b = shmem_idx_b_off +\n\t\t\t\t\t\t\t\t\t\t\t(warpId / BLOCK_COL_WARPS) * (WARP_ROW_TILES * N) * SHARED_OFFSET_B +\n\t\t\t\t\t\t\t\t\t\t\t(j * N) * SHARED_OFFSET_B;\n\t\t\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_b + k_step * K;\n\t\t\t\t\twmma::load_matrix_sync(b[j], tile_ptr, SHARED_OFFSET_B);\n\t\t\t\t}\n\t\t\t\twmma::mma_sync(c[j][i], a[i], b[j], c[j][i]);\n\t\t\t}\n\n        }\n      }\n\n      __syncthreads();\n    }\n\n    // This pointer is used to access the C and D matrix tiles this warp computes.\n\tint *shmem_warp_tile_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * N * WARP_ROW_TILES * SHMEM_STRIDE +\n\t(warpId % BLOCK_COL_WARPS) * SHMEM_OFFSET;\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t#pragma unroll\n\t\t\tfor(int t = 0; t < c[i][j].num_elements; t += 1){\n\t\t\t\tc[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n\t\t\t}\n\t\t\tint *tile_ptr = shmem_warp_tile_ptr + i * N * SHMEM_STRIDE + j * M;\n\t\t\twmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, C_LAYOUT);\n\t\t}\n\t}\n\n    __syncthreads();\n\n\tint *shmem_warp_stream_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N * SHMEM_STRIDE\n\t\t\t\t\t\t\t\t\t+ (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tconst size_t gmem_idx =\n\t\t(block_tile_i * N + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N) * GLOBAL_MEM_STRIDE +\n\t\tblock_tile_j * M + (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tuint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n\tint *shmem_lane_stream_ptr =\n\t\tshmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * SHMEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\t\n\tuint8_t *dst_gmem_lane_stream_ptr =\n\t\tdst_gmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * GLOBAL_MEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\n\tfor(int i = 0; i < WARP_ROW_TILES * N; i += SHARED_TO_GLOBAL_LINES_PER_WARP){\n\t\t*(dst_gmem_lane_stream_ptr + i * GLOBAL_MEM_STRIDE) = (uint8_t)(*(shmem_lane_stream_ptr + i * SHMEM_STRIDE));\n\t}\n\n\t__syncthreads();\n}\n", "launch_config": {"dimBlock": [256, 1], "dimGrid": [256, 1]}}, "117": {"code": "\n__global__ void compute_gemm_imma(float *input0, float *input1, float *input2, float* input3, float* input4, float*input5, float* input6,\n                                  float *output0) {\n    // keys need to be replaced 4096, 1024, K_GLOBA_VALUEL, 0.75, 64\n\n\t//extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B = reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_index = reinterpret_cast<int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n\tconst int WARP_SIZE = 32;\n    const int M = 16;\n    const int K = 16;\n    const int N = 16;\n    const int WMMA_M = 16;\n    const int WMMA_K = 16;\n    const int WMMA_N = 16;\n    const int M_GLOBAL = 4096;\n    const int K_GLOBAL = 1024;\n    const int N_GLOBAL = 1024;\n    const float SPARSITY = 0.75;\n    const int K_GLOBAL_SPARSE = (int(K_GLOBAL * (1-SPARSITY)));\n    const int M_TILES = (M_GLOBAL / M);\n    const int N_TILES = (N_GLOBAL / N);\n    const int K_TILES = (K_GLOBAL / K);\n    const int BLOCK_ROW_WARPS = 2;\n    const int BLOCK_COL_WARPS = 4;\n    const int WARP_ROW_TILES = 4;\n    const int WARP_COL_TILES = 2;\n    const int SKEW_UINT8 = 16;\n    const int BANK_VAL = 32;\n    const int NUM_BANK = (K_GLOBAL / BANK_VAL);\n    /////////// BLOCK_ROW_TILES <= N_TILES ////////////\n    // const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    // ////////// BLOCK_COL_TILES <= M_TILES ////////////\n    // const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    auto C_LAYOUT = wmma::mem_col_major;\n    const int CHUNK_K = 64;\n    const int CHUNK_K_SPARSE = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K_SPARSE = (int((CHUNK_K * K) * (1 - SPARSITY)));\n    const int WARP_COPY_BYTES = (WARP_SIZE * sizeof(int4));\n    const int CHUNK_LINE_BYTES_A (BLOCK_COL_TILES * M * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_A = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_A);\n    const int CHUNK_COPY_LINE_LANES_A = (CHUNK_LINE_BYTES_A / sizeof(int4));\n    const int SHARED_OFFSET_A = (BLOCK_COL_TILES * M + SKEW_UINT8);\n\n    const int CHUNK_LINE_BYTES_B = (BLOCK_SIZE_K_SPARSE * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_B = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_B);\n    const int CHUNK_COPY_LINE_LANES_B = (CHUNK_LINE_BYTES_B / sizeof(int4));\n    const int SHARED_OFFSET_B = (BLOCK_SIZE_K_SPARSE + SKEW_UINT8);\n    const int SHARED_TO_GLOBAL_BYTES_PER_LINE = ((WARP_COL_TILES * M) * sizeof(int));\n    const int SHARED_TO_GLOBAL_BYTES_PER_WARP = (WARP_SIZE * sizeof(int));\n    const int SHARED_TO_GLOBAL_LINES_PER_WARP = (SHARED_TO_GLOBAL_BYTES_PER_WARP / SHARED_TO_GLOBAL_BYTES_PER_LINE);\n    const int SHARED_TO_GLOBAL_LANES_PER_LINE = (WARP_SIZE / SHARED_TO_GLOBAL_LINES_PER_WARP);\n    const int SHARED_TO_GLOBAL_ITERS = ((WARP_ROW_TILES * N) / SHARED_TO_GLOBAL_LINES_PER_WARP);\n\n    const int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const int WARP_STRIDE = (WARP_COL_TILES * M);\n    const int WARPS_PER_BLOCK = (BLOCK_ROW_WARPS * BLOCK_COL_WARPS);\n    const int THREADS_PER_BLOCK = (WARP_SIZE * WARPS_PER_BLOCK);\n    const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    const int GLOBAL_MEM_STRIDE = M_GLOBAL;\n    const int SHMEM_STRIDE = (M * BLOCK_COL_TILES);\n    const int SHMEM_OFFSET = (M * WARP_COL_TILES);\n    const int BLOCK_SIZE_M = (M * BLOCK_COL_TILES);\n    const int BLOCK_SIZE_N = (N * BLOCK_ROW_TILES);\n\n   extern __shared__ uint8_t shmem[];\n\n\t// Warp and lane identification.\n\tconst unsigned int warpId = threadIdx.x / WARP_SIZE;\n\tconst unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n\n\t// Offset in shared memory from which the B matrix is stored.\n\t// const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\tconst size_t shmem_idx_b_off = BLOCK_SIZE_K_SPARSE * SHARED_OFFSET_A;\n\n\n\t// Each CTA slides along the 128 x 128 tiles from the top left corner of the\n\t// matrix to the right and down, and selects the next tile to compute. Once\n\t// there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n\tconst unsigned int block_tile_i =\n\t\t((block_pos * BLOCK_COL_TILES) / M_TILES) * (BLOCK_ROW_TILES);\n\tconst unsigned int block_tile_j = (block_pos * BLOCK_COL_TILES) % M_TILES;\n\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n\twmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_ROW_TILES]\n\t\t\t\t\t\t\t\t\t\t\t\t\t [WARP_COL_TILES];\n\n    // Load the C matrix tiles into fragments from shared memory.\n\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t\twmma::fill_fragment(c[i][j], 0);\n\t\t}\n\t}\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    //for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n    for(int tile_k_idx_sparse = 0, tile_k_idx = 0; tile_k_idx_sparse < K_GLOBAL_SPARSE; tile_k_idx_sparse += BLOCK_SIZE_K_SPARSE, tile_k_idx += BLOCK_SIZE_K){\n\n\t\tsize_t shmem_idx = \n\t\twarpId < (WARPS_PER_BLOCK / 2)\n\t\t\t? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A\n\t\t\t: (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B + shmem_idx_b_off;\n\n\t\tint4 *lane_ptr = NULL;\n\t\tint *lane_ptr_index = NULL;\n\t\tconst uint8_t *warp_ptr = NULL;\n\n\n\t\tif(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\t//warp_ptr = &A[block_tile_j * M] +\n\t\t\t//\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * M_GLOBAL;\n\t\t\twarp_ptr = &A[block_tile_j * M];\n\t\t\t\n\t\t\tconst int *warp_ptr_index = &B_index[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t\t\t\t\t\t((warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A);\n\n\t\t\tlane_ptr_index = (int *)(warp_ptr_index + tile_k_idx_sparse + (laneId / CHUNK_COPY_LINE_LANES_A));\n\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_A) * SHARED_OFFSET_A;\n\t\t}else{\n\t\t\twarp_ptr = &B[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * K_GLOBAL_SPARSE;\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_sparse +\n\t\t\t\t\t\t\t\t(laneId / CHUNK_COPY_LINE_LANES_B) * K_GLOBAL_SPARSE) +\n\t\t\t\t\t\t\t\t(laneId % CHUNK_COPY_LINE_LANES_B);\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_B) * SHARED_OFFSET_B;\n\t\t}\n\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      // shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n\t  int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n\t  \t? BLOCK_SIZE_K_SPARSE / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A)\n\t\t: BLOCK_SIZE_N / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\n\t  /*\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\t  */\n\n\t  /*\n      int tile_k_idx_A;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n          tile_k_idx_A = *(lane_ptr_index);\n      }\n\t  */\n\n\t  #pragma unroll\n\t  for(int i = 0; i < iter_index; i += 1){\n\t\t  if(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\tint tile_k_idx_A = *(lane_ptr_index);\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_A * M_GLOBAL) + (laneId % CHUNK_COPY_LINE_LANES_A);\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_A)) =\n\t\t\t\t*lane_ptr;\n\t\t\t//warp_ptr = (uint8_t *)((uint8_t *)warp_ptr + M_GLOBAL * (WARPS_PER_BLOCK / 2) *CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tlane_ptr_index = (int *)((int *)lane_ptr_index +  (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A;\n\t\t  }else{\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_B)) =\n\t\t\t\t*lane_ptr;\n\t\t\tlane_ptr = (int4 *)((uint8_t *)lane_ptr + K_GLOBAL_SPARSE * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B;\n\t\t  }\n\t  }\n\n      __syncthreads();\n\n\t#pragma unroll\n      for (int k_step = 0; k_step < CHUNK_K_SPARSE; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::col_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n\t#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i += 1) {\n\t\t\tsize_t shmem_idx_a = (warpId % BLOCK_COL_WARPS) * M * WARP_COL_TILES + (i * M);\n\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_a + k_step * K * SHARED_OFFSET_A;\n\n\t\t\twmma::load_matrix_sync(a[i], tile_ptr, SHARED_OFFSET_A);\n\t\t#pragma unroll\n\t\t\tfor(int j = 0; j < WARP_ROW_TILES; j += 1){\n\t\t\t\tif(i == 0){\n\t\t\t\t\tsize_t shmem_idx_b = shmem_idx_b_off +\n\t\t\t\t\t\t\t\t\t\t\t(warpId / BLOCK_COL_WARPS) * (WARP_ROW_TILES * N) * SHARED_OFFSET_B +\n\t\t\t\t\t\t\t\t\t\t\t(j * N) * SHARED_OFFSET_B;\n\t\t\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_b + k_step * K;\n\t\t\t\t\twmma::load_matrix_sync(b[j], tile_ptr, SHARED_OFFSET_B);\n\t\t\t\t}\n\t\t\t\twmma::mma_sync(c[j][i], a[i], b[j], c[j][i]);\n\t\t\t}\n\n        }\n      }\n\n      __syncthreads();\n    }\n\n    // This pointer is used to access the C and D matrix tiles this warp computes.\n\tint *shmem_warp_tile_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * N * WARP_ROW_TILES * SHMEM_STRIDE +\n\t(warpId % BLOCK_COL_WARPS) * SHMEM_OFFSET;\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t#pragma unroll\n\t\t\tfor(int t = 0; t < c[i][j].num_elements; t += 1){\n\t\t\t\tc[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n\t\t\t}\n\t\t\tint *tile_ptr = shmem_warp_tile_ptr + i * N * SHMEM_STRIDE + j * M;\n\t\t\twmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, C_LAYOUT);\n\t\t}\n\t}\n\n    __syncthreads();\n\n\tint *shmem_warp_stream_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N * SHMEM_STRIDE\n\t\t\t\t\t\t\t\t\t+ (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tconst size_t gmem_idx =\n\t\t(block_tile_i * N + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N) * GLOBAL_MEM_STRIDE +\n\t\tblock_tile_j * M + (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tuint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n\tint *shmem_lane_stream_ptr =\n\t\tshmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * SHMEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\t\n\tuint8_t *dst_gmem_lane_stream_ptr =\n\t\tdst_gmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * GLOBAL_MEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\n\tfor(int i = 0; i < WARP_ROW_TILES * N; i += SHARED_TO_GLOBAL_LINES_PER_WARP){\n\t\t*(dst_gmem_lane_stream_ptr + i * GLOBAL_MEM_STRIDE) = (uint8_t)(*(shmem_lane_stream_ptr + i * SHMEM_STRIDE));\n\t}\n\n\t__syncthreads();\n}\n", "launch_config": {"dimBlock": [256, 1], "dimGrid": [256, 1]}}, "118": {"code": "\n__global__ void compute_gemm_imma(float *input0, float *input1, float *input2, float* input3, float* input4, float*input5, float* input6,\n                                  float *output0) {\n    // keys need to be replaced 4096, 1024, K_GLOBA_VALUEL, 0.75, 64\n\n\t//extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B = reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_index = reinterpret_cast<int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n\tconst int WARP_SIZE = 32;\n    const int M = 16;\n    const int K = 16;\n    const int N = 16;\n    const int WMMA_M = 16;\n    const int WMMA_K = 16;\n    const int WMMA_N = 16;\n    const int M_GLOBAL = 4096;\n    const int K_GLOBAL = 1024;\n    const int N_GLOBAL = 1024;\n    const float SPARSITY = 0.75;\n    const int K_GLOBAL_SPARSE = (int(K_GLOBAL * (1-SPARSITY)));\n    const int M_TILES = (M_GLOBAL / M);\n    const int N_TILES = (N_GLOBAL / N);\n    const int K_TILES = (K_GLOBAL / K);\n    const int BLOCK_ROW_WARPS = 2;\n    const int BLOCK_COL_WARPS = 4;\n    const int WARP_ROW_TILES = 4;\n    const int WARP_COL_TILES = 2;\n    const int SKEW_UINT8 = 16;\n    const int BANK_VAL = 32;\n    const int NUM_BANK = (K_GLOBAL / BANK_VAL);\n    /////////// BLOCK_ROW_TILES <= N_TILES ////////////\n    // const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    // ////////// BLOCK_COL_TILES <= M_TILES ////////////\n    // const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    auto C_LAYOUT = wmma::mem_col_major;\n    const int CHUNK_K = 64;\n    const int CHUNK_K_SPARSE = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K_SPARSE = (int((CHUNK_K * K) * (1 - SPARSITY)));\n    const int WARP_COPY_BYTES = (WARP_SIZE * sizeof(int4));\n    const int CHUNK_LINE_BYTES_A (BLOCK_COL_TILES * M * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_A = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_A);\n    const int CHUNK_COPY_LINE_LANES_A = (CHUNK_LINE_BYTES_A / sizeof(int4));\n    const int SHARED_OFFSET_A = (BLOCK_COL_TILES * M + SKEW_UINT8);\n\n    const int CHUNK_LINE_BYTES_B = (BLOCK_SIZE_K_SPARSE * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_B = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_B);\n    const int CHUNK_COPY_LINE_LANES_B = (CHUNK_LINE_BYTES_B / sizeof(int4));\n    const int SHARED_OFFSET_B = (BLOCK_SIZE_K_SPARSE + SKEW_UINT8);\n    const int SHARED_TO_GLOBAL_BYTES_PER_LINE = ((WARP_COL_TILES * M) * sizeof(int));\n    const int SHARED_TO_GLOBAL_BYTES_PER_WARP = (WARP_SIZE * sizeof(int));\n    const int SHARED_TO_GLOBAL_LINES_PER_WARP = (SHARED_TO_GLOBAL_BYTES_PER_WARP / SHARED_TO_GLOBAL_BYTES_PER_LINE);\n    const int SHARED_TO_GLOBAL_LANES_PER_LINE = (WARP_SIZE / SHARED_TO_GLOBAL_LINES_PER_WARP);\n    const int SHARED_TO_GLOBAL_ITERS = ((WARP_ROW_TILES * N) / SHARED_TO_GLOBAL_LINES_PER_WARP);\n\n    const int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const int WARP_STRIDE = (WARP_COL_TILES * M);\n    const int WARPS_PER_BLOCK = (BLOCK_ROW_WARPS * BLOCK_COL_WARPS);\n    const int THREADS_PER_BLOCK = (WARP_SIZE * WARPS_PER_BLOCK);\n    const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    const int GLOBAL_MEM_STRIDE = M_GLOBAL;\n    const int SHMEM_STRIDE = (M * BLOCK_COL_TILES);\n    const int SHMEM_OFFSET = (M * WARP_COL_TILES);\n    const int BLOCK_SIZE_M = (M * BLOCK_COL_TILES);\n    const int BLOCK_SIZE_N = (N * BLOCK_ROW_TILES);\n\n   extern __shared__ uint8_t shmem[];\n\n\t// Warp and lane identification.\n\tconst unsigned int warpId = threadIdx.x / WARP_SIZE;\n\tconst unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n\n\t// Offset in shared memory from which the B matrix is stored.\n\t// const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\tconst size_t shmem_idx_b_off = BLOCK_SIZE_K_SPARSE * SHARED_OFFSET_A;\n\n\n\t// Each CTA slides along the 128 x 128 tiles from the top left corner of the\n\t// matrix to the right and down, and selects the next tile to compute. Once\n\t// there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n\tconst unsigned int block_tile_i =\n\t\t((block_pos * BLOCK_COL_TILES) / M_TILES) * (BLOCK_ROW_TILES);\n\tconst unsigned int block_tile_j = (block_pos * BLOCK_COL_TILES) % M_TILES;\n\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n\twmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_ROW_TILES]\n\t\t\t\t\t\t\t\t\t\t\t\t\t [WARP_COL_TILES];\n\n    // Load the C matrix tiles into fragments from shared memory.\n\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t\twmma::fill_fragment(c[i][j], 0);\n\t\t}\n\t}\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    //for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n    for(int tile_k_idx_sparse = 0, tile_k_idx = 0; tile_k_idx_sparse < K_GLOBAL_SPARSE; tile_k_idx_sparse += BLOCK_SIZE_K_SPARSE, tile_k_idx += BLOCK_SIZE_K){\n\n\t\tsize_t shmem_idx = \n\t\twarpId < (WARPS_PER_BLOCK / 2)\n\t\t\t? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A\n\t\t\t: (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B + shmem_idx_b_off;\n\n\t\tint4 *lane_ptr = NULL;\n\t\tint *lane_ptr_index = NULL;\n\t\tconst uint8_t *warp_ptr = NULL;\n\n\n\t\tif(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\t//warp_ptr = &A[block_tile_j * M] +\n\t\t\t//\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * M_GLOBAL;\n\t\t\twarp_ptr = &A[block_tile_j * M];\n\t\t\t\n\t\t\tconst int *warp_ptr_index = &B_index[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t\t\t\t\t\t((warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A);\n\n\t\t\tlane_ptr_index = (int *)(warp_ptr_index + tile_k_idx_sparse + (laneId / CHUNK_COPY_LINE_LANES_A));\n\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_A) * SHARED_OFFSET_A;\n\t\t}else{\n\t\t\twarp_ptr = &B[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * K_GLOBAL_SPARSE;\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_sparse +\n\t\t\t\t\t\t\t\t(laneId / CHUNK_COPY_LINE_LANES_B) * K_GLOBAL_SPARSE) +\n\t\t\t\t\t\t\t\t(laneId % CHUNK_COPY_LINE_LANES_B);\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_B) * SHARED_OFFSET_B;\n\t\t}\n\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      // shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n\t  int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n\t  \t? BLOCK_SIZE_K_SPARSE / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A)\n\t\t: BLOCK_SIZE_N / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\n\t  /*\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\t  */\n\n\t  /*\n      int tile_k_idx_A;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n          tile_k_idx_A = *(lane_ptr_index);\n      }\n\t  */\n\n\t  #pragma unroll\n\t  for(int i = 0; i < iter_index; i += 1){\n\t\t  if(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\tint tile_k_idx_A = *(lane_ptr_index);\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_A * M_GLOBAL) + (laneId % CHUNK_COPY_LINE_LANES_A);\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_A)) =\n\t\t\t\t*lane_ptr;\n\t\t\t//warp_ptr = (uint8_t *)((uint8_t *)warp_ptr + M_GLOBAL * (WARPS_PER_BLOCK / 2) *CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tlane_ptr_index = (int *)((int *)lane_ptr_index +  (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A;\n\t\t  }else{\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_B)) =\n\t\t\t\t*lane_ptr;\n\t\t\tlane_ptr = (int4 *)((uint8_t *)lane_ptr + K_GLOBAL_SPARSE * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B;\n\t\t  }\n\t  }\n\n      __syncthreads();\n\n\t#pragma unroll\n      for (int k_step = 0; k_step < CHUNK_K_SPARSE; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::col_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n\t#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i += 1) {\n\t\t\tsize_t shmem_idx_a = (warpId % BLOCK_COL_WARPS) * M * WARP_COL_TILES + (i * M);\n\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_a + k_step * K * SHARED_OFFSET_A;\n\n\t\t\twmma::load_matrix_sync(a[i], tile_ptr, SHARED_OFFSET_A);\n\t\t#pragma unroll\n\t\t\tfor(int j = 0; j < WARP_ROW_TILES; j += 1){\n\t\t\t\tif(i == 0){\n\t\t\t\t\tsize_t shmem_idx_b = shmem_idx_b_off +\n\t\t\t\t\t\t\t\t\t\t\t(warpId / BLOCK_COL_WARPS) * (WARP_ROW_TILES * N) * SHARED_OFFSET_B +\n\t\t\t\t\t\t\t\t\t\t\t(j * N) * SHARED_OFFSET_B;\n\t\t\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_b + k_step * K;\n\t\t\t\t\twmma::load_matrix_sync(b[j], tile_ptr, SHARED_OFFSET_B);\n\t\t\t\t}\n\t\t\t\twmma::mma_sync(c[j][i], a[i], b[j], c[j][i]);\n\t\t\t}\n\n        }\n      }\n\n      __syncthreads();\n    }\n\n    // This pointer is used to access the C and D matrix tiles this warp computes.\n\tint *shmem_warp_tile_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * N * WARP_ROW_TILES * SHMEM_STRIDE +\n\t(warpId % BLOCK_COL_WARPS) * SHMEM_OFFSET;\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t#pragma unroll\n\t\t\tfor(int t = 0; t < c[i][j].num_elements; t += 1){\n\t\t\t\tc[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n\t\t\t}\n\t\t\tint *tile_ptr = shmem_warp_tile_ptr + i * N * SHMEM_STRIDE + j * M;\n\t\t\twmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, C_LAYOUT);\n\t\t}\n\t}\n\n    __syncthreads();\n\n\tint *shmem_warp_stream_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N * SHMEM_STRIDE\n\t\t\t\t\t\t\t\t\t+ (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tconst size_t gmem_idx =\n\t\t(block_tile_i * N + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N) * GLOBAL_MEM_STRIDE +\n\t\tblock_tile_j * M + (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tuint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n\tint *shmem_lane_stream_ptr =\n\t\tshmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * SHMEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\t\n\tuint8_t *dst_gmem_lane_stream_ptr =\n\t\tdst_gmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * GLOBAL_MEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\n\tfor(int i = 0; i < WARP_ROW_TILES * N; i += SHARED_TO_GLOBAL_LINES_PER_WARP){\n\t\t*(dst_gmem_lane_stream_ptr + i * GLOBAL_MEM_STRIDE) = (uint8_t)(*(shmem_lane_stream_ptr + i * SHMEM_STRIDE));\n\t}\n\n\t__syncthreads();\n}\n", "launch_config": {"dimBlock": [256, 1], "dimGrid": [256, 1]}}, "119": {"code": "\n__global__ void compute_gemm_imma(float *input0, float *input1, float *input2, float* input3, float* input4, float*input5, float* input6,\n                                  float *output0) {\n    // keys need to be replaced 4096, 4096, K_GLOBA_VALUEL, 0.75, 64\n\n\t//extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B = reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_index = reinterpret_cast<int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n\tconst int WARP_SIZE = 32;\n    const int M = 16;\n    const int K = 16;\n    const int N = 16;\n    const int WMMA_M = 16;\n    const int WMMA_K = 16;\n    const int WMMA_N = 16;\n    const int M_GLOBAL = 4096;\n    const int K_GLOBAL = 1024;\n    const int N_GLOBAL = 4096;\n    const float SPARSITY = 0.75;\n    const int K_GLOBAL_SPARSE = (int(K_GLOBAL * (1-SPARSITY)));\n    const int M_TILES = (M_GLOBAL / M);\n    const int N_TILES = (N_GLOBAL / N);\n    const int K_TILES = (K_GLOBAL / K);\n    const int BLOCK_ROW_WARPS = 2;\n    const int BLOCK_COL_WARPS = 4;\n    const int WARP_ROW_TILES = 4;\n    const int WARP_COL_TILES = 2;\n    const int SKEW_UINT8 = 16;\n    const int BANK_VAL = 32;\n    const int NUM_BANK = (K_GLOBAL / BANK_VAL);\n    /////////// BLOCK_ROW_TILES <= N_TILES ////////////\n    // const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    // ////////// BLOCK_COL_TILES <= M_TILES ////////////\n    // const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    auto C_LAYOUT = wmma::mem_col_major;\n    const int CHUNK_K = 64;\n    const int CHUNK_K_SPARSE = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K_SPARSE = (int((CHUNK_K * K) * (1 - SPARSITY)));\n    const int WARP_COPY_BYTES = (WARP_SIZE * sizeof(int4));\n    const int CHUNK_LINE_BYTES_A (BLOCK_COL_TILES * M * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_A = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_A);\n    const int CHUNK_COPY_LINE_LANES_A = (CHUNK_LINE_BYTES_A / sizeof(int4));\n    const int SHARED_OFFSET_A = (BLOCK_COL_TILES * M + SKEW_UINT8);\n\n    const int CHUNK_LINE_BYTES_B = (BLOCK_SIZE_K_SPARSE * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_B = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_B);\n    const int CHUNK_COPY_LINE_LANES_B = (CHUNK_LINE_BYTES_B / sizeof(int4));\n    const int SHARED_OFFSET_B = (BLOCK_SIZE_K_SPARSE + SKEW_UINT8);\n    const int SHARED_TO_GLOBAL_BYTES_PER_LINE = ((WARP_COL_TILES * M) * sizeof(int));\n    const int SHARED_TO_GLOBAL_BYTES_PER_WARP = (WARP_SIZE * sizeof(int));\n    const int SHARED_TO_GLOBAL_LINES_PER_WARP = (SHARED_TO_GLOBAL_BYTES_PER_WARP / SHARED_TO_GLOBAL_BYTES_PER_LINE);\n    const int SHARED_TO_GLOBAL_LANES_PER_LINE = (WARP_SIZE / SHARED_TO_GLOBAL_LINES_PER_WARP);\n    const int SHARED_TO_GLOBAL_ITERS = ((WARP_ROW_TILES * N) / SHARED_TO_GLOBAL_LINES_PER_WARP);\n\n    const int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const int WARP_STRIDE = (WARP_COL_TILES * M);\n    const int WARPS_PER_BLOCK = (BLOCK_ROW_WARPS * BLOCK_COL_WARPS);\n    const int THREADS_PER_BLOCK = (WARP_SIZE * WARPS_PER_BLOCK);\n    const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    const int GLOBAL_MEM_STRIDE = M_GLOBAL;\n    const int SHMEM_STRIDE = (M * BLOCK_COL_TILES);\n    const int SHMEM_OFFSET = (M * WARP_COL_TILES);\n    const int BLOCK_SIZE_M = (M * BLOCK_COL_TILES);\n    const int BLOCK_SIZE_N = (N * BLOCK_ROW_TILES);\n\n   extern __shared__ uint8_t shmem[];\n\n\t// Warp and lane identification.\n\tconst unsigned int warpId = threadIdx.x / WARP_SIZE;\n\tconst unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n\n\t// Offset in shared memory from which the B matrix is stored.\n\t// const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\tconst size_t shmem_idx_b_off = BLOCK_SIZE_K_SPARSE * SHARED_OFFSET_A;\n\n\n\t// Each CTA slides along the 128 x 128 tiles from the top left corner of the\n\t// matrix to the right and down, and selects the next tile to compute. Once\n\t// there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n\tconst unsigned int block_tile_i =\n\t\t((block_pos * BLOCK_COL_TILES) / M_TILES) * (BLOCK_ROW_TILES);\n\tconst unsigned int block_tile_j = (block_pos * BLOCK_COL_TILES) % M_TILES;\n\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n\twmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_ROW_TILES]\n\t\t\t\t\t\t\t\t\t\t\t\t\t [WARP_COL_TILES];\n\n    // Load the C matrix tiles into fragments from shared memory.\n\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t\twmma::fill_fragment(c[i][j], 0);\n\t\t}\n\t}\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    //for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n    for(int tile_k_idx_sparse = 0, tile_k_idx = 0; tile_k_idx_sparse < K_GLOBAL_SPARSE; tile_k_idx_sparse += BLOCK_SIZE_K_SPARSE, tile_k_idx += BLOCK_SIZE_K){\n\n\t\tsize_t shmem_idx = \n\t\twarpId < (WARPS_PER_BLOCK / 2)\n\t\t\t? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A\n\t\t\t: (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B + shmem_idx_b_off;\n\n\t\tint4 *lane_ptr = NULL;\n\t\tint *lane_ptr_index = NULL;\n\t\tconst uint8_t *warp_ptr = NULL;\n\n\n\t\tif(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\t//warp_ptr = &A[block_tile_j * M] +\n\t\t\t//\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * M_GLOBAL;\n\t\t\twarp_ptr = &A[block_tile_j * M];\n\t\t\t\n\t\t\tconst int *warp_ptr_index = &B_index[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t\t\t\t\t\t((warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A);\n\n\t\t\tlane_ptr_index = (int *)(warp_ptr_index + tile_k_idx_sparse + (laneId / CHUNK_COPY_LINE_LANES_A));\n\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_A) * SHARED_OFFSET_A;\n\t\t}else{\n\t\t\twarp_ptr = &B[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * K_GLOBAL_SPARSE;\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_sparse +\n\t\t\t\t\t\t\t\t(laneId / CHUNK_COPY_LINE_LANES_B) * K_GLOBAL_SPARSE) +\n\t\t\t\t\t\t\t\t(laneId % CHUNK_COPY_LINE_LANES_B);\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_B) * SHARED_OFFSET_B;\n\t\t}\n\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      // shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n\t  int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n\t  \t? BLOCK_SIZE_K_SPARSE / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A)\n\t\t: BLOCK_SIZE_N / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\n\t  /*\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\t  */\n\n\t  /*\n      int tile_k_idx_A;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n          tile_k_idx_A = *(lane_ptr_index);\n      }\n\t  */\n\n\t  #pragma unroll\n\t  for(int i = 0; i < iter_index; i += 1){\n\t\t  if(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\tint tile_k_idx_A = *(lane_ptr_index);\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_A * M_GLOBAL) + (laneId % CHUNK_COPY_LINE_LANES_A);\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_A)) =\n\t\t\t\t*lane_ptr;\n\t\t\t//warp_ptr = (uint8_t *)((uint8_t *)warp_ptr + M_GLOBAL * (WARPS_PER_BLOCK / 2) *CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tlane_ptr_index = (int *)((int *)lane_ptr_index +  (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A;\n\t\t  }else{\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_B)) =\n\t\t\t\t*lane_ptr;\n\t\t\tlane_ptr = (int4 *)((uint8_t *)lane_ptr + K_GLOBAL_SPARSE * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B;\n\t\t  }\n\t  }\n\n      __syncthreads();\n\n\t#pragma unroll\n      for (int k_step = 0; k_step < CHUNK_K_SPARSE; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::col_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n\t#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i += 1) {\n\t\t\tsize_t shmem_idx_a = (warpId % BLOCK_COL_WARPS) * M * WARP_COL_TILES + (i * M);\n\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_a + k_step * K * SHARED_OFFSET_A;\n\n\t\t\twmma::load_matrix_sync(a[i], tile_ptr, SHARED_OFFSET_A);\n\t\t#pragma unroll\n\t\t\tfor(int j = 0; j < WARP_ROW_TILES; j += 1){\n\t\t\t\tif(i == 0){\n\t\t\t\t\tsize_t shmem_idx_b = shmem_idx_b_off +\n\t\t\t\t\t\t\t\t\t\t\t(warpId / BLOCK_COL_WARPS) * (WARP_ROW_TILES * N) * SHARED_OFFSET_B +\n\t\t\t\t\t\t\t\t\t\t\t(j * N) * SHARED_OFFSET_B;\n\t\t\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_b + k_step * K;\n\t\t\t\t\twmma::load_matrix_sync(b[j], tile_ptr, SHARED_OFFSET_B);\n\t\t\t\t}\n\t\t\t\twmma::mma_sync(c[j][i], a[i], b[j], c[j][i]);\n\t\t\t}\n\n        }\n      }\n\n      __syncthreads();\n    }\n\n    // This pointer is used to access the C and D matrix tiles this warp computes.\n\tint *shmem_warp_tile_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * N * WARP_ROW_TILES * SHMEM_STRIDE +\n\t(warpId % BLOCK_COL_WARPS) * SHMEM_OFFSET;\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t#pragma unroll\n\t\t\tfor(int t = 0; t < c[i][j].num_elements; t += 1){\n\t\t\t\tc[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n\t\t\t}\n\t\t\tint *tile_ptr = shmem_warp_tile_ptr + i * N * SHMEM_STRIDE + j * M;\n\t\t\twmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, C_LAYOUT);\n\t\t}\n\t}\n\n    __syncthreads();\n\n\tint *shmem_warp_stream_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N * SHMEM_STRIDE\n\t\t\t\t\t\t\t\t\t+ (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tconst size_t gmem_idx =\n\t\t(block_tile_i * N + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N) * GLOBAL_MEM_STRIDE +\n\t\tblock_tile_j * M + (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tuint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n\tint *shmem_lane_stream_ptr =\n\t\tshmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * SHMEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\t\n\tuint8_t *dst_gmem_lane_stream_ptr =\n\t\tdst_gmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * GLOBAL_MEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\n\tfor(int i = 0; i < WARP_ROW_TILES * N; i += SHARED_TO_GLOBAL_LINES_PER_WARP){\n\t\t*(dst_gmem_lane_stream_ptr + i * GLOBAL_MEM_STRIDE) = (uint8_t)(*(shmem_lane_stream_ptr + i * SHMEM_STRIDE));\n\t}\n\n\t__syncthreads();\n}\n", "launch_config": {"dimBlock": [1024, 1], "dimGrid": [256, 1]}}, "120": {"code": "\n__global__ void compute_gemm_imma(float *input0, float *input1, float *input2, float* input3, float* input4, float*input5, float* input6,\n                                  float *output0) {\n    // keys need to be replaced 4096, 1024, K_GLOBA_VALUEL, 0.75, 64\n\n\t//extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B = reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_index = reinterpret_cast<int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n\tconst int WARP_SIZE = 32;\n    const int M = 16;\n    const int K = 16;\n    const int N = 16;\n    const int WMMA_M = 16;\n    const int WMMA_K = 16;\n    const int WMMA_N = 16;\n    const int M_GLOBAL = 4096;\n    const int K_GLOBAL = 4096;\n    const int N_GLOBAL = 1024;\n    const float SPARSITY = 0.75;\n    const int K_GLOBAL_SPARSE = (int(K_GLOBAL * (1-SPARSITY)));\n    const int M_TILES = (M_GLOBAL / M);\n    const int N_TILES = (N_GLOBAL / N);\n    const int K_TILES = (K_GLOBAL / K);\n    const int BLOCK_ROW_WARPS = 2;\n    const int BLOCK_COL_WARPS = 4;\n    const int WARP_ROW_TILES = 4;\n    const int WARP_COL_TILES = 2;\n    const int SKEW_UINT8 = 16;\n    const int BANK_VAL = 32;\n    const int NUM_BANK = (K_GLOBAL / BANK_VAL);\n    /////////// BLOCK_ROW_TILES <= N_TILES ////////////\n    // const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    // ////////// BLOCK_COL_TILES <= M_TILES ////////////\n    // const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    auto C_LAYOUT = wmma::mem_col_major;\n    const int CHUNK_K = 64;\n    const int CHUNK_K_SPARSE = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K_SPARSE = (int((CHUNK_K * K) * (1 - SPARSITY)));\n    const int WARP_COPY_BYTES = (WARP_SIZE * sizeof(int4));\n    const int CHUNK_LINE_BYTES_A (BLOCK_COL_TILES * M * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_A = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_A);\n    const int CHUNK_COPY_LINE_LANES_A = (CHUNK_LINE_BYTES_A / sizeof(int4));\n    const int SHARED_OFFSET_A = (BLOCK_COL_TILES * M + SKEW_UINT8);\n\n    const int CHUNK_LINE_BYTES_B = (BLOCK_SIZE_K_SPARSE * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_B = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_B);\n    const int CHUNK_COPY_LINE_LANES_B = (CHUNK_LINE_BYTES_B / sizeof(int4));\n    const int SHARED_OFFSET_B = (BLOCK_SIZE_K_SPARSE + SKEW_UINT8);\n    const int SHARED_TO_GLOBAL_BYTES_PER_LINE = ((WARP_COL_TILES * M) * sizeof(int));\n    const int SHARED_TO_GLOBAL_BYTES_PER_WARP = (WARP_SIZE * sizeof(int));\n    const int SHARED_TO_GLOBAL_LINES_PER_WARP = (SHARED_TO_GLOBAL_BYTES_PER_WARP / SHARED_TO_GLOBAL_BYTES_PER_LINE);\n    const int SHARED_TO_GLOBAL_LANES_PER_LINE = (WARP_SIZE / SHARED_TO_GLOBAL_LINES_PER_WARP);\n    const int SHARED_TO_GLOBAL_ITERS = ((WARP_ROW_TILES * N) / SHARED_TO_GLOBAL_LINES_PER_WARP);\n\n    const int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const int WARP_STRIDE = (WARP_COL_TILES * M);\n    const int WARPS_PER_BLOCK = (BLOCK_ROW_WARPS * BLOCK_COL_WARPS);\n    const int THREADS_PER_BLOCK = (WARP_SIZE * WARPS_PER_BLOCK);\n    const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    const int GLOBAL_MEM_STRIDE = M_GLOBAL;\n    const int SHMEM_STRIDE = (M * BLOCK_COL_TILES);\n    const int SHMEM_OFFSET = (M * WARP_COL_TILES);\n    const int BLOCK_SIZE_M = (M * BLOCK_COL_TILES);\n    const int BLOCK_SIZE_N = (N * BLOCK_ROW_TILES);\n\n   extern __shared__ uint8_t shmem[];\n\n\t// Warp and lane identification.\n\tconst unsigned int warpId = threadIdx.x / WARP_SIZE;\n\tconst unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n\n\t// Offset in shared memory from which the B matrix is stored.\n\t// const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\tconst size_t shmem_idx_b_off = BLOCK_SIZE_K_SPARSE * SHARED_OFFSET_A;\n\n\n\t// Each CTA slides along the 128 x 128 tiles from the top left corner of the\n\t// matrix to the right and down, and selects the next tile to compute. Once\n\t// there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n\tconst unsigned int block_tile_i =\n\t\t((block_pos * BLOCK_COL_TILES) / M_TILES) * (BLOCK_ROW_TILES);\n\tconst unsigned int block_tile_j = (block_pos * BLOCK_COL_TILES) % M_TILES;\n\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n\twmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_ROW_TILES]\n\t\t\t\t\t\t\t\t\t\t\t\t\t [WARP_COL_TILES];\n\n    // Load the C matrix tiles into fragments from shared memory.\n\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t\twmma::fill_fragment(c[i][j], 0);\n\t\t}\n\t}\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    //for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n    for(int tile_k_idx_sparse = 0, tile_k_idx = 0; tile_k_idx_sparse < K_GLOBAL_SPARSE; tile_k_idx_sparse += BLOCK_SIZE_K_SPARSE, tile_k_idx += BLOCK_SIZE_K){\n\n\t\tsize_t shmem_idx = \n\t\twarpId < (WARPS_PER_BLOCK / 2)\n\t\t\t? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A\n\t\t\t: (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B + shmem_idx_b_off;\n\n\t\tint4 *lane_ptr = NULL;\n\t\tint *lane_ptr_index = NULL;\n\t\tconst uint8_t *warp_ptr = NULL;\n\n\n\t\tif(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\t//warp_ptr = &A[block_tile_j * M] +\n\t\t\t//\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * M_GLOBAL;\n\t\t\twarp_ptr = &A[block_tile_j * M];\n\t\t\t\n\t\t\tconst int *warp_ptr_index = &B_index[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t\t\t\t\t\t((warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A);\n\n\t\t\tlane_ptr_index = (int *)(warp_ptr_index + tile_k_idx_sparse + (laneId / CHUNK_COPY_LINE_LANES_A));\n\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_A) * SHARED_OFFSET_A;\n\t\t}else{\n\t\t\twarp_ptr = &B[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * K_GLOBAL_SPARSE;\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_sparse +\n\t\t\t\t\t\t\t\t(laneId / CHUNK_COPY_LINE_LANES_B) * K_GLOBAL_SPARSE) +\n\t\t\t\t\t\t\t\t(laneId % CHUNK_COPY_LINE_LANES_B);\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_B) * SHARED_OFFSET_B;\n\t\t}\n\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      // shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n\t  int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n\t  \t? BLOCK_SIZE_K_SPARSE / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A)\n\t\t: BLOCK_SIZE_N / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\n\t  /*\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\t  */\n\n\t  /*\n      int tile_k_idx_A;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n          tile_k_idx_A = *(lane_ptr_index);\n      }\n\t  */\n\n\t  #pragma unroll\n\t  for(int i = 0; i < iter_index; i += 1){\n\t\t  if(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\tint tile_k_idx_A = *(lane_ptr_index);\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_A * M_GLOBAL) + (laneId % CHUNK_COPY_LINE_LANES_A);\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_A)) =\n\t\t\t\t*lane_ptr;\n\t\t\t//warp_ptr = (uint8_t *)((uint8_t *)warp_ptr + M_GLOBAL * (WARPS_PER_BLOCK / 2) *CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tlane_ptr_index = (int *)((int *)lane_ptr_index +  (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A;\n\t\t  }else{\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_B)) =\n\t\t\t\t*lane_ptr;\n\t\t\tlane_ptr = (int4 *)((uint8_t *)lane_ptr + K_GLOBAL_SPARSE * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B;\n\t\t  }\n\t  }\n\n      __syncthreads();\n\n\t#pragma unroll\n      for (int k_step = 0; k_step < CHUNK_K_SPARSE; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::col_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n\t#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i += 1) {\n\t\t\tsize_t shmem_idx_a = (warpId % BLOCK_COL_WARPS) * M * WARP_COL_TILES + (i * M);\n\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_a + k_step * K * SHARED_OFFSET_A;\n\n\t\t\twmma::load_matrix_sync(a[i], tile_ptr, SHARED_OFFSET_A);\n\t\t#pragma unroll\n\t\t\tfor(int j = 0; j < WARP_ROW_TILES; j += 1){\n\t\t\t\tif(i == 0){\n\t\t\t\t\tsize_t shmem_idx_b = shmem_idx_b_off +\n\t\t\t\t\t\t\t\t\t\t\t(warpId / BLOCK_COL_WARPS) * (WARP_ROW_TILES * N) * SHARED_OFFSET_B +\n\t\t\t\t\t\t\t\t\t\t\t(j * N) * SHARED_OFFSET_B;\n\t\t\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_b + k_step * K;\n\t\t\t\t\twmma::load_matrix_sync(b[j], tile_ptr, SHARED_OFFSET_B);\n\t\t\t\t}\n\t\t\t\twmma::mma_sync(c[j][i], a[i], b[j], c[j][i]);\n\t\t\t}\n\n        }\n      }\n\n      __syncthreads();\n    }\n\n    // This pointer is used to access the C and D matrix tiles this warp computes.\n\tint *shmem_warp_tile_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * N * WARP_ROW_TILES * SHMEM_STRIDE +\n\t(warpId % BLOCK_COL_WARPS) * SHMEM_OFFSET;\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t#pragma unroll\n\t\t\tfor(int t = 0; t < c[i][j].num_elements; t += 1){\n\t\t\t\tc[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n\t\t\t}\n\t\t\tint *tile_ptr = shmem_warp_tile_ptr + i * N * SHMEM_STRIDE + j * M;\n\t\t\twmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, C_LAYOUT);\n\t\t}\n\t}\n\n    __syncthreads();\n\n\tint *shmem_warp_stream_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N * SHMEM_STRIDE\n\t\t\t\t\t\t\t\t\t+ (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tconst size_t gmem_idx =\n\t\t(block_tile_i * N + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N) * GLOBAL_MEM_STRIDE +\n\t\tblock_tile_j * M + (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tuint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n\tint *shmem_lane_stream_ptr =\n\t\tshmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * SHMEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\t\n\tuint8_t *dst_gmem_lane_stream_ptr =\n\t\tdst_gmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * GLOBAL_MEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\n\tfor(int i = 0; i < WARP_ROW_TILES * N; i += SHARED_TO_GLOBAL_LINES_PER_WARP){\n\t\t*(dst_gmem_lane_stream_ptr + i * GLOBAL_MEM_STRIDE) = (uint8_t)(*(shmem_lane_stream_ptr + i * SHMEM_STRIDE));\n\t}\n\n\t__syncthreads();\n}\n", "launch_config": {"dimBlock": [256, 1], "dimGrid": [256, 1]}}, "121": {"code": "\n__global__ void compute_gemm_imma(float *input0, float *input1, float *input2, float* input3, float* input4, float*input5, float* input6,\n                                  float *output0) {\n    // keys need to be replaced 4096, 1024, K_GLOBA_VALUEL, 0.75, 64\n\n\t//extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B = reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_index = reinterpret_cast<int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n\tconst int WARP_SIZE = 32;\n    const int M = 16;\n    const int K = 16;\n    const int N = 16;\n    const int WMMA_M = 16;\n    const int WMMA_K = 16;\n    const int WMMA_N = 16;\n    const int M_GLOBAL = 4096;\n    const int K_GLOBAL = 1024;\n    const int N_GLOBAL = 1024;\n    const float SPARSITY = 0.75;\n    const int K_GLOBAL_SPARSE = (int(K_GLOBAL * (1-SPARSITY)));\n    const int M_TILES = (M_GLOBAL / M);\n    const int N_TILES = (N_GLOBAL / N);\n    const int K_TILES = (K_GLOBAL / K);\n    const int BLOCK_ROW_WARPS = 2;\n    const int BLOCK_COL_WARPS = 4;\n    const int WARP_ROW_TILES = 4;\n    const int WARP_COL_TILES = 2;\n    const int SKEW_UINT8 = 16;\n    const int BANK_VAL = 32;\n    const int NUM_BANK = (K_GLOBAL / BANK_VAL);\n    /////////// BLOCK_ROW_TILES <= N_TILES ////////////\n    // const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    // ////////// BLOCK_COL_TILES <= M_TILES ////////////\n    // const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    auto C_LAYOUT = wmma::mem_col_major;\n    const int CHUNK_K = 64;\n    const int CHUNK_K_SPARSE = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K_SPARSE = (int((CHUNK_K * K) * (1 - SPARSITY)));\n    const int WARP_COPY_BYTES = (WARP_SIZE * sizeof(int4));\n    const int CHUNK_LINE_BYTES_A (BLOCK_COL_TILES * M * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_A = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_A);\n    const int CHUNK_COPY_LINE_LANES_A = (CHUNK_LINE_BYTES_A / sizeof(int4));\n    const int SHARED_OFFSET_A = (BLOCK_COL_TILES * M + SKEW_UINT8);\n\n    const int CHUNK_LINE_BYTES_B = (BLOCK_SIZE_K_SPARSE * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_B = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_B);\n    const int CHUNK_COPY_LINE_LANES_B = (CHUNK_LINE_BYTES_B / sizeof(int4));\n    const int SHARED_OFFSET_B = (BLOCK_SIZE_K_SPARSE + SKEW_UINT8);\n    const int SHARED_TO_GLOBAL_BYTES_PER_LINE = ((WARP_COL_TILES * M) * sizeof(int));\n    const int SHARED_TO_GLOBAL_BYTES_PER_WARP = (WARP_SIZE * sizeof(int));\n    const int SHARED_TO_GLOBAL_LINES_PER_WARP = (SHARED_TO_GLOBAL_BYTES_PER_WARP / SHARED_TO_GLOBAL_BYTES_PER_LINE);\n    const int SHARED_TO_GLOBAL_LANES_PER_LINE = (WARP_SIZE / SHARED_TO_GLOBAL_LINES_PER_WARP);\n    const int SHARED_TO_GLOBAL_ITERS = ((WARP_ROW_TILES * N) / SHARED_TO_GLOBAL_LINES_PER_WARP);\n\n    const int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const int WARP_STRIDE = (WARP_COL_TILES * M);\n    const int WARPS_PER_BLOCK = (BLOCK_ROW_WARPS * BLOCK_COL_WARPS);\n    const int THREADS_PER_BLOCK = (WARP_SIZE * WARPS_PER_BLOCK);\n    const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    const int GLOBAL_MEM_STRIDE = M_GLOBAL;\n    const int SHMEM_STRIDE = (M * BLOCK_COL_TILES);\n    const int SHMEM_OFFSET = (M * WARP_COL_TILES);\n    const int BLOCK_SIZE_M = (M * BLOCK_COL_TILES);\n    const int BLOCK_SIZE_N = (N * BLOCK_ROW_TILES);\n\n   extern __shared__ uint8_t shmem[];\n\n\t// Warp and lane identification.\n\tconst unsigned int warpId = threadIdx.x / WARP_SIZE;\n\tconst unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n\n\t// Offset in shared memory from which the B matrix is stored.\n\t// const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\tconst size_t shmem_idx_b_off = BLOCK_SIZE_K_SPARSE * SHARED_OFFSET_A;\n\n\n\t// Each CTA slides along the 128 x 128 tiles from the top left corner of the\n\t// matrix to the right and down, and selects the next tile to compute. Once\n\t// there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n\tconst unsigned int block_tile_i =\n\t\t((block_pos * BLOCK_COL_TILES) / M_TILES) * (BLOCK_ROW_TILES);\n\tconst unsigned int block_tile_j = (block_pos * BLOCK_COL_TILES) % M_TILES;\n\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n\twmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_ROW_TILES]\n\t\t\t\t\t\t\t\t\t\t\t\t\t [WARP_COL_TILES];\n\n    // Load the C matrix tiles into fragments from shared memory.\n\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t\twmma::fill_fragment(c[i][j], 0);\n\t\t}\n\t}\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    //for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n    for(int tile_k_idx_sparse = 0, tile_k_idx = 0; tile_k_idx_sparse < K_GLOBAL_SPARSE; tile_k_idx_sparse += BLOCK_SIZE_K_SPARSE, tile_k_idx += BLOCK_SIZE_K){\n\n\t\tsize_t shmem_idx = \n\t\twarpId < (WARPS_PER_BLOCK / 2)\n\t\t\t? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A\n\t\t\t: (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B + shmem_idx_b_off;\n\n\t\tint4 *lane_ptr = NULL;\n\t\tint *lane_ptr_index = NULL;\n\t\tconst uint8_t *warp_ptr = NULL;\n\n\n\t\tif(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\t//warp_ptr = &A[block_tile_j * M] +\n\t\t\t//\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * M_GLOBAL;\n\t\t\twarp_ptr = &A[block_tile_j * M];\n\t\t\t\n\t\t\tconst int *warp_ptr_index = &B_index[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t\t\t\t\t\t((warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A);\n\n\t\t\tlane_ptr_index = (int *)(warp_ptr_index + tile_k_idx_sparse + (laneId / CHUNK_COPY_LINE_LANES_A));\n\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_A) * SHARED_OFFSET_A;\n\t\t}else{\n\t\t\twarp_ptr = &B[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * K_GLOBAL_SPARSE;\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_sparse +\n\t\t\t\t\t\t\t\t(laneId / CHUNK_COPY_LINE_LANES_B) * K_GLOBAL_SPARSE) +\n\t\t\t\t\t\t\t\t(laneId % CHUNK_COPY_LINE_LANES_B);\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_B) * SHARED_OFFSET_B;\n\t\t}\n\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      // shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n\t  int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n\t  \t? BLOCK_SIZE_K_SPARSE / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A)\n\t\t: BLOCK_SIZE_N / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\n\t  /*\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\t  */\n\n\t  /*\n      int tile_k_idx_A;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n          tile_k_idx_A = *(lane_ptr_index);\n      }\n\t  */\n\n\t  #pragma unroll\n\t  for(int i = 0; i < iter_index; i += 1){\n\t\t  if(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\tint tile_k_idx_A = *(lane_ptr_index);\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_A * M_GLOBAL) + (laneId % CHUNK_COPY_LINE_LANES_A);\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_A)) =\n\t\t\t\t*lane_ptr;\n\t\t\t//warp_ptr = (uint8_t *)((uint8_t *)warp_ptr + M_GLOBAL * (WARPS_PER_BLOCK / 2) *CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tlane_ptr_index = (int *)((int *)lane_ptr_index +  (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A;\n\t\t  }else{\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_B)) =\n\t\t\t\t*lane_ptr;\n\t\t\tlane_ptr = (int4 *)((uint8_t *)lane_ptr + K_GLOBAL_SPARSE * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B;\n\t\t  }\n\t  }\n\n      __syncthreads();\n\n\t#pragma unroll\n      for (int k_step = 0; k_step < CHUNK_K_SPARSE; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::col_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n\t#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i += 1) {\n\t\t\tsize_t shmem_idx_a = (warpId % BLOCK_COL_WARPS) * M * WARP_COL_TILES + (i * M);\n\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_a + k_step * K * SHARED_OFFSET_A;\n\n\t\t\twmma::load_matrix_sync(a[i], tile_ptr, SHARED_OFFSET_A);\n\t\t#pragma unroll\n\t\t\tfor(int j = 0; j < WARP_ROW_TILES; j += 1){\n\t\t\t\tif(i == 0){\n\t\t\t\t\tsize_t shmem_idx_b = shmem_idx_b_off +\n\t\t\t\t\t\t\t\t\t\t\t(warpId / BLOCK_COL_WARPS) * (WARP_ROW_TILES * N) * SHARED_OFFSET_B +\n\t\t\t\t\t\t\t\t\t\t\t(j * N) * SHARED_OFFSET_B;\n\t\t\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_b + k_step * K;\n\t\t\t\t\twmma::load_matrix_sync(b[j], tile_ptr, SHARED_OFFSET_B);\n\t\t\t\t}\n\t\t\t\twmma::mma_sync(c[j][i], a[i], b[j], c[j][i]);\n\t\t\t}\n\n        }\n      }\n\n      __syncthreads();\n    }\n\n    // This pointer is used to access the C and D matrix tiles this warp computes.\n\tint *shmem_warp_tile_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * N * WARP_ROW_TILES * SHMEM_STRIDE +\n\t(warpId % BLOCK_COL_WARPS) * SHMEM_OFFSET;\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t#pragma unroll\n\t\t\tfor(int t = 0; t < c[i][j].num_elements; t += 1){\n\t\t\t\tc[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n\t\t\t}\n\t\t\tint *tile_ptr = shmem_warp_tile_ptr + i * N * SHMEM_STRIDE + j * M;\n\t\t\twmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, C_LAYOUT);\n\t\t}\n\t}\n\n    __syncthreads();\n\n\tint *shmem_warp_stream_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N * SHMEM_STRIDE\n\t\t\t\t\t\t\t\t\t+ (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tconst size_t gmem_idx =\n\t\t(block_tile_i * N + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N) * GLOBAL_MEM_STRIDE +\n\t\tblock_tile_j * M + (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tuint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n\tint *shmem_lane_stream_ptr =\n\t\tshmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * SHMEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\t\n\tuint8_t *dst_gmem_lane_stream_ptr =\n\t\tdst_gmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * GLOBAL_MEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\n\tfor(int i = 0; i < WARP_ROW_TILES * N; i += SHARED_TO_GLOBAL_LINES_PER_WARP){\n\t\t*(dst_gmem_lane_stream_ptr + i * GLOBAL_MEM_STRIDE) = (uint8_t)(*(shmem_lane_stream_ptr + i * SHMEM_STRIDE));\n\t}\n\n\t__syncthreads();\n}\n", "launch_config": {"dimBlock": [256, 1], "dimGrid": [256, 1]}}, "122": {"code": "\n__global__ void compute_gemm_imma(float *input0, float *input1, float *input2, float* input3, float* input4, float*input5, float* input6,\n                                  float *output0) {\n    // keys need to be replaced 4096, 1024, K_GLOBA_VALUEL, 0.75, 64\n\n\t//extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B = reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_index = reinterpret_cast<int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n\tconst int WARP_SIZE = 32;\n    const int M = 16;\n    const int K = 16;\n    const int N = 16;\n    const int WMMA_M = 16;\n    const int WMMA_K = 16;\n    const int WMMA_N = 16;\n    const int M_GLOBAL = 4096;\n    const int K_GLOBAL = 1024;\n    const int N_GLOBAL = 1024;\n    const float SPARSITY = 0.75;\n    const int K_GLOBAL_SPARSE = (int(K_GLOBAL * (1-SPARSITY)));\n    const int M_TILES = (M_GLOBAL / M);\n    const int N_TILES = (N_GLOBAL / N);\n    const int K_TILES = (K_GLOBAL / K);\n    const int BLOCK_ROW_WARPS = 2;\n    const int BLOCK_COL_WARPS = 4;\n    const int WARP_ROW_TILES = 4;\n    const int WARP_COL_TILES = 2;\n    const int SKEW_UINT8 = 16;\n    const int BANK_VAL = 32;\n    const int NUM_BANK = (K_GLOBAL / BANK_VAL);\n    /////////// BLOCK_ROW_TILES <= N_TILES ////////////\n    // const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    // ////////// BLOCK_COL_TILES <= M_TILES ////////////\n    // const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    auto C_LAYOUT = wmma::mem_col_major;\n    const int CHUNK_K = 64;\n    const int CHUNK_K_SPARSE = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K_SPARSE = (int((CHUNK_K * K) * (1 - SPARSITY)));\n    const int WARP_COPY_BYTES = (WARP_SIZE * sizeof(int4));\n    const int CHUNK_LINE_BYTES_A (BLOCK_COL_TILES * M * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_A = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_A);\n    const int CHUNK_COPY_LINE_LANES_A = (CHUNK_LINE_BYTES_A / sizeof(int4));\n    const int SHARED_OFFSET_A = (BLOCK_COL_TILES * M + SKEW_UINT8);\n\n    const int CHUNK_LINE_BYTES_B = (BLOCK_SIZE_K_SPARSE * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_B = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_B);\n    const int CHUNK_COPY_LINE_LANES_B = (CHUNK_LINE_BYTES_B / sizeof(int4));\n    const int SHARED_OFFSET_B = (BLOCK_SIZE_K_SPARSE + SKEW_UINT8);\n    const int SHARED_TO_GLOBAL_BYTES_PER_LINE = ((WARP_COL_TILES * M) * sizeof(int));\n    const int SHARED_TO_GLOBAL_BYTES_PER_WARP = (WARP_SIZE * sizeof(int));\n    const int SHARED_TO_GLOBAL_LINES_PER_WARP = (SHARED_TO_GLOBAL_BYTES_PER_WARP / SHARED_TO_GLOBAL_BYTES_PER_LINE);\n    const int SHARED_TO_GLOBAL_LANES_PER_LINE = (WARP_SIZE / SHARED_TO_GLOBAL_LINES_PER_WARP);\n    const int SHARED_TO_GLOBAL_ITERS = ((WARP_ROW_TILES * N) / SHARED_TO_GLOBAL_LINES_PER_WARP);\n\n    const int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const int WARP_STRIDE = (WARP_COL_TILES * M);\n    const int WARPS_PER_BLOCK = (BLOCK_ROW_WARPS * BLOCK_COL_WARPS);\n    const int THREADS_PER_BLOCK = (WARP_SIZE * WARPS_PER_BLOCK);\n    const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    const int GLOBAL_MEM_STRIDE = M_GLOBAL;\n    const int SHMEM_STRIDE = (M * BLOCK_COL_TILES);\n    const int SHMEM_OFFSET = (M * WARP_COL_TILES);\n    const int BLOCK_SIZE_M = (M * BLOCK_COL_TILES);\n    const int BLOCK_SIZE_N = (N * BLOCK_ROW_TILES);\n\n   extern __shared__ uint8_t shmem[];\n\n\t// Warp and lane identification.\n\tconst unsigned int warpId = threadIdx.x / WARP_SIZE;\n\tconst unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n\n\t// Offset in shared memory from which the B matrix is stored.\n\t// const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\tconst size_t shmem_idx_b_off = BLOCK_SIZE_K_SPARSE * SHARED_OFFSET_A;\n\n\n\t// Each CTA slides along the 128 x 128 tiles from the top left corner of the\n\t// matrix to the right and down, and selects the next tile to compute. Once\n\t// there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n\tconst unsigned int block_tile_i =\n\t\t((block_pos * BLOCK_COL_TILES) / M_TILES) * (BLOCK_ROW_TILES);\n\tconst unsigned int block_tile_j = (block_pos * BLOCK_COL_TILES) % M_TILES;\n\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n\twmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_ROW_TILES]\n\t\t\t\t\t\t\t\t\t\t\t\t\t [WARP_COL_TILES];\n\n    // Load the C matrix tiles into fragments from shared memory.\n\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t\twmma::fill_fragment(c[i][j], 0);\n\t\t}\n\t}\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    //for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n    for(int tile_k_idx_sparse = 0, tile_k_idx = 0; tile_k_idx_sparse < K_GLOBAL_SPARSE; tile_k_idx_sparse += BLOCK_SIZE_K_SPARSE, tile_k_idx += BLOCK_SIZE_K){\n\n\t\tsize_t shmem_idx = \n\t\twarpId < (WARPS_PER_BLOCK / 2)\n\t\t\t? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A\n\t\t\t: (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B + shmem_idx_b_off;\n\n\t\tint4 *lane_ptr = NULL;\n\t\tint *lane_ptr_index = NULL;\n\t\tconst uint8_t *warp_ptr = NULL;\n\n\n\t\tif(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\t//warp_ptr = &A[block_tile_j * M] +\n\t\t\t//\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * M_GLOBAL;\n\t\t\twarp_ptr = &A[block_tile_j * M];\n\t\t\t\n\t\t\tconst int *warp_ptr_index = &B_index[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t\t\t\t\t\t((warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A);\n\n\t\t\tlane_ptr_index = (int *)(warp_ptr_index + tile_k_idx_sparse + (laneId / CHUNK_COPY_LINE_LANES_A));\n\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_A) * SHARED_OFFSET_A;\n\t\t}else{\n\t\t\twarp_ptr = &B[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * K_GLOBAL_SPARSE;\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_sparse +\n\t\t\t\t\t\t\t\t(laneId / CHUNK_COPY_LINE_LANES_B) * K_GLOBAL_SPARSE) +\n\t\t\t\t\t\t\t\t(laneId % CHUNK_COPY_LINE_LANES_B);\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_B) * SHARED_OFFSET_B;\n\t\t}\n\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      // shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n\t  int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n\t  \t? BLOCK_SIZE_K_SPARSE / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A)\n\t\t: BLOCK_SIZE_N / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\n\t  /*\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\t  */\n\n\t  /*\n      int tile_k_idx_A;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n          tile_k_idx_A = *(lane_ptr_index);\n      }\n\t  */\n\n\t  #pragma unroll\n\t  for(int i = 0; i < iter_index; i += 1){\n\t\t  if(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\tint tile_k_idx_A = *(lane_ptr_index);\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_A * M_GLOBAL) + (laneId % CHUNK_COPY_LINE_LANES_A);\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_A)) =\n\t\t\t\t*lane_ptr;\n\t\t\t//warp_ptr = (uint8_t *)((uint8_t *)warp_ptr + M_GLOBAL * (WARPS_PER_BLOCK / 2) *CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tlane_ptr_index = (int *)((int *)lane_ptr_index +  (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A;\n\t\t  }else{\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_B)) =\n\t\t\t\t*lane_ptr;\n\t\t\tlane_ptr = (int4 *)((uint8_t *)lane_ptr + K_GLOBAL_SPARSE * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B;\n\t\t  }\n\t  }\n\n      __syncthreads();\n\n\t#pragma unroll\n      for (int k_step = 0; k_step < CHUNK_K_SPARSE; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::col_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n\t#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i += 1) {\n\t\t\tsize_t shmem_idx_a = (warpId % BLOCK_COL_WARPS) * M * WARP_COL_TILES + (i * M);\n\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_a + k_step * K * SHARED_OFFSET_A;\n\n\t\t\twmma::load_matrix_sync(a[i], tile_ptr, SHARED_OFFSET_A);\n\t\t#pragma unroll\n\t\t\tfor(int j = 0; j < WARP_ROW_TILES; j += 1){\n\t\t\t\tif(i == 0){\n\t\t\t\t\tsize_t shmem_idx_b = shmem_idx_b_off +\n\t\t\t\t\t\t\t\t\t\t\t(warpId / BLOCK_COL_WARPS) * (WARP_ROW_TILES * N) * SHARED_OFFSET_B +\n\t\t\t\t\t\t\t\t\t\t\t(j * N) * SHARED_OFFSET_B;\n\t\t\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_b + k_step * K;\n\t\t\t\t\twmma::load_matrix_sync(b[j], tile_ptr, SHARED_OFFSET_B);\n\t\t\t\t}\n\t\t\t\twmma::mma_sync(c[j][i], a[i], b[j], c[j][i]);\n\t\t\t}\n\n        }\n      }\n\n      __syncthreads();\n    }\n\n    // This pointer is used to access the C and D matrix tiles this warp computes.\n\tint *shmem_warp_tile_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * N * WARP_ROW_TILES * SHMEM_STRIDE +\n\t(warpId % BLOCK_COL_WARPS) * SHMEM_OFFSET;\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t#pragma unroll\n\t\t\tfor(int t = 0; t < c[i][j].num_elements; t += 1){\n\t\t\t\tc[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n\t\t\t}\n\t\t\tint *tile_ptr = shmem_warp_tile_ptr + i * N * SHMEM_STRIDE + j * M;\n\t\t\twmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, C_LAYOUT);\n\t\t}\n\t}\n\n    __syncthreads();\n\n\tint *shmem_warp_stream_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N * SHMEM_STRIDE\n\t\t\t\t\t\t\t\t\t+ (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tconst size_t gmem_idx =\n\t\t(block_tile_i * N + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N) * GLOBAL_MEM_STRIDE +\n\t\tblock_tile_j * M + (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tuint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n\tint *shmem_lane_stream_ptr =\n\t\tshmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * SHMEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\t\n\tuint8_t *dst_gmem_lane_stream_ptr =\n\t\tdst_gmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * GLOBAL_MEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\n\tfor(int i = 0; i < WARP_ROW_TILES * N; i += SHARED_TO_GLOBAL_LINES_PER_WARP){\n\t\t*(dst_gmem_lane_stream_ptr + i * GLOBAL_MEM_STRIDE) = (uint8_t)(*(shmem_lane_stream_ptr + i * SHMEM_STRIDE));\n\t}\n\n\t__syncthreads();\n}\n", "launch_config": {"dimBlock": [256, 1], "dimGrid": [256, 1]}}, "123": {"code": "\n__global__ void compute_gemm_imma(float *input0, float *input1, float *input2, float* input3, float* input4, float*input5, float* input6,\n                                  float *output0) {\n    // keys need to be replaced 4096, 1024, K_GLOBA_VALUEL, 0.75, 64\n\n\t//extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B = reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_index = reinterpret_cast<int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n\tconst int WARP_SIZE = 32;\n    const int M = 16;\n    const int K = 16;\n    const int N = 16;\n    const int WMMA_M = 16;\n    const int WMMA_K = 16;\n    const int WMMA_N = 16;\n    const int M_GLOBAL = 4096;\n    const int K_GLOBAL = 1024;\n    const int N_GLOBAL = 1024;\n    const float SPARSITY = 0.75;\n    const int K_GLOBAL_SPARSE = (int(K_GLOBAL * (1-SPARSITY)));\n    const int M_TILES = (M_GLOBAL / M);\n    const int N_TILES = (N_GLOBAL / N);\n    const int K_TILES = (K_GLOBAL / K);\n    const int BLOCK_ROW_WARPS = 2;\n    const int BLOCK_COL_WARPS = 4;\n    const int WARP_ROW_TILES = 4;\n    const int WARP_COL_TILES = 2;\n    const int SKEW_UINT8 = 16;\n    const int BANK_VAL = 32;\n    const int NUM_BANK = (K_GLOBAL / BANK_VAL);\n    /////////// BLOCK_ROW_TILES <= N_TILES ////////////\n    // const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    // ////////// BLOCK_COL_TILES <= M_TILES ////////////\n    // const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    auto C_LAYOUT = wmma::mem_col_major;\n    const int CHUNK_K = 64;\n    const int CHUNK_K_SPARSE = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K_SPARSE = (int((CHUNK_K * K) * (1 - SPARSITY)));\n    const int WARP_COPY_BYTES = (WARP_SIZE * sizeof(int4));\n    const int CHUNK_LINE_BYTES_A (BLOCK_COL_TILES * M * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_A = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_A);\n    const int CHUNK_COPY_LINE_LANES_A = (CHUNK_LINE_BYTES_A / sizeof(int4));\n    const int SHARED_OFFSET_A = (BLOCK_COL_TILES * M + SKEW_UINT8);\n\n    const int CHUNK_LINE_BYTES_B = (BLOCK_SIZE_K_SPARSE * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_B = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_B);\n    const int CHUNK_COPY_LINE_LANES_B = (CHUNK_LINE_BYTES_B / sizeof(int4));\n    const int SHARED_OFFSET_B = (BLOCK_SIZE_K_SPARSE + SKEW_UINT8);\n    const int SHARED_TO_GLOBAL_BYTES_PER_LINE = ((WARP_COL_TILES * M) * sizeof(int));\n    const int SHARED_TO_GLOBAL_BYTES_PER_WARP = (WARP_SIZE * sizeof(int));\n    const int SHARED_TO_GLOBAL_LINES_PER_WARP = (SHARED_TO_GLOBAL_BYTES_PER_WARP / SHARED_TO_GLOBAL_BYTES_PER_LINE);\n    const int SHARED_TO_GLOBAL_LANES_PER_LINE = (WARP_SIZE / SHARED_TO_GLOBAL_LINES_PER_WARP);\n    const int SHARED_TO_GLOBAL_ITERS = ((WARP_ROW_TILES * N) / SHARED_TO_GLOBAL_LINES_PER_WARP);\n\n    const int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const int WARP_STRIDE = (WARP_COL_TILES * M);\n    const int WARPS_PER_BLOCK = (BLOCK_ROW_WARPS * BLOCK_COL_WARPS);\n    const int THREADS_PER_BLOCK = (WARP_SIZE * WARPS_PER_BLOCK);\n    const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    const int GLOBAL_MEM_STRIDE = M_GLOBAL;\n    const int SHMEM_STRIDE = (M * BLOCK_COL_TILES);\n    const int SHMEM_OFFSET = (M * WARP_COL_TILES);\n    const int BLOCK_SIZE_M = (M * BLOCK_COL_TILES);\n    const int BLOCK_SIZE_N = (N * BLOCK_ROW_TILES);\n\n   extern __shared__ uint8_t shmem[];\n\n\t// Warp and lane identification.\n\tconst unsigned int warpId = threadIdx.x / WARP_SIZE;\n\tconst unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n\n\t// Offset in shared memory from which the B matrix is stored.\n\t// const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\tconst size_t shmem_idx_b_off = BLOCK_SIZE_K_SPARSE * SHARED_OFFSET_A;\n\n\n\t// Each CTA slides along the 128 x 128 tiles from the top left corner of the\n\t// matrix to the right and down, and selects the next tile to compute. Once\n\t// there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n\tconst unsigned int block_tile_i =\n\t\t((block_pos * BLOCK_COL_TILES) / M_TILES) * (BLOCK_ROW_TILES);\n\tconst unsigned int block_tile_j = (block_pos * BLOCK_COL_TILES) % M_TILES;\n\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n\twmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_ROW_TILES]\n\t\t\t\t\t\t\t\t\t\t\t\t\t [WARP_COL_TILES];\n\n    // Load the C matrix tiles into fragments from shared memory.\n\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t\twmma::fill_fragment(c[i][j], 0);\n\t\t}\n\t}\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    //for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n    for(int tile_k_idx_sparse = 0, tile_k_idx = 0; tile_k_idx_sparse < K_GLOBAL_SPARSE; tile_k_idx_sparse += BLOCK_SIZE_K_SPARSE, tile_k_idx += BLOCK_SIZE_K){\n\n\t\tsize_t shmem_idx = \n\t\twarpId < (WARPS_PER_BLOCK / 2)\n\t\t\t? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A\n\t\t\t: (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B + shmem_idx_b_off;\n\n\t\tint4 *lane_ptr = NULL;\n\t\tint *lane_ptr_index = NULL;\n\t\tconst uint8_t *warp_ptr = NULL;\n\n\n\t\tif(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\t//warp_ptr = &A[block_tile_j * M] +\n\t\t\t//\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * M_GLOBAL;\n\t\t\twarp_ptr = &A[block_tile_j * M];\n\t\t\t\n\t\t\tconst int *warp_ptr_index = &B_index[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t\t\t\t\t\t((warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A);\n\n\t\t\tlane_ptr_index = (int *)(warp_ptr_index + tile_k_idx_sparse + (laneId / CHUNK_COPY_LINE_LANES_A));\n\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_A) * SHARED_OFFSET_A;\n\t\t}else{\n\t\t\twarp_ptr = &B[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * K_GLOBAL_SPARSE;\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_sparse +\n\t\t\t\t\t\t\t\t(laneId / CHUNK_COPY_LINE_LANES_B) * K_GLOBAL_SPARSE) +\n\t\t\t\t\t\t\t\t(laneId % CHUNK_COPY_LINE_LANES_B);\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_B) * SHARED_OFFSET_B;\n\t\t}\n\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      // shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n\t  int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n\t  \t? BLOCK_SIZE_K_SPARSE / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A)\n\t\t: BLOCK_SIZE_N / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\n\t  /*\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\t  */\n\n\t  /*\n      int tile_k_idx_A;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n          tile_k_idx_A = *(lane_ptr_index);\n      }\n\t  */\n\n\t  #pragma unroll\n\t  for(int i = 0; i < iter_index; i += 1){\n\t\t  if(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\tint tile_k_idx_A = *(lane_ptr_index);\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_A * M_GLOBAL) + (laneId % CHUNK_COPY_LINE_LANES_A);\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_A)) =\n\t\t\t\t*lane_ptr;\n\t\t\t//warp_ptr = (uint8_t *)((uint8_t *)warp_ptr + M_GLOBAL * (WARPS_PER_BLOCK / 2) *CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tlane_ptr_index = (int *)((int *)lane_ptr_index +  (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A;\n\t\t  }else{\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_B)) =\n\t\t\t\t*lane_ptr;\n\t\t\tlane_ptr = (int4 *)((uint8_t *)lane_ptr + K_GLOBAL_SPARSE * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B;\n\t\t  }\n\t  }\n\n      __syncthreads();\n\n\t#pragma unroll\n      for (int k_step = 0; k_step < CHUNK_K_SPARSE; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::col_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n\t#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i += 1) {\n\t\t\tsize_t shmem_idx_a = (warpId % BLOCK_COL_WARPS) * M * WARP_COL_TILES + (i * M);\n\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_a + k_step * K * SHARED_OFFSET_A;\n\n\t\t\twmma::load_matrix_sync(a[i], tile_ptr, SHARED_OFFSET_A);\n\t\t#pragma unroll\n\t\t\tfor(int j = 0; j < WARP_ROW_TILES; j += 1){\n\t\t\t\tif(i == 0){\n\t\t\t\t\tsize_t shmem_idx_b = shmem_idx_b_off +\n\t\t\t\t\t\t\t\t\t\t\t(warpId / BLOCK_COL_WARPS) * (WARP_ROW_TILES * N) * SHARED_OFFSET_B +\n\t\t\t\t\t\t\t\t\t\t\t(j * N) * SHARED_OFFSET_B;\n\t\t\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_b + k_step * K;\n\t\t\t\t\twmma::load_matrix_sync(b[j], tile_ptr, SHARED_OFFSET_B);\n\t\t\t\t}\n\t\t\t\twmma::mma_sync(c[j][i], a[i], b[j], c[j][i]);\n\t\t\t}\n\n        }\n      }\n\n      __syncthreads();\n    }\n\n    // This pointer is used to access the C and D matrix tiles this warp computes.\n\tint *shmem_warp_tile_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * N * WARP_ROW_TILES * SHMEM_STRIDE +\n\t(warpId % BLOCK_COL_WARPS) * SHMEM_OFFSET;\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t#pragma unroll\n\t\t\tfor(int t = 0; t < c[i][j].num_elements; t += 1){\n\t\t\t\tc[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n\t\t\t}\n\t\t\tint *tile_ptr = shmem_warp_tile_ptr + i * N * SHMEM_STRIDE + j * M;\n\t\t\twmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, C_LAYOUT);\n\t\t}\n\t}\n\n    __syncthreads();\n\n\tint *shmem_warp_stream_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N * SHMEM_STRIDE\n\t\t\t\t\t\t\t\t\t+ (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tconst size_t gmem_idx =\n\t\t(block_tile_i * N + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N) * GLOBAL_MEM_STRIDE +\n\t\tblock_tile_j * M + (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tuint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n\tint *shmem_lane_stream_ptr =\n\t\tshmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * SHMEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\t\n\tuint8_t *dst_gmem_lane_stream_ptr =\n\t\tdst_gmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * GLOBAL_MEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\n\tfor(int i = 0; i < WARP_ROW_TILES * N; i += SHARED_TO_GLOBAL_LINES_PER_WARP){\n\t\t*(dst_gmem_lane_stream_ptr + i * GLOBAL_MEM_STRIDE) = (uint8_t)(*(shmem_lane_stream_ptr + i * SHMEM_STRIDE));\n\t}\n\n\t__syncthreads();\n}\n", "launch_config": {"dimBlock": [256, 1], "dimGrid": [256, 1]}}, "124": {"code": "\n__global__ void compute_gemm_imma(float *input0, float *input1, float *input2, float* input3, float* input4, float*input5, float* input6,\n                                  float *output0) {\n    // keys need to be replaced 4096, 1024, K_GLOBA_VALUEL, 0.75, 64\n\n\t//extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B = reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_index = reinterpret_cast<int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n\tconst int WARP_SIZE = 32;\n    const int M = 16;\n    const int K = 16;\n    const int N = 16;\n    const int WMMA_M = 16;\n    const int WMMA_K = 16;\n    const int WMMA_N = 16;\n    const int M_GLOBAL = 4096;\n    const int K_GLOBAL = 1024;\n    const int N_GLOBAL = 1024;\n    const float SPARSITY = 0.75;\n    const int K_GLOBAL_SPARSE = (int(K_GLOBAL * (1-SPARSITY)));\n    const int M_TILES = (M_GLOBAL / M);\n    const int N_TILES = (N_GLOBAL / N);\n    const int K_TILES = (K_GLOBAL / K);\n    const int BLOCK_ROW_WARPS = 2;\n    const int BLOCK_COL_WARPS = 4;\n    const int WARP_ROW_TILES = 4;\n    const int WARP_COL_TILES = 2;\n    const int SKEW_UINT8 = 16;\n    const int BANK_VAL = 32;\n    const int NUM_BANK = (K_GLOBAL / BANK_VAL);\n    /////////// BLOCK_ROW_TILES <= N_TILES ////////////\n    // const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    // ////////// BLOCK_COL_TILES <= M_TILES ////////////\n    // const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    auto C_LAYOUT = wmma::mem_col_major;\n    const int CHUNK_K = 64;\n    const int CHUNK_K_SPARSE = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K_SPARSE = (int((CHUNK_K * K) * (1 - SPARSITY)));\n    const int WARP_COPY_BYTES = (WARP_SIZE * sizeof(int4));\n    const int CHUNK_LINE_BYTES_A (BLOCK_COL_TILES * M * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_A = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_A);\n    const int CHUNK_COPY_LINE_LANES_A = (CHUNK_LINE_BYTES_A / sizeof(int4));\n    const int SHARED_OFFSET_A = (BLOCK_COL_TILES * M + SKEW_UINT8);\n\n    const int CHUNK_LINE_BYTES_B = (BLOCK_SIZE_K_SPARSE * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_B = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_B);\n    const int CHUNK_COPY_LINE_LANES_B = (CHUNK_LINE_BYTES_B / sizeof(int4));\n    const int SHARED_OFFSET_B = (BLOCK_SIZE_K_SPARSE + SKEW_UINT8);\n    const int SHARED_TO_GLOBAL_BYTES_PER_LINE = ((WARP_COL_TILES * M) * sizeof(int));\n    const int SHARED_TO_GLOBAL_BYTES_PER_WARP = (WARP_SIZE * sizeof(int));\n    const int SHARED_TO_GLOBAL_LINES_PER_WARP = (SHARED_TO_GLOBAL_BYTES_PER_WARP / SHARED_TO_GLOBAL_BYTES_PER_LINE);\n    const int SHARED_TO_GLOBAL_LANES_PER_LINE = (WARP_SIZE / SHARED_TO_GLOBAL_LINES_PER_WARP);\n    const int SHARED_TO_GLOBAL_ITERS = ((WARP_ROW_TILES * N) / SHARED_TO_GLOBAL_LINES_PER_WARP);\n\n    const int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const int WARP_STRIDE = (WARP_COL_TILES * M);\n    const int WARPS_PER_BLOCK = (BLOCK_ROW_WARPS * BLOCK_COL_WARPS);\n    const int THREADS_PER_BLOCK = (WARP_SIZE * WARPS_PER_BLOCK);\n    const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    const int GLOBAL_MEM_STRIDE = M_GLOBAL;\n    const int SHMEM_STRIDE = (M * BLOCK_COL_TILES);\n    const int SHMEM_OFFSET = (M * WARP_COL_TILES);\n    const int BLOCK_SIZE_M = (M * BLOCK_COL_TILES);\n    const int BLOCK_SIZE_N = (N * BLOCK_ROW_TILES);\n\n   extern __shared__ uint8_t shmem[];\n\n\t// Warp and lane identification.\n\tconst unsigned int warpId = threadIdx.x / WARP_SIZE;\n\tconst unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n\n\t// Offset in shared memory from which the B matrix is stored.\n\t// const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\tconst size_t shmem_idx_b_off = BLOCK_SIZE_K_SPARSE * SHARED_OFFSET_A;\n\n\n\t// Each CTA slides along the 128 x 128 tiles from the top left corner of the\n\t// matrix to the right and down, and selects the next tile to compute. Once\n\t// there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n\tconst unsigned int block_tile_i =\n\t\t((block_pos * BLOCK_COL_TILES) / M_TILES) * (BLOCK_ROW_TILES);\n\tconst unsigned int block_tile_j = (block_pos * BLOCK_COL_TILES) % M_TILES;\n\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n\twmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_ROW_TILES]\n\t\t\t\t\t\t\t\t\t\t\t\t\t [WARP_COL_TILES];\n\n    // Load the C matrix tiles into fragments from shared memory.\n\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t\twmma::fill_fragment(c[i][j], 0);\n\t\t}\n\t}\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    //for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n    for(int tile_k_idx_sparse = 0, tile_k_idx = 0; tile_k_idx_sparse < K_GLOBAL_SPARSE; tile_k_idx_sparse += BLOCK_SIZE_K_SPARSE, tile_k_idx += BLOCK_SIZE_K){\n\n\t\tsize_t shmem_idx = \n\t\twarpId < (WARPS_PER_BLOCK / 2)\n\t\t\t? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A\n\t\t\t: (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B + shmem_idx_b_off;\n\n\t\tint4 *lane_ptr = NULL;\n\t\tint *lane_ptr_index = NULL;\n\t\tconst uint8_t *warp_ptr = NULL;\n\n\n\t\tif(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\t//warp_ptr = &A[block_tile_j * M] +\n\t\t\t//\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * M_GLOBAL;\n\t\t\twarp_ptr = &A[block_tile_j * M];\n\t\t\t\n\t\t\tconst int *warp_ptr_index = &B_index[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t\t\t\t\t\t((warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A);\n\n\t\t\tlane_ptr_index = (int *)(warp_ptr_index + tile_k_idx_sparse + (laneId / CHUNK_COPY_LINE_LANES_A));\n\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_A) * SHARED_OFFSET_A;\n\t\t}else{\n\t\t\twarp_ptr = &B[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * K_GLOBAL_SPARSE;\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_sparse +\n\t\t\t\t\t\t\t\t(laneId / CHUNK_COPY_LINE_LANES_B) * K_GLOBAL_SPARSE) +\n\t\t\t\t\t\t\t\t(laneId % CHUNK_COPY_LINE_LANES_B);\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_B) * SHARED_OFFSET_B;\n\t\t}\n\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      // shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n\t  int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n\t  \t? BLOCK_SIZE_K_SPARSE / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A)\n\t\t: BLOCK_SIZE_N / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\n\t  /*\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\t  */\n\n\t  /*\n      int tile_k_idx_A;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n          tile_k_idx_A = *(lane_ptr_index);\n      }\n\t  */\n\n\t  #pragma unroll\n\t  for(int i = 0; i < iter_index; i += 1){\n\t\t  if(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\tint tile_k_idx_A = *(lane_ptr_index);\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_A * M_GLOBAL) + (laneId % CHUNK_COPY_LINE_LANES_A);\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_A)) =\n\t\t\t\t*lane_ptr;\n\t\t\t//warp_ptr = (uint8_t *)((uint8_t *)warp_ptr + M_GLOBAL * (WARPS_PER_BLOCK / 2) *CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tlane_ptr_index = (int *)((int *)lane_ptr_index +  (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A;\n\t\t  }else{\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_B)) =\n\t\t\t\t*lane_ptr;\n\t\t\tlane_ptr = (int4 *)((uint8_t *)lane_ptr + K_GLOBAL_SPARSE * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B;\n\t\t  }\n\t  }\n\n      __syncthreads();\n\n\t#pragma unroll\n      for (int k_step = 0; k_step < CHUNK_K_SPARSE; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::col_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n\t#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i += 1) {\n\t\t\tsize_t shmem_idx_a = (warpId % BLOCK_COL_WARPS) * M * WARP_COL_TILES + (i * M);\n\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_a + k_step * K * SHARED_OFFSET_A;\n\n\t\t\twmma::load_matrix_sync(a[i], tile_ptr, SHARED_OFFSET_A);\n\t\t#pragma unroll\n\t\t\tfor(int j = 0; j < WARP_ROW_TILES; j += 1){\n\t\t\t\tif(i == 0){\n\t\t\t\t\tsize_t shmem_idx_b = shmem_idx_b_off +\n\t\t\t\t\t\t\t\t\t\t\t(warpId / BLOCK_COL_WARPS) * (WARP_ROW_TILES * N) * SHARED_OFFSET_B +\n\t\t\t\t\t\t\t\t\t\t\t(j * N) * SHARED_OFFSET_B;\n\t\t\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_b + k_step * K;\n\t\t\t\t\twmma::load_matrix_sync(b[j], tile_ptr, SHARED_OFFSET_B);\n\t\t\t\t}\n\t\t\t\twmma::mma_sync(c[j][i], a[i], b[j], c[j][i]);\n\t\t\t}\n\n        }\n      }\n\n      __syncthreads();\n    }\n\n    // This pointer is used to access the C and D matrix tiles this warp computes.\n\tint *shmem_warp_tile_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * N * WARP_ROW_TILES * SHMEM_STRIDE +\n\t(warpId % BLOCK_COL_WARPS) * SHMEM_OFFSET;\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t#pragma unroll\n\t\t\tfor(int t = 0; t < c[i][j].num_elements; t += 1){\n\t\t\t\tc[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n\t\t\t}\n\t\t\tint *tile_ptr = shmem_warp_tile_ptr + i * N * SHMEM_STRIDE + j * M;\n\t\t\twmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, C_LAYOUT);\n\t\t}\n\t}\n\n    __syncthreads();\n\n\tint *shmem_warp_stream_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N * SHMEM_STRIDE\n\t\t\t\t\t\t\t\t\t+ (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tconst size_t gmem_idx =\n\t\t(block_tile_i * N + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N) * GLOBAL_MEM_STRIDE +\n\t\tblock_tile_j * M + (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tuint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n\tint *shmem_lane_stream_ptr =\n\t\tshmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * SHMEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\t\n\tuint8_t *dst_gmem_lane_stream_ptr =\n\t\tdst_gmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * GLOBAL_MEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\n\tfor(int i = 0; i < WARP_ROW_TILES * N; i += SHARED_TO_GLOBAL_LINES_PER_WARP){\n\t\t*(dst_gmem_lane_stream_ptr + i * GLOBAL_MEM_STRIDE) = (uint8_t)(*(shmem_lane_stream_ptr + i * SHMEM_STRIDE));\n\t}\n\n\t__syncthreads();\n}\n", "launch_config": {"dimBlock": [256, 1], "dimGrid": [256, 1]}}, "125": {"code": "\n__global__ void compute_gemm_imma(float *input0, float *input1, float *input2, float* input3, float* input4, float*input5, float* input6,\n                                  float *output0) {\n    // keys need to be replaced 4096, 4096, K_GLOBA_VALUEL, 0.75, 64\n\n\t//extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B = reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_index = reinterpret_cast<int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n\tconst int WARP_SIZE = 32;\n    const int M = 16;\n    const int K = 16;\n    const int N = 16;\n    const int WMMA_M = 16;\n    const int WMMA_K = 16;\n    const int WMMA_N = 16;\n    const int M_GLOBAL = 4096;\n    const int K_GLOBAL = 1024;\n    const int N_GLOBAL = 4096;\n    const float SPARSITY = 0.75;\n    const int K_GLOBAL_SPARSE = (int(K_GLOBAL * (1-SPARSITY)));\n    const int M_TILES = (M_GLOBAL / M);\n    const int N_TILES = (N_GLOBAL / N);\n    const int K_TILES = (K_GLOBAL / K);\n    const int BLOCK_ROW_WARPS = 2;\n    const int BLOCK_COL_WARPS = 4;\n    const int WARP_ROW_TILES = 4;\n    const int WARP_COL_TILES = 2;\n    const int SKEW_UINT8 = 16;\n    const int BANK_VAL = 32;\n    const int NUM_BANK = (K_GLOBAL / BANK_VAL);\n    /////////// BLOCK_ROW_TILES <= N_TILES ////////////\n    // const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    // ////////// BLOCK_COL_TILES <= M_TILES ////////////\n    // const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    auto C_LAYOUT = wmma::mem_col_major;\n    const int CHUNK_K = 64;\n    const int CHUNK_K_SPARSE = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K_SPARSE = (int((CHUNK_K * K) * (1 - SPARSITY)));\n    const int WARP_COPY_BYTES = (WARP_SIZE * sizeof(int4));\n    const int CHUNK_LINE_BYTES_A (BLOCK_COL_TILES * M * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_A = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_A);\n    const int CHUNK_COPY_LINE_LANES_A = (CHUNK_LINE_BYTES_A / sizeof(int4));\n    const int SHARED_OFFSET_A = (BLOCK_COL_TILES * M + SKEW_UINT8);\n\n    const int CHUNK_LINE_BYTES_B = (BLOCK_SIZE_K_SPARSE * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_B = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_B);\n    const int CHUNK_COPY_LINE_LANES_B = (CHUNK_LINE_BYTES_B / sizeof(int4));\n    const int SHARED_OFFSET_B = (BLOCK_SIZE_K_SPARSE + SKEW_UINT8);\n    const int SHARED_TO_GLOBAL_BYTES_PER_LINE = ((WARP_COL_TILES * M) * sizeof(int));\n    const int SHARED_TO_GLOBAL_BYTES_PER_WARP = (WARP_SIZE * sizeof(int));\n    const int SHARED_TO_GLOBAL_LINES_PER_WARP = (SHARED_TO_GLOBAL_BYTES_PER_WARP / SHARED_TO_GLOBAL_BYTES_PER_LINE);\n    const int SHARED_TO_GLOBAL_LANES_PER_LINE = (WARP_SIZE / SHARED_TO_GLOBAL_LINES_PER_WARP);\n    const int SHARED_TO_GLOBAL_ITERS = ((WARP_ROW_TILES * N) / SHARED_TO_GLOBAL_LINES_PER_WARP);\n\n    const int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const int WARP_STRIDE = (WARP_COL_TILES * M);\n    const int WARPS_PER_BLOCK = (BLOCK_ROW_WARPS * BLOCK_COL_WARPS);\n    const int THREADS_PER_BLOCK = (WARP_SIZE * WARPS_PER_BLOCK);\n    const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    const int GLOBAL_MEM_STRIDE = M_GLOBAL;\n    const int SHMEM_STRIDE = (M * BLOCK_COL_TILES);\n    const int SHMEM_OFFSET = (M * WARP_COL_TILES);\n    const int BLOCK_SIZE_M = (M * BLOCK_COL_TILES);\n    const int BLOCK_SIZE_N = (N * BLOCK_ROW_TILES);\n\n   extern __shared__ uint8_t shmem[];\n\n\t// Warp and lane identification.\n\tconst unsigned int warpId = threadIdx.x / WARP_SIZE;\n\tconst unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n\n\t// Offset in shared memory from which the B matrix is stored.\n\t// const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\tconst size_t shmem_idx_b_off = BLOCK_SIZE_K_SPARSE * SHARED_OFFSET_A;\n\n\n\t// Each CTA slides along the 128 x 128 tiles from the top left corner of the\n\t// matrix to the right and down, and selects the next tile to compute. Once\n\t// there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n\tconst unsigned int block_tile_i =\n\t\t((block_pos * BLOCK_COL_TILES) / M_TILES) * (BLOCK_ROW_TILES);\n\tconst unsigned int block_tile_j = (block_pos * BLOCK_COL_TILES) % M_TILES;\n\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n\twmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_ROW_TILES]\n\t\t\t\t\t\t\t\t\t\t\t\t\t [WARP_COL_TILES];\n\n    // Load the C matrix tiles into fragments from shared memory.\n\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t\twmma::fill_fragment(c[i][j], 0);\n\t\t}\n\t}\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    //for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n    for(int tile_k_idx_sparse = 0, tile_k_idx = 0; tile_k_idx_sparse < K_GLOBAL_SPARSE; tile_k_idx_sparse += BLOCK_SIZE_K_SPARSE, tile_k_idx += BLOCK_SIZE_K){\n\n\t\tsize_t shmem_idx = \n\t\twarpId < (WARPS_PER_BLOCK / 2)\n\t\t\t? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A\n\t\t\t: (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B + shmem_idx_b_off;\n\n\t\tint4 *lane_ptr = NULL;\n\t\tint *lane_ptr_index = NULL;\n\t\tconst uint8_t *warp_ptr = NULL;\n\n\n\t\tif(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\t//warp_ptr = &A[block_tile_j * M] +\n\t\t\t//\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * M_GLOBAL;\n\t\t\twarp_ptr = &A[block_tile_j * M];\n\t\t\t\n\t\t\tconst int *warp_ptr_index = &B_index[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t\t\t\t\t\t((warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A);\n\n\t\t\tlane_ptr_index = (int *)(warp_ptr_index + tile_k_idx_sparse + (laneId / CHUNK_COPY_LINE_LANES_A));\n\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_A) * SHARED_OFFSET_A;\n\t\t}else{\n\t\t\twarp_ptr = &B[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * K_GLOBAL_SPARSE;\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_sparse +\n\t\t\t\t\t\t\t\t(laneId / CHUNK_COPY_LINE_LANES_B) * K_GLOBAL_SPARSE) +\n\t\t\t\t\t\t\t\t(laneId % CHUNK_COPY_LINE_LANES_B);\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_B) * SHARED_OFFSET_B;\n\t\t}\n\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      // shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n\t  int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n\t  \t? BLOCK_SIZE_K_SPARSE / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A)\n\t\t: BLOCK_SIZE_N / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\n\t  /*\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\t  */\n\n\t  /*\n      int tile_k_idx_A;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n          tile_k_idx_A = *(lane_ptr_index);\n      }\n\t  */\n\n\t  #pragma unroll\n\t  for(int i = 0; i < iter_index; i += 1){\n\t\t  if(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\tint tile_k_idx_A = *(lane_ptr_index);\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_A * M_GLOBAL) + (laneId % CHUNK_COPY_LINE_LANES_A);\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_A)) =\n\t\t\t\t*lane_ptr;\n\t\t\t//warp_ptr = (uint8_t *)((uint8_t *)warp_ptr + M_GLOBAL * (WARPS_PER_BLOCK / 2) *CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tlane_ptr_index = (int *)((int *)lane_ptr_index +  (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A;\n\t\t  }else{\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_B)) =\n\t\t\t\t*lane_ptr;\n\t\t\tlane_ptr = (int4 *)((uint8_t *)lane_ptr + K_GLOBAL_SPARSE * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B;\n\t\t  }\n\t  }\n\n      __syncthreads();\n\n\t#pragma unroll\n      for (int k_step = 0; k_step < CHUNK_K_SPARSE; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::col_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n\t#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i += 1) {\n\t\t\tsize_t shmem_idx_a = (warpId % BLOCK_COL_WARPS) * M * WARP_COL_TILES + (i * M);\n\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_a + k_step * K * SHARED_OFFSET_A;\n\n\t\t\twmma::load_matrix_sync(a[i], tile_ptr, SHARED_OFFSET_A);\n\t\t#pragma unroll\n\t\t\tfor(int j = 0; j < WARP_ROW_TILES; j += 1){\n\t\t\t\tif(i == 0){\n\t\t\t\t\tsize_t shmem_idx_b = shmem_idx_b_off +\n\t\t\t\t\t\t\t\t\t\t\t(warpId / BLOCK_COL_WARPS) * (WARP_ROW_TILES * N) * SHARED_OFFSET_B +\n\t\t\t\t\t\t\t\t\t\t\t(j * N) * SHARED_OFFSET_B;\n\t\t\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_b + k_step * K;\n\t\t\t\t\twmma::load_matrix_sync(b[j], tile_ptr, SHARED_OFFSET_B);\n\t\t\t\t}\n\t\t\t\twmma::mma_sync(c[j][i], a[i], b[j], c[j][i]);\n\t\t\t}\n\n        }\n      }\n\n      __syncthreads();\n    }\n\n    // This pointer is used to access the C and D matrix tiles this warp computes.\n\tint *shmem_warp_tile_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * N * WARP_ROW_TILES * SHMEM_STRIDE +\n\t(warpId % BLOCK_COL_WARPS) * SHMEM_OFFSET;\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t#pragma unroll\n\t\t\tfor(int t = 0; t < c[i][j].num_elements; t += 1){\n\t\t\t\tc[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n\t\t\t}\n\t\t\tint *tile_ptr = shmem_warp_tile_ptr + i * N * SHMEM_STRIDE + j * M;\n\t\t\twmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, C_LAYOUT);\n\t\t}\n\t}\n\n    __syncthreads();\n\n\tint *shmem_warp_stream_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N * SHMEM_STRIDE\n\t\t\t\t\t\t\t\t\t+ (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tconst size_t gmem_idx =\n\t\t(block_tile_i * N + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N) * GLOBAL_MEM_STRIDE +\n\t\tblock_tile_j * M + (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tuint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n\tint *shmem_lane_stream_ptr =\n\t\tshmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * SHMEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\t\n\tuint8_t *dst_gmem_lane_stream_ptr =\n\t\tdst_gmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * GLOBAL_MEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\n\tfor(int i = 0; i < WARP_ROW_TILES * N; i += SHARED_TO_GLOBAL_LINES_PER_WARP){\n\t\t*(dst_gmem_lane_stream_ptr + i * GLOBAL_MEM_STRIDE) = (uint8_t)(*(shmem_lane_stream_ptr + i * SHMEM_STRIDE));\n\t}\n\n\t__syncthreads();\n}\n", "launch_config": {"dimBlock": [1024, 1], "dimGrid": [256, 1]}}, "126": {"code": "\n__global__ void compute_gemm_imma(float *input0, float *input1, float *input2, float* input3, float* input4, float*input5, float* input6,\n                                  float *output0) {\n    // keys need to be replaced 4096, 1024, K_GLOBA_VALUEL, 0.75, 64\n\n\t//extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B = reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_index = reinterpret_cast<int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n\tconst int WARP_SIZE = 32;\n    const int M = 16;\n    const int K = 16;\n    const int N = 16;\n    const int WMMA_M = 16;\n    const int WMMA_K = 16;\n    const int WMMA_N = 16;\n    const int M_GLOBAL = 4096;\n    const int K_GLOBAL = 4096;\n    const int N_GLOBAL = 1024;\n    const float SPARSITY = 0.75;\n    const int K_GLOBAL_SPARSE = (int(K_GLOBAL * (1-SPARSITY)));\n    const int M_TILES = (M_GLOBAL / M);\n    const int N_TILES = (N_GLOBAL / N);\n    const int K_TILES = (K_GLOBAL / K);\n    const int BLOCK_ROW_WARPS = 2;\n    const int BLOCK_COL_WARPS = 4;\n    const int WARP_ROW_TILES = 4;\n    const int WARP_COL_TILES = 2;\n    const int SKEW_UINT8 = 16;\n    const int BANK_VAL = 32;\n    const int NUM_BANK = (K_GLOBAL / BANK_VAL);\n    /////////// BLOCK_ROW_TILES <= N_TILES ////////////\n    // const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    // ////////// BLOCK_COL_TILES <= M_TILES ////////////\n    // const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    auto C_LAYOUT = wmma::mem_col_major;\n    const int CHUNK_K = 64;\n    const int CHUNK_K_SPARSE = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K_SPARSE = (int((CHUNK_K * K) * (1 - SPARSITY)));\n    const int WARP_COPY_BYTES = (WARP_SIZE * sizeof(int4));\n    const int CHUNK_LINE_BYTES_A (BLOCK_COL_TILES * M * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_A = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_A);\n    const int CHUNK_COPY_LINE_LANES_A = (CHUNK_LINE_BYTES_A / sizeof(int4));\n    const int SHARED_OFFSET_A = (BLOCK_COL_TILES * M + SKEW_UINT8);\n\n    const int CHUNK_LINE_BYTES_B = (BLOCK_SIZE_K_SPARSE * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_B = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_B);\n    const int CHUNK_COPY_LINE_LANES_B = (CHUNK_LINE_BYTES_B / sizeof(int4));\n    const int SHARED_OFFSET_B = (BLOCK_SIZE_K_SPARSE + SKEW_UINT8);\n    const int SHARED_TO_GLOBAL_BYTES_PER_LINE = ((WARP_COL_TILES * M) * sizeof(int));\n    const int SHARED_TO_GLOBAL_BYTES_PER_WARP = (WARP_SIZE * sizeof(int));\n    const int SHARED_TO_GLOBAL_LINES_PER_WARP = (SHARED_TO_GLOBAL_BYTES_PER_WARP / SHARED_TO_GLOBAL_BYTES_PER_LINE);\n    const int SHARED_TO_GLOBAL_LANES_PER_LINE = (WARP_SIZE / SHARED_TO_GLOBAL_LINES_PER_WARP);\n    const int SHARED_TO_GLOBAL_ITERS = ((WARP_ROW_TILES * N) / SHARED_TO_GLOBAL_LINES_PER_WARP);\n\n    const int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const int WARP_STRIDE = (WARP_COL_TILES * M);\n    const int WARPS_PER_BLOCK = (BLOCK_ROW_WARPS * BLOCK_COL_WARPS);\n    const int THREADS_PER_BLOCK = (WARP_SIZE * WARPS_PER_BLOCK);\n    const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    const int GLOBAL_MEM_STRIDE = M_GLOBAL;\n    const int SHMEM_STRIDE = (M * BLOCK_COL_TILES);\n    const int SHMEM_OFFSET = (M * WARP_COL_TILES);\n    const int BLOCK_SIZE_M = (M * BLOCK_COL_TILES);\n    const int BLOCK_SIZE_N = (N * BLOCK_ROW_TILES);\n\n   extern __shared__ uint8_t shmem[];\n\n\t// Warp and lane identification.\n\tconst unsigned int warpId = threadIdx.x / WARP_SIZE;\n\tconst unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n\n\t// Offset in shared memory from which the B matrix is stored.\n\t// const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\tconst size_t shmem_idx_b_off = BLOCK_SIZE_K_SPARSE * SHARED_OFFSET_A;\n\n\n\t// Each CTA slides along the 128 x 128 tiles from the top left corner of the\n\t// matrix to the right and down, and selects the next tile to compute. Once\n\t// there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n\tconst unsigned int block_tile_i =\n\t\t((block_pos * BLOCK_COL_TILES) / M_TILES) * (BLOCK_ROW_TILES);\n\tconst unsigned int block_tile_j = (block_pos * BLOCK_COL_TILES) % M_TILES;\n\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n\twmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_ROW_TILES]\n\t\t\t\t\t\t\t\t\t\t\t\t\t [WARP_COL_TILES];\n\n    // Load the C matrix tiles into fragments from shared memory.\n\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t\twmma::fill_fragment(c[i][j], 0);\n\t\t}\n\t}\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    //for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n    for(int tile_k_idx_sparse = 0, tile_k_idx = 0; tile_k_idx_sparse < K_GLOBAL_SPARSE; tile_k_idx_sparse += BLOCK_SIZE_K_SPARSE, tile_k_idx += BLOCK_SIZE_K){\n\n\t\tsize_t shmem_idx = \n\t\twarpId < (WARPS_PER_BLOCK / 2)\n\t\t\t? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A\n\t\t\t: (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B + shmem_idx_b_off;\n\n\t\tint4 *lane_ptr = NULL;\n\t\tint *lane_ptr_index = NULL;\n\t\tconst uint8_t *warp_ptr = NULL;\n\n\n\t\tif(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\t//warp_ptr = &A[block_tile_j * M] +\n\t\t\t//\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * M_GLOBAL;\n\t\t\twarp_ptr = &A[block_tile_j * M];\n\t\t\t\n\t\t\tconst int *warp_ptr_index = &B_index[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t\t\t\t\t\t((warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A);\n\n\t\t\tlane_ptr_index = (int *)(warp_ptr_index + tile_k_idx_sparse + (laneId / CHUNK_COPY_LINE_LANES_A));\n\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_A) * SHARED_OFFSET_A;\n\t\t}else{\n\t\t\twarp_ptr = &B[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * K_GLOBAL_SPARSE;\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_sparse +\n\t\t\t\t\t\t\t\t(laneId / CHUNK_COPY_LINE_LANES_B) * K_GLOBAL_SPARSE) +\n\t\t\t\t\t\t\t\t(laneId % CHUNK_COPY_LINE_LANES_B);\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_B) * SHARED_OFFSET_B;\n\t\t}\n\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      // shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n\t  int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n\t  \t? BLOCK_SIZE_K_SPARSE / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A)\n\t\t: BLOCK_SIZE_N / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\n\t  /*\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\t  */\n\n\t  /*\n      int tile_k_idx_A;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n          tile_k_idx_A = *(lane_ptr_index);\n      }\n\t  */\n\n\t  #pragma unroll\n\t  for(int i = 0; i < iter_index; i += 1){\n\t\t  if(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\tint tile_k_idx_A = *(lane_ptr_index);\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_A * M_GLOBAL) + (laneId % CHUNK_COPY_LINE_LANES_A);\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_A)) =\n\t\t\t\t*lane_ptr;\n\t\t\t//warp_ptr = (uint8_t *)((uint8_t *)warp_ptr + M_GLOBAL * (WARPS_PER_BLOCK / 2) *CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tlane_ptr_index = (int *)((int *)lane_ptr_index +  (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A;\n\t\t  }else{\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_B)) =\n\t\t\t\t*lane_ptr;\n\t\t\tlane_ptr = (int4 *)((uint8_t *)lane_ptr + K_GLOBAL_SPARSE * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B;\n\t\t  }\n\t  }\n\n      __syncthreads();\n\n\t#pragma unroll\n      for (int k_step = 0; k_step < CHUNK_K_SPARSE; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::col_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n\t#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i += 1) {\n\t\t\tsize_t shmem_idx_a = (warpId % BLOCK_COL_WARPS) * M * WARP_COL_TILES + (i * M);\n\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_a + k_step * K * SHARED_OFFSET_A;\n\n\t\t\twmma::load_matrix_sync(a[i], tile_ptr, SHARED_OFFSET_A);\n\t\t#pragma unroll\n\t\t\tfor(int j = 0; j < WARP_ROW_TILES; j += 1){\n\t\t\t\tif(i == 0){\n\t\t\t\t\tsize_t shmem_idx_b = shmem_idx_b_off +\n\t\t\t\t\t\t\t\t\t\t\t(warpId / BLOCK_COL_WARPS) * (WARP_ROW_TILES * N) * SHARED_OFFSET_B +\n\t\t\t\t\t\t\t\t\t\t\t(j * N) * SHARED_OFFSET_B;\n\t\t\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_b + k_step * K;\n\t\t\t\t\twmma::load_matrix_sync(b[j], tile_ptr, SHARED_OFFSET_B);\n\t\t\t\t}\n\t\t\t\twmma::mma_sync(c[j][i], a[i], b[j], c[j][i]);\n\t\t\t}\n\n        }\n      }\n\n      __syncthreads();\n    }\n\n    // This pointer is used to access the C and D matrix tiles this warp computes.\n\tint *shmem_warp_tile_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * N * WARP_ROW_TILES * SHMEM_STRIDE +\n\t(warpId % BLOCK_COL_WARPS) * SHMEM_OFFSET;\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t#pragma unroll\n\t\t\tfor(int t = 0; t < c[i][j].num_elements; t += 1){\n\t\t\t\tc[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n\t\t\t}\n\t\t\tint *tile_ptr = shmem_warp_tile_ptr + i * N * SHMEM_STRIDE + j * M;\n\t\t\twmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, C_LAYOUT);\n\t\t}\n\t}\n\n    __syncthreads();\n\n\tint *shmem_warp_stream_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N * SHMEM_STRIDE\n\t\t\t\t\t\t\t\t\t+ (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tconst size_t gmem_idx =\n\t\t(block_tile_i * N + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N) * GLOBAL_MEM_STRIDE +\n\t\tblock_tile_j * M + (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tuint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n\tint *shmem_lane_stream_ptr =\n\t\tshmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * SHMEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\t\n\tuint8_t *dst_gmem_lane_stream_ptr =\n\t\tdst_gmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * GLOBAL_MEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\n\tfor(int i = 0; i < WARP_ROW_TILES * N; i += SHARED_TO_GLOBAL_LINES_PER_WARP){\n\t\t*(dst_gmem_lane_stream_ptr + i * GLOBAL_MEM_STRIDE) = (uint8_t)(*(shmem_lane_stream_ptr + i * SHMEM_STRIDE));\n\t}\n\n\t__syncthreads();\n}\n", "launch_config": {"dimBlock": [256, 1], "dimGrid": [256, 1]}}, "127": {"code": "\n__global__ void compute_gemm_imma(float *input0, float *input1, float *input2, float* input3, float* input4, float*input5, float* input6,\n                                  float *output0) {\n    // keys need to be replaced 4096, 1024, K_GLOBA_VALUEL, 0.75, 64\n\n\t//extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B = reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_index = reinterpret_cast<int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n\tconst int WARP_SIZE = 32;\n    const int M = 16;\n    const int K = 16;\n    const int N = 16;\n    const int WMMA_M = 16;\n    const int WMMA_K = 16;\n    const int WMMA_N = 16;\n    const int M_GLOBAL = 4096;\n    const int K_GLOBAL = 1024;\n    const int N_GLOBAL = 1024;\n    const float SPARSITY = 0.75;\n    const int K_GLOBAL_SPARSE = (int(K_GLOBAL * (1-SPARSITY)));\n    const int M_TILES = (M_GLOBAL / M);\n    const int N_TILES = (N_GLOBAL / N);\n    const int K_TILES = (K_GLOBAL / K);\n    const int BLOCK_ROW_WARPS = 2;\n    const int BLOCK_COL_WARPS = 4;\n    const int WARP_ROW_TILES = 4;\n    const int WARP_COL_TILES = 2;\n    const int SKEW_UINT8 = 16;\n    const int BANK_VAL = 32;\n    const int NUM_BANK = (K_GLOBAL / BANK_VAL);\n    /////////// BLOCK_ROW_TILES <= N_TILES ////////////\n    // const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    // ////////// BLOCK_COL_TILES <= M_TILES ////////////\n    // const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    auto C_LAYOUT = wmma::mem_col_major;\n    const int CHUNK_K = 64;\n    const int CHUNK_K_SPARSE = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K_SPARSE = (int((CHUNK_K * K) * (1 - SPARSITY)));\n    const int WARP_COPY_BYTES = (WARP_SIZE * sizeof(int4));\n    const int CHUNK_LINE_BYTES_A (BLOCK_COL_TILES * M * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_A = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_A);\n    const int CHUNK_COPY_LINE_LANES_A = (CHUNK_LINE_BYTES_A / sizeof(int4));\n    const int SHARED_OFFSET_A = (BLOCK_COL_TILES * M + SKEW_UINT8);\n\n    const int CHUNK_LINE_BYTES_B = (BLOCK_SIZE_K_SPARSE * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_B = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_B);\n    const int CHUNK_COPY_LINE_LANES_B = (CHUNK_LINE_BYTES_B / sizeof(int4));\n    const int SHARED_OFFSET_B = (BLOCK_SIZE_K_SPARSE + SKEW_UINT8);\n    const int SHARED_TO_GLOBAL_BYTES_PER_LINE = ((WARP_COL_TILES * M) * sizeof(int));\n    const int SHARED_TO_GLOBAL_BYTES_PER_WARP = (WARP_SIZE * sizeof(int));\n    const int SHARED_TO_GLOBAL_LINES_PER_WARP = (SHARED_TO_GLOBAL_BYTES_PER_WARP / SHARED_TO_GLOBAL_BYTES_PER_LINE);\n    const int SHARED_TO_GLOBAL_LANES_PER_LINE = (WARP_SIZE / SHARED_TO_GLOBAL_LINES_PER_WARP);\n    const int SHARED_TO_GLOBAL_ITERS = ((WARP_ROW_TILES * N) / SHARED_TO_GLOBAL_LINES_PER_WARP);\n\n    const int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const int WARP_STRIDE = (WARP_COL_TILES * M);\n    const int WARPS_PER_BLOCK = (BLOCK_ROW_WARPS * BLOCK_COL_WARPS);\n    const int THREADS_PER_BLOCK = (WARP_SIZE * WARPS_PER_BLOCK);\n    const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    const int GLOBAL_MEM_STRIDE = M_GLOBAL;\n    const int SHMEM_STRIDE = (M * BLOCK_COL_TILES);\n    const int SHMEM_OFFSET = (M * WARP_COL_TILES);\n    const int BLOCK_SIZE_M = (M * BLOCK_COL_TILES);\n    const int BLOCK_SIZE_N = (N * BLOCK_ROW_TILES);\n\n   extern __shared__ uint8_t shmem[];\n\n\t// Warp and lane identification.\n\tconst unsigned int warpId = threadIdx.x / WARP_SIZE;\n\tconst unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n\n\t// Offset in shared memory from which the B matrix is stored.\n\t// const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\tconst size_t shmem_idx_b_off = BLOCK_SIZE_K_SPARSE * SHARED_OFFSET_A;\n\n\n\t// Each CTA slides along the 128 x 128 tiles from the top left corner of the\n\t// matrix to the right and down, and selects the next tile to compute. Once\n\t// there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n\tconst unsigned int block_tile_i =\n\t\t((block_pos * BLOCK_COL_TILES) / M_TILES) * (BLOCK_ROW_TILES);\n\tconst unsigned int block_tile_j = (block_pos * BLOCK_COL_TILES) % M_TILES;\n\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n\twmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_ROW_TILES]\n\t\t\t\t\t\t\t\t\t\t\t\t\t [WARP_COL_TILES];\n\n    // Load the C matrix tiles into fragments from shared memory.\n\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t\twmma::fill_fragment(c[i][j], 0);\n\t\t}\n\t}\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    //for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n    for(int tile_k_idx_sparse = 0, tile_k_idx = 0; tile_k_idx_sparse < K_GLOBAL_SPARSE; tile_k_idx_sparse += BLOCK_SIZE_K_SPARSE, tile_k_idx += BLOCK_SIZE_K){\n\n\t\tsize_t shmem_idx = \n\t\twarpId < (WARPS_PER_BLOCK / 2)\n\t\t\t? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A\n\t\t\t: (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B + shmem_idx_b_off;\n\n\t\tint4 *lane_ptr = NULL;\n\t\tint *lane_ptr_index = NULL;\n\t\tconst uint8_t *warp_ptr = NULL;\n\n\n\t\tif(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\t//warp_ptr = &A[block_tile_j * M] +\n\t\t\t//\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * M_GLOBAL;\n\t\t\twarp_ptr = &A[block_tile_j * M];\n\t\t\t\n\t\t\tconst int *warp_ptr_index = &B_index[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t\t\t\t\t\t((warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A);\n\n\t\t\tlane_ptr_index = (int *)(warp_ptr_index + tile_k_idx_sparse + (laneId / CHUNK_COPY_LINE_LANES_A));\n\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_A) * SHARED_OFFSET_A;\n\t\t}else{\n\t\t\twarp_ptr = &B[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * K_GLOBAL_SPARSE;\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_sparse +\n\t\t\t\t\t\t\t\t(laneId / CHUNK_COPY_LINE_LANES_B) * K_GLOBAL_SPARSE) +\n\t\t\t\t\t\t\t\t(laneId % CHUNK_COPY_LINE_LANES_B);\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_B) * SHARED_OFFSET_B;\n\t\t}\n\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      // shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n\t  int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n\t  \t? BLOCK_SIZE_K_SPARSE / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A)\n\t\t: BLOCK_SIZE_N / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\n\t  /*\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\t  */\n\n\t  /*\n      int tile_k_idx_A;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n          tile_k_idx_A = *(lane_ptr_index);\n      }\n\t  */\n\n\t  #pragma unroll\n\t  for(int i = 0; i < iter_index; i += 1){\n\t\t  if(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\tint tile_k_idx_A = *(lane_ptr_index);\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_A * M_GLOBAL) + (laneId % CHUNK_COPY_LINE_LANES_A);\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_A)) =\n\t\t\t\t*lane_ptr;\n\t\t\t//warp_ptr = (uint8_t *)((uint8_t *)warp_ptr + M_GLOBAL * (WARPS_PER_BLOCK / 2) *CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tlane_ptr_index = (int *)((int *)lane_ptr_index +  (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A;\n\t\t  }else{\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_B)) =\n\t\t\t\t*lane_ptr;\n\t\t\tlane_ptr = (int4 *)((uint8_t *)lane_ptr + K_GLOBAL_SPARSE * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B;\n\t\t  }\n\t  }\n\n      __syncthreads();\n\n\t#pragma unroll\n      for (int k_step = 0; k_step < CHUNK_K_SPARSE; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::col_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n\t#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i += 1) {\n\t\t\tsize_t shmem_idx_a = (warpId % BLOCK_COL_WARPS) * M * WARP_COL_TILES + (i * M);\n\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_a + k_step * K * SHARED_OFFSET_A;\n\n\t\t\twmma::load_matrix_sync(a[i], tile_ptr, SHARED_OFFSET_A);\n\t\t#pragma unroll\n\t\t\tfor(int j = 0; j < WARP_ROW_TILES; j += 1){\n\t\t\t\tif(i == 0){\n\t\t\t\t\tsize_t shmem_idx_b = shmem_idx_b_off +\n\t\t\t\t\t\t\t\t\t\t\t(warpId / BLOCK_COL_WARPS) * (WARP_ROW_TILES * N) * SHARED_OFFSET_B +\n\t\t\t\t\t\t\t\t\t\t\t(j * N) * SHARED_OFFSET_B;\n\t\t\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_b + k_step * K;\n\t\t\t\t\twmma::load_matrix_sync(b[j], tile_ptr, SHARED_OFFSET_B);\n\t\t\t\t}\n\t\t\t\twmma::mma_sync(c[j][i], a[i], b[j], c[j][i]);\n\t\t\t}\n\n        }\n      }\n\n      __syncthreads();\n    }\n\n    // This pointer is used to access the C and D matrix tiles this warp computes.\n\tint *shmem_warp_tile_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * N * WARP_ROW_TILES * SHMEM_STRIDE +\n\t(warpId % BLOCK_COL_WARPS) * SHMEM_OFFSET;\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t#pragma unroll\n\t\t\tfor(int t = 0; t < c[i][j].num_elements; t += 1){\n\t\t\t\tc[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n\t\t\t}\n\t\t\tint *tile_ptr = shmem_warp_tile_ptr + i * N * SHMEM_STRIDE + j * M;\n\t\t\twmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, C_LAYOUT);\n\t\t}\n\t}\n\n    __syncthreads();\n\n\tint *shmem_warp_stream_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N * SHMEM_STRIDE\n\t\t\t\t\t\t\t\t\t+ (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tconst size_t gmem_idx =\n\t\t(block_tile_i * N + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N) * GLOBAL_MEM_STRIDE +\n\t\tblock_tile_j * M + (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tuint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n\tint *shmem_lane_stream_ptr =\n\t\tshmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * SHMEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\t\n\tuint8_t *dst_gmem_lane_stream_ptr =\n\t\tdst_gmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * GLOBAL_MEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\n\tfor(int i = 0; i < WARP_ROW_TILES * N; i += SHARED_TO_GLOBAL_LINES_PER_WARP){\n\t\t*(dst_gmem_lane_stream_ptr + i * GLOBAL_MEM_STRIDE) = (uint8_t)(*(shmem_lane_stream_ptr + i * SHMEM_STRIDE));\n\t}\n\n\t__syncthreads();\n}\n", "launch_config": {"dimBlock": [256, 1], "dimGrid": [256, 1]}}, "128": {"code": "\n__global__ void compute_gemm_imma(float *input0, float *input1, float *input2, float* input3, float* input4, float*input5, float* input6,\n                                  float *output0) {\n    // keys need to be replaced 4096, 1024, K_GLOBA_VALUEL, 0.75, 64\n\n\t//extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B = reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_index = reinterpret_cast<int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n\tconst int WARP_SIZE = 32;\n    const int M = 16;\n    const int K = 16;\n    const int N = 16;\n    const int WMMA_M = 16;\n    const int WMMA_K = 16;\n    const int WMMA_N = 16;\n    const int M_GLOBAL = 4096;\n    const int K_GLOBAL = 1024;\n    const int N_GLOBAL = 1024;\n    const float SPARSITY = 0.75;\n    const int K_GLOBAL_SPARSE = (int(K_GLOBAL * (1-SPARSITY)));\n    const int M_TILES = (M_GLOBAL / M);\n    const int N_TILES = (N_GLOBAL / N);\n    const int K_TILES = (K_GLOBAL / K);\n    const int BLOCK_ROW_WARPS = 2;\n    const int BLOCK_COL_WARPS = 4;\n    const int WARP_ROW_TILES = 4;\n    const int WARP_COL_TILES = 2;\n    const int SKEW_UINT8 = 16;\n    const int BANK_VAL = 32;\n    const int NUM_BANK = (K_GLOBAL / BANK_VAL);\n    /////////// BLOCK_ROW_TILES <= N_TILES ////////////\n    // const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    // ////////// BLOCK_COL_TILES <= M_TILES ////////////\n    // const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    auto C_LAYOUT = wmma::mem_col_major;\n    const int CHUNK_K = 64;\n    const int CHUNK_K_SPARSE = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K_SPARSE = (int((CHUNK_K * K) * (1 - SPARSITY)));\n    const int WARP_COPY_BYTES = (WARP_SIZE * sizeof(int4));\n    const int CHUNK_LINE_BYTES_A (BLOCK_COL_TILES * M * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_A = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_A);\n    const int CHUNK_COPY_LINE_LANES_A = (CHUNK_LINE_BYTES_A / sizeof(int4));\n    const int SHARED_OFFSET_A = (BLOCK_COL_TILES * M + SKEW_UINT8);\n\n    const int CHUNK_LINE_BYTES_B = (BLOCK_SIZE_K_SPARSE * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_B = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_B);\n    const int CHUNK_COPY_LINE_LANES_B = (CHUNK_LINE_BYTES_B / sizeof(int4));\n    const int SHARED_OFFSET_B = (BLOCK_SIZE_K_SPARSE + SKEW_UINT8);\n    const int SHARED_TO_GLOBAL_BYTES_PER_LINE = ((WARP_COL_TILES * M) * sizeof(int));\n    const int SHARED_TO_GLOBAL_BYTES_PER_WARP = (WARP_SIZE * sizeof(int));\n    const int SHARED_TO_GLOBAL_LINES_PER_WARP = (SHARED_TO_GLOBAL_BYTES_PER_WARP / SHARED_TO_GLOBAL_BYTES_PER_LINE);\n    const int SHARED_TO_GLOBAL_LANES_PER_LINE = (WARP_SIZE / SHARED_TO_GLOBAL_LINES_PER_WARP);\n    const int SHARED_TO_GLOBAL_ITERS = ((WARP_ROW_TILES * N) / SHARED_TO_GLOBAL_LINES_PER_WARP);\n\n    const int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const int WARP_STRIDE = (WARP_COL_TILES * M);\n    const int WARPS_PER_BLOCK = (BLOCK_ROW_WARPS * BLOCK_COL_WARPS);\n    const int THREADS_PER_BLOCK = (WARP_SIZE * WARPS_PER_BLOCK);\n    const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    const int GLOBAL_MEM_STRIDE = M_GLOBAL;\n    const int SHMEM_STRIDE = (M * BLOCK_COL_TILES);\n    const int SHMEM_OFFSET = (M * WARP_COL_TILES);\n    const int BLOCK_SIZE_M = (M * BLOCK_COL_TILES);\n    const int BLOCK_SIZE_N = (N * BLOCK_ROW_TILES);\n\n   extern __shared__ uint8_t shmem[];\n\n\t// Warp and lane identification.\n\tconst unsigned int warpId = threadIdx.x / WARP_SIZE;\n\tconst unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n\n\t// Offset in shared memory from which the B matrix is stored.\n\t// const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\tconst size_t shmem_idx_b_off = BLOCK_SIZE_K_SPARSE * SHARED_OFFSET_A;\n\n\n\t// Each CTA slides along the 128 x 128 tiles from the top left corner of the\n\t// matrix to the right and down, and selects the next tile to compute. Once\n\t// there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n\tconst unsigned int block_tile_i =\n\t\t((block_pos * BLOCK_COL_TILES) / M_TILES) * (BLOCK_ROW_TILES);\n\tconst unsigned int block_tile_j = (block_pos * BLOCK_COL_TILES) % M_TILES;\n\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n\twmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_ROW_TILES]\n\t\t\t\t\t\t\t\t\t\t\t\t\t [WARP_COL_TILES];\n\n    // Load the C matrix tiles into fragments from shared memory.\n\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t\twmma::fill_fragment(c[i][j], 0);\n\t\t}\n\t}\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    //for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n    for(int tile_k_idx_sparse = 0, tile_k_idx = 0; tile_k_idx_sparse < K_GLOBAL_SPARSE; tile_k_idx_sparse += BLOCK_SIZE_K_SPARSE, tile_k_idx += BLOCK_SIZE_K){\n\n\t\tsize_t shmem_idx = \n\t\twarpId < (WARPS_PER_BLOCK / 2)\n\t\t\t? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A\n\t\t\t: (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B + shmem_idx_b_off;\n\n\t\tint4 *lane_ptr = NULL;\n\t\tint *lane_ptr_index = NULL;\n\t\tconst uint8_t *warp_ptr = NULL;\n\n\n\t\tif(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\t//warp_ptr = &A[block_tile_j * M] +\n\t\t\t//\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * M_GLOBAL;\n\t\t\twarp_ptr = &A[block_tile_j * M];\n\t\t\t\n\t\t\tconst int *warp_ptr_index = &B_index[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t\t\t\t\t\t((warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A);\n\n\t\t\tlane_ptr_index = (int *)(warp_ptr_index + tile_k_idx_sparse + (laneId / CHUNK_COPY_LINE_LANES_A));\n\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_A) * SHARED_OFFSET_A;\n\t\t}else{\n\t\t\twarp_ptr = &B[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * K_GLOBAL_SPARSE;\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_sparse +\n\t\t\t\t\t\t\t\t(laneId / CHUNK_COPY_LINE_LANES_B) * K_GLOBAL_SPARSE) +\n\t\t\t\t\t\t\t\t(laneId % CHUNK_COPY_LINE_LANES_B);\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_B) * SHARED_OFFSET_B;\n\t\t}\n\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      // shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n\t  int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n\t  \t? BLOCK_SIZE_K_SPARSE / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A)\n\t\t: BLOCK_SIZE_N / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\n\t  /*\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\t  */\n\n\t  /*\n      int tile_k_idx_A;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n          tile_k_idx_A = *(lane_ptr_index);\n      }\n\t  */\n\n\t  #pragma unroll\n\t  for(int i = 0; i < iter_index; i += 1){\n\t\t  if(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\tint tile_k_idx_A = *(lane_ptr_index);\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_A * M_GLOBAL) + (laneId % CHUNK_COPY_LINE_LANES_A);\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_A)) =\n\t\t\t\t*lane_ptr;\n\t\t\t//warp_ptr = (uint8_t *)((uint8_t *)warp_ptr + M_GLOBAL * (WARPS_PER_BLOCK / 2) *CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tlane_ptr_index = (int *)((int *)lane_ptr_index +  (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A;\n\t\t  }else{\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_B)) =\n\t\t\t\t*lane_ptr;\n\t\t\tlane_ptr = (int4 *)((uint8_t *)lane_ptr + K_GLOBAL_SPARSE * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B;\n\t\t  }\n\t  }\n\n      __syncthreads();\n\n\t#pragma unroll\n      for (int k_step = 0; k_step < CHUNK_K_SPARSE; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::col_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n\t#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i += 1) {\n\t\t\tsize_t shmem_idx_a = (warpId % BLOCK_COL_WARPS) * M * WARP_COL_TILES + (i * M);\n\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_a + k_step * K * SHARED_OFFSET_A;\n\n\t\t\twmma::load_matrix_sync(a[i], tile_ptr, SHARED_OFFSET_A);\n\t\t#pragma unroll\n\t\t\tfor(int j = 0; j < WARP_ROW_TILES; j += 1){\n\t\t\t\tif(i == 0){\n\t\t\t\t\tsize_t shmem_idx_b = shmem_idx_b_off +\n\t\t\t\t\t\t\t\t\t\t\t(warpId / BLOCK_COL_WARPS) * (WARP_ROW_TILES * N) * SHARED_OFFSET_B +\n\t\t\t\t\t\t\t\t\t\t\t(j * N) * SHARED_OFFSET_B;\n\t\t\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_b + k_step * K;\n\t\t\t\t\twmma::load_matrix_sync(b[j], tile_ptr, SHARED_OFFSET_B);\n\t\t\t\t}\n\t\t\t\twmma::mma_sync(c[j][i], a[i], b[j], c[j][i]);\n\t\t\t}\n\n        }\n      }\n\n      __syncthreads();\n    }\n\n    // This pointer is used to access the C and D matrix tiles this warp computes.\n\tint *shmem_warp_tile_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * N * WARP_ROW_TILES * SHMEM_STRIDE +\n\t(warpId % BLOCK_COL_WARPS) * SHMEM_OFFSET;\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t#pragma unroll\n\t\t\tfor(int t = 0; t < c[i][j].num_elements; t += 1){\n\t\t\t\tc[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n\t\t\t}\n\t\t\tint *tile_ptr = shmem_warp_tile_ptr + i * N * SHMEM_STRIDE + j * M;\n\t\t\twmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, C_LAYOUT);\n\t\t}\n\t}\n\n    __syncthreads();\n\n\tint *shmem_warp_stream_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N * SHMEM_STRIDE\n\t\t\t\t\t\t\t\t\t+ (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tconst size_t gmem_idx =\n\t\t(block_tile_i * N + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N) * GLOBAL_MEM_STRIDE +\n\t\tblock_tile_j * M + (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tuint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n\tint *shmem_lane_stream_ptr =\n\t\tshmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * SHMEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\t\n\tuint8_t *dst_gmem_lane_stream_ptr =\n\t\tdst_gmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * GLOBAL_MEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\n\tfor(int i = 0; i < WARP_ROW_TILES * N; i += SHARED_TO_GLOBAL_LINES_PER_WARP){\n\t\t*(dst_gmem_lane_stream_ptr + i * GLOBAL_MEM_STRIDE) = (uint8_t)(*(shmem_lane_stream_ptr + i * SHMEM_STRIDE));\n\t}\n\n\t__syncthreads();\n}\n", "launch_config": {"dimBlock": [256, 1], "dimGrid": [256, 1]}}, "129": {"code": "\n__global__ void compute_gemm_imma(float *input0, float *input1, float *input2, float* input3, float* input4, float*input5, float* input6,\n                                  float *output0) {\n    // keys need to be replaced 4096, 1024, K_GLOBA_VALUEL, 0.75, 64\n\n\t//extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B = reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_index = reinterpret_cast<int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n\tconst int WARP_SIZE = 32;\n    const int M = 16;\n    const int K = 16;\n    const int N = 16;\n    const int WMMA_M = 16;\n    const int WMMA_K = 16;\n    const int WMMA_N = 16;\n    const int M_GLOBAL = 4096;\n    const int K_GLOBAL = 1024;\n    const int N_GLOBAL = 1024;\n    const float SPARSITY = 0.75;\n    const int K_GLOBAL_SPARSE = (int(K_GLOBAL * (1-SPARSITY)));\n    const int M_TILES = (M_GLOBAL / M);\n    const int N_TILES = (N_GLOBAL / N);\n    const int K_TILES = (K_GLOBAL / K);\n    const int BLOCK_ROW_WARPS = 2;\n    const int BLOCK_COL_WARPS = 4;\n    const int WARP_ROW_TILES = 4;\n    const int WARP_COL_TILES = 2;\n    const int SKEW_UINT8 = 16;\n    const int BANK_VAL = 32;\n    const int NUM_BANK = (K_GLOBAL / BANK_VAL);\n    /////////// BLOCK_ROW_TILES <= N_TILES ////////////\n    // const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    // ////////// BLOCK_COL_TILES <= M_TILES ////////////\n    // const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    auto C_LAYOUT = wmma::mem_col_major;\n    const int CHUNK_K = 64;\n    const int CHUNK_K_SPARSE = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K_SPARSE = (int((CHUNK_K * K) * (1 - SPARSITY)));\n    const int WARP_COPY_BYTES = (WARP_SIZE * sizeof(int4));\n    const int CHUNK_LINE_BYTES_A (BLOCK_COL_TILES * M * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_A = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_A);\n    const int CHUNK_COPY_LINE_LANES_A = (CHUNK_LINE_BYTES_A / sizeof(int4));\n    const int SHARED_OFFSET_A = (BLOCK_COL_TILES * M + SKEW_UINT8);\n\n    const int CHUNK_LINE_BYTES_B = (BLOCK_SIZE_K_SPARSE * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_B = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_B);\n    const int CHUNK_COPY_LINE_LANES_B = (CHUNK_LINE_BYTES_B / sizeof(int4));\n    const int SHARED_OFFSET_B = (BLOCK_SIZE_K_SPARSE + SKEW_UINT8);\n    const int SHARED_TO_GLOBAL_BYTES_PER_LINE = ((WARP_COL_TILES * M) * sizeof(int));\n    const int SHARED_TO_GLOBAL_BYTES_PER_WARP = (WARP_SIZE * sizeof(int));\n    const int SHARED_TO_GLOBAL_LINES_PER_WARP = (SHARED_TO_GLOBAL_BYTES_PER_WARP / SHARED_TO_GLOBAL_BYTES_PER_LINE);\n    const int SHARED_TO_GLOBAL_LANES_PER_LINE = (WARP_SIZE / SHARED_TO_GLOBAL_LINES_PER_WARP);\n    const int SHARED_TO_GLOBAL_ITERS = ((WARP_ROW_TILES * N) / SHARED_TO_GLOBAL_LINES_PER_WARP);\n\n    const int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const int WARP_STRIDE = (WARP_COL_TILES * M);\n    const int WARPS_PER_BLOCK = (BLOCK_ROW_WARPS * BLOCK_COL_WARPS);\n    const int THREADS_PER_BLOCK = (WARP_SIZE * WARPS_PER_BLOCK);\n    const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    const int GLOBAL_MEM_STRIDE = M_GLOBAL;\n    const int SHMEM_STRIDE = (M * BLOCK_COL_TILES);\n    const int SHMEM_OFFSET = (M * WARP_COL_TILES);\n    const int BLOCK_SIZE_M = (M * BLOCK_COL_TILES);\n    const int BLOCK_SIZE_N = (N * BLOCK_ROW_TILES);\n\n   extern __shared__ uint8_t shmem[];\n\n\t// Warp and lane identification.\n\tconst unsigned int warpId = threadIdx.x / WARP_SIZE;\n\tconst unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n\n\t// Offset in shared memory from which the B matrix is stored.\n\t// const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\tconst size_t shmem_idx_b_off = BLOCK_SIZE_K_SPARSE * SHARED_OFFSET_A;\n\n\n\t// Each CTA slides along the 128 x 128 tiles from the top left corner of the\n\t// matrix to the right and down, and selects the next tile to compute. Once\n\t// there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n\tconst unsigned int block_tile_i =\n\t\t((block_pos * BLOCK_COL_TILES) / M_TILES) * (BLOCK_ROW_TILES);\n\tconst unsigned int block_tile_j = (block_pos * BLOCK_COL_TILES) % M_TILES;\n\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n\twmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_ROW_TILES]\n\t\t\t\t\t\t\t\t\t\t\t\t\t [WARP_COL_TILES];\n\n    // Load the C matrix tiles into fragments from shared memory.\n\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t\twmma::fill_fragment(c[i][j], 0);\n\t\t}\n\t}\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    //for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n    for(int tile_k_idx_sparse = 0, tile_k_idx = 0; tile_k_idx_sparse < K_GLOBAL_SPARSE; tile_k_idx_sparse += BLOCK_SIZE_K_SPARSE, tile_k_idx += BLOCK_SIZE_K){\n\n\t\tsize_t shmem_idx = \n\t\twarpId < (WARPS_PER_BLOCK / 2)\n\t\t\t? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A\n\t\t\t: (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B + shmem_idx_b_off;\n\n\t\tint4 *lane_ptr = NULL;\n\t\tint *lane_ptr_index = NULL;\n\t\tconst uint8_t *warp_ptr = NULL;\n\n\n\t\tif(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\t//warp_ptr = &A[block_tile_j * M] +\n\t\t\t//\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * M_GLOBAL;\n\t\t\twarp_ptr = &A[block_tile_j * M];\n\t\t\t\n\t\t\tconst int *warp_ptr_index = &B_index[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t\t\t\t\t\t((warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A);\n\n\t\t\tlane_ptr_index = (int *)(warp_ptr_index + tile_k_idx_sparse + (laneId / CHUNK_COPY_LINE_LANES_A));\n\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_A) * SHARED_OFFSET_A;\n\t\t}else{\n\t\t\twarp_ptr = &B[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * K_GLOBAL_SPARSE;\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_sparse +\n\t\t\t\t\t\t\t\t(laneId / CHUNK_COPY_LINE_LANES_B) * K_GLOBAL_SPARSE) +\n\t\t\t\t\t\t\t\t(laneId % CHUNK_COPY_LINE_LANES_B);\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_B) * SHARED_OFFSET_B;\n\t\t}\n\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      // shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n\t  int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n\t  \t? BLOCK_SIZE_K_SPARSE / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A)\n\t\t: BLOCK_SIZE_N / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\n\t  /*\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\t  */\n\n\t  /*\n      int tile_k_idx_A;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n          tile_k_idx_A = *(lane_ptr_index);\n      }\n\t  */\n\n\t  #pragma unroll\n\t  for(int i = 0; i < iter_index; i += 1){\n\t\t  if(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\tint tile_k_idx_A = *(lane_ptr_index);\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_A * M_GLOBAL) + (laneId % CHUNK_COPY_LINE_LANES_A);\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_A)) =\n\t\t\t\t*lane_ptr;\n\t\t\t//warp_ptr = (uint8_t *)((uint8_t *)warp_ptr + M_GLOBAL * (WARPS_PER_BLOCK / 2) *CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tlane_ptr_index = (int *)((int *)lane_ptr_index +  (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A;\n\t\t  }else{\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_B)) =\n\t\t\t\t*lane_ptr;\n\t\t\tlane_ptr = (int4 *)((uint8_t *)lane_ptr + K_GLOBAL_SPARSE * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B;\n\t\t  }\n\t  }\n\n      __syncthreads();\n\n\t#pragma unroll\n      for (int k_step = 0; k_step < CHUNK_K_SPARSE; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::col_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n\t#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i += 1) {\n\t\t\tsize_t shmem_idx_a = (warpId % BLOCK_COL_WARPS) * M * WARP_COL_TILES + (i * M);\n\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_a + k_step * K * SHARED_OFFSET_A;\n\n\t\t\twmma::load_matrix_sync(a[i], tile_ptr, SHARED_OFFSET_A);\n\t\t#pragma unroll\n\t\t\tfor(int j = 0; j < WARP_ROW_TILES; j += 1){\n\t\t\t\tif(i == 0){\n\t\t\t\t\tsize_t shmem_idx_b = shmem_idx_b_off +\n\t\t\t\t\t\t\t\t\t\t\t(warpId / BLOCK_COL_WARPS) * (WARP_ROW_TILES * N) * SHARED_OFFSET_B +\n\t\t\t\t\t\t\t\t\t\t\t(j * N) * SHARED_OFFSET_B;\n\t\t\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_b + k_step * K;\n\t\t\t\t\twmma::load_matrix_sync(b[j], tile_ptr, SHARED_OFFSET_B);\n\t\t\t\t}\n\t\t\t\twmma::mma_sync(c[j][i], a[i], b[j], c[j][i]);\n\t\t\t}\n\n        }\n      }\n\n      __syncthreads();\n    }\n\n    // This pointer is used to access the C and D matrix tiles this warp computes.\n\tint *shmem_warp_tile_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * N * WARP_ROW_TILES * SHMEM_STRIDE +\n\t(warpId % BLOCK_COL_WARPS) * SHMEM_OFFSET;\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t#pragma unroll\n\t\t\tfor(int t = 0; t < c[i][j].num_elements; t += 1){\n\t\t\t\tc[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n\t\t\t}\n\t\t\tint *tile_ptr = shmem_warp_tile_ptr + i * N * SHMEM_STRIDE + j * M;\n\t\t\twmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, C_LAYOUT);\n\t\t}\n\t}\n\n    __syncthreads();\n\n\tint *shmem_warp_stream_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N * SHMEM_STRIDE\n\t\t\t\t\t\t\t\t\t+ (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tconst size_t gmem_idx =\n\t\t(block_tile_i * N + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N) * GLOBAL_MEM_STRIDE +\n\t\tblock_tile_j * M + (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tuint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n\tint *shmem_lane_stream_ptr =\n\t\tshmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * SHMEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\t\n\tuint8_t *dst_gmem_lane_stream_ptr =\n\t\tdst_gmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * GLOBAL_MEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\n\tfor(int i = 0; i < WARP_ROW_TILES * N; i += SHARED_TO_GLOBAL_LINES_PER_WARP){\n\t\t*(dst_gmem_lane_stream_ptr + i * GLOBAL_MEM_STRIDE) = (uint8_t)(*(shmem_lane_stream_ptr + i * SHMEM_STRIDE));\n\t}\n\n\t__syncthreads();\n}\n", "launch_config": {"dimBlock": [256, 1], "dimGrid": [256, 1]}}, "130": {"code": "\n__global__ void compute_gemm_imma(float *input0, float *input1, float *input2, float* input3, float* input4, float*input5, float* input6,\n                                  float *output0) {\n    // keys need to be replaced 4096, 1024, K_GLOBA_VALUEL, 0.75, 64\n\n\t//extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B = reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_index = reinterpret_cast<int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n\tconst int WARP_SIZE = 32;\n    const int M = 16;\n    const int K = 16;\n    const int N = 16;\n    const int WMMA_M = 16;\n    const int WMMA_K = 16;\n    const int WMMA_N = 16;\n    const int M_GLOBAL = 4096;\n    const int K_GLOBAL = 1024;\n    const int N_GLOBAL = 1024;\n    const float SPARSITY = 0.75;\n    const int K_GLOBAL_SPARSE = (int(K_GLOBAL * (1-SPARSITY)));\n    const int M_TILES = (M_GLOBAL / M);\n    const int N_TILES = (N_GLOBAL / N);\n    const int K_TILES = (K_GLOBAL / K);\n    const int BLOCK_ROW_WARPS = 2;\n    const int BLOCK_COL_WARPS = 4;\n    const int WARP_ROW_TILES = 4;\n    const int WARP_COL_TILES = 2;\n    const int SKEW_UINT8 = 16;\n    const int BANK_VAL = 32;\n    const int NUM_BANK = (K_GLOBAL / BANK_VAL);\n    /////////// BLOCK_ROW_TILES <= N_TILES ////////////\n    // const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    // ////////// BLOCK_COL_TILES <= M_TILES ////////////\n    // const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    auto C_LAYOUT = wmma::mem_col_major;\n    const int CHUNK_K = 64;\n    const int CHUNK_K_SPARSE = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K_SPARSE = (int((CHUNK_K * K) * (1 - SPARSITY)));\n    const int WARP_COPY_BYTES = (WARP_SIZE * sizeof(int4));\n    const int CHUNK_LINE_BYTES_A (BLOCK_COL_TILES * M * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_A = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_A);\n    const int CHUNK_COPY_LINE_LANES_A = (CHUNK_LINE_BYTES_A / sizeof(int4));\n    const int SHARED_OFFSET_A = (BLOCK_COL_TILES * M + SKEW_UINT8);\n\n    const int CHUNK_LINE_BYTES_B = (BLOCK_SIZE_K_SPARSE * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_B = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_B);\n    const int CHUNK_COPY_LINE_LANES_B = (CHUNK_LINE_BYTES_B / sizeof(int4));\n    const int SHARED_OFFSET_B = (BLOCK_SIZE_K_SPARSE + SKEW_UINT8);\n    const int SHARED_TO_GLOBAL_BYTES_PER_LINE = ((WARP_COL_TILES * M) * sizeof(int));\n    const int SHARED_TO_GLOBAL_BYTES_PER_WARP = (WARP_SIZE * sizeof(int));\n    const int SHARED_TO_GLOBAL_LINES_PER_WARP = (SHARED_TO_GLOBAL_BYTES_PER_WARP / SHARED_TO_GLOBAL_BYTES_PER_LINE);\n    const int SHARED_TO_GLOBAL_LANES_PER_LINE = (WARP_SIZE / SHARED_TO_GLOBAL_LINES_PER_WARP);\n    const int SHARED_TO_GLOBAL_ITERS = ((WARP_ROW_TILES * N) / SHARED_TO_GLOBAL_LINES_PER_WARP);\n\n    const int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const int WARP_STRIDE = (WARP_COL_TILES * M);\n    const int WARPS_PER_BLOCK = (BLOCK_ROW_WARPS * BLOCK_COL_WARPS);\n    const int THREADS_PER_BLOCK = (WARP_SIZE * WARPS_PER_BLOCK);\n    const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    const int GLOBAL_MEM_STRIDE = M_GLOBAL;\n    const int SHMEM_STRIDE = (M * BLOCK_COL_TILES);\n    const int SHMEM_OFFSET = (M * WARP_COL_TILES);\n    const int BLOCK_SIZE_M = (M * BLOCK_COL_TILES);\n    const int BLOCK_SIZE_N = (N * BLOCK_ROW_TILES);\n\n   extern __shared__ uint8_t shmem[];\n\n\t// Warp and lane identification.\n\tconst unsigned int warpId = threadIdx.x / WARP_SIZE;\n\tconst unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n\n\t// Offset in shared memory from which the B matrix is stored.\n\t// const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\tconst size_t shmem_idx_b_off = BLOCK_SIZE_K_SPARSE * SHARED_OFFSET_A;\n\n\n\t// Each CTA slides along the 128 x 128 tiles from the top left corner of the\n\t// matrix to the right and down, and selects the next tile to compute. Once\n\t// there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n\tconst unsigned int block_tile_i =\n\t\t((block_pos * BLOCK_COL_TILES) / M_TILES) * (BLOCK_ROW_TILES);\n\tconst unsigned int block_tile_j = (block_pos * BLOCK_COL_TILES) % M_TILES;\n\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n\twmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_ROW_TILES]\n\t\t\t\t\t\t\t\t\t\t\t\t\t [WARP_COL_TILES];\n\n    // Load the C matrix tiles into fragments from shared memory.\n\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t\twmma::fill_fragment(c[i][j], 0);\n\t\t}\n\t}\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    //for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n    for(int tile_k_idx_sparse = 0, tile_k_idx = 0; tile_k_idx_sparse < K_GLOBAL_SPARSE; tile_k_idx_sparse += BLOCK_SIZE_K_SPARSE, tile_k_idx += BLOCK_SIZE_K){\n\n\t\tsize_t shmem_idx = \n\t\twarpId < (WARPS_PER_BLOCK / 2)\n\t\t\t? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A\n\t\t\t: (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B + shmem_idx_b_off;\n\n\t\tint4 *lane_ptr = NULL;\n\t\tint *lane_ptr_index = NULL;\n\t\tconst uint8_t *warp_ptr = NULL;\n\n\n\t\tif(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\t//warp_ptr = &A[block_tile_j * M] +\n\t\t\t//\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * M_GLOBAL;\n\t\t\twarp_ptr = &A[block_tile_j * M];\n\t\t\t\n\t\t\tconst int *warp_ptr_index = &B_index[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t\t\t\t\t\t((warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A);\n\n\t\t\tlane_ptr_index = (int *)(warp_ptr_index + tile_k_idx_sparse + (laneId / CHUNK_COPY_LINE_LANES_A));\n\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_A) * SHARED_OFFSET_A;\n\t\t}else{\n\t\t\twarp_ptr = &B[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * K_GLOBAL_SPARSE;\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_sparse +\n\t\t\t\t\t\t\t\t(laneId / CHUNK_COPY_LINE_LANES_B) * K_GLOBAL_SPARSE) +\n\t\t\t\t\t\t\t\t(laneId % CHUNK_COPY_LINE_LANES_B);\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_B) * SHARED_OFFSET_B;\n\t\t}\n\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      // shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n\t  int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n\t  \t? BLOCK_SIZE_K_SPARSE / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A)\n\t\t: BLOCK_SIZE_N / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\n\t  /*\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\t  */\n\n\t  /*\n      int tile_k_idx_A;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n          tile_k_idx_A = *(lane_ptr_index);\n      }\n\t  */\n\n\t  #pragma unroll\n\t  for(int i = 0; i < iter_index; i += 1){\n\t\t  if(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\tint tile_k_idx_A = *(lane_ptr_index);\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_A * M_GLOBAL) + (laneId % CHUNK_COPY_LINE_LANES_A);\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_A)) =\n\t\t\t\t*lane_ptr;\n\t\t\t//warp_ptr = (uint8_t *)((uint8_t *)warp_ptr + M_GLOBAL * (WARPS_PER_BLOCK / 2) *CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tlane_ptr_index = (int *)((int *)lane_ptr_index +  (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A;\n\t\t  }else{\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_B)) =\n\t\t\t\t*lane_ptr;\n\t\t\tlane_ptr = (int4 *)((uint8_t *)lane_ptr + K_GLOBAL_SPARSE * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B;\n\t\t  }\n\t  }\n\n      __syncthreads();\n\n\t#pragma unroll\n      for (int k_step = 0; k_step < CHUNK_K_SPARSE; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::col_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n\t#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i += 1) {\n\t\t\tsize_t shmem_idx_a = (warpId % BLOCK_COL_WARPS) * M * WARP_COL_TILES + (i * M);\n\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_a + k_step * K * SHARED_OFFSET_A;\n\n\t\t\twmma::load_matrix_sync(a[i], tile_ptr, SHARED_OFFSET_A);\n\t\t#pragma unroll\n\t\t\tfor(int j = 0; j < WARP_ROW_TILES; j += 1){\n\t\t\t\tif(i == 0){\n\t\t\t\t\tsize_t shmem_idx_b = shmem_idx_b_off +\n\t\t\t\t\t\t\t\t\t\t\t(warpId / BLOCK_COL_WARPS) * (WARP_ROW_TILES * N) * SHARED_OFFSET_B +\n\t\t\t\t\t\t\t\t\t\t\t(j * N) * SHARED_OFFSET_B;\n\t\t\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_b + k_step * K;\n\t\t\t\t\twmma::load_matrix_sync(b[j], tile_ptr, SHARED_OFFSET_B);\n\t\t\t\t}\n\t\t\t\twmma::mma_sync(c[j][i], a[i], b[j], c[j][i]);\n\t\t\t}\n\n        }\n      }\n\n      __syncthreads();\n    }\n\n    // This pointer is used to access the C and D matrix tiles this warp computes.\n\tint *shmem_warp_tile_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * N * WARP_ROW_TILES * SHMEM_STRIDE +\n\t(warpId % BLOCK_COL_WARPS) * SHMEM_OFFSET;\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t#pragma unroll\n\t\t\tfor(int t = 0; t < c[i][j].num_elements; t += 1){\n\t\t\t\tc[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n\t\t\t}\n\t\t\tint *tile_ptr = shmem_warp_tile_ptr + i * N * SHMEM_STRIDE + j * M;\n\t\t\twmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, C_LAYOUT);\n\t\t}\n\t}\n\n    __syncthreads();\n\n\tint *shmem_warp_stream_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N * SHMEM_STRIDE\n\t\t\t\t\t\t\t\t\t+ (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tconst size_t gmem_idx =\n\t\t(block_tile_i * N + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N) * GLOBAL_MEM_STRIDE +\n\t\tblock_tile_j * M + (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tuint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n\tint *shmem_lane_stream_ptr =\n\t\tshmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * SHMEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\t\n\tuint8_t *dst_gmem_lane_stream_ptr =\n\t\tdst_gmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * GLOBAL_MEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\n\tfor(int i = 0; i < WARP_ROW_TILES * N; i += SHARED_TO_GLOBAL_LINES_PER_WARP){\n\t\t*(dst_gmem_lane_stream_ptr + i * GLOBAL_MEM_STRIDE) = (uint8_t)(*(shmem_lane_stream_ptr + i * SHMEM_STRIDE));\n\t}\n\n\t__syncthreads();\n}\n", "launch_config": {"dimBlock": [256, 1], "dimGrid": [256, 1]}}, "131": {"code": "\n__global__ void compute_gemm_imma(float *input0, float *input1, float *input2, float* input3, float* input4, float*input5, float* input6,\n                                  float *output0) {\n    // keys need to be replaced 4096, 4096, K_GLOBA_VALUEL, 0.75, 64\n\n\t//extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B = reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_index = reinterpret_cast<int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n\tconst int WARP_SIZE = 32;\n    const int M = 16;\n    const int K = 16;\n    const int N = 16;\n    const int WMMA_M = 16;\n    const int WMMA_K = 16;\n    const int WMMA_N = 16;\n    const int M_GLOBAL = 4096;\n    const int K_GLOBAL = 1024;\n    const int N_GLOBAL = 4096;\n    const float SPARSITY = 0.75;\n    const int K_GLOBAL_SPARSE = (int(K_GLOBAL * (1-SPARSITY)));\n    const int M_TILES = (M_GLOBAL / M);\n    const int N_TILES = (N_GLOBAL / N);\n    const int K_TILES = (K_GLOBAL / K);\n    const int BLOCK_ROW_WARPS = 2;\n    const int BLOCK_COL_WARPS = 4;\n    const int WARP_ROW_TILES = 4;\n    const int WARP_COL_TILES = 2;\n    const int SKEW_UINT8 = 16;\n    const int BANK_VAL = 32;\n    const int NUM_BANK = (K_GLOBAL / BANK_VAL);\n    /////////// BLOCK_ROW_TILES <= N_TILES ////////////\n    // const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    // ////////// BLOCK_COL_TILES <= M_TILES ////////////\n    // const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    auto C_LAYOUT = wmma::mem_col_major;\n    const int CHUNK_K = 64;\n    const int CHUNK_K_SPARSE = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K_SPARSE = (int((CHUNK_K * K) * (1 - SPARSITY)));\n    const int WARP_COPY_BYTES = (WARP_SIZE * sizeof(int4));\n    const int CHUNK_LINE_BYTES_A (BLOCK_COL_TILES * M * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_A = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_A);\n    const int CHUNK_COPY_LINE_LANES_A = (CHUNK_LINE_BYTES_A / sizeof(int4));\n    const int SHARED_OFFSET_A = (BLOCK_COL_TILES * M + SKEW_UINT8);\n\n    const int CHUNK_LINE_BYTES_B = (BLOCK_SIZE_K_SPARSE * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_B = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_B);\n    const int CHUNK_COPY_LINE_LANES_B = (CHUNK_LINE_BYTES_B / sizeof(int4));\n    const int SHARED_OFFSET_B = (BLOCK_SIZE_K_SPARSE + SKEW_UINT8);\n    const int SHARED_TO_GLOBAL_BYTES_PER_LINE = ((WARP_COL_TILES * M) * sizeof(int));\n    const int SHARED_TO_GLOBAL_BYTES_PER_WARP = (WARP_SIZE * sizeof(int));\n    const int SHARED_TO_GLOBAL_LINES_PER_WARP = (SHARED_TO_GLOBAL_BYTES_PER_WARP / SHARED_TO_GLOBAL_BYTES_PER_LINE);\n    const int SHARED_TO_GLOBAL_LANES_PER_LINE = (WARP_SIZE / SHARED_TO_GLOBAL_LINES_PER_WARP);\n    const int SHARED_TO_GLOBAL_ITERS = ((WARP_ROW_TILES * N) / SHARED_TO_GLOBAL_LINES_PER_WARP);\n\n    const int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const int WARP_STRIDE = (WARP_COL_TILES * M);\n    const int WARPS_PER_BLOCK = (BLOCK_ROW_WARPS * BLOCK_COL_WARPS);\n    const int THREADS_PER_BLOCK = (WARP_SIZE * WARPS_PER_BLOCK);\n    const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    const int GLOBAL_MEM_STRIDE = M_GLOBAL;\n    const int SHMEM_STRIDE = (M * BLOCK_COL_TILES);\n    const int SHMEM_OFFSET = (M * WARP_COL_TILES);\n    const int BLOCK_SIZE_M = (M * BLOCK_COL_TILES);\n    const int BLOCK_SIZE_N = (N * BLOCK_ROW_TILES);\n\n   extern __shared__ uint8_t shmem[];\n\n\t// Warp and lane identification.\n\tconst unsigned int warpId = threadIdx.x / WARP_SIZE;\n\tconst unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n\n\t// Offset in shared memory from which the B matrix is stored.\n\t// const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\tconst size_t shmem_idx_b_off = BLOCK_SIZE_K_SPARSE * SHARED_OFFSET_A;\n\n\n\t// Each CTA slides along the 128 x 128 tiles from the top left corner of the\n\t// matrix to the right and down, and selects the next tile to compute. Once\n\t// there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n\tconst unsigned int block_tile_i =\n\t\t((block_pos * BLOCK_COL_TILES) / M_TILES) * (BLOCK_ROW_TILES);\n\tconst unsigned int block_tile_j = (block_pos * BLOCK_COL_TILES) % M_TILES;\n\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n\twmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_ROW_TILES]\n\t\t\t\t\t\t\t\t\t\t\t\t\t [WARP_COL_TILES];\n\n    // Load the C matrix tiles into fragments from shared memory.\n\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t\twmma::fill_fragment(c[i][j], 0);\n\t\t}\n\t}\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    //for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n    for(int tile_k_idx_sparse = 0, tile_k_idx = 0; tile_k_idx_sparse < K_GLOBAL_SPARSE; tile_k_idx_sparse += BLOCK_SIZE_K_SPARSE, tile_k_idx += BLOCK_SIZE_K){\n\n\t\tsize_t shmem_idx = \n\t\twarpId < (WARPS_PER_BLOCK / 2)\n\t\t\t? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A\n\t\t\t: (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B + shmem_idx_b_off;\n\n\t\tint4 *lane_ptr = NULL;\n\t\tint *lane_ptr_index = NULL;\n\t\tconst uint8_t *warp_ptr = NULL;\n\n\n\t\tif(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\t//warp_ptr = &A[block_tile_j * M] +\n\t\t\t//\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * M_GLOBAL;\n\t\t\twarp_ptr = &A[block_tile_j * M];\n\t\t\t\n\t\t\tconst int *warp_ptr_index = &B_index[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t\t\t\t\t\t((warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A);\n\n\t\t\tlane_ptr_index = (int *)(warp_ptr_index + tile_k_idx_sparse + (laneId / CHUNK_COPY_LINE_LANES_A));\n\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_A) * SHARED_OFFSET_A;\n\t\t}else{\n\t\t\twarp_ptr = &B[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * K_GLOBAL_SPARSE;\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_sparse +\n\t\t\t\t\t\t\t\t(laneId / CHUNK_COPY_LINE_LANES_B) * K_GLOBAL_SPARSE) +\n\t\t\t\t\t\t\t\t(laneId % CHUNK_COPY_LINE_LANES_B);\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_B) * SHARED_OFFSET_B;\n\t\t}\n\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      // shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n\t  int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n\t  \t? BLOCK_SIZE_K_SPARSE / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A)\n\t\t: BLOCK_SIZE_N / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\n\t  /*\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\t  */\n\n\t  /*\n      int tile_k_idx_A;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n          tile_k_idx_A = *(lane_ptr_index);\n      }\n\t  */\n\n\t  #pragma unroll\n\t  for(int i = 0; i < iter_index; i += 1){\n\t\t  if(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\tint tile_k_idx_A = *(lane_ptr_index);\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_A * M_GLOBAL) + (laneId % CHUNK_COPY_LINE_LANES_A);\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_A)) =\n\t\t\t\t*lane_ptr;\n\t\t\t//warp_ptr = (uint8_t *)((uint8_t *)warp_ptr + M_GLOBAL * (WARPS_PER_BLOCK / 2) *CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tlane_ptr_index = (int *)((int *)lane_ptr_index +  (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A;\n\t\t  }else{\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_B)) =\n\t\t\t\t*lane_ptr;\n\t\t\tlane_ptr = (int4 *)((uint8_t *)lane_ptr + K_GLOBAL_SPARSE * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B;\n\t\t  }\n\t  }\n\n      __syncthreads();\n\n\t#pragma unroll\n      for (int k_step = 0; k_step < CHUNK_K_SPARSE; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::col_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n\t#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i += 1) {\n\t\t\tsize_t shmem_idx_a = (warpId % BLOCK_COL_WARPS) * M * WARP_COL_TILES + (i * M);\n\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_a + k_step * K * SHARED_OFFSET_A;\n\n\t\t\twmma::load_matrix_sync(a[i], tile_ptr, SHARED_OFFSET_A);\n\t\t#pragma unroll\n\t\t\tfor(int j = 0; j < WARP_ROW_TILES; j += 1){\n\t\t\t\tif(i == 0){\n\t\t\t\t\tsize_t shmem_idx_b = shmem_idx_b_off +\n\t\t\t\t\t\t\t\t\t\t\t(warpId / BLOCK_COL_WARPS) * (WARP_ROW_TILES * N) * SHARED_OFFSET_B +\n\t\t\t\t\t\t\t\t\t\t\t(j * N) * SHARED_OFFSET_B;\n\t\t\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_b + k_step * K;\n\t\t\t\t\twmma::load_matrix_sync(b[j], tile_ptr, SHARED_OFFSET_B);\n\t\t\t\t}\n\t\t\t\twmma::mma_sync(c[j][i], a[i], b[j], c[j][i]);\n\t\t\t}\n\n        }\n      }\n\n      __syncthreads();\n    }\n\n    // This pointer is used to access the C and D matrix tiles this warp computes.\n\tint *shmem_warp_tile_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * N * WARP_ROW_TILES * SHMEM_STRIDE +\n\t(warpId % BLOCK_COL_WARPS) * SHMEM_OFFSET;\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t#pragma unroll\n\t\t\tfor(int t = 0; t < c[i][j].num_elements; t += 1){\n\t\t\t\tc[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n\t\t\t}\n\t\t\tint *tile_ptr = shmem_warp_tile_ptr + i * N * SHMEM_STRIDE + j * M;\n\t\t\twmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, C_LAYOUT);\n\t\t}\n\t}\n\n    __syncthreads();\n\n\tint *shmem_warp_stream_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N * SHMEM_STRIDE\n\t\t\t\t\t\t\t\t\t+ (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tconst size_t gmem_idx =\n\t\t(block_tile_i * N + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N) * GLOBAL_MEM_STRIDE +\n\t\tblock_tile_j * M + (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tuint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n\tint *shmem_lane_stream_ptr =\n\t\tshmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * SHMEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\t\n\tuint8_t *dst_gmem_lane_stream_ptr =\n\t\tdst_gmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * GLOBAL_MEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\n\tfor(int i = 0; i < WARP_ROW_TILES * N; i += SHARED_TO_GLOBAL_LINES_PER_WARP){\n\t\t*(dst_gmem_lane_stream_ptr + i * GLOBAL_MEM_STRIDE) = (uint8_t)(*(shmem_lane_stream_ptr + i * SHMEM_STRIDE));\n\t}\n\n\t__syncthreads();\n}\n", "launch_config": {"dimBlock": [1024, 1], "dimGrid": [256, 1]}}, "132": {"code": "\n__global__ void compute_gemm_imma(float *input0, float *input1, float *input2, float* input3, float* input4, float*input5, float* input6,\n                                  float *output0) {\n    // keys need to be replaced 4096, 1024, K_GLOBA_VALUEL, 0.75, 64\n\n\t//extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B = reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_index = reinterpret_cast<int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n\tconst int WARP_SIZE = 32;\n    const int M = 16;\n    const int K = 16;\n    const int N = 16;\n    const int WMMA_M = 16;\n    const int WMMA_K = 16;\n    const int WMMA_N = 16;\n    const int M_GLOBAL = 4096;\n    const int K_GLOBAL = 4096;\n    const int N_GLOBAL = 1024;\n    const float SPARSITY = 0.75;\n    const int K_GLOBAL_SPARSE = (int(K_GLOBAL * (1-SPARSITY)));\n    const int M_TILES = (M_GLOBAL / M);\n    const int N_TILES = (N_GLOBAL / N);\n    const int K_TILES = (K_GLOBAL / K);\n    const int BLOCK_ROW_WARPS = 2;\n    const int BLOCK_COL_WARPS = 4;\n    const int WARP_ROW_TILES = 4;\n    const int WARP_COL_TILES = 2;\n    const int SKEW_UINT8 = 16;\n    const int BANK_VAL = 32;\n    const int NUM_BANK = (K_GLOBAL / BANK_VAL);\n    /////////// BLOCK_ROW_TILES <= N_TILES ////////////\n    // const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    // ////////// BLOCK_COL_TILES <= M_TILES ////////////\n    // const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    auto C_LAYOUT = wmma::mem_col_major;\n    const int CHUNK_K = 64;\n    const int CHUNK_K_SPARSE = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K_SPARSE = (int((CHUNK_K * K) * (1 - SPARSITY)));\n    const int WARP_COPY_BYTES = (WARP_SIZE * sizeof(int4));\n    const int CHUNK_LINE_BYTES_A (BLOCK_COL_TILES * M * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_A = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_A);\n    const int CHUNK_COPY_LINE_LANES_A = (CHUNK_LINE_BYTES_A / sizeof(int4));\n    const int SHARED_OFFSET_A = (BLOCK_COL_TILES * M + SKEW_UINT8);\n\n    const int CHUNK_LINE_BYTES_B = (BLOCK_SIZE_K_SPARSE * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_B = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_B);\n    const int CHUNK_COPY_LINE_LANES_B = (CHUNK_LINE_BYTES_B / sizeof(int4));\n    const int SHARED_OFFSET_B = (BLOCK_SIZE_K_SPARSE + SKEW_UINT8);\n    const int SHARED_TO_GLOBAL_BYTES_PER_LINE = ((WARP_COL_TILES * M) * sizeof(int));\n    const int SHARED_TO_GLOBAL_BYTES_PER_WARP = (WARP_SIZE * sizeof(int));\n    const int SHARED_TO_GLOBAL_LINES_PER_WARP = (SHARED_TO_GLOBAL_BYTES_PER_WARP / SHARED_TO_GLOBAL_BYTES_PER_LINE);\n    const int SHARED_TO_GLOBAL_LANES_PER_LINE = (WARP_SIZE / SHARED_TO_GLOBAL_LINES_PER_WARP);\n    const int SHARED_TO_GLOBAL_ITERS = ((WARP_ROW_TILES * N) / SHARED_TO_GLOBAL_LINES_PER_WARP);\n\n    const int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const int WARP_STRIDE = (WARP_COL_TILES * M);\n    const int WARPS_PER_BLOCK = (BLOCK_ROW_WARPS * BLOCK_COL_WARPS);\n    const int THREADS_PER_BLOCK = (WARP_SIZE * WARPS_PER_BLOCK);\n    const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    const int GLOBAL_MEM_STRIDE = M_GLOBAL;\n    const int SHMEM_STRIDE = (M * BLOCK_COL_TILES);\n    const int SHMEM_OFFSET = (M * WARP_COL_TILES);\n    const int BLOCK_SIZE_M = (M * BLOCK_COL_TILES);\n    const int BLOCK_SIZE_N = (N * BLOCK_ROW_TILES);\n\n   extern __shared__ uint8_t shmem[];\n\n\t// Warp and lane identification.\n\tconst unsigned int warpId = threadIdx.x / WARP_SIZE;\n\tconst unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n\n\t// Offset in shared memory from which the B matrix is stored.\n\t// const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\tconst size_t shmem_idx_b_off = BLOCK_SIZE_K_SPARSE * SHARED_OFFSET_A;\n\n\n\t// Each CTA slides along the 128 x 128 tiles from the top left corner of the\n\t// matrix to the right and down, and selects the next tile to compute. Once\n\t// there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n\tconst unsigned int block_tile_i =\n\t\t((block_pos * BLOCK_COL_TILES) / M_TILES) * (BLOCK_ROW_TILES);\n\tconst unsigned int block_tile_j = (block_pos * BLOCK_COL_TILES) % M_TILES;\n\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n\twmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_ROW_TILES]\n\t\t\t\t\t\t\t\t\t\t\t\t\t [WARP_COL_TILES];\n\n    // Load the C matrix tiles into fragments from shared memory.\n\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t\twmma::fill_fragment(c[i][j], 0);\n\t\t}\n\t}\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    //for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n    for(int tile_k_idx_sparse = 0, tile_k_idx = 0; tile_k_idx_sparse < K_GLOBAL_SPARSE; tile_k_idx_sparse += BLOCK_SIZE_K_SPARSE, tile_k_idx += BLOCK_SIZE_K){\n\n\t\tsize_t shmem_idx = \n\t\twarpId < (WARPS_PER_BLOCK / 2)\n\t\t\t? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A\n\t\t\t: (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B + shmem_idx_b_off;\n\n\t\tint4 *lane_ptr = NULL;\n\t\tint *lane_ptr_index = NULL;\n\t\tconst uint8_t *warp_ptr = NULL;\n\n\n\t\tif(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\t//warp_ptr = &A[block_tile_j * M] +\n\t\t\t//\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * M_GLOBAL;\n\t\t\twarp_ptr = &A[block_tile_j * M];\n\t\t\t\n\t\t\tconst int *warp_ptr_index = &B_index[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t\t\t\t\t\t((warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A);\n\n\t\t\tlane_ptr_index = (int *)(warp_ptr_index + tile_k_idx_sparse + (laneId / CHUNK_COPY_LINE_LANES_A));\n\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_A) * SHARED_OFFSET_A;\n\t\t}else{\n\t\t\twarp_ptr = &B[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * K_GLOBAL_SPARSE;\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_sparse +\n\t\t\t\t\t\t\t\t(laneId / CHUNK_COPY_LINE_LANES_B) * K_GLOBAL_SPARSE) +\n\t\t\t\t\t\t\t\t(laneId % CHUNK_COPY_LINE_LANES_B);\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_B) * SHARED_OFFSET_B;\n\t\t}\n\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      // shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n\t  int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n\t  \t? BLOCK_SIZE_K_SPARSE / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A)\n\t\t: BLOCK_SIZE_N / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\n\t  /*\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\t  */\n\n\t  /*\n      int tile_k_idx_A;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n          tile_k_idx_A = *(lane_ptr_index);\n      }\n\t  */\n\n\t  #pragma unroll\n\t  for(int i = 0; i < iter_index; i += 1){\n\t\t  if(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\tint tile_k_idx_A = *(lane_ptr_index);\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_A * M_GLOBAL) + (laneId % CHUNK_COPY_LINE_LANES_A);\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_A)) =\n\t\t\t\t*lane_ptr;\n\t\t\t//warp_ptr = (uint8_t *)((uint8_t *)warp_ptr + M_GLOBAL * (WARPS_PER_BLOCK / 2) *CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tlane_ptr_index = (int *)((int *)lane_ptr_index +  (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A;\n\t\t  }else{\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_B)) =\n\t\t\t\t*lane_ptr;\n\t\t\tlane_ptr = (int4 *)((uint8_t *)lane_ptr + K_GLOBAL_SPARSE * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B;\n\t\t  }\n\t  }\n\n      __syncthreads();\n\n\t#pragma unroll\n      for (int k_step = 0; k_step < CHUNK_K_SPARSE; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::col_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n\t#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i += 1) {\n\t\t\tsize_t shmem_idx_a = (warpId % BLOCK_COL_WARPS) * M * WARP_COL_TILES + (i * M);\n\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_a + k_step * K * SHARED_OFFSET_A;\n\n\t\t\twmma::load_matrix_sync(a[i], tile_ptr, SHARED_OFFSET_A);\n\t\t#pragma unroll\n\t\t\tfor(int j = 0; j < WARP_ROW_TILES; j += 1){\n\t\t\t\tif(i == 0){\n\t\t\t\t\tsize_t shmem_idx_b = shmem_idx_b_off +\n\t\t\t\t\t\t\t\t\t\t\t(warpId / BLOCK_COL_WARPS) * (WARP_ROW_TILES * N) * SHARED_OFFSET_B +\n\t\t\t\t\t\t\t\t\t\t\t(j * N) * SHARED_OFFSET_B;\n\t\t\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_b + k_step * K;\n\t\t\t\t\twmma::load_matrix_sync(b[j], tile_ptr, SHARED_OFFSET_B);\n\t\t\t\t}\n\t\t\t\twmma::mma_sync(c[j][i], a[i], b[j], c[j][i]);\n\t\t\t}\n\n        }\n      }\n\n      __syncthreads();\n    }\n\n    // This pointer is used to access the C and D matrix tiles this warp computes.\n\tint *shmem_warp_tile_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * N * WARP_ROW_TILES * SHMEM_STRIDE +\n\t(warpId % BLOCK_COL_WARPS) * SHMEM_OFFSET;\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t#pragma unroll\n\t\t\tfor(int t = 0; t < c[i][j].num_elements; t += 1){\n\t\t\t\tc[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n\t\t\t}\n\t\t\tint *tile_ptr = shmem_warp_tile_ptr + i * N * SHMEM_STRIDE + j * M;\n\t\t\twmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, C_LAYOUT);\n\t\t}\n\t}\n\n    __syncthreads();\n\n\tint *shmem_warp_stream_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N * SHMEM_STRIDE\n\t\t\t\t\t\t\t\t\t+ (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tconst size_t gmem_idx =\n\t\t(block_tile_i * N + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N) * GLOBAL_MEM_STRIDE +\n\t\tblock_tile_j * M + (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tuint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n\tint *shmem_lane_stream_ptr =\n\t\tshmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * SHMEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\t\n\tuint8_t *dst_gmem_lane_stream_ptr =\n\t\tdst_gmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * GLOBAL_MEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\n\tfor(int i = 0; i < WARP_ROW_TILES * N; i += SHARED_TO_GLOBAL_LINES_PER_WARP){\n\t\t*(dst_gmem_lane_stream_ptr + i * GLOBAL_MEM_STRIDE) = (uint8_t)(*(shmem_lane_stream_ptr + i * SHMEM_STRIDE));\n\t}\n\n\t__syncthreads();\n}\n", "launch_config": {"dimBlock": [256, 1], "dimGrid": [256, 1]}}, "133": {"code": "\n__global__ void compute_gemm_imma(float *input0, float *input1, float *input2, float* input3, float* input4, float*input5, float* input6,\n                                  float *output0) {\n    // keys need to be replaced 4096, 1024, K_GLOBA_VALUEL, 0.75, 64\n\n\t//extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B = reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_index = reinterpret_cast<int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n\tconst int WARP_SIZE = 32;\n    const int M = 16;\n    const int K = 16;\n    const int N = 16;\n    const int WMMA_M = 16;\n    const int WMMA_K = 16;\n    const int WMMA_N = 16;\n    const int M_GLOBAL = 4096;\n    const int K_GLOBAL = 1024;\n    const int N_GLOBAL = 1024;\n    const float SPARSITY = 0.75;\n    const int K_GLOBAL_SPARSE = (int(K_GLOBAL * (1-SPARSITY)));\n    const int M_TILES = (M_GLOBAL / M);\n    const int N_TILES = (N_GLOBAL / N);\n    const int K_TILES = (K_GLOBAL / K);\n    const int BLOCK_ROW_WARPS = 2;\n    const int BLOCK_COL_WARPS = 4;\n    const int WARP_ROW_TILES = 4;\n    const int WARP_COL_TILES = 2;\n    const int SKEW_UINT8 = 16;\n    const int BANK_VAL = 32;\n    const int NUM_BANK = (K_GLOBAL / BANK_VAL);\n    /////////// BLOCK_ROW_TILES <= N_TILES ////////////\n    // const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    // ////////// BLOCK_COL_TILES <= M_TILES ////////////\n    // const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    auto C_LAYOUT = wmma::mem_col_major;\n    const int CHUNK_K = 64;\n    const int CHUNK_K_SPARSE = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K_SPARSE = (int((CHUNK_K * K) * (1 - SPARSITY)));\n    const int WARP_COPY_BYTES = (WARP_SIZE * sizeof(int4));\n    const int CHUNK_LINE_BYTES_A (BLOCK_COL_TILES * M * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_A = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_A);\n    const int CHUNK_COPY_LINE_LANES_A = (CHUNK_LINE_BYTES_A / sizeof(int4));\n    const int SHARED_OFFSET_A = (BLOCK_COL_TILES * M + SKEW_UINT8);\n\n    const int CHUNK_LINE_BYTES_B = (BLOCK_SIZE_K_SPARSE * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_B = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_B);\n    const int CHUNK_COPY_LINE_LANES_B = (CHUNK_LINE_BYTES_B / sizeof(int4));\n    const int SHARED_OFFSET_B = (BLOCK_SIZE_K_SPARSE + SKEW_UINT8);\n    const int SHARED_TO_GLOBAL_BYTES_PER_LINE = ((WARP_COL_TILES * M) * sizeof(int));\n    const int SHARED_TO_GLOBAL_BYTES_PER_WARP = (WARP_SIZE * sizeof(int));\n    const int SHARED_TO_GLOBAL_LINES_PER_WARP = (SHARED_TO_GLOBAL_BYTES_PER_WARP / SHARED_TO_GLOBAL_BYTES_PER_LINE);\n    const int SHARED_TO_GLOBAL_LANES_PER_LINE = (WARP_SIZE / SHARED_TO_GLOBAL_LINES_PER_WARP);\n    const int SHARED_TO_GLOBAL_ITERS = ((WARP_ROW_TILES * N) / SHARED_TO_GLOBAL_LINES_PER_WARP);\n\n    const int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const int WARP_STRIDE = (WARP_COL_TILES * M);\n    const int WARPS_PER_BLOCK = (BLOCK_ROW_WARPS * BLOCK_COL_WARPS);\n    const int THREADS_PER_BLOCK = (WARP_SIZE * WARPS_PER_BLOCK);\n    const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    const int GLOBAL_MEM_STRIDE = M_GLOBAL;\n    const int SHMEM_STRIDE = (M * BLOCK_COL_TILES);\n    const int SHMEM_OFFSET = (M * WARP_COL_TILES);\n    const int BLOCK_SIZE_M = (M * BLOCK_COL_TILES);\n    const int BLOCK_SIZE_N = (N * BLOCK_ROW_TILES);\n\n   extern __shared__ uint8_t shmem[];\n\n\t// Warp and lane identification.\n\tconst unsigned int warpId = threadIdx.x / WARP_SIZE;\n\tconst unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n\n\t// Offset in shared memory from which the B matrix is stored.\n\t// const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\tconst size_t shmem_idx_b_off = BLOCK_SIZE_K_SPARSE * SHARED_OFFSET_A;\n\n\n\t// Each CTA slides along the 128 x 128 tiles from the top left corner of the\n\t// matrix to the right and down, and selects the next tile to compute. Once\n\t// there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n\tconst unsigned int block_tile_i =\n\t\t((block_pos * BLOCK_COL_TILES) / M_TILES) * (BLOCK_ROW_TILES);\n\tconst unsigned int block_tile_j = (block_pos * BLOCK_COL_TILES) % M_TILES;\n\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n\twmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_ROW_TILES]\n\t\t\t\t\t\t\t\t\t\t\t\t\t [WARP_COL_TILES];\n\n    // Load the C matrix tiles into fragments from shared memory.\n\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t\twmma::fill_fragment(c[i][j], 0);\n\t\t}\n\t}\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    //for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n    for(int tile_k_idx_sparse = 0, tile_k_idx = 0; tile_k_idx_sparse < K_GLOBAL_SPARSE; tile_k_idx_sparse += BLOCK_SIZE_K_SPARSE, tile_k_idx += BLOCK_SIZE_K){\n\n\t\tsize_t shmem_idx = \n\t\twarpId < (WARPS_PER_BLOCK / 2)\n\t\t\t? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A\n\t\t\t: (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B + shmem_idx_b_off;\n\n\t\tint4 *lane_ptr = NULL;\n\t\tint *lane_ptr_index = NULL;\n\t\tconst uint8_t *warp_ptr = NULL;\n\n\n\t\tif(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\t//warp_ptr = &A[block_tile_j * M] +\n\t\t\t//\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * M_GLOBAL;\n\t\t\twarp_ptr = &A[block_tile_j * M];\n\t\t\t\n\t\t\tconst int *warp_ptr_index = &B_index[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t\t\t\t\t\t((warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A);\n\n\t\t\tlane_ptr_index = (int *)(warp_ptr_index + tile_k_idx_sparse + (laneId / CHUNK_COPY_LINE_LANES_A));\n\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_A) * SHARED_OFFSET_A;\n\t\t}else{\n\t\t\twarp_ptr = &B[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * K_GLOBAL_SPARSE;\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_sparse +\n\t\t\t\t\t\t\t\t(laneId / CHUNK_COPY_LINE_LANES_B) * K_GLOBAL_SPARSE) +\n\t\t\t\t\t\t\t\t(laneId % CHUNK_COPY_LINE_LANES_B);\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_B) * SHARED_OFFSET_B;\n\t\t}\n\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      // shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n\t  int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n\t  \t? BLOCK_SIZE_K_SPARSE / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A)\n\t\t: BLOCK_SIZE_N / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\n\t  /*\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\t  */\n\n\t  /*\n      int tile_k_idx_A;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n          tile_k_idx_A = *(lane_ptr_index);\n      }\n\t  */\n\n\t  #pragma unroll\n\t  for(int i = 0; i < iter_index; i += 1){\n\t\t  if(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\tint tile_k_idx_A = *(lane_ptr_index);\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_A * M_GLOBAL) + (laneId % CHUNK_COPY_LINE_LANES_A);\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_A)) =\n\t\t\t\t*lane_ptr;\n\t\t\t//warp_ptr = (uint8_t *)((uint8_t *)warp_ptr + M_GLOBAL * (WARPS_PER_BLOCK / 2) *CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tlane_ptr_index = (int *)((int *)lane_ptr_index +  (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A;\n\t\t  }else{\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_B)) =\n\t\t\t\t*lane_ptr;\n\t\t\tlane_ptr = (int4 *)((uint8_t *)lane_ptr + K_GLOBAL_SPARSE * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B;\n\t\t  }\n\t  }\n\n      __syncthreads();\n\n\t#pragma unroll\n      for (int k_step = 0; k_step < CHUNK_K_SPARSE; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::col_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n\t#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i += 1) {\n\t\t\tsize_t shmem_idx_a = (warpId % BLOCK_COL_WARPS) * M * WARP_COL_TILES + (i * M);\n\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_a + k_step * K * SHARED_OFFSET_A;\n\n\t\t\twmma::load_matrix_sync(a[i], tile_ptr, SHARED_OFFSET_A);\n\t\t#pragma unroll\n\t\t\tfor(int j = 0; j < WARP_ROW_TILES; j += 1){\n\t\t\t\tif(i == 0){\n\t\t\t\t\tsize_t shmem_idx_b = shmem_idx_b_off +\n\t\t\t\t\t\t\t\t\t\t\t(warpId / BLOCK_COL_WARPS) * (WARP_ROW_TILES * N) * SHARED_OFFSET_B +\n\t\t\t\t\t\t\t\t\t\t\t(j * N) * SHARED_OFFSET_B;\n\t\t\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_b + k_step * K;\n\t\t\t\t\twmma::load_matrix_sync(b[j], tile_ptr, SHARED_OFFSET_B);\n\t\t\t\t}\n\t\t\t\twmma::mma_sync(c[j][i], a[i], b[j], c[j][i]);\n\t\t\t}\n\n        }\n      }\n\n      __syncthreads();\n    }\n\n    // This pointer is used to access the C and D matrix tiles this warp computes.\n\tint *shmem_warp_tile_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * N * WARP_ROW_TILES * SHMEM_STRIDE +\n\t(warpId % BLOCK_COL_WARPS) * SHMEM_OFFSET;\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t#pragma unroll\n\t\t\tfor(int t = 0; t < c[i][j].num_elements; t += 1){\n\t\t\t\tc[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n\t\t\t}\n\t\t\tint *tile_ptr = shmem_warp_tile_ptr + i * N * SHMEM_STRIDE + j * M;\n\t\t\twmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, C_LAYOUT);\n\t\t}\n\t}\n\n    __syncthreads();\n\n\tint *shmem_warp_stream_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N * SHMEM_STRIDE\n\t\t\t\t\t\t\t\t\t+ (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tconst size_t gmem_idx =\n\t\t(block_tile_i * N + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N) * GLOBAL_MEM_STRIDE +\n\t\tblock_tile_j * M + (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tuint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n\tint *shmem_lane_stream_ptr =\n\t\tshmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * SHMEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\t\n\tuint8_t *dst_gmem_lane_stream_ptr =\n\t\tdst_gmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * GLOBAL_MEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\n\tfor(int i = 0; i < WARP_ROW_TILES * N; i += SHARED_TO_GLOBAL_LINES_PER_WARP){\n\t\t*(dst_gmem_lane_stream_ptr + i * GLOBAL_MEM_STRIDE) = (uint8_t)(*(shmem_lane_stream_ptr + i * SHMEM_STRIDE));\n\t}\n\n\t__syncthreads();\n}\n", "launch_config": {"dimBlock": [256, 1], "dimGrid": [256, 1]}}, "134": {"code": "\n__global__ void compute_gemm_imma(float *input0, float *input1, float *input2, float* input3, float* input4, float*input5, float* input6,\n                                  float *output0) {\n    // keys need to be replaced 4096, 1024, K_GLOBA_VALUEL, 0.75, 64\n\n\t//extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B = reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_index = reinterpret_cast<int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n\tconst int WARP_SIZE = 32;\n    const int M = 16;\n    const int K = 16;\n    const int N = 16;\n    const int WMMA_M = 16;\n    const int WMMA_K = 16;\n    const int WMMA_N = 16;\n    const int M_GLOBAL = 4096;\n    const int K_GLOBAL = 1024;\n    const int N_GLOBAL = 1024;\n    const float SPARSITY = 0.75;\n    const int K_GLOBAL_SPARSE = (int(K_GLOBAL * (1-SPARSITY)));\n    const int M_TILES = (M_GLOBAL / M);\n    const int N_TILES = (N_GLOBAL / N);\n    const int K_TILES = (K_GLOBAL / K);\n    const int BLOCK_ROW_WARPS = 2;\n    const int BLOCK_COL_WARPS = 4;\n    const int WARP_ROW_TILES = 4;\n    const int WARP_COL_TILES = 2;\n    const int SKEW_UINT8 = 16;\n    const int BANK_VAL = 32;\n    const int NUM_BANK = (K_GLOBAL / BANK_VAL);\n    /////////// BLOCK_ROW_TILES <= N_TILES ////////////\n    // const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    // ////////// BLOCK_COL_TILES <= M_TILES ////////////\n    // const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    auto C_LAYOUT = wmma::mem_col_major;\n    const int CHUNK_K = 64;\n    const int CHUNK_K_SPARSE = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K_SPARSE = (int((CHUNK_K * K) * (1 - SPARSITY)));\n    const int WARP_COPY_BYTES = (WARP_SIZE * sizeof(int4));\n    const int CHUNK_LINE_BYTES_A (BLOCK_COL_TILES * M * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_A = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_A);\n    const int CHUNK_COPY_LINE_LANES_A = (CHUNK_LINE_BYTES_A / sizeof(int4));\n    const int SHARED_OFFSET_A = (BLOCK_COL_TILES * M + SKEW_UINT8);\n\n    const int CHUNK_LINE_BYTES_B = (BLOCK_SIZE_K_SPARSE * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_B = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_B);\n    const int CHUNK_COPY_LINE_LANES_B = (CHUNK_LINE_BYTES_B / sizeof(int4));\n    const int SHARED_OFFSET_B = (BLOCK_SIZE_K_SPARSE + SKEW_UINT8);\n    const int SHARED_TO_GLOBAL_BYTES_PER_LINE = ((WARP_COL_TILES * M) * sizeof(int));\n    const int SHARED_TO_GLOBAL_BYTES_PER_WARP = (WARP_SIZE * sizeof(int));\n    const int SHARED_TO_GLOBAL_LINES_PER_WARP = (SHARED_TO_GLOBAL_BYTES_PER_WARP / SHARED_TO_GLOBAL_BYTES_PER_LINE);\n    const int SHARED_TO_GLOBAL_LANES_PER_LINE = (WARP_SIZE / SHARED_TO_GLOBAL_LINES_PER_WARP);\n    const int SHARED_TO_GLOBAL_ITERS = ((WARP_ROW_TILES * N) / SHARED_TO_GLOBAL_LINES_PER_WARP);\n\n    const int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const int WARP_STRIDE = (WARP_COL_TILES * M);\n    const int WARPS_PER_BLOCK = (BLOCK_ROW_WARPS * BLOCK_COL_WARPS);\n    const int THREADS_PER_BLOCK = (WARP_SIZE * WARPS_PER_BLOCK);\n    const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    const int GLOBAL_MEM_STRIDE = M_GLOBAL;\n    const int SHMEM_STRIDE = (M * BLOCK_COL_TILES);\n    const int SHMEM_OFFSET = (M * WARP_COL_TILES);\n    const int BLOCK_SIZE_M = (M * BLOCK_COL_TILES);\n    const int BLOCK_SIZE_N = (N * BLOCK_ROW_TILES);\n\n   extern __shared__ uint8_t shmem[];\n\n\t// Warp and lane identification.\n\tconst unsigned int warpId = threadIdx.x / WARP_SIZE;\n\tconst unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n\n\t// Offset in shared memory from which the B matrix is stored.\n\t// const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\tconst size_t shmem_idx_b_off = BLOCK_SIZE_K_SPARSE * SHARED_OFFSET_A;\n\n\n\t// Each CTA slides along the 128 x 128 tiles from the top left corner of the\n\t// matrix to the right and down, and selects the next tile to compute. Once\n\t// there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n\tconst unsigned int block_tile_i =\n\t\t((block_pos * BLOCK_COL_TILES) / M_TILES) * (BLOCK_ROW_TILES);\n\tconst unsigned int block_tile_j = (block_pos * BLOCK_COL_TILES) % M_TILES;\n\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n\twmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_ROW_TILES]\n\t\t\t\t\t\t\t\t\t\t\t\t\t [WARP_COL_TILES];\n\n    // Load the C matrix tiles into fragments from shared memory.\n\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t\twmma::fill_fragment(c[i][j], 0);\n\t\t}\n\t}\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    //for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n    for(int tile_k_idx_sparse = 0, tile_k_idx = 0; tile_k_idx_sparse < K_GLOBAL_SPARSE; tile_k_idx_sparse += BLOCK_SIZE_K_SPARSE, tile_k_idx += BLOCK_SIZE_K){\n\n\t\tsize_t shmem_idx = \n\t\twarpId < (WARPS_PER_BLOCK / 2)\n\t\t\t? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A\n\t\t\t: (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B + shmem_idx_b_off;\n\n\t\tint4 *lane_ptr = NULL;\n\t\tint *lane_ptr_index = NULL;\n\t\tconst uint8_t *warp_ptr = NULL;\n\n\n\t\tif(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\t//warp_ptr = &A[block_tile_j * M] +\n\t\t\t//\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * M_GLOBAL;\n\t\t\twarp_ptr = &A[block_tile_j * M];\n\t\t\t\n\t\t\tconst int *warp_ptr_index = &B_index[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t\t\t\t\t\t((warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A);\n\n\t\t\tlane_ptr_index = (int *)(warp_ptr_index + tile_k_idx_sparse + (laneId / CHUNK_COPY_LINE_LANES_A));\n\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_A) * SHARED_OFFSET_A;\n\t\t}else{\n\t\t\twarp_ptr = &B[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * K_GLOBAL_SPARSE;\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_sparse +\n\t\t\t\t\t\t\t\t(laneId / CHUNK_COPY_LINE_LANES_B) * K_GLOBAL_SPARSE) +\n\t\t\t\t\t\t\t\t(laneId % CHUNK_COPY_LINE_LANES_B);\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_B) * SHARED_OFFSET_B;\n\t\t}\n\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      // shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n\t  int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n\t  \t? BLOCK_SIZE_K_SPARSE / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A)\n\t\t: BLOCK_SIZE_N / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\n\t  /*\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\t  */\n\n\t  /*\n      int tile_k_idx_A;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n          tile_k_idx_A = *(lane_ptr_index);\n      }\n\t  */\n\n\t  #pragma unroll\n\t  for(int i = 0; i < iter_index; i += 1){\n\t\t  if(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\tint tile_k_idx_A = *(lane_ptr_index);\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_A * M_GLOBAL) + (laneId % CHUNK_COPY_LINE_LANES_A);\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_A)) =\n\t\t\t\t*lane_ptr;\n\t\t\t//warp_ptr = (uint8_t *)((uint8_t *)warp_ptr + M_GLOBAL * (WARPS_PER_BLOCK / 2) *CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tlane_ptr_index = (int *)((int *)lane_ptr_index +  (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A;\n\t\t  }else{\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_B)) =\n\t\t\t\t*lane_ptr;\n\t\t\tlane_ptr = (int4 *)((uint8_t *)lane_ptr + K_GLOBAL_SPARSE * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B;\n\t\t  }\n\t  }\n\n      __syncthreads();\n\n\t#pragma unroll\n      for (int k_step = 0; k_step < CHUNK_K_SPARSE; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::col_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n\t#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i += 1) {\n\t\t\tsize_t shmem_idx_a = (warpId % BLOCK_COL_WARPS) * M * WARP_COL_TILES + (i * M);\n\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_a + k_step * K * SHARED_OFFSET_A;\n\n\t\t\twmma::load_matrix_sync(a[i], tile_ptr, SHARED_OFFSET_A);\n\t\t#pragma unroll\n\t\t\tfor(int j = 0; j < WARP_ROW_TILES; j += 1){\n\t\t\t\tif(i == 0){\n\t\t\t\t\tsize_t shmem_idx_b = shmem_idx_b_off +\n\t\t\t\t\t\t\t\t\t\t\t(warpId / BLOCK_COL_WARPS) * (WARP_ROW_TILES * N) * SHARED_OFFSET_B +\n\t\t\t\t\t\t\t\t\t\t\t(j * N) * SHARED_OFFSET_B;\n\t\t\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_b + k_step * K;\n\t\t\t\t\twmma::load_matrix_sync(b[j], tile_ptr, SHARED_OFFSET_B);\n\t\t\t\t}\n\t\t\t\twmma::mma_sync(c[j][i], a[i], b[j], c[j][i]);\n\t\t\t}\n\n        }\n      }\n\n      __syncthreads();\n    }\n\n    // This pointer is used to access the C and D matrix tiles this warp computes.\n\tint *shmem_warp_tile_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * N * WARP_ROW_TILES * SHMEM_STRIDE +\n\t(warpId % BLOCK_COL_WARPS) * SHMEM_OFFSET;\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t#pragma unroll\n\t\t\tfor(int t = 0; t < c[i][j].num_elements; t += 1){\n\t\t\t\tc[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n\t\t\t}\n\t\t\tint *tile_ptr = shmem_warp_tile_ptr + i * N * SHMEM_STRIDE + j * M;\n\t\t\twmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, C_LAYOUT);\n\t\t}\n\t}\n\n    __syncthreads();\n\n\tint *shmem_warp_stream_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N * SHMEM_STRIDE\n\t\t\t\t\t\t\t\t\t+ (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tconst size_t gmem_idx =\n\t\t(block_tile_i * N + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N) * GLOBAL_MEM_STRIDE +\n\t\tblock_tile_j * M + (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tuint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n\tint *shmem_lane_stream_ptr =\n\t\tshmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * SHMEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\t\n\tuint8_t *dst_gmem_lane_stream_ptr =\n\t\tdst_gmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * GLOBAL_MEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\n\tfor(int i = 0; i < WARP_ROW_TILES * N; i += SHARED_TO_GLOBAL_LINES_PER_WARP){\n\t\t*(dst_gmem_lane_stream_ptr + i * GLOBAL_MEM_STRIDE) = (uint8_t)(*(shmem_lane_stream_ptr + i * SHMEM_STRIDE));\n\t}\n\n\t__syncthreads();\n}\n", "launch_config": {"dimBlock": [256, 1], "dimGrid": [256, 1]}}, "135": {"code": "\n__global__ void compute_gemm_imma(float *input0, float *input1, float *input2, float* input3, float* input4, float*input5, float* input6,\n                                  float *output0) {\n    // keys need to be replaced 4096, 1024, K_GLOBA_VALUEL, 0.75, 64\n\n\t//extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B = reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_index = reinterpret_cast<int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n\tconst int WARP_SIZE = 32;\n    const int M = 16;\n    const int K = 16;\n    const int N = 16;\n    const int WMMA_M = 16;\n    const int WMMA_K = 16;\n    const int WMMA_N = 16;\n    const int M_GLOBAL = 4096;\n    const int K_GLOBAL = 1024;\n    const int N_GLOBAL = 1024;\n    const float SPARSITY = 0.75;\n    const int K_GLOBAL_SPARSE = (int(K_GLOBAL * (1-SPARSITY)));\n    const int M_TILES = (M_GLOBAL / M);\n    const int N_TILES = (N_GLOBAL / N);\n    const int K_TILES = (K_GLOBAL / K);\n    const int BLOCK_ROW_WARPS = 2;\n    const int BLOCK_COL_WARPS = 4;\n    const int WARP_ROW_TILES = 4;\n    const int WARP_COL_TILES = 2;\n    const int SKEW_UINT8 = 16;\n    const int BANK_VAL = 32;\n    const int NUM_BANK = (K_GLOBAL / BANK_VAL);\n    /////////// BLOCK_ROW_TILES <= N_TILES ////////////\n    // const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    // ////////// BLOCK_COL_TILES <= M_TILES ////////////\n    // const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    auto C_LAYOUT = wmma::mem_col_major;\n    const int CHUNK_K = 64;\n    const int CHUNK_K_SPARSE = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K_SPARSE = (int((CHUNK_K * K) * (1 - SPARSITY)));\n    const int WARP_COPY_BYTES = (WARP_SIZE * sizeof(int4));\n    const int CHUNK_LINE_BYTES_A (BLOCK_COL_TILES * M * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_A = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_A);\n    const int CHUNK_COPY_LINE_LANES_A = (CHUNK_LINE_BYTES_A / sizeof(int4));\n    const int SHARED_OFFSET_A = (BLOCK_COL_TILES * M + SKEW_UINT8);\n\n    const int CHUNK_LINE_BYTES_B = (BLOCK_SIZE_K_SPARSE * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_B = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_B);\n    const int CHUNK_COPY_LINE_LANES_B = (CHUNK_LINE_BYTES_B / sizeof(int4));\n    const int SHARED_OFFSET_B = (BLOCK_SIZE_K_SPARSE + SKEW_UINT8);\n    const int SHARED_TO_GLOBAL_BYTES_PER_LINE = ((WARP_COL_TILES * M) * sizeof(int));\n    const int SHARED_TO_GLOBAL_BYTES_PER_WARP = (WARP_SIZE * sizeof(int));\n    const int SHARED_TO_GLOBAL_LINES_PER_WARP = (SHARED_TO_GLOBAL_BYTES_PER_WARP / SHARED_TO_GLOBAL_BYTES_PER_LINE);\n    const int SHARED_TO_GLOBAL_LANES_PER_LINE = (WARP_SIZE / SHARED_TO_GLOBAL_LINES_PER_WARP);\n    const int SHARED_TO_GLOBAL_ITERS = ((WARP_ROW_TILES * N) / SHARED_TO_GLOBAL_LINES_PER_WARP);\n\n    const int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const int WARP_STRIDE = (WARP_COL_TILES * M);\n    const int WARPS_PER_BLOCK = (BLOCK_ROW_WARPS * BLOCK_COL_WARPS);\n    const int THREADS_PER_BLOCK = (WARP_SIZE * WARPS_PER_BLOCK);\n    const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    const int GLOBAL_MEM_STRIDE = M_GLOBAL;\n    const int SHMEM_STRIDE = (M * BLOCK_COL_TILES);\n    const int SHMEM_OFFSET = (M * WARP_COL_TILES);\n    const int BLOCK_SIZE_M = (M * BLOCK_COL_TILES);\n    const int BLOCK_SIZE_N = (N * BLOCK_ROW_TILES);\n\n   extern __shared__ uint8_t shmem[];\n\n\t// Warp and lane identification.\n\tconst unsigned int warpId = threadIdx.x / WARP_SIZE;\n\tconst unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n\n\t// Offset in shared memory from which the B matrix is stored.\n\t// const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\tconst size_t shmem_idx_b_off = BLOCK_SIZE_K_SPARSE * SHARED_OFFSET_A;\n\n\n\t// Each CTA slides along the 128 x 128 tiles from the top left corner of the\n\t// matrix to the right and down, and selects the next tile to compute. Once\n\t// there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n\tconst unsigned int block_tile_i =\n\t\t((block_pos * BLOCK_COL_TILES) / M_TILES) * (BLOCK_ROW_TILES);\n\tconst unsigned int block_tile_j = (block_pos * BLOCK_COL_TILES) % M_TILES;\n\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n\twmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_ROW_TILES]\n\t\t\t\t\t\t\t\t\t\t\t\t\t [WARP_COL_TILES];\n\n    // Load the C matrix tiles into fragments from shared memory.\n\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t\twmma::fill_fragment(c[i][j], 0);\n\t\t}\n\t}\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    //for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n    for(int tile_k_idx_sparse = 0, tile_k_idx = 0; tile_k_idx_sparse < K_GLOBAL_SPARSE; tile_k_idx_sparse += BLOCK_SIZE_K_SPARSE, tile_k_idx += BLOCK_SIZE_K){\n\n\t\tsize_t shmem_idx = \n\t\twarpId < (WARPS_PER_BLOCK / 2)\n\t\t\t? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A\n\t\t\t: (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B + shmem_idx_b_off;\n\n\t\tint4 *lane_ptr = NULL;\n\t\tint *lane_ptr_index = NULL;\n\t\tconst uint8_t *warp_ptr = NULL;\n\n\n\t\tif(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\t//warp_ptr = &A[block_tile_j * M] +\n\t\t\t//\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * M_GLOBAL;\n\t\t\twarp_ptr = &A[block_tile_j * M];\n\t\t\t\n\t\t\tconst int *warp_ptr_index = &B_index[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t\t\t\t\t\t((warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A);\n\n\t\t\tlane_ptr_index = (int *)(warp_ptr_index + tile_k_idx_sparse + (laneId / CHUNK_COPY_LINE_LANES_A));\n\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_A) * SHARED_OFFSET_A;\n\t\t}else{\n\t\t\twarp_ptr = &B[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * K_GLOBAL_SPARSE;\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_sparse +\n\t\t\t\t\t\t\t\t(laneId / CHUNK_COPY_LINE_LANES_B) * K_GLOBAL_SPARSE) +\n\t\t\t\t\t\t\t\t(laneId % CHUNK_COPY_LINE_LANES_B);\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_B) * SHARED_OFFSET_B;\n\t\t}\n\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      // shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n\t  int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n\t  \t? BLOCK_SIZE_K_SPARSE / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A)\n\t\t: BLOCK_SIZE_N / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\n\t  /*\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\t  */\n\n\t  /*\n      int tile_k_idx_A;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n          tile_k_idx_A = *(lane_ptr_index);\n      }\n\t  */\n\n\t  #pragma unroll\n\t  for(int i = 0; i < iter_index; i += 1){\n\t\t  if(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\tint tile_k_idx_A = *(lane_ptr_index);\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_A * M_GLOBAL) + (laneId % CHUNK_COPY_LINE_LANES_A);\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_A)) =\n\t\t\t\t*lane_ptr;\n\t\t\t//warp_ptr = (uint8_t *)((uint8_t *)warp_ptr + M_GLOBAL * (WARPS_PER_BLOCK / 2) *CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tlane_ptr_index = (int *)((int *)lane_ptr_index +  (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A;\n\t\t  }else{\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_B)) =\n\t\t\t\t*lane_ptr;\n\t\t\tlane_ptr = (int4 *)((uint8_t *)lane_ptr + K_GLOBAL_SPARSE * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B;\n\t\t  }\n\t  }\n\n      __syncthreads();\n\n\t#pragma unroll\n      for (int k_step = 0; k_step < CHUNK_K_SPARSE; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::col_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n\t#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i += 1) {\n\t\t\tsize_t shmem_idx_a = (warpId % BLOCK_COL_WARPS) * M * WARP_COL_TILES + (i * M);\n\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_a + k_step * K * SHARED_OFFSET_A;\n\n\t\t\twmma::load_matrix_sync(a[i], tile_ptr, SHARED_OFFSET_A);\n\t\t#pragma unroll\n\t\t\tfor(int j = 0; j < WARP_ROW_TILES; j += 1){\n\t\t\t\tif(i == 0){\n\t\t\t\t\tsize_t shmem_idx_b = shmem_idx_b_off +\n\t\t\t\t\t\t\t\t\t\t\t(warpId / BLOCK_COL_WARPS) * (WARP_ROW_TILES * N) * SHARED_OFFSET_B +\n\t\t\t\t\t\t\t\t\t\t\t(j * N) * SHARED_OFFSET_B;\n\t\t\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_b + k_step * K;\n\t\t\t\t\twmma::load_matrix_sync(b[j], tile_ptr, SHARED_OFFSET_B);\n\t\t\t\t}\n\t\t\t\twmma::mma_sync(c[j][i], a[i], b[j], c[j][i]);\n\t\t\t}\n\n        }\n      }\n\n      __syncthreads();\n    }\n\n    // This pointer is used to access the C and D matrix tiles this warp computes.\n\tint *shmem_warp_tile_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * N * WARP_ROW_TILES * SHMEM_STRIDE +\n\t(warpId % BLOCK_COL_WARPS) * SHMEM_OFFSET;\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t#pragma unroll\n\t\t\tfor(int t = 0; t < c[i][j].num_elements; t += 1){\n\t\t\t\tc[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n\t\t\t}\n\t\t\tint *tile_ptr = shmem_warp_tile_ptr + i * N * SHMEM_STRIDE + j * M;\n\t\t\twmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, C_LAYOUT);\n\t\t}\n\t}\n\n    __syncthreads();\n\n\tint *shmem_warp_stream_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N * SHMEM_STRIDE\n\t\t\t\t\t\t\t\t\t+ (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tconst size_t gmem_idx =\n\t\t(block_tile_i * N + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N) * GLOBAL_MEM_STRIDE +\n\t\tblock_tile_j * M + (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tuint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n\tint *shmem_lane_stream_ptr =\n\t\tshmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * SHMEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\t\n\tuint8_t *dst_gmem_lane_stream_ptr =\n\t\tdst_gmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * GLOBAL_MEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\n\tfor(int i = 0; i < WARP_ROW_TILES * N; i += SHARED_TO_GLOBAL_LINES_PER_WARP){\n\t\t*(dst_gmem_lane_stream_ptr + i * GLOBAL_MEM_STRIDE) = (uint8_t)(*(shmem_lane_stream_ptr + i * SHMEM_STRIDE));\n\t}\n\n\t__syncthreads();\n}\n", "launch_config": {"dimBlock": [256, 1], "dimGrid": [256, 1]}}, "136": {"code": "\n__global__ void compute_gemm_imma(float *input0, float *input1, float *input2, float* input3, float* input4, float*input5, float* input6,\n                                  float *output0) {\n    // keys need to be replaced 4096, 1024, K_GLOBA_VALUEL, 0.75, 64\n\n\t//extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B = reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_index = reinterpret_cast<int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n\tconst int WARP_SIZE = 32;\n    const int M = 16;\n    const int K = 16;\n    const int N = 16;\n    const int WMMA_M = 16;\n    const int WMMA_K = 16;\n    const int WMMA_N = 16;\n    const int M_GLOBAL = 4096;\n    const int K_GLOBAL = 1024;\n    const int N_GLOBAL = 1024;\n    const float SPARSITY = 0.75;\n    const int K_GLOBAL_SPARSE = (int(K_GLOBAL * (1-SPARSITY)));\n    const int M_TILES = (M_GLOBAL / M);\n    const int N_TILES = (N_GLOBAL / N);\n    const int K_TILES = (K_GLOBAL / K);\n    const int BLOCK_ROW_WARPS = 2;\n    const int BLOCK_COL_WARPS = 4;\n    const int WARP_ROW_TILES = 4;\n    const int WARP_COL_TILES = 2;\n    const int SKEW_UINT8 = 16;\n    const int BANK_VAL = 32;\n    const int NUM_BANK = (K_GLOBAL / BANK_VAL);\n    /////////// BLOCK_ROW_TILES <= N_TILES ////////////\n    // const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    // ////////// BLOCK_COL_TILES <= M_TILES ////////////\n    // const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    auto C_LAYOUT = wmma::mem_col_major;\n    const int CHUNK_K = 64;\n    const int CHUNK_K_SPARSE = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K_SPARSE = (int((CHUNK_K * K) * (1 - SPARSITY)));\n    const int WARP_COPY_BYTES = (WARP_SIZE * sizeof(int4));\n    const int CHUNK_LINE_BYTES_A (BLOCK_COL_TILES * M * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_A = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_A);\n    const int CHUNK_COPY_LINE_LANES_A = (CHUNK_LINE_BYTES_A / sizeof(int4));\n    const int SHARED_OFFSET_A = (BLOCK_COL_TILES * M + SKEW_UINT8);\n\n    const int CHUNK_LINE_BYTES_B = (BLOCK_SIZE_K_SPARSE * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_B = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_B);\n    const int CHUNK_COPY_LINE_LANES_B = (CHUNK_LINE_BYTES_B / sizeof(int4));\n    const int SHARED_OFFSET_B = (BLOCK_SIZE_K_SPARSE + SKEW_UINT8);\n    const int SHARED_TO_GLOBAL_BYTES_PER_LINE = ((WARP_COL_TILES * M) * sizeof(int));\n    const int SHARED_TO_GLOBAL_BYTES_PER_WARP = (WARP_SIZE * sizeof(int));\n    const int SHARED_TO_GLOBAL_LINES_PER_WARP = (SHARED_TO_GLOBAL_BYTES_PER_WARP / SHARED_TO_GLOBAL_BYTES_PER_LINE);\n    const int SHARED_TO_GLOBAL_LANES_PER_LINE = (WARP_SIZE / SHARED_TO_GLOBAL_LINES_PER_WARP);\n    const int SHARED_TO_GLOBAL_ITERS = ((WARP_ROW_TILES * N) / SHARED_TO_GLOBAL_LINES_PER_WARP);\n\n    const int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const int WARP_STRIDE = (WARP_COL_TILES * M);\n    const int WARPS_PER_BLOCK = (BLOCK_ROW_WARPS * BLOCK_COL_WARPS);\n    const int THREADS_PER_BLOCK = (WARP_SIZE * WARPS_PER_BLOCK);\n    const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    const int GLOBAL_MEM_STRIDE = M_GLOBAL;\n    const int SHMEM_STRIDE = (M * BLOCK_COL_TILES);\n    const int SHMEM_OFFSET = (M * WARP_COL_TILES);\n    const int BLOCK_SIZE_M = (M * BLOCK_COL_TILES);\n    const int BLOCK_SIZE_N = (N * BLOCK_ROW_TILES);\n\n   extern __shared__ uint8_t shmem[];\n\n\t// Warp and lane identification.\n\tconst unsigned int warpId = threadIdx.x / WARP_SIZE;\n\tconst unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n\n\t// Offset in shared memory from which the B matrix is stored.\n\t// const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\tconst size_t shmem_idx_b_off = BLOCK_SIZE_K_SPARSE * SHARED_OFFSET_A;\n\n\n\t// Each CTA slides along the 128 x 128 tiles from the top left corner of the\n\t// matrix to the right and down, and selects the next tile to compute. Once\n\t// there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n\tconst unsigned int block_tile_i =\n\t\t((block_pos * BLOCK_COL_TILES) / M_TILES) * (BLOCK_ROW_TILES);\n\tconst unsigned int block_tile_j = (block_pos * BLOCK_COL_TILES) % M_TILES;\n\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n\twmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_ROW_TILES]\n\t\t\t\t\t\t\t\t\t\t\t\t\t [WARP_COL_TILES];\n\n    // Load the C matrix tiles into fragments from shared memory.\n\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t\twmma::fill_fragment(c[i][j], 0);\n\t\t}\n\t}\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    //for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n    for(int tile_k_idx_sparse = 0, tile_k_idx = 0; tile_k_idx_sparse < K_GLOBAL_SPARSE; tile_k_idx_sparse += BLOCK_SIZE_K_SPARSE, tile_k_idx += BLOCK_SIZE_K){\n\n\t\tsize_t shmem_idx = \n\t\twarpId < (WARPS_PER_BLOCK / 2)\n\t\t\t? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A\n\t\t\t: (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B + shmem_idx_b_off;\n\n\t\tint4 *lane_ptr = NULL;\n\t\tint *lane_ptr_index = NULL;\n\t\tconst uint8_t *warp_ptr = NULL;\n\n\n\t\tif(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\t//warp_ptr = &A[block_tile_j * M] +\n\t\t\t//\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * M_GLOBAL;\n\t\t\twarp_ptr = &A[block_tile_j * M];\n\t\t\t\n\t\t\tconst int *warp_ptr_index = &B_index[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t\t\t\t\t\t((warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A);\n\n\t\t\tlane_ptr_index = (int *)(warp_ptr_index + tile_k_idx_sparse + (laneId / CHUNK_COPY_LINE_LANES_A));\n\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_A) * SHARED_OFFSET_A;\n\t\t}else{\n\t\t\twarp_ptr = &B[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * K_GLOBAL_SPARSE;\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_sparse +\n\t\t\t\t\t\t\t\t(laneId / CHUNK_COPY_LINE_LANES_B) * K_GLOBAL_SPARSE) +\n\t\t\t\t\t\t\t\t(laneId % CHUNK_COPY_LINE_LANES_B);\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_B) * SHARED_OFFSET_B;\n\t\t}\n\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      // shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n\t  int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n\t  \t? BLOCK_SIZE_K_SPARSE / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A)\n\t\t: BLOCK_SIZE_N / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\n\t  /*\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\t  */\n\n\t  /*\n      int tile_k_idx_A;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n          tile_k_idx_A = *(lane_ptr_index);\n      }\n\t  */\n\n\t  #pragma unroll\n\t  for(int i = 0; i < iter_index; i += 1){\n\t\t  if(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\tint tile_k_idx_A = *(lane_ptr_index);\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_A * M_GLOBAL) + (laneId % CHUNK_COPY_LINE_LANES_A);\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_A)) =\n\t\t\t\t*lane_ptr;\n\t\t\t//warp_ptr = (uint8_t *)((uint8_t *)warp_ptr + M_GLOBAL * (WARPS_PER_BLOCK / 2) *CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tlane_ptr_index = (int *)((int *)lane_ptr_index +  (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A;\n\t\t  }else{\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_B)) =\n\t\t\t\t*lane_ptr;\n\t\t\tlane_ptr = (int4 *)((uint8_t *)lane_ptr + K_GLOBAL_SPARSE * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B;\n\t\t  }\n\t  }\n\n      __syncthreads();\n\n\t#pragma unroll\n      for (int k_step = 0; k_step < CHUNK_K_SPARSE; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::col_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n\t#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i += 1) {\n\t\t\tsize_t shmem_idx_a = (warpId % BLOCK_COL_WARPS) * M * WARP_COL_TILES + (i * M);\n\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_a + k_step * K * SHARED_OFFSET_A;\n\n\t\t\twmma::load_matrix_sync(a[i], tile_ptr, SHARED_OFFSET_A);\n\t\t#pragma unroll\n\t\t\tfor(int j = 0; j < WARP_ROW_TILES; j += 1){\n\t\t\t\tif(i == 0){\n\t\t\t\t\tsize_t shmem_idx_b = shmem_idx_b_off +\n\t\t\t\t\t\t\t\t\t\t\t(warpId / BLOCK_COL_WARPS) * (WARP_ROW_TILES * N) * SHARED_OFFSET_B +\n\t\t\t\t\t\t\t\t\t\t\t(j * N) * SHARED_OFFSET_B;\n\t\t\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_b + k_step * K;\n\t\t\t\t\twmma::load_matrix_sync(b[j], tile_ptr, SHARED_OFFSET_B);\n\t\t\t\t}\n\t\t\t\twmma::mma_sync(c[j][i], a[i], b[j], c[j][i]);\n\t\t\t}\n\n        }\n      }\n\n      __syncthreads();\n    }\n\n    // This pointer is used to access the C and D matrix tiles this warp computes.\n\tint *shmem_warp_tile_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * N * WARP_ROW_TILES * SHMEM_STRIDE +\n\t(warpId % BLOCK_COL_WARPS) * SHMEM_OFFSET;\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t#pragma unroll\n\t\t\tfor(int t = 0; t < c[i][j].num_elements; t += 1){\n\t\t\t\tc[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n\t\t\t}\n\t\t\tint *tile_ptr = shmem_warp_tile_ptr + i * N * SHMEM_STRIDE + j * M;\n\t\t\twmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, C_LAYOUT);\n\t\t}\n\t}\n\n    __syncthreads();\n\n\tint *shmem_warp_stream_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N * SHMEM_STRIDE\n\t\t\t\t\t\t\t\t\t+ (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tconst size_t gmem_idx =\n\t\t(block_tile_i * N + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N) * GLOBAL_MEM_STRIDE +\n\t\tblock_tile_j * M + (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tuint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n\tint *shmem_lane_stream_ptr =\n\t\tshmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * SHMEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\t\n\tuint8_t *dst_gmem_lane_stream_ptr =\n\t\tdst_gmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * GLOBAL_MEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\n\tfor(int i = 0; i < WARP_ROW_TILES * N; i += SHARED_TO_GLOBAL_LINES_PER_WARP){\n\t\t*(dst_gmem_lane_stream_ptr + i * GLOBAL_MEM_STRIDE) = (uint8_t)(*(shmem_lane_stream_ptr + i * SHMEM_STRIDE));\n\t}\n\n\t__syncthreads();\n}\n", "launch_config": {"dimBlock": [256, 1], "dimGrid": [256, 1]}}, "137": {"code": "\n__global__ void compute_gemm_imma(float *input0, float *input1, float *input2, float* input3, float* input4, float*input5, float* input6,\n                                  float *output0) {\n    // keys need to be replaced 4096, 4096, K_GLOBA_VALUEL, 0.75, 64\n\n\t//extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B = reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_index = reinterpret_cast<int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n\tconst int WARP_SIZE = 32;\n    const int M = 16;\n    const int K = 16;\n    const int N = 16;\n    const int WMMA_M = 16;\n    const int WMMA_K = 16;\n    const int WMMA_N = 16;\n    const int M_GLOBAL = 4096;\n    const int K_GLOBAL = 1024;\n    const int N_GLOBAL = 4096;\n    const float SPARSITY = 0.75;\n    const int K_GLOBAL_SPARSE = (int(K_GLOBAL * (1-SPARSITY)));\n    const int M_TILES = (M_GLOBAL / M);\n    const int N_TILES = (N_GLOBAL / N);\n    const int K_TILES = (K_GLOBAL / K);\n    const int BLOCK_ROW_WARPS = 2;\n    const int BLOCK_COL_WARPS = 4;\n    const int WARP_ROW_TILES = 4;\n    const int WARP_COL_TILES = 2;\n    const int SKEW_UINT8 = 16;\n    const int BANK_VAL = 32;\n    const int NUM_BANK = (K_GLOBAL / BANK_VAL);\n    /////////// BLOCK_ROW_TILES <= N_TILES ////////////\n    // const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    // ////////// BLOCK_COL_TILES <= M_TILES ////////////\n    // const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    auto C_LAYOUT = wmma::mem_col_major;\n    const int CHUNK_K = 64;\n    const int CHUNK_K_SPARSE = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K_SPARSE = (int((CHUNK_K * K) * (1 - SPARSITY)));\n    const int WARP_COPY_BYTES = (WARP_SIZE * sizeof(int4));\n    const int CHUNK_LINE_BYTES_A (BLOCK_COL_TILES * M * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_A = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_A);\n    const int CHUNK_COPY_LINE_LANES_A = (CHUNK_LINE_BYTES_A / sizeof(int4));\n    const int SHARED_OFFSET_A = (BLOCK_COL_TILES * M + SKEW_UINT8);\n\n    const int CHUNK_LINE_BYTES_B = (BLOCK_SIZE_K_SPARSE * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_B = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_B);\n    const int CHUNK_COPY_LINE_LANES_B = (CHUNK_LINE_BYTES_B / sizeof(int4));\n    const int SHARED_OFFSET_B = (BLOCK_SIZE_K_SPARSE + SKEW_UINT8);\n    const int SHARED_TO_GLOBAL_BYTES_PER_LINE = ((WARP_COL_TILES * M) * sizeof(int));\n    const int SHARED_TO_GLOBAL_BYTES_PER_WARP = (WARP_SIZE * sizeof(int));\n    const int SHARED_TO_GLOBAL_LINES_PER_WARP = (SHARED_TO_GLOBAL_BYTES_PER_WARP / SHARED_TO_GLOBAL_BYTES_PER_LINE);\n    const int SHARED_TO_GLOBAL_LANES_PER_LINE = (WARP_SIZE / SHARED_TO_GLOBAL_LINES_PER_WARP);\n    const int SHARED_TO_GLOBAL_ITERS = ((WARP_ROW_TILES * N) / SHARED_TO_GLOBAL_LINES_PER_WARP);\n\n    const int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const int WARP_STRIDE = (WARP_COL_TILES * M);\n    const int WARPS_PER_BLOCK = (BLOCK_ROW_WARPS * BLOCK_COL_WARPS);\n    const int THREADS_PER_BLOCK = (WARP_SIZE * WARPS_PER_BLOCK);\n    const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    const int GLOBAL_MEM_STRIDE = M_GLOBAL;\n    const int SHMEM_STRIDE = (M * BLOCK_COL_TILES);\n    const int SHMEM_OFFSET = (M * WARP_COL_TILES);\n    const int BLOCK_SIZE_M = (M * BLOCK_COL_TILES);\n    const int BLOCK_SIZE_N = (N * BLOCK_ROW_TILES);\n\n   extern __shared__ uint8_t shmem[];\n\n\t// Warp and lane identification.\n\tconst unsigned int warpId = threadIdx.x / WARP_SIZE;\n\tconst unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n\n\t// Offset in shared memory from which the B matrix is stored.\n\t// const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\tconst size_t shmem_idx_b_off = BLOCK_SIZE_K_SPARSE * SHARED_OFFSET_A;\n\n\n\t// Each CTA slides along the 128 x 128 tiles from the top left corner of the\n\t// matrix to the right and down, and selects the next tile to compute. Once\n\t// there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n\tconst unsigned int block_tile_i =\n\t\t((block_pos * BLOCK_COL_TILES) / M_TILES) * (BLOCK_ROW_TILES);\n\tconst unsigned int block_tile_j = (block_pos * BLOCK_COL_TILES) % M_TILES;\n\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n\twmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_ROW_TILES]\n\t\t\t\t\t\t\t\t\t\t\t\t\t [WARP_COL_TILES];\n\n    // Load the C matrix tiles into fragments from shared memory.\n\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t\twmma::fill_fragment(c[i][j], 0);\n\t\t}\n\t}\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    //for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n    for(int tile_k_idx_sparse = 0, tile_k_idx = 0; tile_k_idx_sparse < K_GLOBAL_SPARSE; tile_k_idx_sparse += BLOCK_SIZE_K_SPARSE, tile_k_idx += BLOCK_SIZE_K){\n\n\t\tsize_t shmem_idx = \n\t\twarpId < (WARPS_PER_BLOCK / 2)\n\t\t\t? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A\n\t\t\t: (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B + shmem_idx_b_off;\n\n\t\tint4 *lane_ptr = NULL;\n\t\tint *lane_ptr_index = NULL;\n\t\tconst uint8_t *warp_ptr = NULL;\n\n\n\t\tif(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\t//warp_ptr = &A[block_tile_j * M] +\n\t\t\t//\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * M_GLOBAL;\n\t\t\twarp_ptr = &A[block_tile_j * M];\n\t\t\t\n\t\t\tconst int *warp_ptr_index = &B_index[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t\t\t\t\t\t((warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A);\n\n\t\t\tlane_ptr_index = (int *)(warp_ptr_index + tile_k_idx_sparse + (laneId / CHUNK_COPY_LINE_LANES_A));\n\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_A) * SHARED_OFFSET_A;\n\t\t}else{\n\t\t\twarp_ptr = &B[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * K_GLOBAL_SPARSE;\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_sparse +\n\t\t\t\t\t\t\t\t(laneId / CHUNK_COPY_LINE_LANES_B) * K_GLOBAL_SPARSE) +\n\t\t\t\t\t\t\t\t(laneId % CHUNK_COPY_LINE_LANES_B);\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_B) * SHARED_OFFSET_B;\n\t\t}\n\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      // shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n\t  int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n\t  \t? BLOCK_SIZE_K_SPARSE / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A)\n\t\t: BLOCK_SIZE_N / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\n\t  /*\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\t  */\n\n\t  /*\n      int tile_k_idx_A;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n          tile_k_idx_A = *(lane_ptr_index);\n      }\n\t  */\n\n\t  #pragma unroll\n\t  for(int i = 0; i < iter_index; i += 1){\n\t\t  if(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\tint tile_k_idx_A = *(lane_ptr_index);\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_A * M_GLOBAL) + (laneId % CHUNK_COPY_LINE_LANES_A);\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_A)) =\n\t\t\t\t*lane_ptr;\n\t\t\t//warp_ptr = (uint8_t *)((uint8_t *)warp_ptr + M_GLOBAL * (WARPS_PER_BLOCK / 2) *CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tlane_ptr_index = (int *)((int *)lane_ptr_index +  (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A;\n\t\t  }else{\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_B)) =\n\t\t\t\t*lane_ptr;\n\t\t\tlane_ptr = (int4 *)((uint8_t *)lane_ptr + K_GLOBAL_SPARSE * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B;\n\t\t  }\n\t  }\n\n      __syncthreads();\n\n\t#pragma unroll\n      for (int k_step = 0; k_step < CHUNK_K_SPARSE; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::col_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n\t#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i += 1) {\n\t\t\tsize_t shmem_idx_a = (warpId % BLOCK_COL_WARPS) * M * WARP_COL_TILES + (i * M);\n\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_a + k_step * K * SHARED_OFFSET_A;\n\n\t\t\twmma::load_matrix_sync(a[i], tile_ptr, SHARED_OFFSET_A);\n\t\t#pragma unroll\n\t\t\tfor(int j = 0; j < WARP_ROW_TILES; j += 1){\n\t\t\t\tif(i == 0){\n\t\t\t\t\tsize_t shmem_idx_b = shmem_idx_b_off +\n\t\t\t\t\t\t\t\t\t\t\t(warpId / BLOCK_COL_WARPS) * (WARP_ROW_TILES * N) * SHARED_OFFSET_B +\n\t\t\t\t\t\t\t\t\t\t\t(j * N) * SHARED_OFFSET_B;\n\t\t\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_b + k_step * K;\n\t\t\t\t\twmma::load_matrix_sync(b[j], tile_ptr, SHARED_OFFSET_B);\n\t\t\t\t}\n\t\t\t\twmma::mma_sync(c[j][i], a[i], b[j], c[j][i]);\n\t\t\t}\n\n        }\n      }\n\n      __syncthreads();\n    }\n\n    // This pointer is used to access the C and D matrix tiles this warp computes.\n\tint *shmem_warp_tile_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * N * WARP_ROW_TILES * SHMEM_STRIDE +\n\t(warpId % BLOCK_COL_WARPS) * SHMEM_OFFSET;\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t#pragma unroll\n\t\t\tfor(int t = 0; t < c[i][j].num_elements; t += 1){\n\t\t\t\tc[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n\t\t\t}\n\t\t\tint *tile_ptr = shmem_warp_tile_ptr + i * N * SHMEM_STRIDE + j * M;\n\t\t\twmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, C_LAYOUT);\n\t\t}\n\t}\n\n    __syncthreads();\n\n\tint *shmem_warp_stream_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N * SHMEM_STRIDE\n\t\t\t\t\t\t\t\t\t+ (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tconst size_t gmem_idx =\n\t\t(block_tile_i * N + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N) * GLOBAL_MEM_STRIDE +\n\t\tblock_tile_j * M + (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tuint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n\tint *shmem_lane_stream_ptr =\n\t\tshmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * SHMEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\t\n\tuint8_t *dst_gmem_lane_stream_ptr =\n\t\tdst_gmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * GLOBAL_MEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\n\tfor(int i = 0; i < WARP_ROW_TILES * N; i += SHARED_TO_GLOBAL_LINES_PER_WARP){\n\t\t*(dst_gmem_lane_stream_ptr + i * GLOBAL_MEM_STRIDE) = (uint8_t)(*(shmem_lane_stream_ptr + i * SHMEM_STRIDE));\n\t}\n\n\t__syncthreads();\n}\n", "launch_config": {"dimBlock": [1024, 1], "dimGrid": [256, 1]}}, "138": {"code": "\n__global__ void compute_gemm_imma(float *input0, float *input1, float *input2, float* input3, float* input4, float*input5, float* input6,\n                                  float *output0) {\n    // keys need to be replaced 4096, 1024, K_GLOBA_VALUEL, 0.75, 64\n\n\t//extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B = reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_index = reinterpret_cast<int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n\tconst int WARP_SIZE = 32;\n    const int M = 16;\n    const int K = 16;\n    const int N = 16;\n    const int WMMA_M = 16;\n    const int WMMA_K = 16;\n    const int WMMA_N = 16;\n    const int M_GLOBAL = 4096;\n    const int K_GLOBAL = 4096;\n    const int N_GLOBAL = 1024;\n    const float SPARSITY = 0.75;\n    const int K_GLOBAL_SPARSE = (int(K_GLOBAL * (1-SPARSITY)));\n    const int M_TILES = (M_GLOBAL / M);\n    const int N_TILES = (N_GLOBAL / N);\n    const int K_TILES = (K_GLOBAL / K);\n    const int BLOCK_ROW_WARPS = 2;\n    const int BLOCK_COL_WARPS = 4;\n    const int WARP_ROW_TILES = 4;\n    const int WARP_COL_TILES = 2;\n    const int SKEW_UINT8 = 16;\n    const int BANK_VAL = 32;\n    const int NUM_BANK = (K_GLOBAL / BANK_VAL);\n    /////////// BLOCK_ROW_TILES <= N_TILES ////////////\n    // const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    // ////////// BLOCK_COL_TILES <= M_TILES ////////////\n    // const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    auto C_LAYOUT = wmma::mem_col_major;\n    const int CHUNK_K = 64;\n    const int CHUNK_K_SPARSE = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K_SPARSE = (int((CHUNK_K * K) * (1 - SPARSITY)));\n    const int WARP_COPY_BYTES = (WARP_SIZE * sizeof(int4));\n    const int CHUNK_LINE_BYTES_A (BLOCK_COL_TILES * M * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_A = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_A);\n    const int CHUNK_COPY_LINE_LANES_A = (CHUNK_LINE_BYTES_A / sizeof(int4));\n    const int SHARED_OFFSET_A = (BLOCK_COL_TILES * M + SKEW_UINT8);\n\n    const int CHUNK_LINE_BYTES_B = (BLOCK_SIZE_K_SPARSE * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_B = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_B);\n    const int CHUNK_COPY_LINE_LANES_B = (CHUNK_LINE_BYTES_B / sizeof(int4));\n    const int SHARED_OFFSET_B = (BLOCK_SIZE_K_SPARSE + SKEW_UINT8);\n    const int SHARED_TO_GLOBAL_BYTES_PER_LINE = ((WARP_COL_TILES * M) * sizeof(int));\n    const int SHARED_TO_GLOBAL_BYTES_PER_WARP = (WARP_SIZE * sizeof(int));\n    const int SHARED_TO_GLOBAL_LINES_PER_WARP = (SHARED_TO_GLOBAL_BYTES_PER_WARP / SHARED_TO_GLOBAL_BYTES_PER_LINE);\n    const int SHARED_TO_GLOBAL_LANES_PER_LINE = (WARP_SIZE / SHARED_TO_GLOBAL_LINES_PER_WARP);\n    const int SHARED_TO_GLOBAL_ITERS = ((WARP_ROW_TILES * N) / SHARED_TO_GLOBAL_LINES_PER_WARP);\n\n    const int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const int WARP_STRIDE = (WARP_COL_TILES * M);\n    const int WARPS_PER_BLOCK = (BLOCK_ROW_WARPS * BLOCK_COL_WARPS);\n    const int THREADS_PER_BLOCK = (WARP_SIZE * WARPS_PER_BLOCK);\n    const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    const int GLOBAL_MEM_STRIDE = M_GLOBAL;\n    const int SHMEM_STRIDE = (M * BLOCK_COL_TILES);\n    const int SHMEM_OFFSET = (M * WARP_COL_TILES);\n    const int BLOCK_SIZE_M = (M * BLOCK_COL_TILES);\n    const int BLOCK_SIZE_N = (N * BLOCK_ROW_TILES);\n\n   extern __shared__ uint8_t shmem[];\n\n\t// Warp and lane identification.\n\tconst unsigned int warpId = threadIdx.x / WARP_SIZE;\n\tconst unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n\n\t// Offset in shared memory from which the B matrix is stored.\n\t// const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\tconst size_t shmem_idx_b_off = BLOCK_SIZE_K_SPARSE * SHARED_OFFSET_A;\n\n\n\t// Each CTA slides along the 128 x 128 tiles from the top left corner of the\n\t// matrix to the right and down, and selects the next tile to compute. Once\n\t// there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n\tconst unsigned int block_tile_i =\n\t\t((block_pos * BLOCK_COL_TILES) / M_TILES) * (BLOCK_ROW_TILES);\n\tconst unsigned int block_tile_j = (block_pos * BLOCK_COL_TILES) % M_TILES;\n\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n\twmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_ROW_TILES]\n\t\t\t\t\t\t\t\t\t\t\t\t\t [WARP_COL_TILES];\n\n    // Load the C matrix tiles into fragments from shared memory.\n\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t\twmma::fill_fragment(c[i][j], 0);\n\t\t}\n\t}\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    //for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n    for(int tile_k_idx_sparse = 0, tile_k_idx = 0; tile_k_idx_sparse < K_GLOBAL_SPARSE; tile_k_idx_sparse += BLOCK_SIZE_K_SPARSE, tile_k_idx += BLOCK_SIZE_K){\n\n\t\tsize_t shmem_idx = \n\t\twarpId < (WARPS_PER_BLOCK / 2)\n\t\t\t? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A\n\t\t\t: (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B + shmem_idx_b_off;\n\n\t\tint4 *lane_ptr = NULL;\n\t\tint *lane_ptr_index = NULL;\n\t\tconst uint8_t *warp_ptr = NULL;\n\n\n\t\tif(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\t//warp_ptr = &A[block_tile_j * M] +\n\t\t\t//\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * M_GLOBAL;\n\t\t\twarp_ptr = &A[block_tile_j * M];\n\t\t\t\n\t\t\tconst int *warp_ptr_index = &B_index[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t\t\t\t\t\t((warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A);\n\n\t\t\tlane_ptr_index = (int *)(warp_ptr_index + tile_k_idx_sparse + (laneId / CHUNK_COPY_LINE_LANES_A));\n\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_A) * SHARED_OFFSET_A;\n\t\t}else{\n\t\t\twarp_ptr = &B[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * K_GLOBAL_SPARSE;\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_sparse +\n\t\t\t\t\t\t\t\t(laneId / CHUNK_COPY_LINE_LANES_B) * K_GLOBAL_SPARSE) +\n\t\t\t\t\t\t\t\t(laneId % CHUNK_COPY_LINE_LANES_B);\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_B) * SHARED_OFFSET_B;\n\t\t}\n\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      // shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n\t  int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n\t  \t? BLOCK_SIZE_K_SPARSE / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A)\n\t\t: BLOCK_SIZE_N / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\n\t  /*\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\t  */\n\n\t  /*\n      int tile_k_idx_A;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n          tile_k_idx_A = *(lane_ptr_index);\n      }\n\t  */\n\n\t  #pragma unroll\n\t  for(int i = 0; i < iter_index; i += 1){\n\t\t  if(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\tint tile_k_idx_A = *(lane_ptr_index);\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_A * M_GLOBAL) + (laneId % CHUNK_COPY_LINE_LANES_A);\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_A)) =\n\t\t\t\t*lane_ptr;\n\t\t\t//warp_ptr = (uint8_t *)((uint8_t *)warp_ptr + M_GLOBAL * (WARPS_PER_BLOCK / 2) *CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tlane_ptr_index = (int *)((int *)lane_ptr_index +  (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A;\n\t\t  }else{\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_B)) =\n\t\t\t\t*lane_ptr;\n\t\t\tlane_ptr = (int4 *)((uint8_t *)lane_ptr + K_GLOBAL_SPARSE * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B;\n\t\t  }\n\t  }\n\n      __syncthreads();\n\n\t#pragma unroll\n      for (int k_step = 0; k_step < CHUNK_K_SPARSE; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::col_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n\t#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i += 1) {\n\t\t\tsize_t shmem_idx_a = (warpId % BLOCK_COL_WARPS) * M * WARP_COL_TILES + (i * M);\n\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_a + k_step * K * SHARED_OFFSET_A;\n\n\t\t\twmma::load_matrix_sync(a[i], tile_ptr, SHARED_OFFSET_A);\n\t\t#pragma unroll\n\t\t\tfor(int j = 0; j < WARP_ROW_TILES; j += 1){\n\t\t\t\tif(i == 0){\n\t\t\t\t\tsize_t shmem_idx_b = shmem_idx_b_off +\n\t\t\t\t\t\t\t\t\t\t\t(warpId / BLOCK_COL_WARPS) * (WARP_ROW_TILES * N) * SHARED_OFFSET_B +\n\t\t\t\t\t\t\t\t\t\t\t(j * N) * SHARED_OFFSET_B;\n\t\t\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_b + k_step * K;\n\t\t\t\t\twmma::load_matrix_sync(b[j], tile_ptr, SHARED_OFFSET_B);\n\t\t\t\t}\n\t\t\t\twmma::mma_sync(c[j][i], a[i], b[j], c[j][i]);\n\t\t\t}\n\n        }\n      }\n\n      __syncthreads();\n    }\n\n    // This pointer is used to access the C and D matrix tiles this warp computes.\n\tint *shmem_warp_tile_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * N * WARP_ROW_TILES * SHMEM_STRIDE +\n\t(warpId % BLOCK_COL_WARPS) * SHMEM_OFFSET;\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t#pragma unroll\n\t\t\tfor(int t = 0; t < c[i][j].num_elements; t += 1){\n\t\t\t\tc[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n\t\t\t}\n\t\t\tint *tile_ptr = shmem_warp_tile_ptr + i * N * SHMEM_STRIDE + j * M;\n\t\t\twmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, C_LAYOUT);\n\t\t}\n\t}\n\n    __syncthreads();\n\n\tint *shmem_warp_stream_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N * SHMEM_STRIDE\n\t\t\t\t\t\t\t\t\t+ (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tconst size_t gmem_idx =\n\t\t(block_tile_i * N + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N) * GLOBAL_MEM_STRIDE +\n\t\tblock_tile_j * M + (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tuint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n\tint *shmem_lane_stream_ptr =\n\t\tshmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * SHMEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\t\n\tuint8_t *dst_gmem_lane_stream_ptr =\n\t\tdst_gmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * GLOBAL_MEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\n\tfor(int i = 0; i < WARP_ROW_TILES * N; i += SHARED_TO_GLOBAL_LINES_PER_WARP){\n\t\t*(dst_gmem_lane_stream_ptr + i * GLOBAL_MEM_STRIDE) = (uint8_t)(*(shmem_lane_stream_ptr + i * SHMEM_STRIDE));\n\t}\n\n\t__syncthreads();\n}\n", "launch_config": {"dimBlock": [256, 1], "dimGrid": [256, 1]}}, "139": {"code": "\n__global__ void compute_gemm_imma(float *input0, float *input1, float *input2, float* input3, float* input4, float*input5, float* input6,\n                                  float *output0) {\n    // keys need to be replaced 4096, 1024, K_GLOBA_VALUEL, 0.75, 64\n\n\t//extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B = reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_index = reinterpret_cast<int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n\tconst int WARP_SIZE = 32;\n    const int M = 16;\n    const int K = 16;\n    const int N = 16;\n    const int WMMA_M = 16;\n    const int WMMA_K = 16;\n    const int WMMA_N = 16;\n    const int M_GLOBAL = 4096;\n    const int K_GLOBAL = 1024;\n    const int N_GLOBAL = 1024;\n    const float SPARSITY = 0.75;\n    const int K_GLOBAL_SPARSE = (int(K_GLOBAL * (1-SPARSITY)));\n    const int M_TILES = (M_GLOBAL / M);\n    const int N_TILES = (N_GLOBAL / N);\n    const int K_TILES = (K_GLOBAL / K);\n    const int BLOCK_ROW_WARPS = 2;\n    const int BLOCK_COL_WARPS = 4;\n    const int WARP_ROW_TILES = 4;\n    const int WARP_COL_TILES = 2;\n    const int SKEW_UINT8 = 16;\n    const int BANK_VAL = 32;\n    const int NUM_BANK = (K_GLOBAL / BANK_VAL);\n    /////////// BLOCK_ROW_TILES <= N_TILES ////////////\n    // const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    // ////////// BLOCK_COL_TILES <= M_TILES ////////////\n    // const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    auto C_LAYOUT = wmma::mem_col_major;\n    const int CHUNK_K = 64;\n    const int CHUNK_K_SPARSE = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K_SPARSE = (int((CHUNK_K * K) * (1 - SPARSITY)));\n    const int WARP_COPY_BYTES = (WARP_SIZE * sizeof(int4));\n    const int CHUNK_LINE_BYTES_A (BLOCK_COL_TILES * M * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_A = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_A);\n    const int CHUNK_COPY_LINE_LANES_A = (CHUNK_LINE_BYTES_A / sizeof(int4));\n    const int SHARED_OFFSET_A = (BLOCK_COL_TILES * M + SKEW_UINT8);\n\n    const int CHUNK_LINE_BYTES_B = (BLOCK_SIZE_K_SPARSE * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_B = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_B);\n    const int CHUNK_COPY_LINE_LANES_B = (CHUNK_LINE_BYTES_B / sizeof(int4));\n    const int SHARED_OFFSET_B = (BLOCK_SIZE_K_SPARSE + SKEW_UINT8);\n    const int SHARED_TO_GLOBAL_BYTES_PER_LINE = ((WARP_COL_TILES * M) * sizeof(int));\n    const int SHARED_TO_GLOBAL_BYTES_PER_WARP = (WARP_SIZE * sizeof(int));\n    const int SHARED_TO_GLOBAL_LINES_PER_WARP = (SHARED_TO_GLOBAL_BYTES_PER_WARP / SHARED_TO_GLOBAL_BYTES_PER_LINE);\n    const int SHARED_TO_GLOBAL_LANES_PER_LINE = (WARP_SIZE / SHARED_TO_GLOBAL_LINES_PER_WARP);\n    const int SHARED_TO_GLOBAL_ITERS = ((WARP_ROW_TILES * N) / SHARED_TO_GLOBAL_LINES_PER_WARP);\n\n    const int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const int WARP_STRIDE = (WARP_COL_TILES * M);\n    const int WARPS_PER_BLOCK = (BLOCK_ROW_WARPS * BLOCK_COL_WARPS);\n    const int THREADS_PER_BLOCK = (WARP_SIZE * WARPS_PER_BLOCK);\n    const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    const int GLOBAL_MEM_STRIDE = M_GLOBAL;\n    const int SHMEM_STRIDE = (M * BLOCK_COL_TILES);\n    const int SHMEM_OFFSET = (M * WARP_COL_TILES);\n    const int BLOCK_SIZE_M = (M * BLOCK_COL_TILES);\n    const int BLOCK_SIZE_N = (N * BLOCK_ROW_TILES);\n\n   extern __shared__ uint8_t shmem[];\n\n\t// Warp and lane identification.\n\tconst unsigned int warpId = threadIdx.x / WARP_SIZE;\n\tconst unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n\n\t// Offset in shared memory from which the B matrix is stored.\n\t// const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\tconst size_t shmem_idx_b_off = BLOCK_SIZE_K_SPARSE * SHARED_OFFSET_A;\n\n\n\t// Each CTA slides along the 128 x 128 tiles from the top left corner of the\n\t// matrix to the right and down, and selects the next tile to compute. Once\n\t// there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n\tconst unsigned int block_tile_i =\n\t\t((block_pos * BLOCK_COL_TILES) / M_TILES) * (BLOCK_ROW_TILES);\n\tconst unsigned int block_tile_j = (block_pos * BLOCK_COL_TILES) % M_TILES;\n\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n\twmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_ROW_TILES]\n\t\t\t\t\t\t\t\t\t\t\t\t\t [WARP_COL_TILES];\n\n    // Load the C matrix tiles into fragments from shared memory.\n\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t\twmma::fill_fragment(c[i][j], 0);\n\t\t}\n\t}\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    //for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n    for(int tile_k_idx_sparse = 0, tile_k_idx = 0; tile_k_idx_sparse < K_GLOBAL_SPARSE; tile_k_idx_sparse += BLOCK_SIZE_K_SPARSE, tile_k_idx += BLOCK_SIZE_K){\n\n\t\tsize_t shmem_idx = \n\t\twarpId < (WARPS_PER_BLOCK / 2)\n\t\t\t? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A\n\t\t\t: (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B + shmem_idx_b_off;\n\n\t\tint4 *lane_ptr = NULL;\n\t\tint *lane_ptr_index = NULL;\n\t\tconst uint8_t *warp_ptr = NULL;\n\n\n\t\tif(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\t//warp_ptr = &A[block_tile_j * M] +\n\t\t\t//\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * M_GLOBAL;\n\t\t\twarp_ptr = &A[block_tile_j * M];\n\t\t\t\n\t\t\tconst int *warp_ptr_index = &B_index[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t\t\t\t\t\t((warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A);\n\n\t\t\tlane_ptr_index = (int *)(warp_ptr_index + tile_k_idx_sparse + (laneId / CHUNK_COPY_LINE_LANES_A));\n\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_A) * SHARED_OFFSET_A;\n\t\t}else{\n\t\t\twarp_ptr = &B[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * K_GLOBAL_SPARSE;\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_sparse +\n\t\t\t\t\t\t\t\t(laneId / CHUNK_COPY_LINE_LANES_B) * K_GLOBAL_SPARSE) +\n\t\t\t\t\t\t\t\t(laneId % CHUNK_COPY_LINE_LANES_B);\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_B) * SHARED_OFFSET_B;\n\t\t}\n\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      // shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n\t  int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n\t  \t? BLOCK_SIZE_K_SPARSE / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A)\n\t\t: BLOCK_SIZE_N / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\n\t  /*\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\t  */\n\n\t  /*\n      int tile_k_idx_A;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n          tile_k_idx_A = *(lane_ptr_index);\n      }\n\t  */\n\n\t  #pragma unroll\n\t  for(int i = 0; i < iter_index; i += 1){\n\t\t  if(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\tint tile_k_idx_A = *(lane_ptr_index);\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_A * M_GLOBAL) + (laneId % CHUNK_COPY_LINE_LANES_A);\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_A)) =\n\t\t\t\t*lane_ptr;\n\t\t\t//warp_ptr = (uint8_t *)((uint8_t *)warp_ptr + M_GLOBAL * (WARPS_PER_BLOCK / 2) *CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tlane_ptr_index = (int *)((int *)lane_ptr_index +  (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A;\n\t\t  }else{\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_B)) =\n\t\t\t\t*lane_ptr;\n\t\t\tlane_ptr = (int4 *)((uint8_t *)lane_ptr + K_GLOBAL_SPARSE * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B;\n\t\t  }\n\t  }\n\n      __syncthreads();\n\n\t#pragma unroll\n      for (int k_step = 0; k_step < CHUNK_K_SPARSE; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::col_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n\t#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i += 1) {\n\t\t\tsize_t shmem_idx_a = (warpId % BLOCK_COL_WARPS) * M * WARP_COL_TILES + (i * M);\n\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_a + k_step * K * SHARED_OFFSET_A;\n\n\t\t\twmma::load_matrix_sync(a[i], tile_ptr, SHARED_OFFSET_A);\n\t\t#pragma unroll\n\t\t\tfor(int j = 0; j < WARP_ROW_TILES; j += 1){\n\t\t\t\tif(i == 0){\n\t\t\t\t\tsize_t shmem_idx_b = shmem_idx_b_off +\n\t\t\t\t\t\t\t\t\t\t\t(warpId / BLOCK_COL_WARPS) * (WARP_ROW_TILES * N) * SHARED_OFFSET_B +\n\t\t\t\t\t\t\t\t\t\t\t(j * N) * SHARED_OFFSET_B;\n\t\t\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_b + k_step * K;\n\t\t\t\t\twmma::load_matrix_sync(b[j], tile_ptr, SHARED_OFFSET_B);\n\t\t\t\t}\n\t\t\t\twmma::mma_sync(c[j][i], a[i], b[j], c[j][i]);\n\t\t\t}\n\n        }\n      }\n\n      __syncthreads();\n    }\n\n    // This pointer is used to access the C and D matrix tiles this warp computes.\n\tint *shmem_warp_tile_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * N * WARP_ROW_TILES * SHMEM_STRIDE +\n\t(warpId % BLOCK_COL_WARPS) * SHMEM_OFFSET;\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t#pragma unroll\n\t\t\tfor(int t = 0; t < c[i][j].num_elements; t += 1){\n\t\t\t\tc[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n\t\t\t}\n\t\t\tint *tile_ptr = shmem_warp_tile_ptr + i * N * SHMEM_STRIDE + j * M;\n\t\t\twmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, C_LAYOUT);\n\t\t}\n\t}\n\n    __syncthreads();\n\n\tint *shmem_warp_stream_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N * SHMEM_STRIDE\n\t\t\t\t\t\t\t\t\t+ (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tconst size_t gmem_idx =\n\t\t(block_tile_i * N + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N) * GLOBAL_MEM_STRIDE +\n\t\tblock_tile_j * M + (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tuint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n\tint *shmem_lane_stream_ptr =\n\t\tshmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * SHMEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\t\n\tuint8_t *dst_gmem_lane_stream_ptr =\n\t\tdst_gmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * GLOBAL_MEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\n\tfor(int i = 0; i < WARP_ROW_TILES * N; i += SHARED_TO_GLOBAL_LINES_PER_WARP){\n\t\t*(dst_gmem_lane_stream_ptr + i * GLOBAL_MEM_STRIDE) = (uint8_t)(*(shmem_lane_stream_ptr + i * SHMEM_STRIDE));\n\t}\n\n\t__syncthreads();\n}\n", "launch_config": {"dimBlock": [256, 1], "dimGrid": [256, 1]}}, "140": {"code": "\n__global__ void compute_gemm_imma(float *input0, float *input1, float *input2, float* input3, float* input4, float*input5, float* input6,\n                                  float *output0) {\n    // keys need to be replaced 4096, 1024, K_GLOBA_VALUEL, 0.75, 64\n\n\t//extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B = reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_index = reinterpret_cast<int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n\tconst int WARP_SIZE = 32;\n    const int M = 16;\n    const int K = 16;\n    const int N = 16;\n    const int WMMA_M = 16;\n    const int WMMA_K = 16;\n    const int WMMA_N = 16;\n    const int M_GLOBAL = 4096;\n    const int K_GLOBAL = 1024;\n    const int N_GLOBAL = 1024;\n    const float SPARSITY = 0.75;\n    const int K_GLOBAL_SPARSE = (int(K_GLOBAL * (1-SPARSITY)));\n    const int M_TILES = (M_GLOBAL / M);\n    const int N_TILES = (N_GLOBAL / N);\n    const int K_TILES = (K_GLOBAL / K);\n    const int BLOCK_ROW_WARPS = 2;\n    const int BLOCK_COL_WARPS = 4;\n    const int WARP_ROW_TILES = 4;\n    const int WARP_COL_TILES = 2;\n    const int SKEW_UINT8 = 16;\n    const int BANK_VAL = 32;\n    const int NUM_BANK = (K_GLOBAL / BANK_VAL);\n    /////////// BLOCK_ROW_TILES <= N_TILES ////////////\n    // const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    // ////////// BLOCK_COL_TILES <= M_TILES ////////////\n    // const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    auto C_LAYOUT = wmma::mem_col_major;\n    const int CHUNK_K = 64;\n    const int CHUNK_K_SPARSE = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K_SPARSE = (int((CHUNK_K * K) * (1 - SPARSITY)));\n    const int WARP_COPY_BYTES = (WARP_SIZE * sizeof(int4));\n    const int CHUNK_LINE_BYTES_A (BLOCK_COL_TILES * M * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_A = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_A);\n    const int CHUNK_COPY_LINE_LANES_A = (CHUNK_LINE_BYTES_A / sizeof(int4));\n    const int SHARED_OFFSET_A = (BLOCK_COL_TILES * M + SKEW_UINT8);\n\n    const int CHUNK_LINE_BYTES_B = (BLOCK_SIZE_K_SPARSE * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_B = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_B);\n    const int CHUNK_COPY_LINE_LANES_B = (CHUNK_LINE_BYTES_B / sizeof(int4));\n    const int SHARED_OFFSET_B = (BLOCK_SIZE_K_SPARSE + SKEW_UINT8);\n    const int SHARED_TO_GLOBAL_BYTES_PER_LINE = ((WARP_COL_TILES * M) * sizeof(int));\n    const int SHARED_TO_GLOBAL_BYTES_PER_WARP = (WARP_SIZE * sizeof(int));\n    const int SHARED_TO_GLOBAL_LINES_PER_WARP = (SHARED_TO_GLOBAL_BYTES_PER_WARP / SHARED_TO_GLOBAL_BYTES_PER_LINE);\n    const int SHARED_TO_GLOBAL_LANES_PER_LINE = (WARP_SIZE / SHARED_TO_GLOBAL_LINES_PER_WARP);\n    const int SHARED_TO_GLOBAL_ITERS = ((WARP_ROW_TILES * N) / SHARED_TO_GLOBAL_LINES_PER_WARP);\n\n    const int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const int WARP_STRIDE = (WARP_COL_TILES * M);\n    const int WARPS_PER_BLOCK = (BLOCK_ROW_WARPS * BLOCK_COL_WARPS);\n    const int THREADS_PER_BLOCK = (WARP_SIZE * WARPS_PER_BLOCK);\n    const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    const int GLOBAL_MEM_STRIDE = M_GLOBAL;\n    const int SHMEM_STRIDE = (M * BLOCK_COL_TILES);\n    const int SHMEM_OFFSET = (M * WARP_COL_TILES);\n    const int BLOCK_SIZE_M = (M * BLOCK_COL_TILES);\n    const int BLOCK_SIZE_N = (N * BLOCK_ROW_TILES);\n\n   extern __shared__ uint8_t shmem[];\n\n\t// Warp and lane identification.\n\tconst unsigned int warpId = threadIdx.x / WARP_SIZE;\n\tconst unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n\n\t// Offset in shared memory from which the B matrix is stored.\n\t// const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\tconst size_t shmem_idx_b_off = BLOCK_SIZE_K_SPARSE * SHARED_OFFSET_A;\n\n\n\t// Each CTA slides along the 128 x 128 tiles from the top left corner of the\n\t// matrix to the right and down, and selects the next tile to compute. Once\n\t// there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n\tconst unsigned int block_tile_i =\n\t\t((block_pos * BLOCK_COL_TILES) / M_TILES) * (BLOCK_ROW_TILES);\n\tconst unsigned int block_tile_j = (block_pos * BLOCK_COL_TILES) % M_TILES;\n\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n\twmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_ROW_TILES]\n\t\t\t\t\t\t\t\t\t\t\t\t\t [WARP_COL_TILES];\n\n    // Load the C matrix tiles into fragments from shared memory.\n\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t\twmma::fill_fragment(c[i][j], 0);\n\t\t}\n\t}\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    //for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n    for(int tile_k_idx_sparse = 0, tile_k_idx = 0; tile_k_idx_sparse < K_GLOBAL_SPARSE; tile_k_idx_sparse += BLOCK_SIZE_K_SPARSE, tile_k_idx += BLOCK_SIZE_K){\n\n\t\tsize_t shmem_idx = \n\t\twarpId < (WARPS_PER_BLOCK / 2)\n\t\t\t? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A\n\t\t\t: (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B + shmem_idx_b_off;\n\n\t\tint4 *lane_ptr = NULL;\n\t\tint *lane_ptr_index = NULL;\n\t\tconst uint8_t *warp_ptr = NULL;\n\n\n\t\tif(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\t//warp_ptr = &A[block_tile_j * M] +\n\t\t\t//\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * M_GLOBAL;\n\t\t\twarp_ptr = &A[block_tile_j * M];\n\t\t\t\n\t\t\tconst int *warp_ptr_index = &B_index[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t\t\t\t\t\t((warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A);\n\n\t\t\tlane_ptr_index = (int *)(warp_ptr_index + tile_k_idx_sparse + (laneId / CHUNK_COPY_LINE_LANES_A));\n\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_A) * SHARED_OFFSET_A;\n\t\t}else{\n\t\t\twarp_ptr = &B[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * K_GLOBAL_SPARSE;\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_sparse +\n\t\t\t\t\t\t\t\t(laneId / CHUNK_COPY_LINE_LANES_B) * K_GLOBAL_SPARSE) +\n\t\t\t\t\t\t\t\t(laneId % CHUNK_COPY_LINE_LANES_B);\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_B) * SHARED_OFFSET_B;\n\t\t}\n\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      // shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n\t  int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n\t  \t? BLOCK_SIZE_K_SPARSE / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A)\n\t\t: BLOCK_SIZE_N / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\n\t  /*\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\t  */\n\n\t  /*\n      int tile_k_idx_A;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n          tile_k_idx_A = *(lane_ptr_index);\n      }\n\t  */\n\n\t  #pragma unroll\n\t  for(int i = 0; i < iter_index; i += 1){\n\t\t  if(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\tint tile_k_idx_A = *(lane_ptr_index);\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_A * M_GLOBAL) + (laneId % CHUNK_COPY_LINE_LANES_A);\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_A)) =\n\t\t\t\t*lane_ptr;\n\t\t\t//warp_ptr = (uint8_t *)((uint8_t *)warp_ptr + M_GLOBAL * (WARPS_PER_BLOCK / 2) *CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tlane_ptr_index = (int *)((int *)lane_ptr_index +  (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A;\n\t\t  }else{\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_B)) =\n\t\t\t\t*lane_ptr;\n\t\t\tlane_ptr = (int4 *)((uint8_t *)lane_ptr + K_GLOBAL_SPARSE * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B;\n\t\t  }\n\t  }\n\n      __syncthreads();\n\n\t#pragma unroll\n      for (int k_step = 0; k_step < CHUNK_K_SPARSE; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::col_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n\t#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i += 1) {\n\t\t\tsize_t shmem_idx_a = (warpId % BLOCK_COL_WARPS) * M * WARP_COL_TILES + (i * M);\n\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_a + k_step * K * SHARED_OFFSET_A;\n\n\t\t\twmma::load_matrix_sync(a[i], tile_ptr, SHARED_OFFSET_A);\n\t\t#pragma unroll\n\t\t\tfor(int j = 0; j < WARP_ROW_TILES; j += 1){\n\t\t\t\tif(i == 0){\n\t\t\t\t\tsize_t shmem_idx_b = shmem_idx_b_off +\n\t\t\t\t\t\t\t\t\t\t\t(warpId / BLOCK_COL_WARPS) * (WARP_ROW_TILES * N) * SHARED_OFFSET_B +\n\t\t\t\t\t\t\t\t\t\t\t(j * N) * SHARED_OFFSET_B;\n\t\t\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_b + k_step * K;\n\t\t\t\t\twmma::load_matrix_sync(b[j], tile_ptr, SHARED_OFFSET_B);\n\t\t\t\t}\n\t\t\t\twmma::mma_sync(c[j][i], a[i], b[j], c[j][i]);\n\t\t\t}\n\n        }\n      }\n\n      __syncthreads();\n    }\n\n    // This pointer is used to access the C and D matrix tiles this warp computes.\n\tint *shmem_warp_tile_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * N * WARP_ROW_TILES * SHMEM_STRIDE +\n\t(warpId % BLOCK_COL_WARPS) * SHMEM_OFFSET;\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t#pragma unroll\n\t\t\tfor(int t = 0; t < c[i][j].num_elements; t += 1){\n\t\t\t\tc[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n\t\t\t}\n\t\t\tint *tile_ptr = shmem_warp_tile_ptr + i * N * SHMEM_STRIDE + j * M;\n\t\t\twmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, C_LAYOUT);\n\t\t}\n\t}\n\n    __syncthreads();\n\n\tint *shmem_warp_stream_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N * SHMEM_STRIDE\n\t\t\t\t\t\t\t\t\t+ (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tconst size_t gmem_idx =\n\t\t(block_tile_i * N + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N) * GLOBAL_MEM_STRIDE +\n\t\tblock_tile_j * M + (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tuint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n\tint *shmem_lane_stream_ptr =\n\t\tshmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * SHMEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\t\n\tuint8_t *dst_gmem_lane_stream_ptr =\n\t\tdst_gmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * GLOBAL_MEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\n\tfor(int i = 0; i < WARP_ROW_TILES * N; i += SHARED_TO_GLOBAL_LINES_PER_WARP){\n\t\t*(dst_gmem_lane_stream_ptr + i * GLOBAL_MEM_STRIDE) = (uint8_t)(*(shmem_lane_stream_ptr + i * SHMEM_STRIDE));\n\t}\n\n\t__syncthreads();\n}\n", "launch_config": {"dimBlock": [256, 1], "dimGrid": [256, 1]}}, "141": {"code": "\n__global__ void compute_gemm_imma(float *input0, float *input1, float *input2, float* input3, float* input4, float*input5, float* input6,\n                                  float *output0) {\n    // keys need to be replaced 4096, 1024, K_GLOBA_VALUEL, 0.75, 64\n\n\t//extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B = reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_index = reinterpret_cast<int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n\tconst int WARP_SIZE = 32;\n    const int M = 16;\n    const int K = 16;\n    const int N = 16;\n    const int WMMA_M = 16;\n    const int WMMA_K = 16;\n    const int WMMA_N = 16;\n    const int M_GLOBAL = 4096;\n    const int K_GLOBAL = 1024;\n    const int N_GLOBAL = 1024;\n    const float SPARSITY = 0.75;\n    const int K_GLOBAL_SPARSE = (int(K_GLOBAL * (1-SPARSITY)));\n    const int M_TILES = (M_GLOBAL / M);\n    const int N_TILES = (N_GLOBAL / N);\n    const int K_TILES = (K_GLOBAL / K);\n    const int BLOCK_ROW_WARPS = 2;\n    const int BLOCK_COL_WARPS = 4;\n    const int WARP_ROW_TILES = 4;\n    const int WARP_COL_TILES = 2;\n    const int SKEW_UINT8 = 16;\n    const int BANK_VAL = 32;\n    const int NUM_BANK = (K_GLOBAL / BANK_VAL);\n    /////////// BLOCK_ROW_TILES <= N_TILES ////////////\n    // const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    // ////////// BLOCK_COL_TILES <= M_TILES ////////////\n    // const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    auto C_LAYOUT = wmma::mem_col_major;\n    const int CHUNK_K = 64;\n    const int CHUNK_K_SPARSE = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K_SPARSE = (int((CHUNK_K * K) * (1 - SPARSITY)));\n    const int WARP_COPY_BYTES = (WARP_SIZE * sizeof(int4));\n    const int CHUNK_LINE_BYTES_A (BLOCK_COL_TILES * M * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_A = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_A);\n    const int CHUNK_COPY_LINE_LANES_A = (CHUNK_LINE_BYTES_A / sizeof(int4));\n    const int SHARED_OFFSET_A = (BLOCK_COL_TILES * M + SKEW_UINT8);\n\n    const int CHUNK_LINE_BYTES_B = (BLOCK_SIZE_K_SPARSE * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_B = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_B);\n    const int CHUNK_COPY_LINE_LANES_B = (CHUNK_LINE_BYTES_B / sizeof(int4));\n    const int SHARED_OFFSET_B = (BLOCK_SIZE_K_SPARSE + SKEW_UINT8);\n    const int SHARED_TO_GLOBAL_BYTES_PER_LINE = ((WARP_COL_TILES * M) * sizeof(int));\n    const int SHARED_TO_GLOBAL_BYTES_PER_WARP = (WARP_SIZE * sizeof(int));\n    const int SHARED_TO_GLOBAL_LINES_PER_WARP = (SHARED_TO_GLOBAL_BYTES_PER_WARP / SHARED_TO_GLOBAL_BYTES_PER_LINE);\n    const int SHARED_TO_GLOBAL_LANES_PER_LINE = (WARP_SIZE / SHARED_TO_GLOBAL_LINES_PER_WARP);\n    const int SHARED_TO_GLOBAL_ITERS = ((WARP_ROW_TILES * N) / SHARED_TO_GLOBAL_LINES_PER_WARP);\n\n    const int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const int WARP_STRIDE = (WARP_COL_TILES * M);\n    const int WARPS_PER_BLOCK = (BLOCK_ROW_WARPS * BLOCK_COL_WARPS);\n    const int THREADS_PER_BLOCK = (WARP_SIZE * WARPS_PER_BLOCK);\n    const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    const int GLOBAL_MEM_STRIDE = M_GLOBAL;\n    const int SHMEM_STRIDE = (M * BLOCK_COL_TILES);\n    const int SHMEM_OFFSET = (M * WARP_COL_TILES);\n    const int BLOCK_SIZE_M = (M * BLOCK_COL_TILES);\n    const int BLOCK_SIZE_N = (N * BLOCK_ROW_TILES);\n\n   extern __shared__ uint8_t shmem[];\n\n\t// Warp and lane identification.\n\tconst unsigned int warpId = threadIdx.x / WARP_SIZE;\n\tconst unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n\n\t// Offset in shared memory from which the B matrix is stored.\n\t// const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\tconst size_t shmem_idx_b_off = BLOCK_SIZE_K_SPARSE * SHARED_OFFSET_A;\n\n\n\t// Each CTA slides along the 128 x 128 tiles from the top left corner of the\n\t// matrix to the right and down, and selects the next tile to compute. Once\n\t// there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n\tconst unsigned int block_tile_i =\n\t\t((block_pos * BLOCK_COL_TILES) / M_TILES) * (BLOCK_ROW_TILES);\n\tconst unsigned int block_tile_j = (block_pos * BLOCK_COL_TILES) % M_TILES;\n\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n\twmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_ROW_TILES]\n\t\t\t\t\t\t\t\t\t\t\t\t\t [WARP_COL_TILES];\n\n    // Load the C matrix tiles into fragments from shared memory.\n\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t\twmma::fill_fragment(c[i][j], 0);\n\t\t}\n\t}\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    //for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n    for(int tile_k_idx_sparse = 0, tile_k_idx = 0; tile_k_idx_sparse < K_GLOBAL_SPARSE; tile_k_idx_sparse += BLOCK_SIZE_K_SPARSE, tile_k_idx += BLOCK_SIZE_K){\n\n\t\tsize_t shmem_idx = \n\t\twarpId < (WARPS_PER_BLOCK / 2)\n\t\t\t? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A\n\t\t\t: (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B + shmem_idx_b_off;\n\n\t\tint4 *lane_ptr = NULL;\n\t\tint *lane_ptr_index = NULL;\n\t\tconst uint8_t *warp_ptr = NULL;\n\n\n\t\tif(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\t//warp_ptr = &A[block_tile_j * M] +\n\t\t\t//\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * M_GLOBAL;\n\t\t\twarp_ptr = &A[block_tile_j * M];\n\t\t\t\n\t\t\tconst int *warp_ptr_index = &B_index[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t\t\t\t\t\t((warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A);\n\n\t\t\tlane_ptr_index = (int *)(warp_ptr_index + tile_k_idx_sparse + (laneId / CHUNK_COPY_LINE_LANES_A));\n\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_A) * SHARED_OFFSET_A;\n\t\t}else{\n\t\t\twarp_ptr = &B[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * K_GLOBAL_SPARSE;\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_sparse +\n\t\t\t\t\t\t\t\t(laneId / CHUNK_COPY_LINE_LANES_B) * K_GLOBAL_SPARSE) +\n\t\t\t\t\t\t\t\t(laneId % CHUNK_COPY_LINE_LANES_B);\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_B) * SHARED_OFFSET_B;\n\t\t}\n\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      // shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n\t  int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n\t  \t? BLOCK_SIZE_K_SPARSE / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A)\n\t\t: BLOCK_SIZE_N / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\n\t  /*\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\t  */\n\n\t  /*\n      int tile_k_idx_A;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n          tile_k_idx_A = *(lane_ptr_index);\n      }\n\t  */\n\n\t  #pragma unroll\n\t  for(int i = 0; i < iter_index; i += 1){\n\t\t  if(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\tint tile_k_idx_A = *(lane_ptr_index);\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_A * M_GLOBAL) + (laneId % CHUNK_COPY_LINE_LANES_A);\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_A)) =\n\t\t\t\t*lane_ptr;\n\t\t\t//warp_ptr = (uint8_t *)((uint8_t *)warp_ptr + M_GLOBAL * (WARPS_PER_BLOCK / 2) *CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tlane_ptr_index = (int *)((int *)lane_ptr_index +  (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A;\n\t\t  }else{\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_B)) =\n\t\t\t\t*lane_ptr;\n\t\t\tlane_ptr = (int4 *)((uint8_t *)lane_ptr + K_GLOBAL_SPARSE * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B;\n\t\t  }\n\t  }\n\n      __syncthreads();\n\n\t#pragma unroll\n      for (int k_step = 0; k_step < CHUNK_K_SPARSE; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::col_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n\t#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i += 1) {\n\t\t\tsize_t shmem_idx_a = (warpId % BLOCK_COL_WARPS) * M * WARP_COL_TILES + (i * M);\n\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_a + k_step * K * SHARED_OFFSET_A;\n\n\t\t\twmma::load_matrix_sync(a[i], tile_ptr, SHARED_OFFSET_A);\n\t\t#pragma unroll\n\t\t\tfor(int j = 0; j < WARP_ROW_TILES; j += 1){\n\t\t\t\tif(i == 0){\n\t\t\t\t\tsize_t shmem_idx_b = shmem_idx_b_off +\n\t\t\t\t\t\t\t\t\t\t\t(warpId / BLOCK_COL_WARPS) * (WARP_ROW_TILES * N) * SHARED_OFFSET_B +\n\t\t\t\t\t\t\t\t\t\t\t(j * N) * SHARED_OFFSET_B;\n\t\t\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_b + k_step * K;\n\t\t\t\t\twmma::load_matrix_sync(b[j], tile_ptr, SHARED_OFFSET_B);\n\t\t\t\t}\n\t\t\t\twmma::mma_sync(c[j][i], a[i], b[j], c[j][i]);\n\t\t\t}\n\n        }\n      }\n\n      __syncthreads();\n    }\n\n    // This pointer is used to access the C and D matrix tiles this warp computes.\n\tint *shmem_warp_tile_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * N * WARP_ROW_TILES * SHMEM_STRIDE +\n\t(warpId % BLOCK_COL_WARPS) * SHMEM_OFFSET;\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t#pragma unroll\n\t\t\tfor(int t = 0; t < c[i][j].num_elements; t += 1){\n\t\t\t\tc[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n\t\t\t}\n\t\t\tint *tile_ptr = shmem_warp_tile_ptr + i * N * SHMEM_STRIDE + j * M;\n\t\t\twmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, C_LAYOUT);\n\t\t}\n\t}\n\n    __syncthreads();\n\n\tint *shmem_warp_stream_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N * SHMEM_STRIDE\n\t\t\t\t\t\t\t\t\t+ (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tconst size_t gmem_idx =\n\t\t(block_tile_i * N + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N) * GLOBAL_MEM_STRIDE +\n\t\tblock_tile_j * M + (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tuint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n\tint *shmem_lane_stream_ptr =\n\t\tshmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * SHMEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\t\n\tuint8_t *dst_gmem_lane_stream_ptr =\n\t\tdst_gmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * GLOBAL_MEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\n\tfor(int i = 0; i < WARP_ROW_TILES * N; i += SHARED_TO_GLOBAL_LINES_PER_WARP){\n\t\t*(dst_gmem_lane_stream_ptr + i * GLOBAL_MEM_STRIDE) = (uint8_t)(*(shmem_lane_stream_ptr + i * SHMEM_STRIDE));\n\t}\n\n\t__syncthreads();\n}\n", "launch_config": {"dimBlock": [256, 1], "dimGrid": [256, 1]}}, "142": {"code": "\n__global__ void compute_gemm_imma(float *input0, float *input1, float *input2, float* input3, float* input4, float*input5, float* input6,\n                                  float *output0) {\n    // keys need to be replaced 4096, 1024, K_GLOBA_VALUEL, 0.75, 64\n\n\t//extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B = reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_index = reinterpret_cast<int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n\tconst int WARP_SIZE = 32;\n    const int M = 16;\n    const int K = 16;\n    const int N = 16;\n    const int WMMA_M = 16;\n    const int WMMA_K = 16;\n    const int WMMA_N = 16;\n    const int M_GLOBAL = 4096;\n    const int K_GLOBAL = 1024;\n    const int N_GLOBAL = 1024;\n    const float SPARSITY = 0.75;\n    const int K_GLOBAL_SPARSE = (int(K_GLOBAL * (1-SPARSITY)));\n    const int M_TILES = (M_GLOBAL / M);\n    const int N_TILES = (N_GLOBAL / N);\n    const int K_TILES = (K_GLOBAL / K);\n    const int BLOCK_ROW_WARPS = 2;\n    const int BLOCK_COL_WARPS = 4;\n    const int WARP_ROW_TILES = 4;\n    const int WARP_COL_TILES = 2;\n    const int SKEW_UINT8 = 16;\n    const int BANK_VAL = 32;\n    const int NUM_BANK = (K_GLOBAL / BANK_VAL);\n    /////////// BLOCK_ROW_TILES <= N_TILES ////////////\n    // const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    // ////////// BLOCK_COL_TILES <= M_TILES ////////////\n    // const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    auto C_LAYOUT = wmma::mem_col_major;\n    const int CHUNK_K = 64;\n    const int CHUNK_K_SPARSE = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K_SPARSE = (int((CHUNK_K * K) * (1 - SPARSITY)));\n    const int WARP_COPY_BYTES = (WARP_SIZE * sizeof(int4));\n    const int CHUNK_LINE_BYTES_A (BLOCK_COL_TILES * M * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_A = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_A);\n    const int CHUNK_COPY_LINE_LANES_A = (CHUNK_LINE_BYTES_A / sizeof(int4));\n    const int SHARED_OFFSET_A = (BLOCK_COL_TILES * M + SKEW_UINT8);\n\n    const int CHUNK_LINE_BYTES_B = (BLOCK_SIZE_K_SPARSE * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_B = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_B);\n    const int CHUNK_COPY_LINE_LANES_B = (CHUNK_LINE_BYTES_B / sizeof(int4));\n    const int SHARED_OFFSET_B = (BLOCK_SIZE_K_SPARSE + SKEW_UINT8);\n    const int SHARED_TO_GLOBAL_BYTES_PER_LINE = ((WARP_COL_TILES * M) * sizeof(int));\n    const int SHARED_TO_GLOBAL_BYTES_PER_WARP = (WARP_SIZE * sizeof(int));\n    const int SHARED_TO_GLOBAL_LINES_PER_WARP = (SHARED_TO_GLOBAL_BYTES_PER_WARP / SHARED_TO_GLOBAL_BYTES_PER_LINE);\n    const int SHARED_TO_GLOBAL_LANES_PER_LINE = (WARP_SIZE / SHARED_TO_GLOBAL_LINES_PER_WARP);\n    const int SHARED_TO_GLOBAL_ITERS = ((WARP_ROW_TILES * N) / SHARED_TO_GLOBAL_LINES_PER_WARP);\n\n    const int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const int WARP_STRIDE = (WARP_COL_TILES * M);\n    const int WARPS_PER_BLOCK = (BLOCK_ROW_WARPS * BLOCK_COL_WARPS);\n    const int THREADS_PER_BLOCK = (WARP_SIZE * WARPS_PER_BLOCK);\n    const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    const int GLOBAL_MEM_STRIDE = M_GLOBAL;\n    const int SHMEM_STRIDE = (M * BLOCK_COL_TILES);\n    const int SHMEM_OFFSET = (M * WARP_COL_TILES);\n    const int BLOCK_SIZE_M = (M * BLOCK_COL_TILES);\n    const int BLOCK_SIZE_N = (N * BLOCK_ROW_TILES);\n\n   extern __shared__ uint8_t shmem[];\n\n\t// Warp and lane identification.\n\tconst unsigned int warpId = threadIdx.x / WARP_SIZE;\n\tconst unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n\n\t// Offset in shared memory from which the B matrix is stored.\n\t// const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\tconst size_t shmem_idx_b_off = BLOCK_SIZE_K_SPARSE * SHARED_OFFSET_A;\n\n\n\t// Each CTA slides along the 128 x 128 tiles from the top left corner of the\n\t// matrix to the right and down, and selects the next tile to compute. Once\n\t// there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n\tconst unsigned int block_tile_i =\n\t\t((block_pos * BLOCK_COL_TILES) / M_TILES) * (BLOCK_ROW_TILES);\n\tconst unsigned int block_tile_j = (block_pos * BLOCK_COL_TILES) % M_TILES;\n\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n\twmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_ROW_TILES]\n\t\t\t\t\t\t\t\t\t\t\t\t\t [WARP_COL_TILES];\n\n    // Load the C matrix tiles into fragments from shared memory.\n\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t\twmma::fill_fragment(c[i][j], 0);\n\t\t}\n\t}\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    //for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n    for(int tile_k_idx_sparse = 0, tile_k_idx = 0; tile_k_idx_sparse < K_GLOBAL_SPARSE; tile_k_idx_sparse += BLOCK_SIZE_K_SPARSE, tile_k_idx += BLOCK_SIZE_K){\n\n\t\tsize_t shmem_idx = \n\t\twarpId < (WARPS_PER_BLOCK / 2)\n\t\t\t? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A\n\t\t\t: (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B + shmem_idx_b_off;\n\n\t\tint4 *lane_ptr = NULL;\n\t\tint *lane_ptr_index = NULL;\n\t\tconst uint8_t *warp_ptr = NULL;\n\n\n\t\tif(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\t//warp_ptr = &A[block_tile_j * M] +\n\t\t\t//\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * M_GLOBAL;\n\t\t\twarp_ptr = &A[block_tile_j * M];\n\t\t\t\n\t\t\tconst int *warp_ptr_index = &B_index[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t\t\t\t\t\t((warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A);\n\n\t\t\tlane_ptr_index = (int *)(warp_ptr_index + tile_k_idx_sparse + (laneId / CHUNK_COPY_LINE_LANES_A));\n\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_A) * SHARED_OFFSET_A;\n\t\t}else{\n\t\t\twarp_ptr = &B[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * K_GLOBAL_SPARSE;\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_sparse +\n\t\t\t\t\t\t\t\t(laneId / CHUNK_COPY_LINE_LANES_B) * K_GLOBAL_SPARSE) +\n\t\t\t\t\t\t\t\t(laneId % CHUNK_COPY_LINE_LANES_B);\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_B) * SHARED_OFFSET_B;\n\t\t}\n\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      // shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n\t  int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n\t  \t? BLOCK_SIZE_K_SPARSE / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A)\n\t\t: BLOCK_SIZE_N / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\n\t  /*\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\t  */\n\n\t  /*\n      int tile_k_idx_A;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n          tile_k_idx_A = *(lane_ptr_index);\n      }\n\t  */\n\n\t  #pragma unroll\n\t  for(int i = 0; i < iter_index; i += 1){\n\t\t  if(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\tint tile_k_idx_A = *(lane_ptr_index);\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_A * M_GLOBAL) + (laneId % CHUNK_COPY_LINE_LANES_A);\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_A)) =\n\t\t\t\t*lane_ptr;\n\t\t\t//warp_ptr = (uint8_t *)((uint8_t *)warp_ptr + M_GLOBAL * (WARPS_PER_BLOCK / 2) *CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tlane_ptr_index = (int *)((int *)lane_ptr_index +  (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A;\n\t\t  }else{\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_B)) =\n\t\t\t\t*lane_ptr;\n\t\t\tlane_ptr = (int4 *)((uint8_t *)lane_ptr + K_GLOBAL_SPARSE * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B;\n\t\t  }\n\t  }\n\n      __syncthreads();\n\n\t#pragma unroll\n      for (int k_step = 0; k_step < CHUNK_K_SPARSE; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::col_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n\t#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i += 1) {\n\t\t\tsize_t shmem_idx_a = (warpId % BLOCK_COL_WARPS) * M * WARP_COL_TILES + (i * M);\n\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_a + k_step * K * SHARED_OFFSET_A;\n\n\t\t\twmma::load_matrix_sync(a[i], tile_ptr, SHARED_OFFSET_A);\n\t\t#pragma unroll\n\t\t\tfor(int j = 0; j < WARP_ROW_TILES; j += 1){\n\t\t\t\tif(i == 0){\n\t\t\t\t\tsize_t shmem_idx_b = shmem_idx_b_off +\n\t\t\t\t\t\t\t\t\t\t\t(warpId / BLOCK_COL_WARPS) * (WARP_ROW_TILES * N) * SHARED_OFFSET_B +\n\t\t\t\t\t\t\t\t\t\t\t(j * N) * SHARED_OFFSET_B;\n\t\t\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_b + k_step * K;\n\t\t\t\t\twmma::load_matrix_sync(b[j], tile_ptr, SHARED_OFFSET_B);\n\t\t\t\t}\n\t\t\t\twmma::mma_sync(c[j][i], a[i], b[j], c[j][i]);\n\t\t\t}\n\n        }\n      }\n\n      __syncthreads();\n    }\n\n    // This pointer is used to access the C and D matrix tiles this warp computes.\n\tint *shmem_warp_tile_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * N * WARP_ROW_TILES * SHMEM_STRIDE +\n\t(warpId % BLOCK_COL_WARPS) * SHMEM_OFFSET;\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t#pragma unroll\n\t\t\tfor(int t = 0; t < c[i][j].num_elements; t += 1){\n\t\t\t\tc[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n\t\t\t}\n\t\t\tint *tile_ptr = shmem_warp_tile_ptr + i * N * SHMEM_STRIDE + j * M;\n\t\t\twmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, C_LAYOUT);\n\t\t}\n\t}\n\n    __syncthreads();\n\n\tint *shmem_warp_stream_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N * SHMEM_STRIDE\n\t\t\t\t\t\t\t\t\t+ (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tconst size_t gmem_idx =\n\t\t(block_tile_i * N + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N) * GLOBAL_MEM_STRIDE +\n\t\tblock_tile_j * M + (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tuint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n\tint *shmem_lane_stream_ptr =\n\t\tshmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * SHMEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\t\n\tuint8_t *dst_gmem_lane_stream_ptr =\n\t\tdst_gmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * GLOBAL_MEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\n\tfor(int i = 0; i < WARP_ROW_TILES * N; i += SHARED_TO_GLOBAL_LINES_PER_WARP){\n\t\t*(dst_gmem_lane_stream_ptr + i * GLOBAL_MEM_STRIDE) = (uint8_t)(*(shmem_lane_stream_ptr + i * SHMEM_STRIDE));\n\t}\n\n\t__syncthreads();\n}\n", "launch_config": {"dimBlock": [256, 1], "dimGrid": [256, 1]}}, "143": {"code": "\n__global__ void compute_gemm_imma(float *input0, float *input1, float *input2, float* input3, float* input4, float*input5, float* input6,\n                                  float *output0) {\n    // keys need to be replaced 4096, 4096, K_GLOBA_VALUEL, 0.75, 64\n\n\t//extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B = reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_index = reinterpret_cast<int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n\tconst int WARP_SIZE = 32;\n    const int M = 16;\n    const int K = 16;\n    const int N = 16;\n    const int WMMA_M = 16;\n    const int WMMA_K = 16;\n    const int WMMA_N = 16;\n    const int M_GLOBAL = 4096;\n    const int K_GLOBAL = 1024;\n    const int N_GLOBAL = 4096;\n    const float SPARSITY = 0.75;\n    const int K_GLOBAL_SPARSE = (int(K_GLOBAL * (1-SPARSITY)));\n    const int M_TILES = (M_GLOBAL / M);\n    const int N_TILES = (N_GLOBAL / N);\n    const int K_TILES = (K_GLOBAL / K);\n    const int BLOCK_ROW_WARPS = 2;\n    const int BLOCK_COL_WARPS = 4;\n    const int WARP_ROW_TILES = 4;\n    const int WARP_COL_TILES = 2;\n    const int SKEW_UINT8 = 16;\n    const int BANK_VAL = 32;\n    const int NUM_BANK = (K_GLOBAL / BANK_VAL);\n    /////////// BLOCK_ROW_TILES <= N_TILES ////////////\n    // const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    // ////////// BLOCK_COL_TILES <= M_TILES ////////////\n    // const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    auto C_LAYOUT = wmma::mem_col_major;\n    const int CHUNK_K = 64;\n    const int CHUNK_K_SPARSE = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K_SPARSE = (int((CHUNK_K * K) * (1 - SPARSITY)));\n    const int WARP_COPY_BYTES = (WARP_SIZE * sizeof(int4));\n    const int CHUNK_LINE_BYTES_A (BLOCK_COL_TILES * M * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_A = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_A);\n    const int CHUNK_COPY_LINE_LANES_A = (CHUNK_LINE_BYTES_A / sizeof(int4));\n    const int SHARED_OFFSET_A = (BLOCK_COL_TILES * M + SKEW_UINT8);\n\n    const int CHUNK_LINE_BYTES_B = (BLOCK_SIZE_K_SPARSE * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_B = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_B);\n    const int CHUNK_COPY_LINE_LANES_B = (CHUNK_LINE_BYTES_B / sizeof(int4));\n    const int SHARED_OFFSET_B = (BLOCK_SIZE_K_SPARSE + SKEW_UINT8);\n    const int SHARED_TO_GLOBAL_BYTES_PER_LINE = ((WARP_COL_TILES * M) * sizeof(int));\n    const int SHARED_TO_GLOBAL_BYTES_PER_WARP = (WARP_SIZE * sizeof(int));\n    const int SHARED_TO_GLOBAL_LINES_PER_WARP = (SHARED_TO_GLOBAL_BYTES_PER_WARP / SHARED_TO_GLOBAL_BYTES_PER_LINE);\n    const int SHARED_TO_GLOBAL_LANES_PER_LINE = (WARP_SIZE / SHARED_TO_GLOBAL_LINES_PER_WARP);\n    const int SHARED_TO_GLOBAL_ITERS = ((WARP_ROW_TILES * N) / SHARED_TO_GLOBAL_LINES_PER_WARP);\n\n    const int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const int WARP_STRIDE = (WARP_COL_TILES * M);\n    const int WARPS_PER_BLOCK = (BLOCK_ROW_WARPS * BLOCK_COL_WARPS);\n    const int THREADS_PER_BLOCK = (WARP_SIZE * WARPS_PER_BLOCK);\n    const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    const int GLOBAL_MEM_STRIDE = M_GLOBAL;\n    const int SHMEM_STRIDE = (M * BLOCK_COL_TILES);\n    const int SHMEM_OFFSET = (M * WARP_COL_TILES);\n    const int BLOCK_SIZE_M = (M * BLOCK_COL_TILES);\n    const int BLOCK_SIZE_N = (N * BLOCK_ROW_TILES);\n\n   extern __shared__ uint8_t shmem[];\n\n\t// Warp and lane identification.\n\tconst unsigned int warpId = threadIdx.x / WARP_SIZE;\n\tconst unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n\n\t// Offset in shared memory from which the B matrix is stored.\n\t// const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\tconst size_t shmem_idx_b_off = BLOCK_SIZE_K_SPARSE * SHARED_OFFSET_A;\n\n\n\t// Each CTA slides along the 128 x 128 tiles from the top left corner of the\n\t// matrix to the right and down, and selects the next tile to compute. Once\n\t// there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n\tconst unsigned int block_tile_i =\n\t\t((block_pos * BLOCK_COL_TILES) / M_TILES) * (BLOCK_ROW_TILES);\n\tconst unsigned int block_tile_j = (block_pos * BLOCK_COL_TILES) % M_TILES;\n\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n\twmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_ROW_TILES]\n\t\t\t\t\t\t\t\t\t\t\t\t\t [WARP_COL_TILES];\n\n    // Load the C matrix tiles into fragments from shared memory.\n\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t\twmma::fill_fragment(c[i][j], 0);\n\t\t}\n\t}\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    //for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n    for(int tile_k_idx_sparse = 0, tile_k_idx = 0; tile_k_idx_sparse < K_GLOBAL_SPARSE; tile_k_idx_sparse += BLOCK_SIZE_K_SPARSE, tile_k_idx += BLOCK_SIZE_K){\n\n\t\tsize_t shmem_idx = \n\t\twarpId < (WARPS_PER_BLOCK / 2)\n\t\t\t? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A\n\t\t\t: (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B + shmem_idx_b_off;\n\n\t\tint4 *lane_ptr = NULL;\n\t\tint *lane_ptr_index = NULL;\n\t\tconst uint8_t *warp_ptr = NULL;\n\n\n\t\tif(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\t//warp_ptr = &A[block_tile_j * M] +\n\t\t\t//\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * M_GLOBAL;\n\t\t\twarp_ptr = &A[block_tile_j * M];\n\t\t\t\n\t\t\tconst int *warp_ptr_index = &B_index[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t\t\t\t\t\t((warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A);\n\n\t\t\tlane_ptr_index = (int *)(warp_ptr_index + tile_k_idx_sparse + (laneId / CHUNK_COPY_LINE_LANES_A));\n\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_A) * SHARED_OFFSET_A;\n\t\t}else{\n\t\t\twarp_ptr = &B[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * K_GLOBAL_SPARSE;\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_sparse +\n\t\t\t\t\t\t\t\t(laneId / CHUNK_COPY_LINE_LANES_B) * K_GLOBAL_SPARSE) +\n\t\t\t\t\t\t\t\t(laneId % CHUNK_COPY_LINE_LANES_B);\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_B) * SHARED_OFFSET_B;\n\t\t}\n\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      // shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n\t  int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n\t  \t? BLOCK_SIZE_K_SPARSE / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A)\n\t\t: BLOCK_SIZE_N / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\n\t  /*\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\t  */\n\n\t  /*\n      int tile_k_idx_A;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n          tile_k_idx_A = *(lane_ptr_index);\n      }\n\t  */\n\n\t  #pragma unroll\n\t  for(int i = 0; i < iter_index; i += 1){\n\t\t  if(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\tint tile_k_idx_A = *(lane_ptr_index);\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_A * M_GLOBAL) + (laneId % CHUNK_COPY_LINE_LANES_A);\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_A)) =\n\t\t\t\t*lane_ptr;\n\t\t\t//warp_ptr = (uint8_t *)((uint8_t *)warp_ptr + M_GLOBAL * (WARPS_PER_BLOCK / 2) *CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tlane_ptr_index = (int *)((int *)lane_ptr_index +  (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A;\n\t\t  }else{\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_B)) =\n\t\t\t\t*lane_ptr;\n\t\t\tlane_ptr = (int4 *)((uint8_t *)lane_ptr + K_GLOBAL_SPARSE * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B;\n\t\t  }\n\t  }\n\n      __syncthreads();\n\n\t#pragma unroll\n      for (int k_step = 0; k_step < CHUNK_K_SPARSE; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::col_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n\t#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i += 1) {\n\t\t\tsize_t shmem_idx_a = (warpId % BLOCK_COL_WARPS) * M * WARP_COL_TILES + (i * M);\n\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_a + k_step * K * SHARED_OFFSET_A;\n\n\t\t\twmma::load_matrix_sync(a[i], tile_ptr, SHARED_OFFSET_A);\n\t\t#pragma unroll\n\t\t\tfor(int j = 0; j < WARP_ROW_TILES; j += 1){\n\t\t\t\tif(i == 0){\n\t\t\t\t\tsize_t shmem_idx_b = shmem_idx_b_off +\n\t\t\t\t\t\t\t\t\t\t\t(warpId / BLOCK_COL_WARPS) * (WARP_ROW_TILES * N) * SHARED_OFFSET_B +\n\t\t\t\t\t\t\t\t\t\t\t(j * N) * SHARED_OFFSET_B;\n\t\t\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_b + k_step * K;\n\t\t\t\t\twmma::load_matrix_sync(b[j], tile_ptr, SHARED_OFFSET_B);\n\t\t\t\t}\n\t\t\t\twmma::mma_sync(c[j][i], a[i], b[j], c[j][i]);\n\t\t\t}\n\n        }\n      }\n\n      __syncthreads();\n    }\n\n    // This pointer is used to access the C and D matrix tiles this warp computes.\n\tint *shmem_warp_tile_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * N * WARP_ROW_TILES * SHMEM_STRIDE +\n\t(warpId % BLOCK_COL_WARPS) * SHMEM_OFFSET;\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t#pragma unroll\n\t\t\tfor(int t = 0; t < c[i][j].num_elements; t += 1){\n\t\t\t\tc[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n\t\t\t}\n\t\t\tint *tile_ptr = shmem_warp_tile_ptr + i * N * SHMEM_STRIDE + j * M;\n\t\t\twmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, C_LAYOUT);\n\t\t}\n\t}\n\n    __syncthreads();\n\n\tint *shmem_warp_stream_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N * SHMEM_STRIDE\n\t\t\t\t\t\t\t\t\t+ (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tconst size_t gmem_idx =\n\t\t(block_tile_i * N + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N) * GLOBAL_MEM_STRIDE +\n\t\tblock_tile_j * M + (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tuint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n\tint *shmem_lane_stream_ptr =\n\t\tshmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * SHMEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\t\n\tuint8_t *dst_gmem_lane_stream_ptr =\n\t\tdst_gmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * GLOBAL_MEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\n\tfor(int i = 0; i < WARP_ROW_TILES * N; i += SHARED_TO_GLOBAL_LINES_PER_WARP){\n\t\t*(dst_gmem_lane_stream_ptr + i * GLOBAL_MEM_STRIDE) = (uint8_t)(*(shmem_lane_stream_ptr + i * SHMEM_STRIDE));\n\t}\n\n\t__syncthreads();\n}\n", "launch_config": {"dimBlock": [1024, 1], "dimGrid": [256, 1]}}, "144": {"code": "\n__global__ void compute_gemm_imma(float *input0, float *input1, float *input2, float* input3, float* input4, float*input5, float* input6,\n                                  float *output0) {\n    // keys need to be replaced 4096, 1024, K_GLOBA_VALUEL, 0.75, 64\n\n\t//extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B = reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_index = reinterpret_cast<int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n\tconst int WARP_SIZE = 32;\n    const int M = 16;\n    const int K = 16;\n    const int N = 16;\n    const int WMMA_M = 16;\n    const int WMMA_K = 16;\n    const int WMMA_N = 16;\n    const int M_GLOBAL = 4096;\n    const int K_GLOBAL = 4096;\n    const int N_GLOBAL = 1024;\n    const float SPARSITY = 0.75;\n    const int K_GLOBAL_SPARSE = (int(K_GLOBAL * (1-SPARSITY)));\n    const int M_TILES = (M_GLOBAL / M);\n    const int N_TILES = (N_GLOBAL / N);\n    const int K_TILES = (K_GLOBAL / K);\n    const int BLOCK_ROW_WARPS = 2;\n    const int BLOCK_COL_WARPS = 4;\n    const int WARP_ROW_TILES = 4;\n    const int WARP_COL_TILES = 2;\n    const int SKEW_UINT8 = 16;\n    const int BANK_VAL = 32;\n    const int NUM_BANK = (K_GLOBAL / BANK_VAL);\n    /////////// BLOCK_ROW_TILES <= N_TILES ////////////\n    // const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    // ////////// BLOCK_COL_TILES <= M_TILES ////////////\n    // const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    auto C_LAYOUT = wmma::mem_col_major;\n    const int CHUNK_K = 64;\n    const int CHUNK_K_SPARSE = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K_SPARSE = (int((CHUNK_K * K) * (1 - SPARSITY)));\n    const int WARP_COPY_BYTES = (WARP_SIZE * sizeof(int4));\n    const int CHUNK_LINE_BYTES_A (BLOCK_COL_TILES * M * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_A = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_A);\n    const int CHUNK_COPY_LINE_LANES_A = (CHUNK_LINE_BYTES_A / sizeof(int4));\n    const int SHARED_OFFSET_A = (BLOCK_COL_TILES * M + SKEW_UINT8);\n\n    const int CHUNK_LINE_BYTES_B = (BLOCK_SIZE_K_SPARSE * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_B = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_B);\n    const int CHUNK_COPY_LINE_LANES_B = (CHUNK_LINE_BYTES_B / sizeof(int4));\n    const int SHARED_OFFSET_B = (BLOCK_SIZE_K_SPARSE + SKEW_UINT8);\n    const int SHARED_TO_GLOBAL_BYTES_PER_LINE = ((WARP_COL_TILES * M) * sizeof(int));\n    const int SHARED_TO_GLOBAL_BYTES_PER_WARP = (WARP_SIZE * sizeof(int));\n    const int SHARED_TO_GLOBAL_LINES_PER_WARP = (SHARED_TO_GLOBAL_BYTES_PER_WARP / SHARED_TO_GLOBAL_BYTES_PER_LINE);\n    const int SHARED_TO_GLOBAL_LANES_PER_LINE = (WARP_SIZE / SHARED_TO_GLOBAL_LINES_PER_WARP);\n    const int SHARED_TO_GLOBAL_ITERS = ((WARP_ROW_TILES * N) / SHARED_TO_GLOBAL_LINES_PER_WARP);\n\n    const int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const int WARP_STRIDE = (WARP_COL_TILES * M);\n    const int WARPS_PER_BLOCK = (BLOCK_ROW_WARPS * BLOCK_COL_WARPS);\n    const int THREADS_PER_BLOCK = (WARP_SIZE * WARPS_PER_BLOCK);\n    const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    const int GLOBAL_MEM_STRIDE = M_GLOBAL;\n    const int SHMEM_STRIDE = (M * BLOCK_COL_TILES);\n    const int SHMEM_OFFSET = (M * WARP_COL_TILES);\n    const int BLOCK_SIZE_M = (M * BLOCK_COL_TILES);\n    const int BLOCK_SIZE_N = (N * BLOCK_ROW_TILES);\n\n   extern __shared__ uint8_t shmem[];\n\n\t// Warp and lane identification.\n\tconst unsigned int warpId = threadIdx.x / WARP_SIZE;\n\tconst unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n\n\t// Offset in shared memory from which the B matrix is stored.\n\t// const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\tconst size_t shmem_idx_b_off = BLOCK_SIZE_K_SPARSE * SHARED_OFFSET_A;\n\n\n\t// Each CTA slides along the 128 x 128 tiles from the top left corner of the\n\t// matrix to the right and down, and selects the next tile to compute. Once\n\t// there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n\tconst unsigned int block_tile_i =\n\t\t((block_pos * BLOCK_COL_TILES) / M_TILES) * (BLOCK_ROW_TILES);\n\tconst unsigned int block_tile_j = (block_pos * BLOCK_COL_TILES) % M_TILES;\n\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n\twmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_ROW_TILES]\n\t\t\t\t\t\t\t\t\t\t\t\t\t [WARP_COL_TILES];\n\n    // Load the C matrix tiles into fragments from shared memory.\n\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t\twmma::fill_fragment(c[i][j], 0);\n\t\t}\n\t}\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    //for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n    for(int tile_k_idx_sparse = 0, tile_k_idx = 0; tile_k_idx_sparse < K_GLOBAL_SPARSE; tile_k_idx_sparse += BLOCK_SIZE_K_SPARSE, tile_k_idx += BLOCK_SIZE_K){\n\n\t\tsize_t shmem_idx = \n\t\twarpId < (WARPS_PER_BLOCK / 2)\n\t\t\t? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A\n\t\t\t: (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B + shmem_idx_b_off;\n\n\t\tint4 *lane_ptr = NULL;\n\t\tint *lane_ptr_index = NULL;\n\t\tconst uint8_t *warp_ptr = NULL;\n\n\n\t\tif(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\t//warp_ptr = &A[block_tile_j * M] +\n\t\t\t//\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * M_GLOBAL;\n\t\t\twarp_ptr = &A[block_tile_j * M];\n\t\t\t\n\t\t\tconst int *warp_ptr_index = &B_index[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t\t\t\t\t\t((warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A);\n\n\t\t\tlane_ptr_index = (int *)(warp_ptr_index + tile_k_idx_sparse + (laneId / CHUNK_COPY_LINE_LANES_A));\n\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_A) * SHARED_OFFSET_A;\n\t\t}else{\n\t\t\twarp_ptr = &B[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * K_GLOBAL_SPARSE;\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_sparse +\n\t\t\t\t\t\t\t\t(laneId / CHUNK_COPY_LINE_LANES_B) * K_GLOBAL_SPARSE) +\n\t\t\t\t\t\t\t\t(laneId % CHUNK_COPY_LINE_LANES_B);\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_B) * SHARED_OFFSET_B;\n\t\t}\n\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      // shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n\t  int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n\t  \t? BLOCK_SIZE_K_SPARSE / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A)\n\t\t: BLOCK_SIZE_N / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\n\t  /*\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\t  */\n\n\t  /*\n      int tile_k_idx_A;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n          tile_k_idx_A = *(lane_ptr_index);\n      }\n\t  */\n\n\t  #pragma unroll\n\t  for(int i = 0; i < iter_index; i += 1){\n\t\t  if(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\tint tile_k_idx_A = *(lane_ptr_index);\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_A * M_GLOBAL) + (laneId % CHUNK_COPY_LINE_LANES_A);\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_A)) =\n\t\t\t\t*lane_ptr;\n\t\t\t//warp_ptr = (uint8_t *)((uint8_t *)warp_ptr + M_GLOBAL * (WARPS_PER_BLOCK / 2) *CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tlane_ptr_index = (int *)((int *)lane_ptr_index +  (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A;\n\t\t  }else{\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_B)) =\n\t\t\t\t*lane_ptr;\n\t\t\tlane_ptr = (int4 *)((uint8_t *)lane_ptr + K_GLOBAL_SPARSE * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B;\n\t\t  }\n\t  }\n\n      __syncthreads();\n\n\t#pragma unroll\n      for (int k_step = 0; k_step < CHUNK_K_SPARSE; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::col_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n\t#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i += 1) {\n\t\t\tsize_t shmem_idx_a = (warpId % BLOCK_COL_WARPS) * M * WARP_COL_TILES + (i * M);\n\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_a + k_step * K * SHARED_OFFSET_A;\n\n\t\t\twmma::load_matrix_sync(a[i], tile_ptr, SHARED_OFFSET_A);\n\t\t#pragma unroll\n\t\t\tfor(int j = 0; j < WARP_ROW_TILES; j += 1){\n\t\t\t\tif(i == 0){\n\t\t\t\t\tsize_t shmem_idx_b = shmem_idx_b_off +\n\t\t\t\t\t\t\t\t\t\t\t(warpId / BLOCK_COL_WARPS) * (WARP_ROW_TILES * N) * SHARED_OFFSET_B +\n\t\t\t\t\t\t\t\t\t\t\t(j * N) * SHARED_OFFSET_B;\n\t\t\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_b + k_step * K;\n\t\t\t\t\twmma::load_matrix_sync(b[j], tile_ptr, SHARED_OFFSET_B);\n\t\t\t\t}\n\t\t\t\twmma::mma_sync(c[j][i], a[i], b[j], c[j][i]);\n\t\t\t}\n\n        }\n      }\n\n      __syncthreads();\n    }\n\n    // This pointer is used to access the C and D matrix tiles this warp computes.\n\tint *shmem_warp_tile_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * N * WARP_ROW_TILES * SHMEM_STRIDE +\n\t(warpId % BLOCK_COL_WARPS) * SHMEM_OFFSET;\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t#pragma unroll\n\t\t\tfor(int t = 0; t < c[i][j].num_elements; t += 1){\n\t\t\t\tc[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n\t\t\t}\n\t\t\tint *tile_ptr = shmem_warp_tile_ptr + i * N * SHMEM_STRIDE + j * M;\n\t\t\twmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, C_LAYOUT);\n\t\t}\n\t}\n\n    __syncthreads();\n\n\tint *shmem_warp_stream_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N * SHMEM_STRIDE\n\t\t\t\t\t\t\t\t\t+ (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tconst size_t gmem_idx =\n\t\t(block_tile_i * N + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N) * GLOBAL_MEM_STRIDE +\n\t\tblock_tile_j * M + (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tuint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n\tint *shmem_lane_stream_ptr =\n\t\tshmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * SHMEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\t\n\tuint8_t *dst_gmem_lane_stream_ptr =\n\t\tdst_gmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * GLOBAL_MEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\n\tfor(int i = 0; i < WARP_ROW_TILES * N; i += SHARED_TO_GLOBAL_LINES_PER_WARP){\n\t\t*(dst_gmem_lane_stream_ptr + i * GLOBAL_MEM_STRIDE) = (uint8_t)(*(shmem_lane_stream_ptr + i * SHMEM_STRIDE));\n\t}\n\n\t__syncthreads();\n}\n", "launch_config": {"dimBlock": [256, 1], "dimGrid": [256, 1]}}, "145": {"code": "\n__global__ void compute_gemm_imma(float *input0, float *input1, float *input2, float* input3, float* input4, float*input5, float* input6,\n                                  float *output0) {\n    // keys need to be replaced 32, 1024, K_GLOBA_VALUEL, 0.75, 64\n\n\t//extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B = reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_index = reinterpret_cast<int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n\tconst int WARP_SIZE = 32;\n    const int M = 16;\n    const int K = 16;\n    const int N = 16;\n    const int WMMA_M = 16;\n    const int WMMA_K = 16;\n    const int WMMA_N = 16;\n    const int M_GLOBAL = 32;\n    const int K_GLOBAL = 1024;\n    const int N_GLOBAL = 1024;\n    const float SPARSITY = 0.75;\n    const int K_GLOBAL_SPARSE = (int(K_GLOBAL * (1-SPARSITY)));\n    const int M_TILES = (M_GLOBAL / M);\n    const int N_TILES = (N_GLOBAL / N);\n    const int K_TILES = (K_GLOBAL / K);\n    const int BLOCK_ROW_WARPS = 2;\n    const int BLOCK_COL_WARPS = 4;\n    const int WARP_ROW_TILES = 4;\n    const int WARP_COL_TILES = 2;\n    const int SKEW_UINT8 = 16;\n    const int BANK_VAL = 32;\n    const int NUM_BANK = (K_GLOBAL / BANK_VAL);\n    /////////// BLOCK_ROW_TILES <= N_TILES ////////////\n    // const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    // ////////// BLOCK_COL_TILES <= M_TILES ////////////\n    // const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    auto C_LAYOUT = wmma::mem_col_major;\n    const int CHUNK_K = 64;\n    const int CHUNK_K_SPARSE = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K = (int(CHUNK_K * (1-SPARSITY)));\n    const int BLOCK_SIZE_K_SPARSE = (int((CHUNK_K * K) * (1 - SPARSITY)));\n    const int WARP_COPY_BYTES = (WARP_SIZE * sizeof(int4));\n    const int CHUNK_LINE_BYTES_A (BLOCK_COL_TILES * M * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_A = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_A);\n    const int CHUNK_COPY_LINE_LANES_A = (CHUNK_LINE_BYTES_A / sizeof(int4));\n    const int SHARED_OFFSET_A = (BLOCK_COL_TILES * M + SKEW_UINT8);\n\n    const int CHUNK_LINE_BYTES_B = (BLOCK_SIZE_K_SPARSE * sizeof(uint8_t));\n    const int CHUNK_COPY_LINES_PER_WARP_B = (WARP_COPY_BYTES / CHUNK_LINE_BYTES_B);\n    const int CHUNK_COPY_LINE_LANES_B = (CHUNK_LINE_BYTES_B / sizeof(int4));\n    const int SHARED_OFFSET_B = (BLOCK_SIZE_K_SPARSE + SKEW_UINT8);\n    const int SHARED_TO_GLOBAL_BYTES_PER_LINE = ((WARP_COL_TILES * M) * sizeof(int));\n    const int SHARED_TO_GLOBAL_BYTES_PER_WARP = (WARP_SIZE * sizeof(int));\n    const int SHARED_TO_GLOBAL_LINES_PER_WARP = (SHARED_TO_GLOBAL_BYTES_PER_WARP / SHARED_TO_GLOBAL_BYTES_PER_LINE);\n    const int SHARED_TO_GLOBAL_LANES_PER_LINE = (WARP_SIZE / SHARED_TO_GLOBAL_LINES_PER_WARP);\n    const int SHARED_TO_GLOBAL_ITERS = ((WARP_ROW_TILES * N) / SHARED_TO_GLOBAL_LINES_PER_WARP);\n\n    const int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const int WARP_STRIDE = (WARP_COL_TILES * M);\n    const int WARPS_PER_BLOCK = (BLOCK_ROW_WARPS * BLOCK_COL_WARPS);\n    const int THREADS_PER_BLOCK = (WARP_SIZE * WARPS_PER_BLOCK);\n    const int BLOCK_ROW_TILES = (WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const int BLOCK_COL_TILES = (WARP_COL_TILES * BLOCK_COL_WARPS);\n    const int GLOBAL_MEM_STRIDE = M_GLOBAL;\n    const int SHMEM_STRIDE = (M * BLOCK_COL_TILES);\n    const int SHMEM_OFFSET = (M * WARP_COL_TILES);\n    const int BLOCK_SIZE_M = (M * BLOCK_COL_TILES);\n    const int BLOCK_SIZE_N = (N * BLOCK_ROW_TILES);\n\n   extern __shared__ uint8_t shmem[];\n\n\t// Warp and lane identification.\n\tconst unsigned int warpId = threadIdx.x / WARP_SIZE;\n\tconst unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n\n\t// Offset in shared memory from which the B matrix is stored.\n\t// const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\tconst size_t shmem_idx_b_off = BLOCK_SIZE_K_SPARSE * SHARED_OFFSET_A;\n\n\n\t// Each CTA slides along the 128 x 128 tiles from the top left corner of the\n\t// matrix to the right and down, and selects the next tile to compute. Once\n\t// there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n\tconst unsigned int block_tile_i =\n\t\t((block_pos * BLOCK_COL_TILES) / M_TILES) * (BLOCK_ROW_TILES);\n\tconst unsigned int block_tile_j = (block_pos * BLOCK_COL_TILES) % M_TILES;\n\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n\twmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_ROW_TILES]\n\t\t\t\t\t\t\t\t\t\t\t\t\t [WARP_COL_TILES];\n\n    // Load the C matrix tiles into fragments from shared memory.\n\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t\twmma::fill_fragment(c[i][j], 0);\n\t\t}\n\t}\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    //for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n    for(int tile_k_idx_sparse = 0, tile_k_idx = 0; tile_k_idx_sparse < K_GLOBAL_SPARSE; tile_k_idx_sparse += BLOCK_SIZE_K_SPARSE, tile_k_idx += BLOCK_SIZE_K){\n\n\t\tsize_t shmem_idx = \n\t\twarpId < (WARPS_PER_BLOCK / 2)\n\t\t\t? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A\n\t\t\t: (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B + shmem_idx_b_off;\n\n\t\tint4 *lane_ptr = NULL;\n\t\tint *lane_ptr_index = NULL;\n\t\tconst uint8_t *warp_ptr = NULL;\n\n\n\t\tif(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\t//warp_ptr = &A[block_tile_j * M] +\n\t\t\t//\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A * M_GLOBAL;\n\t\t\twarp_ptr = &A[block_tile_j * M];\n\t\t\t\n\t\t\tconst int *warp_ptr_index = &B_index[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t\t\t\t\t\t((warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_A);\n\n\t\t\tlane_ptr_index = (int *)(warp_ptr_index + tile_k_idx_sparse + (laneId / CHUNK_COPY_LINE_LANES_A));\n\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_A) * SHARED_OFFSET_A;\n\t\t}else{\n\t\t\twarp_ptr = &B[block_tile_i * N * K_GLOBAL_SPARSE] +\n\t\t\t\t(warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP_B * K_GLOBAL_SPARSE;\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_sparse +\n\t\t\t\t\t\t\t\t(laneId / CHUNK_COPY_LINE_LANES_B) * K_GLOBAL_SPARSE) +\n\t\t\t\t\t\t\t\t(laneId % CHUNK_COPY_LINE_LANES_B);\n\t\t\tshmem_idx += (laneId / CHUNK_COPY_LINE_LANES_B) * SHARED_OFFSET_B;\n\t\t}\n\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      // shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n\t  int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n\t  \t? BLOCK_SIZE_K_SPARSE / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A)\n\t\t: BLOCK_SIZE_N / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\n\t  /*\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\t  */\n\n\t  /*\n      int tile_k_idx_A;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n          tile_k_idx_A = *(lane_ptr_index);\n      }\n\t  */\n\n\t  #pragma unroll\n\t  for(int i = 0; i < iter_index; i += 1){\n\t\t  if(warpId < (WARPS_PER_BLOCK / 2)){\n\t\t\tint tile_k_idx_A = *(lane_ptr_index);\n\t\t\tlane_ptr = (int4 *)(warp_ptr + tile_k_idx_A * M_GLOBAL) + (laneId % CHUNK_COPY_LINE_LANES_A);\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_A)) =\n\t\t\t\t*lane_ptr;\n\t\t\t//warp_ptr = (uint8_t *)((uint8_t *)warp_ptr + M_GLOBAL * (WARPS_PER_BLOCK / 2) *CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tlane_ptr_index = (int *)((int *)lane_ptr_index +  (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_A * SHARED_OFFSET_A;\n\t\t  }else{\n\t\t\t*((int4 *)&shmem[shmem_idx] + (laneId % CHUNK_COPY_LINE_LANES_B)) =\n\t\t\t\t*lane_ptr;\n\t\t\tlane_ptr = (int4 *)((uint8_t *)lane_ptr + K_GLOBAL_SPARSE * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B);\n\t\t\tshmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP_B * SHARED_OFFSET_B;\n\t\t  }\n\t  }\n\n      __syncthreads();\n\n\t#pragma unroll\n      for (int k_step = 0; k_step < CHUNK_K_SPARSE; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::col_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n\t#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i += 1) {\n\t\t\tsize_t shmem_idx_a = (warpId % BLOCK_COL_WARPS) * M * WARP_COL_TILES + (i * M);\n\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_a + k_step * K * SHARED_OFFSET_A;\n\n\t\t\twmma::load_matrix_sync(a[i], tile_ptr, SHARED_OFFSET_A);\n\t\t#pragma unroll\n\t\t\tfor(int j = 0; j < WARP_ROW_TILES; j += 1){\n\t\t\t\tif(i == 0){\n\t\t\t\t\tsize_t shmem_idx_b = shmem_idx_b_off +\n\t\t\t\t\t\t\t\t\t\t\t(warpId / BLOCK_COL_WARPS) * (WARP_ROW_TILES * N) * SHARED_OFFSET_B +\n\t\t\t\t\t\t\t\t\t\t\t(j * N) * SHARED_OFFSET_B;\n\t\t\t\t\tconst uint8_t *tile_ptr = shmem + shmem_idx_b + k_step * K;\n\t\t\t\t\twmma::load_matrix_sync(b[j], tile_ptr, SHARED_OFFSET_B);\n\t\t\t\t}\n\t\t\t\twmma::mma_sync(c[j][i], a[i], b[j], c[j][i]);\n\t\t\t}\n\n        }\n      }\n\n      __syncthreads();\n    }\n\n    // This pointer is used to access the C and D matrix tiles this warp computes.\n\tint *shmem_warp_tile_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * N * WARP_ROW_TILES * SHMEM_STRIDE +\n\t(warpId % BLOCK_COL_WARPS) * SHMEM_OFFSET;\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n\tfor(int i = 0; i < WARP_ROW_TILES; i += 1){\n\t#pragma unroll\n\t\tfor(int j = 0; j < WARP_COL_TILES; j += 1){\n\t\t#pragma unroll\n\t\t\tfor(int t = 0; t < c[i][j].num_elements; t += 1){\n\t\t\t\tc[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n\t\t\t}\n\t\t\tint *tile_ptr = shmem_warp_tile_ptr + i * N * SHMEM_STRIDE + j * M;\n\t\t\twmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, C_LAYOUT);\n\t\t}\n\t}\n\n    __syncthreads();\n\n\tint *shmem_warp_stream_ptr = (int *)shmem + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N * SHMEM_STRIDE\n\t\t\t\t\t\t\t\t\t+ (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tconst size_t gmem_idx =\n\t\t(block_tile_i * N + (warpId / BLOCK_COL_WARPS) * WARP_ROW_TILES * N) * GLOBAL_MEM_STRIDE +\n\t\tblock_tile_j * M + (warpId % BLOCK_COL_WARPS) * WARP_COL_TILES * M;\n\tuint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n\tint *shmem_lane_stream_ptr =\n\t\tshmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * SHMEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\t\n\tuint8_t *dst_gmem_lane_stream_ptr =\n\t\tdst_gmem_warp_stream_ptr +\n\t\t(laneId / SHARED_TO_GLOBAL_LANES_PER_LINE) * GLOBAL_MEM_STRIDE +\n\t\t(laneId % SHARED_TO_GLOBAL_LANES_PER_LINE);\n\n\tfor(int i = 0; i < WARP_ROW_TILES * N; i += SHARED_TO_GLOBAL_LINES_PER_WARP){\n\t\t*(dst_gmem_lane_stream_ptr + i * GLOBAL_MEM_STRIDE) = (uint8_t)(*(shmem_lane_stream_ptr + i * SHMEM_STRIDE));\n\t}\n\n\t__syncthreads();\n}\n", "launch_config": {"dimBlock": [2, 1], "dimGrid": [256, 1]}}}