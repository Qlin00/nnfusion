[
    {
        "tvm_func_name": "MatrixMulCUDA_8bit",
        "op_type": "QuantizeDotAddRelu",
        "parameters": {
            "arg0_shape": [
                256,
                256
            ],
            "arg1_shape": [
                256,
                256
            ],
            "arg2_shape": [
                256,
                256
            ],
            "arg3_shape": [
                256,
                256
            ],
            "arg4_shape": [
                256,
                256
            ],
            "arg5_shape": [
                1
            ],
            "arg6_shape": [
                1
            ],
            "arg7_shape": [
                256
            ],
            "arg8_shape": [
                1
            ],
            "out_shape": [
                256,
                256
            ],
            "transpose_A": false,
            "transpose_B": false,
            "in_quantize_bit": 8,
            "out_quantize_bit": 8,
            "identifier_suffix": "AddRelu"

        },
        "code": "// This kernel's input datatype and output datatype are all 8bit, using cuda core.\nextern \"C\" __global__ void MatrixMulCUDA_8bit_biasRelu(float *input0, float *input1, float *input2, float *input3, float *input4, float *input5, float * input6,float  *input7, float *input8, float *output0){\n    /*Prepare Stage*/\n    // set the block size and matrix size constants here\n    const int BLOCK_SIZE=32;\n    const int k=256, n=256;\n    // convert the input pointer to the correct type\n    uint8_t * A =  reinterpret_cast<uint8_t*>(input0);\n    uint8_t * W =  reinterpret_cast<uint8_t*>(input1);\n    uint8_t * C =  reinterpret_cast<uint8_t*>(output0);\n    unsigned int * Multi_w_zp =  reinterpret_cast<unsigned int *>(input2);\n    uint8_t * W_zp =  reinterpret_cast<uint8_t *>(input3);\n    unsigned int * ZP_accu =  reinterpret_cast<unsigned int *>(input4);\n    const int integer = (int)(*input5);\n    const int shift_val = (int)(*input6);\n    unsigned int * bias = reinterpret_cast<unsigned int *>(input7);\n    uint8_t output_zp = (uint8_t)(*input8);\n\n    \n    int bx = blockIdx.x;\n    int by = blockIdx.y;\n    int tx = threadIdx.x;\n    int ty = threadIdx.y;\n \n    int aBegin = BLOCK_SIZE * by * k;\n    int aEnd = aBegin + k - 1;\n \n    int bBegin = BLOCK_SIZE * bx * k;\n \n    __shared__ uint8_t As[BLOCK_SIZE][BLOCK_SIZE];\n    __shared__ uint8_t Bs[BLOCK_SIZE][BLOCK_SIZE];\n    __shared__ uint8_t Wzp[BLOCK_SIZE][BLOCK_SIZE];\n \n    unsigned int cSub = 0;\n    unsigned int azpSub = 0;\n    for(int i = aBegin, j = bBegin; i <= aEnd; i += BLOCK_SIZE, j += BLOCK_SIZE){\n        As[ty][tx] = A[i + ty * k + tx];\n        Bs[ty][tx] = W[j + ty * k + tx];\n        Wzp[ty][tx] = W_zp[j + ty * k + tx];\n \n        __syncthreads();\n \n        for(int k = 0; k < 32; k += 4){\n            unsigned int pack_val1 = FETCH_UINT32(As[ty][k]);\n            unsigned int pack_val2 = FETCH_UINT32(Bs[tx][k]);\n            unsigned int pack_val3 = FETCH_UINT32(Wzp[tx][k]);\n            cSub = __dp4a(pack_val1, pack_val2, cSub);\n            azpSub = __dp4a(pack_val1, pack_val3, azpSub);\n        }\n        __syncthreads();\n    }\n    \n    int cx = by * BLOCK_SIZE + ty;\n    int cy = bx * BLOCK_SIZE + tx;\n \n    cSub = cSub - Multi_w_zp[cx*n+cy] - azpSub + ZP_accu[cx*n+cy] + bias[cy];\n    cSub = (cSub * integer) >> shift_val;\n \n    C[cx*n+cy] = (uint8_t)cSub;\n    if(C[cx*n+cy] < output_zp) C[cx*n+cy] = 0;\n}",
        "gridDim": [
            8,
            8,
            1
        ],
        "blockDim": [
            32,
            32,
            1
        ]
    }
]